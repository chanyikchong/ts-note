{"documents": {"docs/en/index.md": {"path": "docs/en/index.md", "title": "Time Series Study Notes", "content": "# Time Series Study Notes\n\n<div class=\"interview-summary\">\n<strong>Welcome to the Time Series Study Notes</strong> — a comprehensive, bilingual (English/中文) resource for learning time series analysis and forecasting, designed for interview preparation and practical application.\n</div>\n\n## What's Included\n\nThis knowledge base covers:\n\n- **Foundations**: Stationarity, autocorrelation, partial autocorrelation\n- **Time Domain Models**: AR, MA, ARMA, ARIMA, SARIMA with identification and estimation\n- **Exponential Smoothing**: SES, Holt, Holt-Winters, ETS framework\n- **Decomposition**: STL, classical decomposition, handling seasonality\n- **Forecasting**: Prediction intervals, multi-step strategies, rolling evaluation\n- **Model Selection**: AIC/BIC, cross-validation for time series, residual diagnostics\n- **Spectral Analysis**: Periodogram, frequency domain basics\n- **State Space Models**: Kalman filter, local level and trend models\n- **Multivariate TS**: VAR, VARMA, Granger causality\n- **Change Detection**: Change-point and anomaly detection methods\n- **Feature Engineering**: Classical pipelines, scaling, missing data handling\n- **Deep Learning**: RNN/LSTM/TCN, Transformers for time series\n- **Practical Modeling**: Backtesting, deployment, common pitfalls\n\n## Page Structure\n\nEvery topic page follows a consistent 8-section format:\n\n1. **Interview Summary** — Key points in 3-6 lines\n2. **Core Definitions** — Essential terminology and concepts\n3. **Math and Derivations** — Rigorous mathematical foundations\n4. **Algorithm/Model Sketch** — How the method works\n5. **Common Pitfalls** — Mistakes to avoid\n6. **Mini Example** — Quick illustration\n7. **Quiz** — 5+ questions with hidden answers (click to reveal)\n8. **References** — Further reading\n\n## Getting Started\n\nChoose a topic from the sidebar to begin. Each page is self-contained but builds on foundational concepts.\n\n**Recommended learning path for beginners:**\n\n1. Start with [Stationarity](foundations/stationarity.md) and [Autocorrelation](foundations/autocorrelation.md)\n2. Move to [AR Models](time-domain/ar.md) → [MA Models](time-domain/ma.md) → [ARMA](time-domain/arma.md) → [ARIMA](time-domain/arima.md)\n3. Learn [Model Identification](time-domain/identification.md) and [Residual Diagnostics](model-selection/residual-diagnostics.md)\n4. Explore [Exponential Smoothing](exponential-smoothing/ses.md) and [Decomposition](decomposition/stl.md)\n5. Advance to [State Space Models](state-space/kalman-filter.md) and [Multivariate TS](multivariate/var.md)\n\n## Code Examples\n\nRunnable Python demos are available in the `ts_examples/` directory. Run them with:\n\n```bash\npython -m ts_examples.run --demo <demo_name>\n```\n\nAvailable demos: `arima`, `ets`, `stl`, `kalman`, `var`, `changepoint`, `backtest`, `metrics`\n\n## Language Toggle\n\nUse the language selector in the header to switch between English and 中文. The site maintains parallel content in both languages.\n\n---\n\n*This is an open, extensible knowledge base. See the repository README for instructions on adding new content.*\n", "sections": [{"heading": "Time Series Study Notes", "content": "<div class=\"interview-summary\">\n<strong>Welcome to the Time Series Study Notes</strong> — a comprehensive, bilingual (English/中文) resource for learning time series analysis and forecasting, designed for interview preparation and practical application.\n</div>", "line_start": 1, "level": 2}, {"heading": "What's Included", "content": "This knowledge base covers:\n\n- **Foundations**: Stationarity, autocorrelation, partial autocorrelation\n- **Time Domain Models**: AR, MA, ARMA, ARIMA, SARIMA with identification and estimation\n- **Exponential Smoothing**: SES, Holt, Holt-Winters, ETS framework\n- **Decomposition**: STL, classical decomposition, handling seasonality\n- **Forecasting**: Prediction intervals, multi-step strategies, rolling evaluation\n- **Model Selection**: AIC/BIC, cross-validation for time series, residual diagnostics\n- **Spectral Analysis**: Periodogram, frequency domain basics\n- **State Space Models**: Kalman filter, local level and trend models\n- **Multivariate TS**: VAR, VARMA, Granger causality\n- **Change Detection**: Change-point and anomaly detection methods\n- **Feature Engineering**: Classical pipelines, scaling, missing data handling\n- **Deep Learning**: RNN/LSTM/TCN, Transformers for time series\n- **Practical Modeling**: Backtesting, deployment, common pitfalls", "line_start": 7, "level": 2}, {"heading": "Page Structure", "content": "Every topic page follows a consistent 8-section format:\n\n1. **Interview Summary** — Key points in 3-6 lines\n2. **Core Definitions** — Essential terminology and concepts\n3. **Math and Derivations** — Rigorous mathematical foundations\n4. **Algorithm/Model Sketch** — How the method works\n5. **Common Pitfalls** — Mistakes to avoid\n6. **Mini Example** — Quick illustration\n7. **Quiz** — 5+ questions with hidden answers (click to reveal)\n8. **References** — Further reading", "line_start": 25, "level": 2}, {"heading": "Getting Started", "content": "Choose a topic from the sidebar to begin. Each page is self-contained but builds on foundational concepts.\n\n**Recommended learning path for beginners:**\n\n1. Start with [Stationarity](foundations/stationarity.md) and [Autocorrelation](foundations/autocorrelation.md)\n2. Move to [AR Models](time-domain/ar.md) → [MA Models](time-domain/ma.md) → [ARMA](time-domain/arma.md) → [ARIMA](time-domain/arima.md)\n3. Learn [Model Identification](time-domain/identification.md) and [Residual Diagnostics](model-selection/residual-diagnostics.md)\n4. Explore [Exponential Smoothing](exponential-smoothing/ses.md) and [Decomposition](decomposition/stl.md)\n5. Advance to [State Space Models](state-space/kalman-filter.md) and [Multivariate TS](multivariate/var.md)", "line_start": 38, "level": 2}, {"heading": "Code Examples", "content": "Runnable Python demos are available in the `ts_examples/` directory. Run them with:\n\n```bash\npython -m ts_examples.run --demo <demo_name>\n```\n\nAvailable demos: `arima`, `ets`, `stl`, `kalman`, `var`, `changepoint`, `backtest`, `metrics`", "line_start": 50, "level": 2}, {"heading": "Language Toggle", "content": "Use the language selector in the header to switch between English and 中文. The site maintains parallel content in both languages.\n\n---\n\n*This is an open, extensible knowledge base. See the repository README for instructions on adding new content.*", "line_start": 60, "level": 1}]}, "docs/en/spectral/spectral-analysis.md": {"path": "docs/en/spectral/spectral-analysis.md", "title": "Spectral Analysis", "content": "# Spectral Analysis\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Spectral analysis decomposes time series into frequency components. The periodogram estimates power at each frequency. Peaks indicate dominant cycles. For stationary series, spectral density is the Fourier transform of autocovariance. Key insight: AR produces smooth spectrum; MA produces peaks. Useful for detecting hidden periodicities and understanding cyclical behavior.\n</div>\n\n## Core Definitions\n\n**Spectral Density:** For a stationary process, the spectral density $f(\\omega)$ represents power at frequency $\\omega$:\n$$f(\\omega) = \\frac{1}{2\\pi}\\sum_{h=-\\infty}^{\\infty}\\gamma(h)e^{-i\\omega h}$$\n\n**Periodogram:** Sample estimate of spectral density:\n$$I(\\omega_j) = \\frac{1}{n}\\left|\\sum_{t=1}^{n}y_t e^{-i\\omega_j t}\\right|^2$$\n\nat Fourier frequencies $\\omega_j = 2\\pi j/n$ for $j = 0, 1, \\ldots, n/2$.\n\n**Key Frequencies:**\n- $\\omega = 0$: Mean level (zero frequency)\n- $\\omega = 2\\pi/m$: Period of m time units\n- $\\omega = \\pi$: Nyquist frequency (fastest observable cycle = 2 time units)\n\n## Math and Derivations\n\n### Fourier Transform Relationship\n\nThe autocovariance and spectral density are Fourier transform pairs:\n$$f(\\omega) = \\frac{1}{2\\pi}\\sum_{h=-\\infty}^{\\infty}\\gamma(h)e^{-i\\omega h}$$\n$$\\gamma(h) = \\int_{-\\pi}^{\\pi}f(\\omega)e^{i\\omega h}d\\omega$$\n\n**Parseval's relation:**\n$$\\gamma(0) = \\text{Var}(y_t) = \\int_{-\\pi}^{\\pi}f(\\omega)d\\omega$$\n\nTotal variance decomposes across frequencies.\n\n### Spectral Density of AR(1)\n\nFor $y_t = \\phi y_{t-1} + \\epsilon_t$:\n$$f(\\omega) = \\frac{\\sigma^2}{2\\pi|1-\\phi e^{-i\\omega}|^2} = \\frac{\\sigma^2}{2\\pi(1+\\phi^2-2\\phi\\cos\\omega)}$$\n\nProperties:\n- $\\phi > 0$: Peak at $\\omega = 0$ (low-frequency dominance)\n- $\\phi < 0$: Peak at $\\omega = \\pi$ (high-frequency dominance)\n\n### Spectral Density of MA(1)\n\nFor $y_t = \\epsilon_t + \\theta\\epsilon_{t-1}$:\n$$f(\\omega) = \\frac{\\sigma^2}{2\\pi}|1+\\theta e^{-i\\omega}|^2 = \\frac{\\sigma^2}{2\\pi}(1+\\theta^2+2\\theta\\cos\\omega)$$\n\n### Period Detection\n\nIf periodogram has peak at $\\omega_j$, the dominant period is:\n$$T = \\frac{2\\pi}{\\omega_j} = \\frac{n}{j}$$\n\nFor monthly data with annual cycle: peak at $\\omega = 2\\pi/12 \\approx 0.524$.\n\n## Algorithm/Model Sketch\n\n**Spectral Analysis Procedure:**\n\n```\n1. Remove mean (and trend if necessary)\n   y_centered = y - mean(y)\n\n2. Apply window (optional, reduces leakage)\n   Common: Hanning, Hamming, Blackman\n\n3. Compute FFT\n   Y = FFT(y_centered)\n\n4. Compute periodogram\n   I[j] = |Y[j]|² / n\n\n5. Smooth periodogram (optional)\n   - Daniell kernel\n   - Log-smoothing\n   - Welch's method\n\n6. Identify peaks\n   - Compare to red/white noise baseline\n   - Test significance (F-test against continuum)\n\n7. Interpret\n   - Peaks → dominant periods\n   - Smooth decay → AR-like behavior\n   - Flat → white noise\n```\n\n**Frequency to Period Conversion:**\n$$\\text{Period} = \\frac{n}{\\text{index}} = \\frac{2\\pi}{\\omega}$$\n\n## Common Pitfalls\n\n1. **Spectral leakage**: Sharp peaks in true spectrum appear spread out due to finite sample. Use windowing to reduce.\n\n2. **Confusing periodogram with spectral density**: Periodogram is inconsistent (doesn't converge). Smooth it for density estimation.\n\n3. **Ignoring aliasing**: Cycles faster than Nyquist (period < 2) appear at wrong frequencies. Ensure adequate sampling.\n\n4. **Non-stationarity**: Spectral analysis assumes stationarity. Trend causes low-frequency blow-up. Detrend first.\n\n5. **Over-interpreting peaks**: Random fluctuations create spurious peaks. Test significance against noise baseline.\n\n6. **Wrong frequency interpretation**: $\\omega = 0.5$ doesn't mean period = 0.5. Period = $2\\pi/0.5 \\approx 12.6$.\n\n## Mini Example\n\n```python\nimport numpy as np\nfrom scipy import signal\nimport matplotlib.pyplot as plt\n\n# Generate signal with known frequencies\nnp.random.seed(42)\nn = 500\nt = np.arange(n)\n\n# Components: trend + period 50 + period 12 + noise\ny = (0.01 * t +                          # trend\n     5 * np.sin(2 * np.pi * t / 50) +    # period 50\n     3 * np.sin(2 * np.pi * t / 12) +    # period 12\n     np.random.randn(n))                  # noise\n\n# Detrend\ny_detrended = signal.detrend(y)\n\n# Compute periodogram\nfreqs, psd = signal.periodogram(y_detrended, fs=1.0)\n\n# Find peaks\npeaks, _ = signal.find_peaks(psd, height=np.percentile(psd, 90))\n\nprint(\"Detected periods:\")\nfor p in peaks:\n    if freqs[p] > 0:\n        period = 1 / freqs[p]\n        print(f\"  Frequency {freqs[p]:.4f} → Period {period:.1f}\")\n\n# Expected: peaks near period 50 and period 12\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What is the relationship between the autocovariance function and spectral density?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> They are Fourier transform pairs.\n\n$$f(\\omega) = \\frac{1}{2\\pi}\\sum_{h=-\\infty}^{\\infty}\\gamma(h)e^{-i\\omega h}$$\n\n$$\\gamma(h) = \\int_{-\\pi}^{\\pi}f(\\omega)e^{i\\omega h}d\\omega$$\n\n**Interpretation:**\n- ACF describes correlation in time domain\n- Spectral density describes power in frequency domain\n- Same information, different representation\n\n**Key insight:** At $h=0$:\n$$\\gamma(0) = \\text{Var}(y_t) = \\int f(\\omega)d\\omega$$\n\nTotal variance = integral of spectral density (power across all frequencies).\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Thinking frequency and time domain analyses give different information. They're equivalent representations—choose based on what's easier to interpret for your problem.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Why does an AR(1) with positive φ have peak at frequency zero?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Positive AR(1) coefficient creates persistence—values tend to stay above or below mean for extended periods. This translates to slow oscillations (low frequency).\n\n**Mathematical explanation:**\nSpectral density: $f(\\omega) \\propto \\frac{1}{1+\\phi^2-2\\phi\\cos\\omega}$\n\nAt $\\omega = 0$: $f(0) \\propto \\frac{1}{(1-\\phi)^2}$ (maximum for $\\phi > 0$)\nAt $\\omega = \\pi$: $f(\\pi) \\propto \\frac{1}{(1+\\phi)^2}$ (minimum for $\\phi > 0$)\n\n**Intuition:**\n- $\\phi > 0$: Today's value predicts tomorrow's → smooth, low-frequency behavior\n- $\\phi < 0$: Today's value predicts opposite tomorrow → choppy, high-frequency behavior\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Expecting all AR processes to look similar in spectrum. The sign and magnitude of φ drastically change spectral shape.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Derive the Nyquist frequency and explain why frequencies above it cannot be detected.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Nyquist frequency = $\\pi$ rad/sample = 0.5 cycles/sample.\n\n**Derivation:**\nTo observe a cycle, we need at least 2 samples per period (sample at peak and trough).\n\nMinimum period detectable = 2 sample intervals\nMaximum frequency = 1/(2 sample intervals) = 0.5 cycles/sample\n\nIn angular frequency: $\\omega_{Nyquist} = 2\\pi \\times 0.5 = \\pi$\n\n**Aliasing:**\nA signal with frequency $\\omega > \\pi$ appears as frequency $2\\pi - \\omega$ (reflected).\n\nExample: True frequency 0.6 cycles/sample appears as 0.4 cycles/sample.\n\n**Consequence:** Without higher sampling rate, we cannot distinguish $\\omega$ from $2\\pi - \\omega$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Trying to detect daily cycles from monthly data. Monthly sampling (Nyquist = 2 months period) cannot see anything faster than bimonthly oscillations.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Why is the raw periodogram an inconsistent estimator of spectral density?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The periodogram variance doesn't decrease as sample size increases.\n\n**Technical explanation:**\n$$\\text{Var}(I(\\omega)) \\approx f(\\omega)^2$$\n\nfor $\\omega \\neq 0, \\pi$. The variance equals the squared mean—relative error stays constant!\n\n**Why this happens:**\n- Periodogram at each frequency uses information from the entire series\n- But at each Fourier frequency, we essentially have one \"observation\"\n- More data → more frequencies, but still one estimate per frequency\n\n**Solution:** Smooth the periodogram\n- Average nearby frequencies (reduces variance)\n- Or use multitaper methods\n- Trade bias for variance\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Treating raw periodogram peaks as definitive. Peaks can be noise; always smooth or test significance.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You analyze hourly temperature data and see a strong peak at period 24 hours, but also unexpected peaks at periods 12, 8, 6 hours. What's happening?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> These are **harmonics** of the fundamental daily cycle.\n\n**Explanation:**\nA pure 24-hour cycle would show only one peak at period 24. But real temperature patterns aren't pure sinusoids—they have:\n- Sharp morning rise\n- Gradual afternoon decline\n- These non-sinusoidal shapes require multiple frequencies to represent\n\n**Fourier's theorem:** Any periodic signal is a sum of harmonics:\n$$y(t) = \\sum_{k=1}^{\\infty} a_k \\cos(2\\pi kt/24) + b_k \\sin(2\\pi kt/24)$$\n\nHarmonics at periods 24/2=12, 24/3=8, 24/4=6, etc.\n\n**Interpretation:**\n- Period 24: Fundamental daily cycle\n- Period 12: Asymmetry (morning ≠ evening)\n- Period 8, 6: Further shape details\n\n**Action:** This is normal for non-sinusoidal cycles. Focus on fundamental; harmonics indicate shape.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Interpreting harmonics as separate physical phenomena. They're mathematical artifacts of non-sinusoidal shape.\n</div>\n</div>\n</details>\n\n## References\n\n1. Shumway, R. H., & Stoffer, D. S. (2017). *Time Series Analysis and Its Applications*. Springer. Chapter 4.\n2. Brockwell, P. J., & Davis, R. A. (2016). *Introduction to Time Series and Forecasting*. Springer. Chapter 4.\n3. Priestley, M. B. (1981). *Spectral Analysis and Time Series*. Academic Press.\n4. Percival, D. B., & Walden, A. T. (1993). *Spectral Analysis for Physical Applications*. Cambridge University Press.\n", "sections": [{"heading": "Spectral Analysis", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Spectral analysis decomposes time series into frequency components. The periodogram estimates power at each frequency. Peaks indicate dominant cycles. For stationary series, spectral density is the Fourier transform of autocovariance. Key insight: AR produces smooth spectrum; MA produces peaks. Useful for detecting hidden periodicities and understanding cyclical behavior.\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**Spectral Density:** For a stationary process, the spectral density $f(\\omega)$ represents power at frequency $\\omega$:\n$$f(\\omega) = \\frac{1}{2\\pi}\\sum_{h=-\\infty}^{\\infty}\\gamma(h)e^{-i\\omega h}$$\n\n**Periodogram:** Sample estimate of spectral density:\n$$I(\\omega_j) = \\frac{1}{n}\\left|\\sum_{t=1}^{n}y_t e^{-i\\omega_j t}\\right|^2$$\n\nat Fourier frequencies $\\omega_j = 2\\pi j/n$ for $j = 0, 1, \\ldots, n/2$.\n\n**Key Frequencies:**\n- $\\omega = 0$: Mean level (zero frequency)\n- $\\omega = 2\\pi/m$: Period of m time units\n- $\\omega = \\pi$: Nyquist frequency (fastest observable cycle = 2 time units)", "line_start": 7, "level": 2}, {"heading": "Fourier Transform Relationship", "content": "The autocovariance and spectral density are Fourier transform pairs:\n$$f(\\omega) = \\frac{1}{2\\pi}\\sum_{h=-\\infty}^{\\infty}\\gamma(h)e^{-i\\omega h}$$\n$$\\gamma(h) = \\int_{-\\pi}^{\\pi}f(\\omega)e^{i\\omega h}d\\omega$$\n\n**Parseval's relation:**\n$$\\gamma(0) = \\text{Var}(y_t) = \\int_{-\\pi}^{\\pi}f(\\omega)d\\omega$$\n\nTotal variance decomposes across frequencies.", "line_start": 24, "level": 3}, {"heading": "Spectral Density of AR(1)", "content": "For $y_t = \\phi y_{t-1} + \\epsilon_t$:\n$$f(\\omega) = \\frac{\\sigma^2}{2\\pi|1-\\phi e^{-i\\omega}|^2} = \\frac{\\sigma^2}{2\\pi(1+\\phi^2-2\\phi\\cos\\omega)}$$\n\nProperties:\n- $\\phi > 0$: Peak at $\\omega = 0$ (low-frequency dominance)\n- $\\phi < 0$: Peak at $\\omega = \\pi$ (high-frequency dominance)", "line_start": 35, "level": 3}, {"heading": "Spectral Density of MA(1)", "content": "For $y_t = \\epsilon_t + \\theta\\epsilon_{t-1}$:\n$$f(\\omega) = \\frac{\\sigma^2}{2\\pi}|1+\\theta e^{-i\\omega}|^2 = \\frac{\\sigma^2}{2\\pi}(1+\\theta^2+2\\theta\\cos\\omega)$$", "line_start": 44, "level": 3}, {"heading": "Period Detection", "content": "If periodogram has peak at $\\omega_j$, the dominant period is:\n$$T = \\frac{2\\pi}{\\omega_j} = \\frac{n}{j}$$\n\nFor monthly data with annual cycle: peak at $\\omega = 2\\pi/12 \\approx 0.524$.", "line_start": 49, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**Spectral Analysis Procedure:**\n\n```\n1. Remove mean (and trend if necessary)\n   y_centered = y - mean(y)\n\n2. Apply window (optional, reduces leakage)\n   Common: Hanning, Hamming, Blackman\n\n3. Compute FFT\n   Y = FFT(y_centered)\n\n4. Compute periodogram\n   I[j] = |Y[j]|² / n\n\n5. Smooth periodogram (optional)\n   - Daniell kernel\n   - Log-smoothing\n   - Welch's method\n\n6. Identify peaks\n   - Compare to red/white noise baseline\n   - Test significance (F-test against continuum)\n\n7. Interpret\n   - Peaks → dominant periods\n   - Smooth decay → AR-like behavior\n   - Flat → white noise\n```\n\n**Frequency to Period Conversion:**\n$$\\text{Period} = \\frac{n}{\\text{index}} = \\frac{2\\pi}{\\omega}$$", "line_start": 56, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Spectral leakage**: Sharp peaks in true spectrum appear spread out due to finite sample. Use windowing to reduce.\n\n2. **Confusing periodogram with spectral density**: Periodogram is inconsistent (doesn't converge). Smooth it for density estimation.\n\n3. **Ignoring aliasing**: Cycles faster than Nyquist (period < 2) appear at wrong frequencies. Ensure adequate sampling.\n\n4. **Non-stationarity**: Spectral analysis assumes stationarity. Trend causes low-frequency blow-up. Detrend first.\n\n5. **Over-interpreting peaks**: Random fluctuations create spurious peaks. Test significance against noise baseline.\n\n6. **Wrong frequency interpretation**: $\\omega = 0.5$ doesn't mean period = 0.5. Period = $2\\pi/0.5 \\approx 12.6$.", "line_start": 91, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom scipy import signal\nimport matplotlib.pyplot as plt", "line_start": 105, "level": 1}, {"heading": "Generate signal with known frequencies", "content": "np.random.seed(42)\nn = 500\nt = np.arange(n)", "line_start": 112, "level": 1}, {"heading": "Components: trend + period 50 + period 12 + noise", "content": "y = (0.01 * t +                          # trend\n     5 * np.sin(2 * np.pi * t / 50) +    # period 50\n     3 * np.sin(2 * np.pi * t / 12) +    # period 12\n     np.random.randn(n))                  # noise", "line_start": 117, "level": 1}, {"heading": "Detrend", "content": "y_detrended = signal.detrend(y)", "line_start": 123, "level": 1}, {"heading": "Compute periodogram", "content": "freqs, psd = signal.periodogram(y_detrended, fs=1.0)", "line_start": 126, "level": 1}, {"heading": "Find peaks", "content": "peaks, _ = signal.find_peaks(psd, height=np.percentile(psd, 90))\n\nprint(\"Detected periods:\")\nfor p in peaks:\n    if freqs[p] > 0:\n        period = 1 / freqs[p]\n        print(f\"  Frequency {freqs[p]:.4f} → Period {period:.1f}\")", "line_start": 129, "level": 1}, {"heading": "Expected: peaks near period 50 and period 12", "content": "```", "line_start": 138, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What is the relationship between the autocovariance function and spectral density?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> They are Fourier transform pairs.\n\n$$f(\\omega) = \\frac{1}{2\\pi}\\sum_{h=-\\infty}^{\\infty}\\gamma(h)e^{-i\\omega h}$$\n\n$$\\gamma(h) = \\int_{-\\pi}^{\\pi}f(\\omega)e^{i\\omega h}d\\omega$$\n\n**Interpretation:**\n- ACF describes correlation in time domain\n- Spectral density describes power in frequency domain\n- Same information, different representation\n\n**Key insight:** At $h=0$:\n$$\\gamma(0) = \\text{Var}(y_t) = \\int f(\\omega)d\\omega$$\n\nTotal variance = integral of spectral density (power across all frequencies).\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Thinking frequency and time domain analyses give different information. They're equivalent representations—choose based on what's easier to interpret for your problem.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Why does an AR(1) with positive φ have peak at frequency zero?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Positive AR(1) coefficient creates persistence—values tend to stay above or below mean for extended periods. This translates to slow oscillations (low frequency).\n\n**Mathematical explanation:**\nSpectral density: $f(\\omega) \\propto \\frac{1}{1+\\phi^2-2\\phi\\cos\\omega}$\n\nAt $\\omega = 0$: $f(0) \\propto \\frac{1}{(1-\\phi)^2}$ (maximum for $\\phi > 0$)\nAt $\\omega = \\pi$: $f(\\pi) \\propto \\frac{1}{(1+\\phi)^2}$ (minimum for $\\phi > 0$)\n\n**Intuition:**\n- $\\phi > 0$: Today's value predicts tomorrow's → smooth, low-frequency behavior\n- $\\phi < 0$: Today's value predicts opposite tomorrow → choppy, high-frequency behavior\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Expecting all AR processes to look similar in spectrum. The sign and magnitude of φ drastically change spectral shape.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Derive the Nyquist frequency and explain why frequencies above it cannot be detected.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Nyquist frequency = $\\pi$ rad/sample = 0.5 cycles/sample.\n\n**Derivation:**\nTo observe a cycle, we need at least 2 samples per period (sample at peak and trough).\n\nMinimum period detectable = 2 sample intervals\nMaximum frequency = 1/(2 sample intervals) = 0.5 cycles/sample\n\nIn angular frequency: $\\omega_{Nyquist} = 2\\pi \\times 0.5 = \\pi$\n\n**Aliasing:**\nA signal with frequency $\\omega > \\pi$ appears as frequency $2\\pi - \\omega$ (reflected).\n\nExample: True frequency 0.6 cycles/sample appears as 0.4 cycles/sample.\n\n**Consequence:** Without higher sampling rate, we cannot distinguish $\\omega$ from $2\\pi - \\omega$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Trying to detect daily cycles from monthly data. Monthly sampling (Nyquist = 2 months period) cannot see anything faster than bimonthly oscillations.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Why is the raw periodogram an inconsistent estimator of spectral density?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The periodogram variance doesn't decrease as sample size increases.\n\n**Technical explanation:**\n$$\\text{Var}(I(\\omega)) \\approx f(\\omega)^2$$\n\nfor $\\omega \\neq 0, \\pi$. The variance equals the squared mean—relative error stays constant!\n\n**Why this happens:**\n- Periodogram at each frequency uses information from the entire series\n- But at each Fourier frequency, we essentially have one \"observation\"\n- More data → more frequencies, but still one estimate per frequency\n\n**Solution:** Smooth the periodogram\n- Average nearby frequencies (reduces variance)\n- Or use multitaper methods\n- Trade bias for variance\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Treating raw periodogram peaks as definitive. Peaks can be noise; always smooth or test significance.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You analyze hourly temperature data and see a strong peak at period 24 hours, but also unexpected peaks at periods 12, 8, 6 hours. What's happening?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> These are **harmonics** of the fundamental daily cycle.\n\n**Explanation:**\nA pure 24-hour cycle would show only one peak at period 24. But real temperature patterns aren't pure sinusoids—they have:\n- Sharp morning rise\n- Gradual afternoon decline\n- These non-sinusoidal shapes require multiple frequencies to represent\n\n**Fourier's theorem:** Any periodic signal is a sum of harmonics:\n$$y(t) = \\sum_{k=1}^{\\infty} a_k \\cos(2\\pi kt/24) + b_k \\sin(2\\pi kt/24)$$\n\nHarmonics at periods 24/2=12, 24/3=8, 24/4=6, etc.\n\n**Interpretation:**\n- Period 24: Fundamental daily cycle\n- Period 12: Asymmetry (morning ≠ evening)\n- Period 8, 6: Further shape details\n\n**Action:** This is normal for non-sinusoidal cycles. Focus on fundamental; harmonics indicate shape.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Interpreting harmonics as separate physical phenomena. They're mathematical artifacts of non-sinusoidal shape.\n</div>\n</div>\n</details>", "line_start": 141, "level": 2}, {"heading": "References", "content": "1. Shumway, R. H., & Stoffer, D. S. (2017). *Time Series Analysis and Its Applications*. Springer. Chapter 4.\n2. Brockwell, P. J., & Davis, R. A. (2016). *Introduction to Time Series and Forecasting*. Springer. Chapter 4.\n3. Priestley, M. B. (1981). *Spectral Analysis and Time Series*. Academic Press.\n4. Percival, D. B., & Walden, A. T. (1993). *Spectral Analysis for Physical Applications*. Cambridge University Press.", "line_start": 275, "level": 1}]}, "docs/en/multivariate/granger-causality.md": {"path": "docs/en/multivariate/granger-causality.md", "title": "Granger Causality", "content": "# Granger Causality\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Granger causality tests whether past values of X help predict Y beyond Y's own history. It's about predictive precedence, not true causation. Test via F-test on VAR coefficients. X Granger-causes Y if coefficients on lagged X in Y's equation are jointly significant. Bidirectional causality is possible. Sensitive to omitted variables and lag selection.\n</div>\n\n## Core Definitions\n\n**Granger Causality:** X Granger-causes Y if:\n$$E[Y_t | Y_{t-1}, Y_{t-2}, \\ldots, X_{t-1}, X_{t-2}, \\ldots] \\neq E[Y_t | Y_{t-1}, Y_{t-2}, \\ldots]$$\n\nPast X provides predictive information about Y beyond Y's own past.\n\n**Non-causality:** X does NOT Granger-cause Y if knowing past X doesn't improve prediction of Y.\n\n**Bivariate VAR Test:**\n$$y_t = c + \\sum_{i=1}^{p}\\alpha_i y_{t-i} + \\sum_{i=1}^{p}\\beta_i x_{t-i} + \\epsilon_t$$\n\n$H_0$: $\\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0$ (X does not Granger-cause Y)\n\n## Math and Derivations\n\n### F-Test for Granger Causality\n\n**Restricted model:** AR(p) for Y only\n$$y_t = c + \\sum_{i=1}^{p}\\alpha_i y_{t-i} + u_t$$\n\n**Unrestricted model:** VAR including X\n$$y_t = c + \\sum_{i=1}^{p}\\alpha_i y_{t-i} + \\sum_{i=1}^{p}\\beta_i x_{t-i} + \\epsilon_t$$\n\n**F-statistic:**\n$$F = \\frac{(RSS_R - RSS_U)/p}{RSS_U/(T-2p-1)}$$\n\nUnder $H_0$: $F \\sim F_{p, T-2p-1}$\n\n### Wald Test (for VAR)\n\nIn VAR framework, test:\n$$H_0: \\mathbf{R}\\boldsymbol{\\beta} = \\mathbf{0}$$\n\nwhere $\\mathbf{R}$ selects the coefficients on lagged X in Y's equation.\n\nWald statistic: $W = (\\mathbf{R}\\hat{\\boldsymbol{\\beta}})'[\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}'\\hat{\\sigma}^2]^{-1}(\\mathbf{R}\\hat{\\boldsymbol{\\beta}})$\n\nUnder $H_0$: $W \\sim \\chi^2_p$\n\n### Instantaneous Causality\n\nTests whether current X helps predict current Y (beyond lagged effects):\n\n$$\\text{Cov}(\\epsilon_{yt}, \\epsilon_{xt}) \\neq 0$$\n\nThis tests contemporaneous correlation, not temporal precedence.\n\n### Block Exogeneity\n\nIn multivariate system, test whether a group of variables Granger-causes another group.\n\nJoint test on all relevant coefficient matrices.\n\n## Algorithm/Model Sketch\n\n**Granger Causality Test Procedure:**\n\n```\n1. Determine if series are stationary\n   - If I(1), difference or use Toda-Yamamoto approach\n   - Standard GC tests require stationarity\n\n2. Select optimal lag order\n   - Use AIC/BIC on bivariate VAR\n   - Or use same p for all tests (consistency)\n\n3. Estimate unrestricted VAR(p)\n\n4. Perform Wald/F test:\n   - H0: Coefficients on lagged X = 0 (in Y equation)\n   - Reject → X Granger-causes Y\n\n5. Test reverse direction:\n   - H0: Coefficients on lagged Y = 0 (in X equation)\n   - Reject → Y Granger-causes X\n\n6. Interpret:\n   - Both reject: bidirectional causality\n   - One rejects: unidirectional causality\n   - Neither rejects: no Granger causality\n```\n\n**Toda-Yamamoto Approach (for I(1) series):**\n1. Determine maximum integration order d_max\n2. Fit VAR(p + d_max)\n3. Test only coefficients on first p lags\n4. Avoids issues with pretesting for unit roots\n\n## Common Pitfalls\n\n1. **Confusing with true causation**: Granger causality is predictive precedence, not causal mechanism. Correlation can arise from common causes.\n\n2. **Omitted variable bias**: If Z causes both X and Y with different lags, you may find spurious GC between X and Y.\n\n3. **Wrong lag selection**: Too few lags → miss true effects. Too many → lose power and introduce noise.\n\n4. **Non-stationary data**: Standard F-tests have wrong distribution with unit roots. Use augmented lag approach or error correction.\n\n5. **Multiple testing**: Testing many pairs inflates Type I error. Adjust significance level.\n\n6. **Contemporaneous effects only**: If X and Y move together within a period but not across periods, GC won't detect it.\n\n## Mini Example\n\n```python\nimport numpy as np\nfrom statsmodels.tsa.stattools import grangercausalitytests\nfrom statsmodels.tsa.api import VAR\n\n# Generate data where X Granger-causes Y but not vice versa\nnp.random.seed(42)\nn = 200\n\n# X is independent AR(1)\nx = np.zeros(n)\nfor t in range(1, n):\n    x[t] = 0.7 * x[t-1] + np.random.randn()\n\n# Y depends on own lag and lagged X\ny = np.zeros(n)\nfor t in range(1, n):\n    y[t] = 0.5 * y[t-1] + 0.4 * x[t-1] + np.random.randn()\n\n# Stack data\ndata = np.column_stack([y, x])\n\n# Granger causality tests\nprint(\"=== Does X Granger-cause Y? ===\")\ngc_x_to_y = grangercausalitytests(data, maxlag=4, verbose=True)\n\nprint(\"\\n=== Does Y Granger-cause X? ===\")\ngc_y_to_x = grangercausalitytests(data[:, ::-1], maxlag=4, verbose=True)\n\n# Using VAR\nmodel = VAR(data)\nresults = model.fit(2)\nprint(\"\\n=== VAR-based Granger Causality ===\")\nprint(results.test_causality('y1', 'y2', kind='f'))  # X → Y\nprint(results.test_causality('y2', 'y1', kind='f'))  # Y → X\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why is Granger causality not the same as true causation?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Granger causality only measures predictive precedence—whether past X helps predict Y. It doesn't establish causal mechanism.\n\n**Why they differ:**\n1. **Omitted variables**: Z may cause both X and Y with different lags, creating spurious GC\n2. **Common causes**: X and Y may both respond to unobserved factor\n3. **Spurious correlation**: Can find GC even in independent series by chance\n4. **Measurement timing**: If X and Y are measured at different times, GC reflects measurement, not causation\n\n**Example:** Ice cream sales Granger-cause drownings (both caused by summer heat with different lags).\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Claiming X causes Y based on GC test. Always say \"X Granger-causes Y\" or \"X has predictive power for Y\"—not \"X causes Y.\"\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> What does bidirectional Granger causality mean? Is this common?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Bidirectional GC means both X → Y and Y → X (each helps predict the other). This is common in economics.\n\n**Examples:**\n- GDP ↔ Employment (economic activity and labor market interact)\n- Prices ↔ Wages (wage-price spiral)\n- Interest rates ↔ Exchange rates (monetary policy and currency markets)\n\n**Interpretation:**\n- Feedback relationship\n- Both variables contain unique predictive information\n- System is interdependent\n\n**Caution:** Bidirectional GC doesn't mean simultaneous causation—it means mutual predictive value across time.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Expecting one-way causality. In complex systems, feedback is the rule rather than exception.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> What are the degrees of freedom for the Granger causality F-test with p lags and T observations?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> $F_{p, T-2p-1}$ (numerator df = p, denominator df = T - 2p - 1)\n\n**Derivation:**\n- Restricted model: p parameters (p lags of Y + constant)\n- Unrestricted model: 2p + 1 parameters (p lags of Y + p lags of X + constant)\n- Restriction: p parameters set to zero\n- Observations used: T - p (lose p for lags)\n\nNumerator df = number of restrictions = p\nDenominator df = T - p - (2p + 1) = T - 3p - 1\n\n(Some formulations differ slightly depending on whether constant is counted.)\n\n**Practical:** Use software; these details are handled automatically.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> With short series and many lags, degrees of freedom are low, reducing test power. Balance p against sample size.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> How does the Toda-Yamamoto approach handle non-stationary series in Granger causality testing?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Toda-Yamamoto (1995) avoids pretesting for unit roots:\n\n1. Determine maximum integration order $d_{max}$ (usually 1 or 2)\n2. Fit VAR(p + $d_{max}$) in levels (don't difference)\n3. Test Granger causality on first p lags only\n4. Extra $d_{max}$ lags absorb non-stationarity\n\n**Why it works:**\n- VAR in levels with extra lags has standard asymptotic distribution for Wald test\n- No need to pretest for cointegration\n- Robust to I(1) or I(0) series\n\n**Test:**\n$$H_0: \\beta_1 = \\cdots = \\beta_p = 0$$\n\n(Ignore $\\beta_{p+1}, \\ldots, \\beta_{p+d_{max}}$)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Testing all coefficients including extra lags. Only test first p; the extra $d_{max}$ are \"nuisance\" parameters.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You test Granger causality between oil prices and stock returns using lags 1-8. Results vary: significant at lags 2, 4, 5 but not others. How do you interpret?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> This pattern suggests:\n\n1. **Lag selection matters**: Results are sensitive to specification\n2. **Possible weak relationship**: Significance appears at some lags by chance\n3. **Multiple testing**: Testing 8 specifications inflates false positives\n\n**Recommended approach:**\n1. Select lag order FIRST using information criteria (not GC results)\n2. Report single test at optimal lag\n3. If sensitivity analysis needed, report all results and acknowledge instability\n4. Consider Bonferroni correction for multiple tests\n5. Validate on out-of-sample data\n\n**If results are inconsistent:**\n- Weak evidence for Granger causality\n- Relationship may be nonlinear or time-varying\n- Consider threshold VAR or regime-switching model\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Selecting lag that gives desired result (\"lag shopping\"). This is p-hacking; report pre-specified lag or all results.\n</div>\n</div>\n</details>\n\n## References\n\n1. Granger, C. W. J. (1969). Investigating causal relations by econometric models and cross-spectral methods. *Econometrica*, 37(3), 424-438.\n2. Toda, H. Y., & Yamamoto, T. (1995). Statistical inference in vector autoregressions with possibly integrated processes. *Journal of Econometrics*, 66(1-2), 225-250.\n3. Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press. Chapter 11.\n4. Lütkepohl, H. (2005). *New Introduction to Multiple Time Series Analysis*. Springer. Chapter 2.\n", "sections": [{"heading": "Granger Causality", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Granger causality tests whether past values of X help predict Y beyond Y's own history. It's about predictive precedence, not true causation. Test via F-test on VAR coefficients. X Granger-causes Y if coefficients on lagged X in Y's equation are jointly significant. Bidirectional causality is possible. Sensitive to omitted variables and lag selection.\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**Granger Causality:** X Granger-causes Y if:\n$$E[Y_t | Y_{t-1}, Y_{t-2}, \\ldots, X_{t-1}, X_{t-2}, \\ldots] \\neq E[Y_t | Y_{t-1}, Y_{t-2}, \\ldots]$$\n\nPast X provides predictive information about Y beyond Y's own past.\n\n**Non-causality:** X does NOT Granger-cause Y if knowing past X doesn't improve prediction of Y.\n\n**Bivariate VAR Test:**\n$$y_t = c + \\sum_{i=1}^{p}\\alpha_i y_{t-i} + \\sum_{i=1}^{p}\\beta_i x_{t-i} + \\epsilon_t$$\n\n$H_0$: $\\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0$ (X does not Granger-cause Y)", "line_start": 7, "level": 2}, {"heading": "F-Test for Granger Causality", "content": "**Restricted model:** AR(p) for Y only\n$$y_t = c + \\sum_{i=1}^{p}\\alpha_i y_{t-i} + u_t$$\n\n**Unrestricted model:** VAR including X\n$$y_t = c + \\sum_{i=1}^{p}\\alpha_i y_{t-i} + \\sum_{i=1}^{p}\\beta_i x_{t-i} + \\epsilon_t$$\n\n**F-statistic:**\n$$F = \\frac{(RSS_R - RSS_U)/p}{RSS_U/(T-2p-1)}$$\n\nUnder $H_0$: $F \\sim F_{p, T-2p-1}$", "line_start": 23, "level": 3}, {"heading": "Wald Test (for VAR)", "content": "In VAR framework, test:\n$$H_0: \\mathbf{R}\\boldsymbol{\\beta} = \\mathbf{0}$$\n\nwhere $\\mathbf{R}$ selects the coefficients on lagged X in Y's equation.\n\nWald statistic: $W = (\\mathbf{R}\\hat{\\boldsymbol{\\beta}})'[\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}'\\hat{\\sigma}^2]^{-1}(\\mathbf{R}\\hat{\\boldsymbol{\\beta}})$\n\nUnder $H_0$: $W \\sim \\chi^2_p$", "line_start": 36, "level": 3}, {"heading": "Instantaneous Causality", "content": "Tests whether current X helps predict current Y (beyond lagged effects):\n\n$$\\text{Cov}(\\epsilon_{yt}, \\epsilon_{xt}) \\neq 0$$\n\nThis tests contemporaneous correlation, not temporal precedence.", "line_start": 47, "level": 3}, {"heading": "Block Exogeneity", "content": "In multivariate system, test whether a group of variables Granger-causes another group.\n\nJoint test on all relevant coefficient matrices.", "line_start": 55, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**Granger Causality Test Procedure:**\n\n```\n1. Determine if series are stationary\n   - If I(1), difference or use Toda-Yamamoto approach\n   - Standard GC tests require stationarity\n\n2. Select optimal lag order\n   - Use AIC/BIC on bivariate VAR\n   - Or use same p for all tests (consistency)\n\n3. Estimate unrestricted VAR(p)\n\n4. Perform Wald/F test:\n   - H0: Coefficients on lagged X = 0 (in Y equation)\n   - Reject → X Granger-causes Y\n\n5. Test reverse direction:\n   - H0: Coefficients on lagged Y = 0 (in X equation)\n   - Reject → Y Granger-causes X\n\n6. Interpret:\n   - Both reject: bidirectional causality\n   - One rejects: unidirectional causality\n   - Neither rejects: no Granger causality\n```\n\n**Toda-Yamamoto Approach (for I(1) series):**\n1. Determine maximum integration order d_max\n2. Fit VAR(p + d_max)\n3. Test only coefficients on first p lags\n4. Avoids issues with pretesting for unit roots", "line_start": 61, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Confusing with true causation**: Granger causality is predictive precedence, not causal mechanism. Correlation can arise from common causes.\n\n2. **Omitted variable bias**: If Z causes both X and Y with different lags, you may find spurious GC between X and Y.\n\n3. **Wrong lag selection**: Too few lags → miss true effects. Too many → lose power and introduce noise.\n\n4. **Non-stationary data**: Standard F-tests have wrong distribution with unit roots. Use augmented lag approach or error correction.\n\n5. **Multiple testing**: Testing many pairs inflates Type I error. Adjust significance level.\n\n6. **Contemporaneous effects only**: If X and Y move together within a period but not across periods, GC won't detect it.", "line_start": 96, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.stattools import grangercausalitytests\nfrom statsmodels.tsa.api import VAR", "line_start": 110, "level": 1}, {"heading": "Generate data where X Granger-causes Y but not vice versa", "content": "np.random.seed(42)\nn = 200", "line_start": 117, "level": 1}, {"heading": "X is independent AR(1)", "content": "x = np.zeros(n)\nfor t in range(1, n):\n    x[t] = 0.7 * x[t-1] + np.random.randn()", "line_start": 121, "level": 1}, {"heading": "Y depends on own lag and lagged X", "content": "y = np.zeros(n)\nfor t in range(1, n):\n    y[t] = 0.5 * y[t-1] + 0.4 * x[t-1] + np.random.randn()", "line_start": 126, "level": 1}, {"heading": "Stack data", "content": "data = np.column_stack([y, x])", "line_start": 131, "level": 1}, {"heading": "Granger causality tests", "content": "print(\"=== Does X Granger-cause Y? ===\")\ngc_x_to_y = grangercausalitytests(data, maxlag=4, verbose=True)\n\nprint(\"\\n=== Does Y Granger-cause X? ===\")\ngc_y_to_x = grangercausalitytests(data[:, ::-1], maxlag=4, verbose=True)", "line_start": 134, "level": 1}, {"heading": "Using VAR", "content": "model = VAR(data)\nresults = model.fit(2)\nprint(\"\\n=== VAR-based Granger Causality ===\")\nprint(results.test_causality('y1', 'y2', kind='f'))  # X → Y\nprint(results.test_causality('y2', 'y1', kind='f'))  # Y → X\n```", "line_start": 141, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why is Granger causality not the same as true causation?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Granger causality only measures predictive precedence—whether past X helps predict Y. It doesn't establish causal mechanism.\n\n**Why they differ:**\n1. **Omitted variables**: Z may cause both X and Y with different lags, creating spurious GC\n2. **Common causes**: X and Y may both respond to unobserved factor\n3. **Spurious correlation**: Can find GC even in independent series by chance\n4. **Measurement timing**: If X and Y are measured at different times, GC reflects measurement, not causation\n\n**Example:** Ice cream sales Granger-cause drownings (both caused by summer heat with different lags).\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Claiming X causes Y based on GC test. Always say \"X Granger-causes Y\" or \"X has predictive power for Y\"—not \"X causes Y.\"\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> What does bidirectional Granger causality mean? Is this common?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Bidirectional GC means both X → Y and Y → X (each helps predict the other). This is common in economics.\n\n**Examples:**\n- GDP ↔ Employment (economic activity and labor market interact)\n- Prices ↔ Wages (wage-price spiral)\n- Interest rates ↔ Exchange rates (monetary policy and currency markets)\n\n**Interpretation:**\n- Feedback relationship\n- Both variables contain unique predictive information\n- System is interdependent\n\n**Caution:** Bidirectional GC doesn't mean simultaneous causation—it means mutual predictive value across time.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Expecting one-way causality. In complex systems, feedback is the rule rather than exception.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> What are the degrees of freedom for the Granger causality F-test with p lags and T observations?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> $F_{p, T-2p-1}$ (numerator df = p, denominator df = T - 2p - 1)\n\n**Derivation:**\n- Restricted model: p parameters (p lags of Y + constant)\n- Unrestricted model: 2p + 1 parameters (p lags of Y + p lags of X + constant)\n- Restriction: p parameters set to zero\n- Observations used: T - p (lose p for lags)\n\nNumerator df = number of restrictions = p\nDenominator df = T - p - (2p + 1) = T - 3p - 1\n\n(Some formulations differ slightly depending on whether constant is counted.)\n\n**Practical:** Use software; these details are handled automatically.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> With short series and many lags, degrees of freedom are low, reducing test power. Balance p against sample size.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> How does the Toda-Yamamoto approach handle non-stationary series in Granger causality testing?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Toda-Yamamoto (1995) avoids pretesting for unit roots:\n\n1. Determine maximum integration order $d_{max}$ (usually 1 or 2)\n2. Fit VAR(p + $d_{max}$) in levels (don't difference)\n3. Test Granger causality on first p lags only\n4. Extra $d_{max}$ lags absorb non-stationarity\n\n**Why it works:**\n- VAR in levels with extra lags has standard asymptotic distribution for Wald test\n- No need to pretest for cointegration\n- Robust to I(1) or I(0) series\n\n**Test:**\n$$H_0: \\beta_1 = \\cdots = \\beta_p = 0$$\n\n(Ignore $\\beta_{p+1}, \\ldots, \\beta_{p+d_{max}}$)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Testing all coefficients including extra lags. Only test first p; the extra $d_{max}$ are \"nuisance\" parameters.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You test Granger causality between oil prices and stock returns using lags 1-8. Results vary: significant at lags 2, 4, 5 but not others. How do you interpret?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> This pattern suggests:\n\n1. **Lag selection matters**: Results are sensitive to specification\n2. **Possible weak relationship**: Significance appears at some lags by chance\n3. **Multiple testing**: Testing 8 specifications inflates false positives\n\n**Recommended approach:**\n1. Select lag order FIRST using information criteria (not GC results)\n2. Report single test at optimal lag\n3. If sensitivity analysis needed, report all results and acknowledge instability\n4. Consider Bonferroni correction for multiple tests\n5. Validate on out-of-sample data\n\n**If results are inconsistent:**\n- Weak evidence for Granger causality\n- Relationship may be nonlinear or time-varying\n- Consider threshold VAR or regime-switching model\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Selecting lag that gives desired result (\"lag shopping\"). This is p-hacking; report pre-specified lag or all results.\n</div>\n</div>\n</details>", "line_start": 149, "level": 2}, {"heading": "References", "content": "1. Granger, C. W. J. (1969). Investigating causal relations by econometric models and cross-spectral methods. *Econometrica*, 37(3), 424-438.\n2. Toda, H. Y., & Yamamoto, T. (1995). Statistical inference in vector autoregressions with possibly integrated processes. *Journal of Econometrics*, 66(1-2), 225-250.\n3. Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press. Chapter 11.\n4. Lütkepohl, H. (2005). *New Introduction to Multiple Time Series Analysis*. Springer. Chapter 2.", "line_start": 275, "level": 1}]}, "docs/en/multivariate/var.md": {"path": "docs/en/multivariate/var.md", "title": "Vector Autoregression (VAR)", "content": "# Vector Autoregression (VAR)\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> VAR models multiple time series jointly, where each variable depends on its own lags and lags of others. VAR(p): $\\mathbf{y}_t = \\mathbf{c} + \\mathbf{A}_1\\mathbf{y}_{t-1} + \\cdots + \\mathbf{A}_p\\mathbf{y}_{t-p} + \\boldsymbol{\\epsilon}_t$. Useful for forecasting interrelated series and analyzing dynamic relationships. Estimate by OLS equation-by-equation. Select order with AIC/BIC. Check stability via eigenvalues.\n</div>\n\n## Core Definitions\n\n**VAR(p) Model:**\n$$\\mathbf{y}_t = \\mathbf{c} + \\mathbf{A}_1\\mathbf{y}_{t-1} + \\mathbf{A}_2\\mathbf{y}_{t-2} + \\cdots + \\mathbf{A}_p\\mathbf{y}_{t-p} + \\boldsymbol{\\epsilon}_t$$\n\nwhere:\n- $\\mathbf{y}_t$: k×1 vector of variables\n- $\\mathbf{c}$: k×1 constant vector\n- $\\mathbf{A}_i$: k×k coefficient matrices\n- $\\boldsymbol{\\epsilon}_t \\sim N(\\mathbf{0}, \\boldsymbol{\\Sigma})$: k×1 error vector\n\n**Compact Form:**\n$$\\mathbf{A}(L)\\mathbf{y}_t = \\mathbf{c} + \\boldsymbol{\\epsilon}_t$$\n\nwhere $\\mathbf{A}(L) = \\mathbf{I} - \\mathbf{A}_1 L - \\cdots - \\mathbf{A}_p L^p$\n\n**Stationarity Condition:** All eigenvalues of companion matrix inside unit circle.\n\n## Math and Derivations\n\n### Bivariate VAR(1) Example\n\nFor variables $(y_{1t}, y_{2t})$:\n$$\\begin{pmatrix} y_{1t} \\\\ y_{2t} \\end{pmatrix} = \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} + \\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{pmatrix}\\begin{pmatrix} y_{1,t-1} \\\\ y_{2,t-1} \\end{pmatrix} + \\begin{pmatrix} \\epsilon_{1t} \\\\ \\epsilon_{2t} \\end{pmatrix}$$\n\nWritten out:\n$$y_{1t} = c_1 + a_{11}y_{1,t-1} + a_{12}y_{2,t-1} + \\epsilon_{1t}$$\n$$y_{2t} = c_2 + a_{21}y_{1,t-1} + a_{22}y_{2,t-1} + \\epsilon_{2t}$$\n\nCross-coefficients $a_{12}, a_{21}$ capture dynamic spillovers.\n\n### Companion Form\n\nVAR(p) can be written as VAR(1) in higher dimension:\n$$\\boldsymbol{\\xi}_t = \\mathbf{F}\\boldsymbol{\\xi}_{t-1} + \\mathbf{v}_t$$\n\nwhere $\\boldsymbol{\\xi}_t = (\\mathbf{y}_t', \\mathbf{y}_{t-1}', \\ldots, \\mathbf{y}_{t-p+1}')'$ and $\\mathbf{F}$ is the companion matrix.\n\nStationarity: eigenvalues of $\\mathbf{F}$ have modulus < 1.\n\n### MA(∞) Representation\n\nStationary VAR has moving average form:\n$$\\mathbf{y}_t = \\boldsymbol{\\mu} + \\sum_{i=0}^{\\infty}\\boldsymbol{\\Phi}_i\\boldsymbol{\\epsilon}_{t-i}$$\n\n$\\boldsymbol{\\Phi}_i$ are impulse response matrices: $\\boldsymbol{\\Phi}_i^{jk}$ = response of variable j to shock in variable k at lag i.\n\n### Forecast Error Variance Decomposition\n\nVariance of h-step forecast error for variable j:\n$$\\sigma_j^2(h) = \\sum_{i=0}^{h-1}\\sum_{k=1}^{K}(\\Phi_i^{jk})^2\\sigma_k^2$$\n\nContribution of variable k to variance of j at horizon h.\n\n## Algorithm/Model Sketch\n\n**VAR Estimation:**\n\n```\n1. Determine optimal lag order p:\n   - Fit VAR(1), VAR(2), ..., VAR(p_max)\n   - Select p minimizing AIC or BIC\n\n2. Estimate by OLS:\n   - Each equation can be estimated separately\n   - OLS is consistent and efficient (same regressors)\n\n3. Check stability:\n   - Compute eigenvalues of companion matrix\n   - All |λᵢ| < 1 for stationarity\n\n4. Diagnostics:\n   - Test residuals for autocorrelation (multivariate LB)\n   - Test for normality\n   - Check for heteroskedasticity\n\n5. Analysis:\n   - Impulse responses\n   - Forecast error variance decomposition\n   - Granger causality tests\n```\n\n## Common Pitfalls\n\n1. **Too many parameters**: VAR(p) with k variables has k + k²p parameters. Overfitting is easy with high k or p.\n\n2. **Non-stationary variables**: VAR requires stationarity. Use differences or VECM for I(1) variables.\n\n3. **Structural interpretation**: Reduced-form VAR shows correlations, not causation. Use structural VAR (SVAR) for causal claims.\n\n4. **Ignoring cointegration**: If variables are cointegrated, restricted VECM is more efficient than unrestricted VAR in differences.\n\n5. **Over-interpreting IRFs**: Impulse responses depend on ordering (Cholesky) or identification assumptions.\n\n6. **Forgetting contemporaneous correlation**: $\\boldsymbol{\\Sigma}$ is not diagonal; shocks are correlated across equations.\n\n## Mini Example\n\n```python\nimport numpy as np\nfrom statsmodels.tsa.api import VAR\n\n# Generate bivariate VAR(1) data\nnp.random.seed(42)\nn = 200\nA = np.array([[0.5, 0.3],\n              [0.2, 0.4]])\nc = np.array([1, 2])\n\ny = np.zeros((n, 2))\nfor t in range(1, n):\n    y[t] = c + A @ y[t-1] + np.random.randn(2) * 0.5\n\n# Fit VAR\nmodel = VAR(y)\n\n# Select lag order\nlag_order = model.select_order(maxlags=8)\nprint(\"Lag selection:\")\nprint(lag_order.summary())\n\n# Fit VAR(1)\nresults = model.fit(1)\nprint(\"\\nCoefficient matrix A:\")\nprint(results.coefs[0])\nprint(f\"\\nTrue A:\\n{A}\")\n\n# Impulse response\nirf = results.irf(10)\nprint(f\"\\nIRF: Response of y1 to y2 shock at lag 5: {irf.irfs[5, 0, 1]:.3f}\")\n\n# Forecast\nforecast = results.forecast(y[-1:], steps=5)\nprint(f\"\\n5-step forecast:\\n{forecast}\")\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What is the advantage of VAR over fitting separate univariate models?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> VAR captures cross-variable dynamics:\n\n1. **Dynamic interactions**: How $y_1$ affects future $y_2$ and vice versa\n2. **Joint forecasting**: Uses information from all variables\n3. **Correlated errors**: Accounts for contemporaneous shocks\n4. **Policy analysis**: Impulse responses show system-wide effects\n\n**Example:** GDP and inflation. VAR captures:\n- Past GDP affecting future inflation (demand effects)\n- Past inflation affecting future GDP (real balance effects)\n- Correlated supply shocks hitting both\n\nSeparate ARIMAs miss these interactions.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using VAR when variables are unrelated. With k variables, you estimate k² coefficients per lag—wasteful if many are zero. Consider sparse VAR or variable selection.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Why is the ordering of variables important for impulse response analysis?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Standard IRFs use Cholesky decomposition of $\\boldsymbol{\\Sigma}$, which depends on variable ordering.\n\n**Cholesky:** $\\boldsymbol{\\Sigma} = \\mathbf{PP}'$ where $\\mathbf{P}$ is lower triangular.\n\nThis implies:\n- First variable's shock is \"structural\" (not affected by others contemporaneously)\n- Later variables respond to earlier ones within same period\n\n**Different orderings → different IRFs**\n\n**Example:** Order (GDP, Inflation) vs (Inflation, GDP)\n- First ordering: GDP shock affects inflation immediately\n- Second ordering: Inflation shock affects GDP immediately\n\n**Solutions:**\n- Use theory to justify ordering\n- Use structural VAR with explicit identification\n- Report sensitivity to ordering\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Reporting IRFs without stating ordering or justifying identification. Results may be driven by arbitrary ordering choice.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> For VAR(1) with coefficient matrix $\\mathbf{A}$, what is the stationarity condition?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> All eigenvalues of $\\mathbf{A}$ must have modulus less than 1.\n\n**Why:**\nVAR(1): $\\mathbf{y}_t = \\mathbf{c} + \\mathbf{A}\\mathbf{y}_{t-1} + \\boldsymbol{\\epsilon}_t$\n\nIterating backward:\n$$\\mathbf{y}_t = (\\mathbf{I} + \\mathbf{A} + \\mathbf{A}^2 + \\cdots)\\mathbf{c} + \\sum_{j=0}^{\\infty}\\mathbf{A}^j\\boldsymbol{\\epsilon}_{t-j}$$\n\nThis converges iff $\\mathbf{A}^j \\to 0$, which requires all eigenvalues inside unit circle.\n\n**For bivariate:**\nIf $\\mathbf{A} = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$:\n\nEigenvalues: $\\lambda = \\frac{(a+d) \\pm \\sqrt{(a+d)^2 - 4(ad-bc)}}{2}$\n\nNeed $|\\lambda_1| < 1$ and $|\\lambda_2| < 1$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Checking only diagonal elements. Even if $|a|, |d| < 1$, off-diagonal terms can make system unstable.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> How many parameters does a VAR(p) model with k variables have?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> $k + k^2 p + \\frac{k(k+1)}{2}$\n\n**Breakdown:**\n- $k$ constant terms (vector $\\mathbf{c}$)\n- $k^2 \\times p$ coefficients (p matrices of size k×k)\n- $\\frac{k(k+1)}{2}$ variance-covariance parameters (symmetric $\\boldsymbol{\\Sigma}$)\n\n**Example:** k=3 variables, p=4 lags:\n- Constants: 3\n- AR coefficients: 9 × 4 = 36\n- Covariance: 6\n- Total: 45 parameters\n\n**Implications:**\n- Parameters grow as $k^2$\n- With limited data, overfitting is severe\n- Consider restricted VAR, BVAR, or variable selection\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Fitting large VAR with small samples. Rule of thumb: need at least 10-20 observations per parameter.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You fit VAR(2) to 3 quarterly macro variables. The residual autocorrelation test rejects at lag 4. What do you do?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Lag 4 autocorrelation with quarterly data suggests annual seasonality. Options:\n\n1. **Increase lag order**: Try VAR(4) or VAR(5) to capture annual dynamics\n\n2. **Add seasonal dummies**: Include Q1, Q2, Q3 indicators as exogenous variables\n\n3. **Seasonally adjust**: Pre-filter data to remove seasonality\n\n4. **VARX**: Add seasonal Fourier terms as exogenous regressors\n\n**Diagnostic process:**\n1. Check if all three residuals show lag-4 pattern\n2. Fit VAR(4) and re-test\n3. Compare AIC: VAR(2) with seasonals vs VAR(4)\n4. Verify residuals now pass tests\n\n**Consideration:** More lags = more parameters. If sample is small, prefer seasonal dummies over VAR(4).\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Ignoring seasonal patterns in macro data. Annual effects are common; quarterly VAR should capture them explicitly.\n</div>\n</div>\n</details>\n\n## References\n\n1. Lütkepohl, H. (2005). *New Introduction to Multiple Time Series Analysis*. Springer.\n2. Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press. Chapters 10-11.\n3. Sims, C. A. (1980). Macroeconomics and reality. *Econometrica*, 48(1), 1-48.\n4. Stock, J. H., & Watson, M. W. (2001). Vector autoregressions. *Journal of Economic Perspectives*, 15(4), 101-115.\n", "sections": [{"heading": "Vector Autoregression (VAR)", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> VAR models multiple time series jointly, where each variable depends on its own lags and lags of others. VAR(p): $\\mathbf{y}_t = \\mathbf{c} + \\mathbf{A}_1\\mathbf{y}_{t-1} + \\cdots + \\mathbf{A}_p\\mathbf{y}_{t-p} + \\boldsymbol{\\epsilon}_t$. Useful for forecasting interrelated series and analyzing dynamic relationships. Estimate by OLS equation-by-equation. Select order with AIC/BIC. Check stability via eigenvalues.\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**VAR(p) Model:**\n$$\\mathbf{y}_t = \\mathbf{c} + \\mathbf{A}_1\\mathbf{y}_{t-1} + \\mathbf{A}_2\\mathbf{y}_{t-2} + \\cdots + \\mathbf{A}_p\\mathbf{y}_{t-p} + \\boldsymbol{\\epsilon}_t$$\n\nwhere:\n- $\\mathbf{y}_t$: k×1 vector of variables\n- $\\mathbf{c}$: k×1 constant vector\n- $\\mathbf{A}_i$: k×k coefficient matrices\n- $\\boldsymbol{\\epsilon}_t \\sim N(\\mathbf{0}, \\boldsymbol{\\Sigma})$: k×1 error vector\n\n**Compact Form:**\n$$\\mathbf{A}(L)\\mathbf{y}_t = \\mathbf{c} + \\boldsymbol{\\epsilon}_t$$\n\nwhere $\\mathbf{A}(L) = \\mathbf{I} - \\mathbf{A}_1 L - \\cdots - \\mathbf{A}_p L^p$\n\n**Stationarity Condition:** All eigenvalues of companion matrix inside unit circle.", "line_start": 7, "level": 2}, {"heading": "Bivariate VAR(1) Example", "content": "For variables $(y_{1t}, y_{2t})$:\n$$\\begin{pmatrix} y_{1t} \\\\ y_{2t} \\end{pmatrix} = \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} + \\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{pmatrix}\\begin{pmatrix} y_{1,t-1} \\\\ y_{2,t-1} \\end{pmatrix} + \\begin{pmatrix} \\epsilon_{1t} \\\\ \\epsilon_{2t} \\end{pmatrix}$$\n\nWritten out:\n$$y_{1t} = c_1 + a_{11}y_{1,t-1} + a_{12}y_{2,t-1} + \\epsilon_{1t}$$\n$$y_{2t} = c_2 + a_{21}y_{1,t-1} + a_{22}y_{2,t-1} + \\epsilon_{2t}$$\n\nCross-coefficients $a_{12}, a_{21}$ capture dynamic spillovers.", "line_start": 27, "level": 3}, {"heading": "Companion Form", "content": "VAR(p) can be written as VAR(1) in higher dimension:\n$$\\boldsymbol{\\xi}_t = \\mathbf{F}\\boldsymbol{\\xi}_{t-1} + \\mathbf{v}_t$$\n\nwhere $\\boldsymbol{\\xi}_t = (\\mathbf{y}_t', \\mathbf{y}_{t-1}', \\ldots, \\mathbf{y}_{t-p+1}')'$ and $\\mathbf{F}$ is the companion matrix.\n\nStationarity: eigenvalues of $\\mathbf{F}$ have modulus < 1.", "line_start": 38, "level": 3}, {"heading": "MA(∞) Representation", "content": "Stationary VAR has moving average form:\n$$\\mathbf{y}_t = \\boldsymbol{\\mu} + \\sum_{i=0}^{\\infty}\\boldsymbol{\\Phi}_i\\boldsymbol{\\epsilon}_{t-i}$$\n\n$\\boldsymbol{\\Phi}_i$ are impulse response matrices: $\\boldsymbol{\\Phi}_i^{jk}$ = response of variable j to shock in variable k at lag i.", "line_start": 47, "level": 3}, {"heading": "Forecast Error Variance Decomposition", "content": "Variance of h-step forecast error for variable j:\n$$\\sigma_j^2(h) = \\sum_{i=0}^{h-1}\\sum_{k=1}^{K}(\\Phi_i^{jk})^2\\sigma_k^2$$\n\nContribution of variable k to variance of j at horizon h.", "line_start": 54, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**VAR Estimation:**\n\n```\n1. Determine optimal lag order p:\n   - Fit VAR(1), VAR(2), ..., VAR(p_max)\n   - Select p minimizing AIC or BIC\n\n2. Estimate by OLS:\n   - Each equation can be estimated separately\n   - OLS is consistent and efficient (same regressors)\n\n3. Check stability:\n   - Compute eigenvalues of companion matrix\n   - All |λᵢ| < 1 for stationarity\n\n4. Diagnostics:\n   - Test residuals for autocorrelation (multivariate LB)\n   - Test for normality\n   - Check for heteroskedasticity\n\n5. Analysis:\n   - Impulse responses\n   - Forecast error variance decomposition\n   - Granger causality tests\n```", "line_start": 61, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Too many parameters**: VAR(p) with k variables has k + k²p parameters. Overfitting is easy with high k or p.\n\n2. **Non-stationary variables**: VAR requires stationarity. Use differences or VECM for I(1) variables.\n\n3. **Structural interpretation**: Reduced-form VAR shows correlations, not causation. Use structural VAR (SVAR) for causal claims.\n\n4. **Ignoring cointegration**: If variables are cointegrated, restricted VECM is more efficient than unrestricted VAR in differences.\n\n5. **Over-interpreting IRFs**: Impulse responses depend on ordering (Cholesky) or identification assumptions.\n\n6. **Forgetting contemporaneous correlation**: $\\boldsymbol{\\Sigma}$ is not diagonal; shocks are correlated across equations.", "line_start": 89, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.api import VAR", "line_start": 103, "level": 1}, {"heading": "Generate bivariate VAR(1) data", "content": "np.random.seed(42)\nn = 200\nA = np.array([[0.5, 0.3],\n              [0.2, 0.4]])\nc = np.array([1, 2])\n\ny = np.zeros((n, 2))\nfor t in range(1, n):\n    y[t] = c + A @ y[t-1] + np.random.randn(2) * 0.5", "line_start": 109, "level": 1}, {"heading": "Fit VAR", "content": "model = VAR(y)", "line_start": 120, "level": 1}, {"heading": "Select lag order", "content": "lag_order = model.select_order(maxlags=8)\nprint(\"Lag selection:\")\nprint(lag_order.summary())", "line_start": 123, "level": 1}, {"heading": "Fit VAR(1)", "content": "results = model.fit(1)\nprint(\"\\nCoefficient matrix A:\")\nprint(results.coefs[0])\nprint(f\"\\nTrue A:\\n{A}\")", "line_start": 128, "level": 1}, {"heading": "Impulse response", "content": "irf = results.irf(10)\nprint(f\"\\nIRF: Response of y1 to y2 shock at lag 5: {irf.irfs[5, 0, 1]:.3f}\")", "line_start": 134, "level": 1}, {"heading": "Forecast", "content": "forecast = results.forecast(y[-1:], steps=5)\nprint(f\"\\n5-step forecast:\\n{forecast}\")\n```", "line_start": 138, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What is the advantage of VAR over fitting separate univariate models?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> VAR captures cross-variable dynamics:\n\n1. **Dynamic interactions**: How $y_1$ affects future $y_2$ and vice versa\n2. **Joint forecasting**: Uses information from all variables\n3. **Correlated errors**: Accounts for contemporaneous shocks\n4. **Policy analysis**: Impulse responses show system-wide effects\n\n**Example:** GDP and inflation. VAR captures:\n- Past GDP affecting future inflation (demand effects)\n- Past inflation affecting future GDP (real balance effects)\n- Correlated supply shocks hitting both\n\nSeparate ARIMAs miss these interactions.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using VAR when variables are unrelated. With k variables, you estimate k² coefficients per lag—wasteful if many are zero. Consider sparse VAR or variable selection.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Why is the ordering of variables important for impulse response analysis?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Standard IRFs use Cholesky decomposition of $\\boldsymbol{\\Sigma}$, which depends on variable ordering.\n\n**Cholesky:** $\\boldsymbol{\\Sigma} = \\mathbf{PP}'$ where $\\mathbf{P}$ is lower triangular.\n\nThis implies:\n- First variable's shock is \"structural\" (not affected by others contemporaneously)\n- Later variables respond to earlier ones within same period\n\n**Different orderings → different IRFs**\n\n**Example:** Order (GDP, Inflation) vs (Inflation, GDP)\n- First ordering: GDP shock affects inflation immediately\n- Second ordering: Inflation shock affects GDP immediately\n\n**Solutions:**\n- Use theory to justify ordering\n- Use structural VAR with explicit identification\n- Report sensitivity to ordering\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Reporting IRFs without stating ordering or justifying identification. Results may be driven by arbitrary ordering choice.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> For VAR(1) with coefficient matrix $\\mathbf{A}$, what is the stationarity condition?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> All eigenvalues of $\\mathbf{A}$ must have modulus less than 1.\n\n**Why:**\nVAR(1): $\\mathbf{y}_t = \\mathbf{c} + \\mathbf{A}\\mathbf{y}_{t-1} + \\boldsymbol{\\epsilon}_t$\n\nIterating backward:\n$$\\mathbf{y}_t = (\\mathbf{I} + \\mathbf{A} + \\mathbf{A}^2 + \\cdots)\\mathbf{c} + \\sum_{j=0}^{\\infty}\\mathbf{A}^j\\boldsymbol{\\epsilon}_{t-j}$$\n\nThis converges iff $\\mathbf{A}^j \\to 0$, which requires all eigenvalues inside unit circle.\n\n**For bivariate:**\nIf $\\mathbf{A} = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$:\n\nEigenvalues: $\\lambda = \\frac{(a+d) \\pm \\sqrt{(a+d)^2 - 4(ad-bc)}}{2}$\n\nNeed $|\\lambda_1| < 1$ and $|\\lambda_2| < 1$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Checking only diagonal elements. Even if $|a|, |d| < 1$, off-diagonal terms can make system unstable.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> How many parameters does a VAR(p) model with k variables have?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> $k + k^2 p + \\frac{k(k+1)}{2}$\n\n**Breakdown:**\n- $k$ constant terms (vector $\\mathbf{c}$)\n- $k^2 \\times p$ coefficients (p matrices of size k×k)\n- $\\frac{k(k+1)}{2}$ variance-covariance parameters (symmetric $\\boldsymbol{\\Sigma}$)\n\n**Example:** k=3 variables, p=4 lags:\n- Constants: 3\n- AR coefficients: 9 × 4 = 36\n- Covariance: 6\n- Total: 45 parameters\n\n**Implications:**\n- Parameters grow as $k^2$\n- With limited data, overfitting is severe\n- Consider restricted VAR, BVAR, or variable selection\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Fitting large VAR with small samples. Rule of thumb: need at least 10-20 observations per parameter.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You fit VAR(2) to 3 quarterly macro variables. The residual autocorrelation test rejects at lag 4. What do you do?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Lag 4 autocorrelation with quarterly data suggests annual seasonality. Options:\n\n1. **Increase lag order**: Try VAR(4) or VAR(5) to capture annual dynamics\n\n2. **Add seasonal dummies**: Include Q1, Q2, Q3 indicators as exogenous variables\n\n3. **Seasonally adjust**: Pre-filter data to remove seasonality\n\n4. **VARX**: Add seasonal Fourier terms as exogenous regressors\n\n**Diagnostic process:**\n1. Check if all three residuals show lag-4 pattern\n2. Fit VAR(4) and re-test\n3. Compare AIC: VAR(2) with seasonals vs VAR(4)\n4. Verify residuals now pass tests\n\n**Consideration:** More lags = more parameters. If sample is small, prefer seasonal dummies over VAR(4).\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Ignoring seasonal patterns in macro data. Annual effects are common; quarterly VAR should capture them explicitly.\n</div>\n</div>\n</details>", "line_start": 143, "level": 2}, {"heading": "References", "content": "1. Lütkepohl, H. (2005). *New Introduction to Multiple Time Series Analysis*. Springer.\n2. Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press. Chapters 10-11.\n3. Sims, C. A. (1980). Macroeconomics and reality. *Econometrica*, 48(1), 1-48.\n4. Stock, J. H., & Watson, M. W. (2001). Vector autoregressions. *Journal of Economic Perspectives*, 15(4), 101-115.", "line_start": 281, "level": 1}]}, "docs/en/time-domain/ma.md": {"path": "docs/en/time-domain/ma.md", "title": "Moving Average (MA) Models", "content": "# Moving Average (MA) Models\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> MA(q) models express current value as a linear combination of current and past q noise terms. MA processes are always stationary (finite linear combination of white noise). ACF cuts off after lag q; PACF decays exponentially. Estimation requires nonlinear optimization (MLE). Invertibility requires roots outside unit circle.\n</div>\n\n## Core Definitions\n\n**MA(q) Model**:\n$$X_t = \\mu + \\epsilon_t + \\theta_1\\epsilon_{t-1} + \\theta_2\\epsilon_{t-2} + \\cdots + \\theta_q\\epsilon_{t-q}$$\n\nwhere $\\epsilon_t \\sim WN(0, \\sigma^2)$.\n\n**Lag Operator Form**:\n$$X_t = \\mu + \\Theta(L)\\epsilon_t$$\n\nwhere $\\Theta(L) = 1 + \\theta_1 L + \\theta_2 L^2 + \\cdots + \\theta_q L^q$.\n\n**Characteristic Polynomial**:\n$$\\Theta(z) = 1 + \\theta_1 z + \\theta_2 z^2 + \\cdots + \\theta_q z^q$$\n\n**Invertibility Condition**: All roots of $\\Theta(z) = 0$ must lie outside the unit circle.\n\n## Math and Derivations\n\n### MA(1) Model: $X_t = \\mu + \\epsilon_t + \\theta\\epsilon_{t-1}$\n\n**Mean**: $E[X_t] = \\mu$\n\n**Variance**:\n$$\\gamma(0) = \\text{Var}(X_t) = \\sigma^2(1 + \\theta^2)$$\n\n**Autocovariance at lag 1**:\n$$\\gamma(1) = E[(\\epsilon_t + \\theta\\epsilon_{t-1})(\\epsilon_{t+1} + \\theta\\epsilon_t)] = \\theta\\sigma^2$$\n\n**Autocovariance at lag $h \\geq 2$**: $\\gamma(h) = 0$\n\n**ACF**:\n$$\\rho(1) = \\frac{\\theta}{1+\\theta^2}, \\quad \\rho(h) = 0 \\text{ for } h \\geq 2$$\n\n**Note**: Maximum $|\\rho(1)| = 0.5$ at $\\theta = \\pm 1$.\n\n### MA(q) General ACF\n\n$$\\gamma(h) = \\begin{cases} \\sigma^2 \\sum_{j=0}^{q-h} \\theta_j \\theta_{j+h} & 0 \\leq h \\leq q \\\\ 0 & h > q \\end{cases}$$\n\nwhere $\\theta_0 = 1$.\n\n$$\\rho(h) = \\frac{\\sum_{j=0}^{q-h} \\theta_j \\theta_{j+h}}{\\sum_{j=0}^{q} \\theta_j^2}$$\n\n### Invertibility and the AR(∞) Representation\n\nAn invertible MA(1) can be written as AR(∞):\n$$X_t = \\mu + \\epsilon_t + \\theta\\epsilon_{t-1}$$\n\nIf $|\\theta| < 1$:\n$$\\epsilon_t = \\sum_{j=0}^{\\infty}(-\\theta)^j(X_{t-j} - \\mu)$$\n\nThis gives:\n$$X_t = \\mu(1+\\theta) - \\theta X_{t-1} + \\theta^2 X_{t-2} - \\theta^3 X_{t-3} + \\cdots + \\epsilon_t$$\n\n**Why invertibility matters**: Allows expressing shocks in terms of observables. Required for proper forecasting and model interpretation.\n\n### PACF of MA(1)\n\nThe PACF of MA(1) decays exponentially:\n$$\\phi_{hh} = \\frac{-(-\\theta)^h(1-\\theta^2)}{1-\\theta^{2(h+1)}}$$\n\nFor large $h$: $\\phi_{hh} \\approx -(-\\theta)^h$\n\n## Algorithm/Model Sketch\n\n**Estimation Methods:**\n\n1. **Innovation Algorithm**: Recursive method to compute MA coefficients from autocovariances.\n\n2. **Conditional Sum of Squares (CSS)**:\n   - Set pre-sample $\\epsilon$ values to zero\n   - Minimize $\\sum \\epsilon_t^2$\n   - Fast but may be biased\n\n3. **Exact Maximum Likelihood (MLE)**:\n   - Accounts for initial conditions\n   - Uses Kalman filter or direct likelihood\n   - Most efficient asymptotically\n\n**Estimation Challenges:**\n- MA estimation is nonlinear (unlike AR)\n- Multiple local optima possible\n- Need good starting values\n- Invertibility constraints must be enforced\n\n**Order Selection:**\n```\n1. Examine ACF - cutoff suggests MA order\n2. If ACF cuts off after lag q, start with MA(q)\n3. Fit candidate models\n4. Compare AIC/BIC\n5. Check residual ACF/PACF\n```\n\n## Common Pitfalls\n\n1. **Parameter identification**: MA(1) with $\\theta$ and MA(1) with $1/\\theta$ give the same ACF! Always enforce invertibility to get unique solution.\n\n2. **Estimation difficulty**: MA models are harder to estimate than AR. Poor starting values lead to convergence issues. Use method=\"innovations\" or CSS for initial estimates.\n\n3. **Confusing MA order with differencing**: Large negative spike at lag 1 in ACF after differencing often indicates over-differencing, not MA(1).\n\n4. **Misinterpreting ACF cutoff**: \"Cutoff\" means abrupt drop to zero, not just decay. AR processes also show ACF patterns—check PACF to distinguish.\n\n5. **Non-invertible estimates**: If estimated $|\\theta| > 1$, the model is non-invertible. Either flip to $1/\\theta$ or reconsider model specification.\n\n6. **Ignoring the unit root boundary**: $\\theta = -1$ or $\\theta = 1$ are non-invertible. Near these values, standard inference breaks down.\n\n## Mini Example\n\n```python\nimport numpy as np\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.stattools import acf\n\n# Generate MA(2) process\nnp.random.seed(42)\nn = 300\ntheta1, theta2 = 0.6, 0.3\neps = np.random.randn(n + 2)\nX = np.zeros(n)\n\nfor t in range(n):\n    X[t] = eps[t+2] + theta1*eps[t+1] + theta2*eps[t]\n\n# Check ACF (should cut off after lag 2)\nacf_values = acf(X, nlags=10)\nprint(\"ACF:\", np.round(acf_values, 3))\n# Expected: significant at lags 1, 2; near zero after\n\n# Fit MA(2) model\nmodel = ARIMA(X, order=(0, 0, 2)).fit()\nprint(f\"True: theta1={theta1}, theta2={theta2}\")\nprint(f\"Estimated: theta1={model.maparams[0]:.3f}, theta2={model.maparams[1]:.3f}\")\n\n# Theoretical ACF for comparison\ngamma0 = 1 + theta1**2 + theta2**2\nrho1 = (theta1 + theta1*theta2) / gamma0\nrho2 = theta2 / gamma0\nprint(f\"Theoretical rho(1)={rho1:.3f}, rho(2)={rho2:.3f}\")\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why are MA processes always stationary regardless of parameter values?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> MA(q) is a finite linear combination of white noise: $X_t = \\mu + \\sum_{j=0}^{q}\\theta_j\\epsilon_{t-j}$. The mean is constant ($\\mu$), variance is $\\sigma^2\\sum\\theta_j^2$ (constant), and autocovariance depends only on lag (not time).\n\n<strong>Explanation:</strong>\nStationarity requires:\n1. Constant mean: $E[X_t] = \\mu$ ✓\n2. Constant variance: $\\text{Var}(X_t) = \\sigma^2(1+\\theta_1^2+\\cdots+\\theta_q^2)$ ✓\n3. Autocovariance depends only on lag: $\\gamma(h)$ doesn't depend on $t$ ✓\n\nAll conditions are satisfied for any finite $\\theta$ values because white noise is stationary and finite linear combinations preserve stationarity.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Confusing stationarity with invertibility. MA is always stationary but not always invertible. Invertibility is about the AR(∞) representation, not stationarity.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Explain the concept of invertibility. Why do we care about it?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Invertibility means we can express the unobservable shocks $\\epsilon_t$ as a convergent function of past observables $X_t, X_{t-1}, \\ldots$. We care because:\n1. It ensures unique model identification\n2. It enables computing residuals for diagnostics\n3. It's needed for proper forecasting updates\n\n<strong>Technical detail:</strong> For MA(1): $X_t = \\epsilon_t + \\theta\\epsilon_{t-1}$\n\nIf $|\\theta| < 1$: $\\epsilon_t = X_t - \\theta X_{t-1} + \\theta^2 X_{t-2} - \\cdots$ (converges)\nIf $|\\theta| > 1$: the expansion diverges\n\n**Key equation:** MA(q) is invertible iff all roots of $\\Theta(z) = 0$ lie outside the unit circle.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Models with $\\theta$ and $1/\\theta$ produce identical ACFs but different forecasts. Without enforcing invertibility, you might get the \"wrong\" model that performs poorly.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Show that for MA(1), the ACF satisfies $|\\rho(1)| \\leq 0.5$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> We have $\\rho(1) = \\frac{\\theta}{1+\\theta^2}$. Taking the derivative and setting to zero finds the maximum.\n\n<strong>Derivation:</strong>\n$$\\frac{d\\rho(1)}{d\\theta} = \\frac{(1+\\theta^2) - \\theta(2\\theta)}{(1+\\theta^2)^2} = \\frac{1-\\theta^2}{(1+\\theta^2)^2}$$\n\nSetting equal to zero: $\\theta = \\pm 1$\n\nAt $\\theta = 1$: $\\rho(1) = \\frac{1}{1+1} = 0.5$\nAt $\\theta = -1$: $\\rho(1) = \\frac{-1}{1+1} = -0.5$\n\nAs $\\theta \\to 0$: $\\rho(1) \\to 0$\nAs $|\\theta| \\to \\infty$: $\\rho(1) \\to 0$\n\nTherefore $|\\rho(1)| \\leq 0.5$ with equality at $\\theta = \\pm 1$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> If you observe sample $|\\hat{\\rho}(1)| > 0.5$, it's unlikely to be pure MA(1). Consider AR(1) (which can have any $|\\rho(1)| < 1$) or mixed ARMA.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Derive the variance $\\gamma(0)$ for MA(2): $X_t = \\epsilon_t + \\theta_1\\epsilon_{t-1} + \\theta_2\\epsilon_{t-2}$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> $\\gamma(0) = \\sigma^2(1 + \\theta_1^2 + \\theta_2^2)$\n\n<strong>Derivation:</strong>\n$$\\gamma(0) = \\text{Var}(X_t) = \\text{Var}(\\epsilon_t + \\theta_1\\epsilon_{t-1} + \\theta_2\\epsilon_{t-2})$$\n\nSince $\\epsilon_t$, $\\epsilon_{t-1}$, $\\epsilon_{t-2}$ are independent:\n$$= \\text{Var}(\\epsilon_t) + \\theta_1^2\\text{Var}(\\epsilon_{t-1}) + \\theta_2^2\\text{Var}(\\epsilon_{t-2})$$\n$$= \\sigma^2 + \\theta_1^2\\sigma^2 + \\theta_2^2\\sigma^2$$\n$$= \\sigma^2(1 + \\theta_1^2 + \\theta_2^2)$$\n\n**General formula for MA(q):**\n$$\\gamma(0) = \\sigma^2\\sum_{j=0}^{q}\\theta_j^2 \\text{ where } \\theta_0 = 1$$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Forgetting that $\\theta_0 = 1$ by convention. The \"1\" in the formula comes from the $\\epsilon_t$ term (coefficient 1).\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You fit an MA(1) model and get $\\hat{\\theta} = 1.2$. What should you do?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The estimate $\\hat{\\theta} = 1.2$ is outside the invertibility region. Options:\n1. Flip to the invertible form: use $\\theta' = 1/1.2 = 0.833$\n2. Re-fit with invertibility constraints enforced\n3. Reconsider model specification (maybe ARMA is better)\n\n<strong>Explanation:</strong>\nMA(1) with $\\theta = 1.2$ and MA(1) with $\\theta = 0.833$ produce identical ACFs:\n- $\\rho(1) = \\frac{1.2}{1+1.44} = \\frac{0.833}{1+0.694} = 0.492$\n\nBut only $\\theta = 0.833$ is invertible. When computing forecasts or residuals, the invertible form is needed.\n\n**Action plan:**\n1. Check if software enforces invertibility automatically\n2. If not, manually transform: $\\theta_{new} = 1/\\hat{\\theta}$\n3. Adjust variance estimate: $\\sigma^2_{new} = \\hat{\\sigma}^2 \\cdot \\hat{\\theta}^2$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Ignoring non-invertibility warnings. The non-invertible model will give poor forecasts because the AR(∞) expansion diverges.\n</div>\n</div>\n</details>\n\n## References\n\n1. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis*. Wiley. Chapter 4.\n2. Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press. Chapter 4.\n3. Brockwell, P. J., & Davis, R. A. (2016). *Introduction to Time Series and Forecasting*. Springer. Chapter 3.\n4. Shumway, R. H., & Stoffer, D. S. (2017). *Time Series Analysis and Its Applications*. Springer. Chapter 3.\n", "sections": [{"heading": "Moving Average (MA) Models", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> MA(q) models express current value as a linear combination of current and past q noise terms. MA processes are always stationary (finite linear combination of white noise). ACF cuts off after lag q; PACF decays exponentially. Estimation requires nonlinear optimization (MLE). Invertibility requires roots outside unit circle.\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**MA(q) Model**:\n$$X_t = \\mu + \\epsilon_t + \\theta_1\\epsilon_{t-1} + \\theta_2\\epsilon_{t-2} + \\cdots + \\theta_q\\epsilon_{t-q}$$\n\nwhere $\\epsilon_t \\sim WN(0, \\sigma^2)$.\n\n**Lag Operator Form**:\n$$X_t = \\mu + \\Theta(L)\\epsilon_t$$\n\nwhere $\\Theta(L) = 1 + \\theta_1 L + \\theta_2 L^2 + \\cdots + \\theta_q L^q$.\n\n**Characteristic Polynomial**:\n$$\\Theta(z) = 1 + \\theta_1 z + \\theta_2 z^2 + \\cdots + \\theta_q z^q$$\n\n**Invertibility Condition**: All roots of $\\Theta(z) = 0$ must lie outside the unit circle.", "line_start": 7, "level": 2}, {"heading": "MA(1) Model: $X_t = \\mu + \\epsilon_t + \\theta\\epsilon_{t-1}$", "content": "**Mean**: $E[X_t] = \\mu$\n\n**Variance**:\n$$\\gamma(0) = \\text{Var}(X_t) = \\sigma^2(1 + \\theta^2)$$\n\n**Autocovariance at lag 1**:\n$$\\gamma(1) = E[(\\epsilon_t + \\theta\\epsilon_{t-1})(\\epsilon_{t+1} + \\theta\\epsilon_t)] = \\theta\\sigma^2$$\n\n**Autocovariance at lag $h \\geq 2$**: $\\gamma(h) = 0$\n\n**ACF**:\n$$\\rho(1) = \\frac{\\theta}{1+\\theta^2}, \\quad \\rho(h) = 0 \\text{ for } h \\geq 2$$\n\n**Note**: Maximum $|\\rho(1)| = 0.5$ at $\\theta = \\pm 1$.", "line_start": 26, "level": 3}, {"heading": "MA(q) General ACF", "content": "$$\\gamma(h) = \\begin{cases} \\sigma^2 \\sum_{j=0}^{q-h} \\theta_j \\theta_{j+h} & 0 \\leq h \\leq q \\\\ 0 & h > q \\end{cases}$$\n\nwhere $\\theta_0 = 1$.\n\n$$\\rho(h) = \\frac{\\sum_{j=0}^{q-h} \\theta_j \\theta_{j+h}}{\\sum_{j=0}^{q} \\theta_j^2}$$", "line_start": 43, "level": 3}, {"heading": "Invertibility and the AR(∞) Representation", "content": "An invertible MA(1) can be written as AR(∞):\n$$X_t = \\mu + \\epsilon_t + \\theta\\epsilon_{t-1}$$\n\nIf $|\\theta| < 1$:\n$$\\epsilon_t = \\sum_{j=0}^{\\infty}(-\\theta)^j(X_{t-j} - \\mu)$$\n\nThis gives:\n$$X_t = \\mu(1+\\theta) - \\theta X_{t-1} + \\theta^2 X_{t-2} - \\theta^3 X_{t-3} + \\cdots + \\epsilon_t$$\n\n**Why invertibility matters**: Allows expressing shocks in terms of observables. Required for proper forecasting and model interpretation.", "line_start": 51, "level": 3}, {"heading": "PACF of MA(1)", "content": "The PACF of MA(1) decays exponentially:\n$$\\phi_{hh} = \\frac{-(-\\theta)^h(1-\\theta^2)}{1-\\theta^{2(h+1)}}$$\n\nFor large $h$: $\\phi_{hh} \\approx -(-\\theta)^h$", "line_start": 64, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**Estimation Methods:**\n\n1. **Innovation Algorithm**: Recursive method to compute MA coefficients from autocovariances.\n\n2. **Conditional Sum of Squares (CSS)**:\n   - Set pre-sample $\\epsilon$ values to zero\n   - Minimize $\\sum \\epsilon_t^2$\n   - Fast but may be biased\n\n3. **Exact Maximum Likelihood (MLE)**:\n   - Accounts for initial conditions\n   - Uses Kalman filter or direct likelihood\n   - Most efficient asymptotically\n\n**Estimation Challenges:**\n- MA estimation is nonlinear (unlike AR)\n- Multiple local optima possible\n- Need good starting values\n- Invertibility constraints must be enforced\n\n**Order Selection:**\n```\n1. Examine ACF - cutoff suggests MA order\n2. If ACF cuts off after lag q, start with MA(q)\n3. Fit candidate models\n4. Compare AIC/BIC\n5. Check residual ACF/PACF\n```", "line_start": 71, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Parameter identification**: MA(1) with $\\theta$ and MA(1) with $1/\\theta$ give the same ACF! Always enforce invertibility to get unique solution.\n\n2. **Estimation difficulty**: MA models are harder to estimate than AR. Poor starting values lead to convergence issues. Use method=\"innovations\" or CSS for initial estimates.\n\n3. **Confusing MA order with differencing**: Large negative spike at lag 1 in ACF after differencing often indicates over-differencing, not MA(1).\n\n4. **Misinterpreting ACF cutoff**: \"Cutoff\" means abrupt drop to zero, not just decay. AR processes also show ACF patterns—check PACF to distinguish.\n\n5. **Non-invertible estimates**: If estimated $|\\theta| > 1$, the model is non-invertible. Either flip to $1/\\theta$ or reconsider model specification.\n\n6. **Ignoring the unit root boundary**: $\\theta = -1$ or $\\theta = 1$ are non-invertible. Near these values, standard inference breaks down.", "line_start": 102, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.stattools import acf", "line_start": 116, "level": 1}, {"heading": "Generate MA(2) process", "content": "np.random.seed(42)\nn = 300\ntheta1, theta2 = 0.6, 0.3\neps = np.random.randn(n + 2)\nX = np.zeros(n)\n\nfor t in range(n):\n    X[t] = eps[t+2] + theta1*eps[t+1] + theta2*eps[t]", "line_start": 123, "level": 1}, {"heading": "Check ACF (should cut off after lag 2)", "content": "acf_values = acf(X, nlags=10)\nprint(\"ACF:\", np.round(acf_values, 3))", "line_start": 133, "level": 1}, {"heading": "Fit MA(2) model", "content": "model = ARIMA(X, order=(0, 0, 2)).fit()\nprint(f\"True: theta1={theta1}, theta2={theta2}\")\nprint(f\"Estimated: theta1={model.maparams[0]:.3f}, theta2={model.maparams[1]:.3f}\")", "line_start": 138, "level": 1}, {"heading": "Theoretical ACF for comparison", "content": "gamma0 = 1 + theta1**2 + theta2**2\nrho1 = (theta1 + theta1*theta2) / gamma0\nrho2 = theta2 / gamma0\nprint(f\"Theoretical rho(1)={rho1:.3f}, rho(2)={rho2:.3f}\")\n```", "line_start": 143, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why are MA processes always stationary regardless of parameter values?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> MA(q) is a finite linear combination of white noise: $X_t = \\mu + \\sum_{j=0}^{q}\\theta_j\\epsilon_{t-j}$. The mean is constant ($\\mu$), variance is $\\sigma^2\\sum\\theta_j^2$ (constant), and autocovariance depends only on lag (not time).\n\n<strong>Explanation:</strong>\nStationarity requires:\n1. Constant mean: $E[X_t] = \\mu$ ✓\n2. Constant variance: $\\text{Var}(X_t) = \\sigma^2(1+\\theta_1^2+\\cdots+\\theta_q^2)$ ✓\n3. Autocovariance depends only on lag: $\\gamma(h)$ doesn't depend on $t$ ✓\n\nAll conditions are satisfied for any finite $\\theta$ values because white noise is stationary and finite linear combinations preserve stationarity.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Confusing stationarity with invertibility. MA is always stationary but not always invertible. Invertibility is about the AR(∞) representation, not stationarity.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Explain the concept of invertibility. Why do we care about it?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Invertibility means we can express the unobservable shocks $\\epsilon_t$ as a convergent function of past observables $X_t, X_{t-1}, \\ldots$. We care because:\n1. It ensures unique model identification\n2. It enables computing residuals for diagnostics\n3. It's needed for proper forecasting updates\n\n<strong>Technical detail:</strong> For MA(1): $X_t = \\epsilon_t + \\theta\\epsilon_{t-1}$\n\nIf $|\\theta| < 1$: $\\epsilon_t = X_t - \\theta X_{t-1} + \\theta^2 X_{t-2} - \\cdots$ (converges)\nIf $|\\theta| > 1$: the expansion diverges\n\n**Key equation:** MA(q) is invertible iff all roots of $\\Theta(z) = 0$ lie outside the unit circle.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Models with $\\theta$ and $1/\\theta$ produce identical ACFs but different forecasts. Without enforcing invertibility, you might get the \"wrong\" model that performs poorly.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Show that for MA(1), the ACF satisfies $|\\rho(1)| \\leq 0.5$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> We have $\\rho(1) = \\frac{\\theta}{1+\\theta^2}$. Taking the derivative and setting to zero finds the maximum.\n\n<strong>Derivation:</strong>\n$$\\frac{d\\rho(1)}{d\\theta} = \\frac{(1+\\theta^2) - \\theta(2\\theta)}{(1+\\theta^2)^2} = \\frac{1-\\theta^2}{(1+\\theta^2)^2}$$\n\nSetting equal to zero: $\\theta = \\pm 1$\n\nAt $\\theta = 1$: $\\rho(1) = \\frac{1}{1+1} = 0.5$\nAt $\\theta = -1$: $\\rho(1) = \\frac{-1}{1+1} = -0.5$\n\nAs $\\theta \\to 0$: $\\rho(1) \\to 0$\nAs $|\\theta| \\to \\infty$: $\\rho(1) \\to 0$\n\nTherefore $|\\rho(1)| \\leq 0.5$ with equality at $\\theta = \\pm 1$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> If you observe sample $|\\hat{\\rho}(1)| > 0.5$, it's unlikely to be pure MA(1). Consider AR(1) (which can have any $|\\rho(1)| < 1$) or mixed ARMA.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Derive the variance $\\gamma(0)$ for MA(2): $X_t = \\epsilon_t + \\theta_1\\epsilon_{t-1} + \\theta_2\\epsilon_{t-2}$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> $\\gamma(0) = \\sigma^2(1 + \\theta_1^2 + \\theta_2^2)$\n\n<strong>Derivation:</strong>\n$$\\gamma(0) = \\text{Var}(X_t) = \\text{Var}(\\epsilon_t + \\theta_1\\epsilon_{t-1} + \\theta_2\\epsilon_{t-2})$$\n\nSince $\\epsilon_t$, $\\epsilon_{t-1}$, $\\epsilon_{t-2}$ are independent:\n$$= \\text{Var}(\\epsilon_t) + \\theta_1^2\\text{Var}(\\epsilon_{t-1}) + \\theta_2^2\\text{Var}(\\epsilon_{t-2})$$\n$$= \\sigma^2 + \\theta_1^2\\sigma^2 + \\theta_2^2\\sigma^2$$\n$$= \\sigma^2(1 + \\theta_1^2 + \\theta_2^2)$$\n\n**General formula for MA(q):**\n$$\\gamma(0) = \\sigma^2\\sum_{j=0}^{q}\\theta_j^2 \\text{ where } \\theta_0 = 1$$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Forgetting that $\\theta_0 = 1$ by convention. The \"1\" in the formula comes from the $\\epsilon_t$ term (coefficient 1).\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You fit an MA(1) model and get $\\hat{\\theta} = 1.2$. What should you do?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The estimate $\\hat{\\theta} = 1.2$ is outside the invertibility region. Options:\n1. Flip to the invertible form: use $\\theta' = 1/1.2 = 0.833$\n2. Re-fit with invertibility constraints enforced\n3. Reconsider model specification (maybe ARMA is better)\n\n<strong>Explanation:</strong>\nMA(1) with $\\theta = 1.2$ and MA(1) with $\\theta = 0.833$ produce identical ACFs:\n- $\\rho(1) = \\frac{1.2}{1+1.44} = \\frac{0.833}{1+0.694} = 0.492$\n\nBut only $\\theta = 0.833$ is invertible. When computing forecasts or residuals, the invertible form is needed.\n\n**Action plan:**\n1. Check if software enforces invertibility automatically\n2. If not, manually transform: $\\theta_{new} = 1/\\hat{\\theta}$\n3. Adjust variance estimate: $\\sigma^2_{new} = \\hat{\\sigma}^2 \\cdot \\hat{\\theta}^2$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Ignoring non-invertibility warnings. The non-invertible model will give poor forecasts because the AR(∞) expansion diverges.\n</div>\n</div>\n</details>", "line_start": 150, "level": 2}, {"heading": "References", "content": "1. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis*. Wiley. Chapter 4.\n2. Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press. Chapter 4.\n3. Brockwell, P. J., & Davis, R. A. (2016). *Introduction to Time Series and Forecasting*. Springer. Chapter 3.\n4. Shumway, R. H., & Stoffer, D. S. (2017). *Time Series Analysis and Its Applications*. Springer. Chapter 3.", "line_start": 268, "level": 1}]}, "docs/en/time-domain/arma.md": {"path": "docs/en/time-domain/arma.md", "title": "ARMA Models", "content": "# ARMA Models\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> ARMA(p,q) combines AR and MA components: $\\Phi(L)X_t = \\Theta(L)\\epsilon_t$. Both ACF and PACF tail off (decay). Stationarity depends on AR part; invertibility depends on MA part. Estimation via MLE. More parsimonious than pure AR or MA when both patterns present.\n</div>\n\n## Core Definitions\n\n**ARMA(p,q) Model**:\n$$X_t = c + \\phi_1 X_{t-1} + \\cdots + \\phi_p X_{t-p} + \\epsilon_t + \\theta_1\\epsilon_{t-1} + \\cdots + \\theta_q\\epsilon_{t-q}$$\n\n**Lag Operator Form**:\n$$\\Phi(L)X_t = c + \\Theta(L)\\epsilon_t$$\n\nwhere:\n- $\\Phi(L) = 1 - \\phi_1 L - \\cdots - \\phi_p L^p$ (AR polynomial)\n- $\\Theta(L) = 1 + \\theta_1 L + \\cdots + \\theta_q L^q$ (MA polynomial)\n\n**Mean**: $\\mu = \\frac{c}{1 - \\phi_1 - \\cdots - \\phi_p}$\n\n**Stationarity**: Roots of $\\Phi(z) = 0$ outside unit circle\n\n**Invertibility**: Roots of $\\Theta(z) = 0$ outside unit circle\n\n## Math and Derivations\n\n### ARMA(1,1): $X_t = c + \\phi X_{t-1} + \\epsilon_t + \\theta\\epsilon_{t-1}$\n\n**Stationarity**: $|\\phi| < 1$\n\n**Invertibility**: $|\\theta| < 1$\n\n**Mean**: $\\mu = \\frac{c}{1-\\phi}$\n\n**Variance** (assuming $\\mu = 0$ for simplicity):\n$$\\gamma(0) = \\phi\\gamma(1) + \\sigma^2(1 + \\theta\\phi + \\theta^2)$$\n\nSolving with $\\gamma(1) = \\phi\\gamma(0) + \\theta\\sigma^2$:\n$$\\gamma(0) = \\sigma^2 \\frac{1 + 2\\theta\\phi + \\theta^2}{1-\\phi^2}$$\n\n**ACF**:\n$$\\rho(1) = \\frac{(1+\\theta\\phi)(\\phi+\\theta)}{1 + 2\\theta\\phi + \\theta^2}$$\n$$\\rho(h) = \\phi\\rho(h-1) \\text{ for } h \\geq 2$$\n\nNote: ACF decays like AR(1) after lag 1, but $\\rho(1)$ differs from AR(1).\n\n### General ARMA(p,q) ACF\n\nFor $h > q$:\n$$\\gamma(h) = \\phi_1\\gamma(h-1) + \\phi_2\\gamma(h-2) + \\cdots + \\phi_p\\gamma(h-p)$$\n\nThe ACF satisfies the same recursion as AR(p) for lags beyond q. Initial values $\\gamma(0), \\ldots, \\gamma(q)$ depend on both AR and MA parameters.\n\n### Causal and Invertible Representations\n\n**Causal (MA(∞)) form**: If stationary:\n$$X_t = \\mu + \\sum_{j=0}^{\\infty}\\psi_j\\epsilon_{t-j}$$\n\nwhere $\\psi_j$ coefficients come from $\\Psi(L) = \\Theta(L)/\\Phi(L)$.\n\n**Invertible (AR(∞)) form**: If invertible:\n$$\\Pi(L)(X_t - \\mu) = \\epsilon_t$$\n\nwhere $\\Pi(L) = \\Phi(L)/\\Theta(L)$.\n\n### Parameter Redundancy\n\n**Critical**: Ensure AR and MA polynomials share no common roots (factors).\n\nExample: $X_t = 0.5X_{t-1} + \\epsilon_t - 0.5\\epsilon_{t-1}$\n\nHere $(1-0.5L)X_t = (1-0.5L)\\epsilon_t$, which simplifies to $X_t = \\epsilon_t$ (white noise!).\n\nThis is called a **common factor** or **parameter redundancy**.\n\n## Algorithm/Model Sketch\n\n**Identification:**\n```\n1. Check stationarity; difference if needed\n2. Examine ACF and PACF:\n   - Both tail off → ARMA (not pure AR or MA)\n   - ACF cuts off → likely MA\n   - PACF cuts off → likely AR\n3. Use EACF (Extended ACF) or information criteria\n4. Fit candidate models\n5. Compare AIC/BIC, check residuals\n```\n\n**Extended ACF (EACF) Method:**\n\nEACF simplifies identification by iteratively removing AR structure. The resulting table shows \"O\" pattern indicating (p,q) order.\n\n**Estimation:**\n\n1. **Conditional MLE**: Condition on initial values, maximize likelihood\n2. **Exact MLE**: Properly accounts for initial conditions\n3. **CSS (Conditional Sum of Squares)**: Minimize squared residuals\n\nMost software uses exact MLE by default.\n\n## Common Pitfalls\n\n1. **Over-parameterization**: ARMA(2,2) often not better than ARMA(1,1). Parsimony matters for forecasting.\n\n2. **Common factor problem**: ARMA(p,q) may reduce to ARMA(p-1,q-1) if polynomials share a root. Check for parameter redundancy.\n\n3. **Local optima**: ARMA likelihood can have multiple modes. Try different starting values.\n\n4. **Near-cancellation**: Parameters close to canceling (e.g., $\\phi \\approx \\theta$) cause estimation instability and inflated standard errors.\n\n5. **Identification confusion**: Both ACF and PACF tail off, but the patterns differ. Focus on overall decay rate and compare with theoretical patterns.\n\n6. **Forgetting conditions**: Need both stationarity (AR roots) AND invertibility (MA roots) outside unit circle.\n\n## Mini Example\n\n```python\nimport numpy as np\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.stattools import acf, pacf\n\n# Generate ARMA(1,1) process\nnp.random.seed(42)\nn = 500\nphi, theta = 0.7, 0.4\neps = np.random.randn(n + 1)\nX = np.zeros(n)\n\nX[0] = eps[1] + theta * eps[0]\nfor t in range(1, n):\n    X[t] = phi * X[t-1] + eps[t+1] + theta * eps[t]\n\n# Both ACF and PACF should tail off\nprint(\"ACF (first 6):\", np.round(acf(X, nlags=5), 3))\nprint(\"PACF (first 6):\", np.round(pacf(X, nlags=5), 3))\n\n# Fit ARMA(1,1)\nmodel = ARIMA(X, order=(1, 0, 1)).fit()\nprint(f\"\\nTrue: phi={phi}, theta={theta}\")\nprint(f\"Estimated: phi={model.arparams[0]:.3f}, theta={model.maparams[0]:.3f}\")\n\n# Check for parameter redundancy\nprint(f\"\\nPhi - Theta = {abs(model.arparams[0] - model.maparams[0]):.3f}\")\n# If close to 0, possible near-cancellation\n\n# Compare with pure AR and pure MA\nar_aic = ARIMA(X, order=(2, 0, 0)).fit().aic\nma_aic = ARIMA(X, order=(0, 0, 2)).fit().aic\narma_aic = model.aic\nprint(f\"\\nAIC: AR(2)={ar_aic:.1f}, MA(2)={ma_aic:.1f}, ARMA(1,1)={arma_aic:.1f}\")\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why can ARMA models be more parsimonious than pure AR or MA models?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Many real processes have both autoregressive dynamics (momentum/persistence) and shock effects that dissipate over time. Modeling this with pure AR or MA requires many parameters, while ARMA captures both with fewer parameters.\n\n<strong>Example:</strong> A process requiring AR(10) or MA(10) might be well-approximated by ARMA(1,1) with just 2 parameters.\n\n**Key insight:** ARMA(1,1) has infinite ACF decay (like AR(∞)) and infinite PACF decay (like MA(∞)), achieving complex correlation structure parsimoniously.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Assuming more parameters is better. ARMA(3,3) often overfits. Start simple—ARMA(1,1) is frequently sufficient.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> What is parameter redundancy in ARMA models and why is it problematic?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Parameter redundancy (common factor problem) occurs when AR and MA polynomials share a root, causing them to cancel. The model reduces to a lower-order ARMA.\n\n<strong>Example:</strong>\n$$(1 - 0.5L)X_t = (1 - 0.5L)\\epsilon_t$$\n\nBoth sides have factor $(1-0.5L)$. Canceling gives $X_t = \\epsilon_t$.\n\n**Problems:**\n1. Extra parameters don't improve fit\n2. Estimation becomes unstable (nearly singular Hessian)\n3. Standard errors explode\n4. Misleading model complexity\n\n**Detection:** Check if AR and MA roots are close. Large standard errors suggest near-redundancy.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Not checking for common factors. Software may fit ARMA(2,2) when ARMA(1,1) suffices, leading to unstable estimates.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> For ARMA(1,1), show that the ACF follows $\\rho(h) = \\phi^{h-1}\\rho(1)$ for $h \\geq 1$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> For $h \\geq 2$, the autocovariance satisfies the AR(1) recursion: $\\gamma(h) = \\phi\\gamma(h-1)$.\n\n<strong>Derivation:</strong>\nMultiply both sides of $X_t = \\phi X_{t-1} + \\epsilon_t + \\theta\\epsilon_{t-1}$ by $X_{t-h}$ and take expectations:\n\nFor $h \\geq 2$:\n$$E[X_t X_{t-h}] = \\phi E[X_{t-1}X_{t-h}] + E[\\epsilon_t X_{t-h}] + \\theta E[\\epsilon_{t-1}X_{t-h}]$$\n\nSince $\\epsilon_t$ and $\\epsilon_{t-1}$ are uncorrelated with $X_{t-h}$ when $h \\geq 2$:\n$$\\gamma(h) = \\phi\\gamma(h-1)$$\n\nTherefore:\n$$\\gamma(h) = \\phi^{h-1}\\gamma(1)$$\n$$\\rho(h) = \\phi^{h-1}\\rho(1)$$\n\n**Note:** $\\rho(1)$ itself is not simply $\\phi$ — it depends on both $\\phi$ and $\\theta$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Assuming ARMA(1,1) ACF at lag 1 equals $\\phi$. The MA component modifies $\\rho(1)$; only subsequent lags follow pure AR(1) decay.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Derive the condition for an ARMA(1,1) to be both stationary and invertible.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Stationarity requires $|\\phi| < 1$; invertibility requires $|\\theta| < 1$.\n\n<strong>Derivation:</strong>\n\n**Stationarity:**\nAR polynomial: $\\Phi(z) = 1 - \\phi z$\nRoot: $z = 1/\\phi$\nOutside unit circle: $|1/\\phi| > 1 \\Rightarrow |\\phi| < 1$\n\n**Invertibility:**\nMA polynomial: $\\Theta(z) = 1 + \\theta z$\nRoot: $z = -1/\\theta$\nOutside unit circle: $|{-1/\\theta}| > 1 \\Rightarrow |\\theta| < 1$\n\n**Combined:** The process is stationary and invertible iff $|\\phi| < 1$ AND $|\\theta| < 1$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Checking only stationarity. A stationary but non-invertible ARMA has improper AR(∞) representation, causing forecasting and diagnostic issues.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You fit several models and get: AR(2) AIC=520, MA(2) AIC=525, ARMA(1,1) AIC=515, ARMA(2,1) AIC=514. Which model do you choose and why?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Choose ARMA(1,1) despite ARMA(2,1) having slightly lower AIC.\n\n<strong>Reasoning:</strong>\n1. AIC difference of 1 is negligible (within noise)\n2. Parsimony principle: simpler model preferred when performance similar\n3. ARMA(1,1) is more stable and interpretable\n4. Additional AR parameter unlikely to improve forecasts\n\n**Decision framework:**\n- AIC difference < 2: models essentially equivalent\n- AIC difference 2-7: some evidence for lower AIC model\n- AIC difference > 10: strong evidence for lower AIC model\n\nAlso consider:\n- BIC (penalizes complexity more)\n- Out-of-sample forecast accuracy\n- Residual diagnostics for all candidates\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Blindly choosing lowest AIC. Small AIC differences are not meaningful. Always consider parsimony and validate with holdout data.\n</div>\n</div>\n</details>\n\n## References\n\n1. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis*. Wiley. Chapter 4.\n2. Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press. Chapter 4.\n3. Tsay, R. S. (2010). *Analysis of Financial Time Series*. Wiley. Chapter 2.\n4. Shumway, R. H., & Stoffer, D. S. (2017). *Time Series Analysis and Its Applications*. Springer. Chapter 3.\n", "sections": [{"heading": "ARMA Models", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> ARMA(p,q) combines AR and MA components: $\\Phi(L)X_t = \\Theta(L)\\epsilon_t$. Both ACF and PACF tail off (decay). Stationarity depends on AR part; invertibility depends on MA part. Estimation via MLE. More parsimonious than pure AR or MA when both patterns present.\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**ARMA(p,q) Model**:\n$$X_t = c + \\phi_1 X_{t-1} + \\cdots + \\phi_p X_{t-p} + \\epsilon_t + \\theta_1\\epsilon_{t-1} + \\cdots + \\theta_q\\epsilon_{t-q}$$\n\n**Lag Operator Form**:\n$$\\Phi(L)X_t = c + \\Theta(L)\\epsilon_t$$\n\nwhere:\n- $\\Phi(L) = 1 - \\phi_1 L - \\cdots - \\phi_p L^p$ (AR polynomial)\n- $\\Theta(L) = 1 + \\theta_1 L + \\cdots + \\theta_q L^q$ (MA polynomial)\n\n**Mean**: $\\mu = \\frac{c}{1 - \\phi_1 - \\cdots - \\phi_p}$\n\n**Stationarity**: Roots of $\\Phi(z) = 0$ outside unit circle\n\n**Invertibility**: Roots of $\\Theta(z) = 0$ outside unit circle", "line_start": 7, "level": 2}, {"heading": "ARMA(1,1): $X_t = c + \\phi X_{t-1} + \\epsilon_t + \\theta\\epsilon_{t-1}$", "content": "**Stationarity**: $|\\phi| < 1$\n\n**Invertibility**: $|\\theta| < 1$\n\n**Mean**: $\\mu = \\frac{c}{1-\\phi}$\n\n**Variance** (assuming $\\mu = 0$ for simplicity):\n$$\\gamma(0) = \\phi\\gamma(1) + \\sigma^2(1 + \\theta\\phi + \\theta^2)$$\n\nSolving with $\\gamma(1) = \\phi\\gamma(0) + \\theta\\sigma^2$:\n$$\\gamma(0) = \\sigma^2 \\frac{1 + 2\\theta\\phi + \\theta^2}{1-\\phi^2}$$\n\n**ACF**:\n$$\\rho(1) = \\frac{(1+\\theta\\phi)(\\phi+\\theta)}{1 + 2\\theta\\phi + \\theta^2}$$\n$$\\rho(h) = \\phi\\rho(h-1) \\text{ for } h \\geq 2$$\n\nNote: ACF decays like AR(1) after lag 1, but $\\rho(1)$ differs from AR(1).", "line_start": 27, "level": 3}, {"heading": "General ARMA(p,q) ACF", "content": "For $h > q$:\n$$\\gamma(h) = \\phi_1\\gamma(h-1) + \\phi_2\\gamma(h-2) + \\cdots + \\phi_p\\gamma(h-p)$$\n\nThe ACF satisfies the same recursion as AR(p) for lags beyond q. Initial values $\\gamma(0), \\ldots, \\gamma(q)$ depend on both AR and MA parameters.", "line_start": 47, "level": 3}, {"heading": "Causal and Invertible Representations", "content": "**Causal (MA(∞)) form**: If stationary:\n$$X_t = \\mu + \\sum_{j=0}^{\\infty}\\psi_j\\epsilon_{t-j}$$\n\nwhere $\\psi_j$ coefficients come from $\\Psi(L) = \\Theta(L)/\\Phi(L)$.\n\n**Invertible (AR(∞)) form**: If invertible:\n$$\\Pi(L)(X_t - \\mu) = \\epsilon_t$$\n\nwhere $\\Pi(L) = \\Phi(L)/\\Theta(L)$.", "line_start": 54, "level": 3}, {"heading": "Parameter Redundancy", "content": "**Critical**: Ensure AR and MA polynomials share no common roots (factors).\n\nExample: $X_t = 0.5X_{t-1} + \\epsilon_t - 0.5\\epsilon_{t-1}$\n\nHere $(1-0.5L)X_t = (1-0.5L)\\epsilon_t$, which simplifies to $X_t = \\epsilon_t$ (white noise!).\n\nThis is called a **common factor** or **parameter redundancy**.", "line_start": 66, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**Identification:**\n```\n1. Check stationarity; difference if needed\n2. Examine ACF and PACF:\n   - Both tail off → ARMA (not pure AR or MA)\n   - ACF cuts off → likely MA\n   - PACF cuts off → likely AR\n3. Use EACF (Extended ACF) or information criteria\n4. Fit candidate models\n5. Compare AIC/BIC, check residuals\n```\n\n**Extended ACF (EACF) Method:**\n\nEACF simplifies identification by iteratively removing AR structure. The resulting table shows \"O\" pattern indicating (p,q) order.\n\n**Estimation:**\n\n1. **Conditional MLE**: Condition on initial values, maximize likelihood\n2. **Exact MLE**: Properly accounts for initial conditions\n3. **CSS (Conditional Sum of Squares)**: Minimize squared residuals\n\nMost software uses exact MLE by default.", "line_start": 76, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Over-parameterization**: ARMA(2,2) often not better than ARMA(1,1). Parsimony matters for forecasting.\n\n2. **Common factor problem**: ARMA(p,q) may reduce to ARMA(p-1,q-1) if polynomials share a root. Check for parameter redundancy.\n\n3. **Local optima**: ARMA likelihood can have multiple modes. Try different starting values.\n\n4. **Near-cancellation**: Parameters close to canceling (e.g., $\\phi \\approx \\theta$) cause estimation instability and inflated standard errors.\n\n5. **Identification confusion**: Both ACF and PACF tail off, but the patterns differ. Focus on overall decay rate and compare with theoretical patterns.\n\n6. **Forgetting conditions**: Need both stationarity (AR roots) AND invertibility (MA roots) outside unit circle.", "line_start": 102, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.stattools import acf, pacf", "line_start": 116, "level": 1}, {"heading": "Generate ARMA(1,1) process", "content": "np.random.seed(42)\nn = 500\nphi, theta = 0.7, 0.4\neps = np.random.randn(n + 1)\nX = np.zeros(n)\n\nX[0] = eps[1] + theta * eps[0]\nfor t in range(1, n):\n    X[t] = phi * X[t-1] + eps[t+1] + theta * eps[t]", "line_start": 123, "level": 1}, {"heading": "Both ACF and PACF should tail off", "content": "print(\"ACF (first 6):\", np.round(acf(X, nlags=5), 3))\nprint(\"PACF (first 6):\", np.round(pacf(X, nlags=5), 3))", "line_start": 134, "level": 1}, {"heading": "Fit ARMA(1,1)", "content": "model = ARIMA(X, order=(1, 0, 1)).fit()\nprint(f\"\\nTrue: phi={phi}, theta={theta}\")\nprint(f\"Estimated: phi={model.arparams[0]:.3f}, theta={model.maparams[0]:.3f}\")", "line_start": 138, "level": 1}, {"heading": "Check for parameter redundancy", "content": "print(f\"\\nPhi - Theta = {abs(model.arparams[0] - model.maparams[0]):.3f}\")", "line_start": 143, "level": 1}, {"heading": "Compare with pure AR and pure MA", "content": "ar_aic = ARIMA(X, order=(2, 0, 0)).fit().aic\nma_aic = ARIMA(X, order=(0, 0, 2)).fit().aic\narma_aic = model.aic\nprint(f\"\\nAIC: AR(2)={ar_aic:.1f}, MA(2)={ma_aic:.1f}, ARMA(1,1)={arma_aic:.1f}\")\n```", "line_start": 147, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why can ARMA models be more parsimonious than pure AR or MA models?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Many real processes have both autoregressive dynamics (momentum/persistence) and shock effects that dissipate over time. Modeling this with pure AR or MA requires many parameters, while ARMA captures both with fewer parameters.\n\n<strong>Example:</strong> A process requiring AR(10) or MA(10) might be well-approximated by ARMA(1,1) with just 2 parameters.\n\n**Key insight:** ARMA(1,1) has infinite ACF decay (like AR(∞)) and infinite PACF decay (like MA(∞)), achieving complex correlation structure parsimoniously.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Assuming more parameters is better. ARMA(3,3) often overfits. Start simple—ARMA(1,1) is frequently sufficient.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> What is parameter redundancy in ARMA models and why is it problematic?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Parameter redundancy (common factor problem) occurs when AR and MA polynomials share a root, causing them to cancel. The model reduces to a lower-order ARMA.\n\n<strong>Example:</strong>\n$$(1 - 0.5L)X_t = (1 - 0.5L)\\epsilon_t$$\n\nBoth sides have factor $(1-0.5L)$. Canceling gives $X_t = \\epsilon_t$.\n\n**Problems:**\n1. Extra parameters don't improve fit\n2. Estimation becomes unstable (nearly singular Hessian)\n3. Standard errors explode\n4. Misleading model complexity\n\n**Detection:** Check if AR and MA roots are close. Large standard errors suggest near-redundancy.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Not checking for common factors. Software may fit ARMA(2,2) when ARMA(1,1) suffices, leading to unstable estimates.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> For ARMA(1,1), show that the ACF follows $\\rho(h) = \\phi^{h-1}\\rho(1)$ for $h \\geq 1$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> For $h \\geq 2$, the autocovariance satisfies the AR(1) recursion: $\\gamma(h) = \\phi\\gamma(h-1)$.\n\n<strong>Derivation:</strong>\nMultiply both sides of $X_t = \\phi X_{t-1} + \\epsilon_t + \\theta\\epsilon_{t-1}$ by $X_{t-h}$ and take expectations:\n\nFor $h \\geq 2$:\n$$E[X_t X_{t-h}] = \\phi E[X_{t-1}X_{t-h}] + E[\\epsilon_t X_{t-h}] + \\theta E[\\epsilon_{t-1}X_{t-h}]$$\n\nSince $\\epsilon_t$ and $\\epsilon_{t-1}$ are uncorrelated with $X_{t-h}$ when $h \\geq 2$:\n$$\\gamma(h) = \\phi\\gamma(h-1)$$\n\nTherefore:\n$$\\gamma(h) = \\phi^{h-1}\\gamma(1)$$\n$$\\rho(h) = \\phi^{h-1}\\rho(1)$$\n\n**Note:** $\\rho(1)$ itself is not simply $\\phi$ — it depends on both $\\phi$ and $\\theta$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Assuming ARMA(1,1) ACF at lag 1 equals $\\phi$. The MA component modifies $\\rho(1)$; only subsequent lags follow pure AR(1) decay.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Derive the condition for an ARMA(1,1) to be both stationary and invertible.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Stationarity requires $|\\phi| < 1$; invertibility requires $|\\theta| < 1$.\n\n<strong>Derivation:</strong>\n\n**Stationarity:**\nAR polynomial: $\\Phi(z) = 1 - \\phi z$\nRoot: $z = 1/\\phi$\nOutside unit circle: $|1/\\phi| > 1 \\Rightarrow |\\phi| < 1$\n\n**Invertibility:**\nMA polynomial: $\\Theta(z) = 1 + \\theta z$\nRoot: $z = -1/\\theta$\nOutside unit circle: $|{-1/\\theta}| > 1 \\Rightarrow |\\theta| < 1$\n\n**Combined:** The process is stationary and invertible iff $|\\phi| < 1$ AND $|\\theta| < 1$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Checking only stationarity. A stationary but non-invertible ARMA has improper AR(∞) representation, causing forecasting and diagnostic issues.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You fit several models and get: AR(2) AIC=520, MA(2) AIC=525, ARMA(1,1) AIC=515, ARMA(2,1) AIC=514. Which model do you choose and why?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Choose ARMA(1,1) despite ARMA(2,1) having slightly lower AIC.\n\n<strong>Reasoning:</strong>\n1. AIC difference of 1 is negligible (within noise)\n2. Parsimony principle: simpler model preferred when performance similar\n3. ARMA(1,1) is more stable and interpretable\n4. Additional AR parameter unlikely to improve forecasts\n\n**Decision framework:**\n- AIC difference < 2: models essentially equivalent\n- AIC difference 2-7: some evidence for lower AIC model\n- AIC difference > 10: strong evidence for lower AIC model\n\nAlso consider:\n- BIC (penalizes complexity more)\n- Out-of-sample forecast accuracy\n- Residual diagnostics for all candidates\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Blindly choosing lowest AIC. Small AIC differences are not meaningful. Always consider parsimony and validate with holdout data.\n</div>\n</div>\n</details>", "line_start": 154, "level": 2}, {"heading": "References", "content": "1. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis*. Wiley. Chapter 4.\n2. Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press. Chapter 4.\n3. Tsay, R. S. (2010). *Analysis of Financial Time Series*. Wiley. Chapter 2.\n4. Shumway, R. H., & Stoffer, D. S. (2017). *Time Series Analysis and Its Applications*. Springer. Chapter 3.", "line_start": 278, "level": 1}]}, "docs/en/time-domain/ar.md": {"path": "docs/en/time-domain/ar.md", "title": "Autoregressive (AR) Models", "content": "# Autoregressive (AR) Models\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> AR(p) models express current value as a linear combination of p past values plus noise. Stationarity requires roots of characteristic polynomial outside unit circle (equivalently, $|\\phi| < 1$ for AR(1)). ACF decays exponentially/sinusoidally; PACF cuts off after lag p. Estimate via Yule-Walker, OLS, or MLE.\n</div>\n\n## Core Definitions\n\n**AR(p) Model**:\n$$X_t = c + \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + \\cdots + \\phi_p X_{t-p} + \\epsilon_t$$\n\nwhere $\\epsilon_t \\sim WN(0, \\sigma^2)$ (white noise).\n\n**Lag Operator Form**:\n$$\\Phi(L)X_t = c + \\epsilon_t$$\n\nwhere $\\Phi(L) = 1 - \\phi_1 L - \\phi_2 L^2 - \\cdots - \\phi_p L^p$ and $LX_t = X_{t-1}$.\n\n**Characteristic Polynomial**:\n$$\\Phi(z) = 1 - \\phi_1 z - \\phi_2 z^2 - \\cdots - \\phi_p z^p$$\n\n**Stationarity Condition**: All roots of $\\Phi(z) = 0$ must lie outside the unit circle (|z| > 1).\n\n**Mean of Stationary AR(p)**:\n$$\\mu = E[X_t] = \\frac{c}{1 - \\phi_1 - \\phi_2 - \\cdots - \\phi_p}$$\n\n## Math and Derivations\n\n### AR(1) Model: $X_t = c + \\phi X_{t-1} + \\epsilon_t$\n\n**Stationarity condition**: $|\\phi| < 1$\n\n**Mean**: $\\mu = \\frac{c}{1-\\phi}$\n\n**Variance**:\n$$\\gamma(0) = \\text{Var}(X_t) = \\frac{\\sigma^2}{1-\\phi^2}$$\n\n**Autocovariance**:\n$$\\gamma(h) = \\phi^{|h|} \\gamma(0) = \\frac{\\phi^{|h|} \\sigma^2}{1-\\phi^2}$$\n\n**ACF**: $\\rho(h) = \\phi^{|h|}$\n\n**PACF**: $\\phi_{11} = \\phi$, $\\phi_{hh} = 0$ for $h > 1$\n\n### AR(2) Model: $X_t = c + \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + \\epsilon_t$\n\n**Stationarity conditions** (all must hold):\n1. $\\phi_1 + \\phi_2 < 1$\n2. $\\phi_2 - \\phi_1 < 1$\n3. $|\\phi_2| < 1$\n\n**Characteristic roots**: Solutions to $1 - \\phi_1 z - \\phi_2 z^2 = 0$\n- If roots are real: monotonic decay in ACF\n- If roots are complex: damped sinusoidal ACF\n\n**Yule-Walker equations for AR(2)**:\n$$\\rho(1) = \\phi_1 + \\phi_2\\rho(1) \\Rightarrow \\rho(1) = \\frac{\\phi_1}{1-\\phi_2}$$\n$$\\rho(2) = \\phi_1\\rho(1) + \\phi_2$$\n\n### General AR(p) Yule-Walker Equations\n\n$$\\gamma(h) = \\phi_1\\gamma(h-1) + \\phi_2\\gamma(h-2) + \\cdots + \\phi_p\\gamma(h-p) \\text{ for } h > 0$$\n\nIn matrix form:\n$$\\begin{pmatrix} \\gamma(0) & \\gamma(1) & \\cdots & \\gamma(p-1) \\\\ \\gamma(1) & \\gamma(0) & \\cdots & \\gamma(p-2) \\\\ \\vdots & & \\ddots & \\vdots \\\\ \\gamma(p-1) & \\cdots & \\gamma(1) & \\gamma(0) \\end{pmatrix} \\begin{pmatrix} \\phi_1 \\\\ \\phi_2 \\\\ \\vdots \\\\ \\phi_p \\end{pmatrix} = \\begin{pmatrix} \\gamma(1) \\\\ \\gamma(2) \\\\ \\vdots \\\\ \\gamma(p) \\end{pmatrix}$$\n\nOr in terms of autocorrelations:\n$$\\mathbf{R}\\boldsymbol{\\phi} = \\boldsymbol{\\rho}$$\n\n### Infinite MA Representation\n\nA stationary AR(p) can be written as an infinite MA:\n$$X_t = \\mu + \\sum_{j=0}^{\\infty} \\psi_j \\epsilon_{t-j}$$\n\nFor AR(1): $\\psi_j = \\phi^j$\n\nThis shows AR processes have infinite memory but with exponentially decaying weights.\n\n## Algorithm/Model Sketch\n\n**Estimation Methods:**\n\n1. **Yule-Walker (Method of Moments)**:\n   - Replace $\\gamma(h)$ with $\\hat{\\gamma}(h)$\n   - Solve linear system for $\\hat{\\phi}$\n   - Always yields stationary estimates\n   - May be inefficient for small samples\n\n2. **Ordinary Least Squares (OLS)**:\n   - Regress $X_t$ on $X_{t-1}, \\ldots, X_{t-p}$\n   - Simple but loses first $p$ observations\n   - May give non-stationary estimates\n\n3. **Maximum Likelihood (MLE)**:\n   - Most efficient asymptotically\n   - Accounts for initial conditions\n   - Requires distributional assumption (usually Gaussian)\n   - Use numerical optimization\n\n**Order Selection:**\n```\n1. Examine PACF - significant spikes suggest AR order\n2. Fit AR(1), AR(2), ..., AR(p_max)\n3. Compare AIC/BIC values\n4. Select model with lowest information criterion\n5. Verify residuals are white noise\n```\n\n## Common Pitfalls\n\n1. **Ignoring stationarity check**: Always verify estimated parameters satisfy stationarity conditions. Non-stationary AR leads to explosive forecasts.\n\n2. **Over-fitting with too many lags**: AIC may favor larger models. BIC penalizes complexity more and often gives better forecasts.\n\n3. **Assuming causality**: AR models capture correlation, not causation. $X_{t-1}$ predicting $X_t$ doesn't mean past causes future.\n\n4. **Neglecting seasonality**: Standard AR doesn't capture seasonal patterns at lag $s$. Consider SARIMA or include $X_{t-s}$ explicitly.\n\n5. **Using OLS without correction**: Standard OLS standard errors are invalid for time series with autocorrelation. Use HAC standard errors or proper likelihood-based inference.\n\n6. **Confusing AR(1) coefficient sign**: Positive $\\phi$ gives positive autocorrelation (momentum). Negative $\\phi$ gives alternating signs (mean reversion).\n\n## Mini Example\n\n```python\nimport numpy as np\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom statsmodels.tsa.stattools import pacf\n\n# Generate AR(2) data\nnp.random.seed(42)\nn = 300\nphi1, phi2 = 0.6, -0.3\nX = np.zeros(n)\neps = np.random.randn(n)\n\nfor t in range(2, n):\n    X[t] = phi1 * X[t-1] + phi2 * X[t-2] + eps[t]\n\n# Check PACF (should cut off after lag 2)\npacf_values = pacf(X, nlags=10)\nprint(\"PACF:\", np.round(pacf_values, 3))\n\n# Fit AR model using AIC to select order\nfrom statsmodels.tsa.ar_model import ar_select_order\nsel = ar_select_order(X, maxlag=10, ic='aic')\nprint(f\"Selected order: {sel.ar_lags}\")\n\n# Fit AR(2) and check estimates\nmodel = AutoReg(X, lags=2).fit()\nprint(f\"True: phi1={phi1}, phi2={phi2}\")\nprint(f\"Estimated: phi1={model.params[1]:.3f}, phi2={model.params[2]:.3f}\")\n\n# Forecast\nforecast = model.forecast(steps=5)\nprint(\"5-step forecast:\", forecast)\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Explain intuitively why the stationarity condition for AR(1) is $|\\phi| < 1$. What happens when $\\phi = 1$ or $\\phi > 1$?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> When $|\\phi| < 1$, shocks decay over time, keeping variance bounded. When $\\phi = 1$, we have a random walk where shocks persist forever (unit root). When $|\\phi| > 1$, the process explodes exponentially.\n\n<strong>Explanation:</strong>\nThe AR(1) can be written as:\n$$X_t = \\phi^t X_0 + \\sum_{j=0}^{t-1} \\phi^j \\epsilon_{t-j}$$\n\n- If $|\\phi| < 1$: $\\phi^t \\to 0$ and the MA representation converges (bounded variance)\n- If $\\phi = 1$: $X_t = X_0 + \\sum \\epsilon_j$ (random walk, variance $\\to \\infty$)\n- If $|\\phi| > 1$: $\\phi^t \\to \\infty$ (explosive)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Thinking $\\phi = 0.99$ is \"close enough\" to stationary. While technically stationary, near-unit-root processes behave like random walks in finite samples. Predictions degrade quickly.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Why does the PACF of an AR(p) process cut off after lag p while the ACF decays gradually?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> PACF measures direct correlation after controlling for intermediate lags. AR(p) by definition only has direct dependence on $p$ past values, so PACF is zero beyond lag $p$. ACF includes indirect effects through intermediate values, causing gradual decay.\n\n<strong>Explanation:</strong>\nFor AR(2): $X_t = \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + \\epsilon_t$\n- Direct effects: only from $X_{t-1}$ and $X_{t-2}$\n- PACF at lag 3: After controlling for $X_{t-1}, X_{t-2}$, $X_{t-3}$ has no additional predictive power\n- But ACF at lag 3: $X_t$ correlates with $X_{t-3}$ through the chain $X_{t-1} \\to X_{t-2} \\to X_{t-3}$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Expecting perfectly zero PACF beyond lag $p$ in samples. Due to estimation error, you'll see small nonzero values. Use confidence bands to judge significance.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Derive the variance of a stationary AR(1) process: $\\text{Var}(X_t) = \\frac{\\sigma^2}{1-\\phi^2}$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Starting from $X_t = \\phi X_{t-1} + \\epsilon_t$, take variance of both sides and use stationarity.\n\n<strong>Derivation:</strong>\n$$\\text{Var}(X_t) = \\text{Var}(\\phi X_{t-1} + \\epsilon_t)$$\n$$= \\phi^2 \\text{Var}(X_{t-1}) + \\text{Var}(\\epsilon_t) + 2\\phi\\text{Cov}(X_{t-1}, \\epsilon_t)$$\n\nSince $\\epsilon_t$ is independent of $X_{t-1}$:\n$$\\gamma(0) = \\phi^2 \\gamma(0) + \\sigma^2$$\n\nBy stationarity, $\\text{Var}(X_t) = \\text{Var}(X_{t-1}) = \\gamma(0)$:\n$$\\gamma(0) - \\phi^2\\gamma(0) = \\sigma^2$$\n$$\\gamma(0)(1 - \\phi^2) = \\sigma^2$$\n$$\\gamma(0) = \\frac{\\sigma^2}{1-\\phi^2}$$\n\n**Note:** Requires $|\\phi| < 1$ for positive variance.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Forgetting that variance increases as $|\\phi| \\to 1$. Near-unit-root processes have large variance, making them look more volatile than white noise with the same $\\sigma^2$.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> For AR(2), show that the stationarity region in the $(\\phi_1, \\phi_2)$ plane is triangular with vertices at $(2, -1)$, $(-2, -1)$, and $(0, 1)$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The three stationarity conditions $\\phi_1 + \\phi_2 < 1$, $\\phi_2 - \\phi_1 < 1$, and $|\\phi_2| < 1$ define a triangular region.\n\n<strong>Derivation:</strong>\nThe characteristic equation $1 - \\phi_1 z - \\phi_2 z^2 = 0$ must have roots outside unit circle.\n\nSetting $z = 1$: $1 - \\phi_1 - \\phi_2 > 0 \\Rightarrow \\phi_1 + \\phi_2 < 1$\nSetting $z = -1$: $1 + \\phi_1 - \\phi_2 > 0 \\Rightarrow \\phi_2 - \\phi_1 < 1$\n\nFor complex roots, discriminant analysis gives: $|\\phi_2| < 1$\n\nBoundary lines:\n- $\\phi_1 + \\phi_2 = 1$ (passes through $(2, -1)$ and $(0, 1)$)\n- $\\phi_2 - \\phi_1 = 1$ (passes through $(-2, -1)$ and $(0, 1)$)\n- $\\phi_2 = -1$ (connects $(2, -1)$ and $(-2, -1)$)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Checking only one condition. All three must hold simultaneously. A model can satisfy two conditions but fail the third and still be non-stationary.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You fit an AR(3) model and get estimates $\\hat{\\phi}_1 = 0.5$, $\\hat{\\phi}_2 = 0.3$, $\\hat{\\phi}_3 = 0.25$. The residual ACF shows a significant spike at lag 1. What might be wrong?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> A significant residual autocorrelation at lag 1 indicates model misspecification. Possible causes:\n1. MA component is needed (ARMA instead of pure AR)\n2. Structural break in the data\n3. Outliers affecting estimation\n4. Non-stationarity not fully addressed\n\n<strong>Diagnostic steps:</strong>\n1. Perform Ljung-Box test on residuals\n2. Try fitting ARMA(3,1) or ARMA(3,2)\n3. Plot residuals over time to check for patterns\n4. Check for outliers or level shifts\n5. Verify original series was stationary\n\n**Key insight:** Pure AR residuals should be white noise. Significant residual autocorrelation means the model hasn't captured all temporal dependence.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Adding more AR lags to fix residual correlation. Sometimes an MA term is more parsimonious. Compare AIC between AR(4), AR(5) and ARMA(3,1).\n</div>\n</div>\n</details>\n\n## References\n\n1. Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press. Chapter 3.\n2. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis*. Wiley. Chapter 3.\n3. Brockwell, P. J., & Davis, R. A. (2016). *Introduction to Time Series and Forecasting*. Springer. Chapter 3.\n4. Lütkepohl, H. (2005). *New Introduction to Multiple Time Series Analysis*. Springer. Chapter 2.\n", "sections": [{"heading": "Autoregressive (AR) Models", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> AR(p) models express current value as a linear combination of p past values plus noise. Stationarity requires roots of characteristic polynomial outside unit circle (equivalently, $|\\phi| < 1$ for AR(1)). ACF decays exponentially/sinusoidally; PACF cuts off after lag p. Estimate via Yule-Walker, OLS, or MLE.\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**AR(p) Model**:\n$$X_t = c + \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + \\cdots + \\phi_p X_{t-p} + \\epsilon_t$$\n\nwhere $\\epsilon_t \\sim WN(0, \\sigma^2)$ (white noise).\n\n**Lag Operator Form**:\n$$\\Phi(L)X_t = c + \\epsilon_t$$\n\nwhere $\\Phi(L) = 1 - \\phi_1 L - \\phi_2 L^2 - \\cdots - \\phi_p L^p$ and $LX_t = X_{t-1}$.\n\n**Characteristic Polynomial**:\n$$\\Phi(z) = 1 - \\phi_1 z - \\phi_2 z^2 - \\cdots - \\phi_p z^p$$\n\n**Stationarity Condition**: All roots of $\\Phi(z) = 0$ must lie outside the unit circle (|z| > 1).\n\n**Mean of Stationary AR(p)**:\n$$\\mu = E[X_t] = \\frac{c}{1 - \\phi_1 - \\phi_2 - \\cdots - \\phi_p}$$", "line_start": 7, "level": 2}, {"heading": "AR(1) Model: $X_t = c + \\phi X_{t-1} + \\epsilon_t$", "content": "**Stationarity condition**: $|\\phi| < 1$\n\n**Mean**: $\\mu = \\frac{c}{1-\\phi}$\n\n**Variance**:\n$$\\gamma(0) = \\text{Var}(X_t) = \\frac{\\sigma^2}{1-\\phi^2}$$\n\n**Autocovariance**:\n$$\\gamma(h) = \\phi^{|h|} \\gamma(0) = \\frac{\\phi^{|h|} \\sigma^2}{1-\\phi^2}$$\n\n**ACF**: $\\rho(h) = \\phi^{|h|}$\n\n**PACF**: $\\phi_{11} = \\phi$, $\\phi_{hh} = 0$ for $h > 1$", "line_start": 29, "level": 3}, {"heading": "AR(2) Model: $X_t = c + \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + \\epsilon_t$", "content": "**Stationarity conditions** (all must hold):\n1. $\\phi_1 + \\phi_2 < 1$\n2. $\\phi_2 - \\phi_1 < 1$\n3. $|\\phi_2| < 1$\n\n**Characteristic roots**: Solutions to $1 - \\phi_1 z - \\phi_2 z^2 = 0$\n- If roots are real: monotonic decay in ACF\n- If roots are complex: damped sinusoidal ACF\n\n**Yule-Walker equations for AR(2)**:\n$$\\rho(1) = \\phi_1 + \\phi_2\\rho(1) \\Rightarrow \\rho(1) = \\frac{\\phi_1}{1-\\phi_2}$$\n$$\\rho(2) = \\phi_1\\rho(1) + \\phi_2$$", "line_start": 45, "level": 3}, {"heading": "General AR(p) Yule-Walker Equations", "content": "$$\\gamma(h) = \\phi_1\\gamma(h-1) + \\phi_2\\gamma(h-2) + \\cdots + \\phi_p\\gamma(h-p) \\text{ for } h > 0$$\n\nIn matrix form:\n$$\\begin{pmatrix} \\gamma(0) & \\gamma(1) & \\cdots & \\gamma(p-1) \\\\ \\gamma(1) & \\gamma(0) & \\cdots & \\gamma(p-2) \\\\ \\vdots & & \\ddots & \\vdots \\\\ \\gamma(p-1) & \\cdots & \\gamma(1) & \\gamma(0) \\end{pmatrix} \\begin{pmatrix} \\phi_1 \\\\ \\phi_2 \\\\ \\vdots \\\\ \\phi_p \\end{pmatrix} = \\begin{pmatrix} \\gamma(1) \\\\ \\gamma(2) \\\\ \\vdots \\\\ \\gamma(p) \\end{pmatrix}$$\n\nOr in terms of autocorrelations:\n$$\\mathbf{R}\\boldsymbol{\\phi} = \\boldsymbol{\\rho}$$", "line_start": 60, "level": 3}, {"heading": "Infinite MA Representation", "content": "A stationary AR(p) can be written as an infinite MA:\n$$X_t = \\mu + \\sum_{j=0}^{\\infty} \\psi_j \\epsilon_{t-j}$$\n\nFor AR(1): $\\psi_j = \\phi^j$\n\nThis shows AR processes have infinite memory but with exponentially decaying weights.", "line_start": 70, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**Estimation Methods:**\n\n1. **Yule-Walker (Method of Moments)**:\n   - Replace $\\gamma(h)$ with $\\hat{\\gamma}(h)$\n   - Solve linear system for $\\hat{\\phi}$\n   - Always yields stationary estimates\n   - May be inefficient for small samples\n\n2. **Ordinary Least Squares (OLS)**:\n   - Regress $X_t$ on $X_{t-1}, \\ldots, X_{t-p}$\n   - Simple but loses first $p$ observations\n   - May give non-stationary estimates\n\n3. **Maximum Likelihood (MLE)**:\n   - Most efficient asymptotically\n   - Accounts for initial conditions\n   - Requires distributional assumption (usually Gaussian)\n   - Use numerical optimization\n\n**Order Selection:**\n```\n1. Examine PACF - significant spikes suggest AR order\n2. Fit AR(1), AR(2), ..., AR(p_max)\n3. Compare AIC/BIC values\n4. Select model with lowest information criterion\n5. Verify residuals are white noise\n```", "line_start": 79, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Ignoring stationarity check**: Always verify estimated parameters satisfy stationarity conditions. Non-stationary AR leads to explosive forecasts.\n\n2. **Over-fitting with too many lags**: AIC may favor larger models. BIC penalizes complexity more and often gives better forecasts.\n\n3. **Assuming causality**: AR models capture correlation, not causation. $X_{t-1}$ predicting $X_t$ doesn't mean past causes future.\n\n4. **Neglecting seasonality**: Standard AR doesn't capture seasonal patterns at lag $s$. Consider SARIMA or include $X_{t-s}$ explicitly.\n\n5. **Using OLS without correction**: Standard OLS standard errors are invalid for time series with autocorrelation. Use HAC standard errors or proper likelihood-based inference.\n\n6. **Confusing AR(1) coefficient sign**: Positive $\\phi$ gives positive autocorrelation (momentum). Negative $\\phi$ gives alternating signs (mean reversion).", "line_start": 109, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom statsmodels.tsa.stattools import pacf", "line_start": 123, "level": 1}, {"heading": "Generate AR(2) data", "content": "np.random.seed(42)\nn = 300\nphi1, phi2 = 0.6, -0.3\nX = np.zeros(n)\neps = np.random.randn(n)\n\nfor t in range(2, n):\n    X[t] = phi1 * X[t-1] + phi2 * X[t-2] + eps[t]", "line_start": 130, "level": 1}, {"heading": "Check PACF (should cut off after lag 2)", "content": "pacf_values = pacf(X, nlags=10)\nprint(\"PACF:\", np.round(pacf_values, 3))", "line_start": 140, "level": 1}, {"heading": "Fit AR model using AIC to select order", "content": "from statsmodels.tsa.ar_model import ar_select_order\nsel = ar_select_order(X, maxlag=10, ic='aic')\nprint(f\"Selected order: {sel.ar_lags}\")", "line_start": 144, "level": 1}, {"heading": "Fit AR(2) and check estimates", "content": "model = AutoReg(X, lags=2).fit()\nprint(f\"True: phi1={phi1}, phi2={phi2}\")\nprint(f\"Estimated: phi1={model.params[1]:.3f}, phi2={model.params[2]:.3f}\")", "line_start": 149, "level": 1}, {"heading": "Forecast", "content": "forecast = model.forecast(steps=5)\nprint(\"5-step forecast:\", forecast)\n```", "line_start": 154, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Explain intuitively why the stationarity condition for AR(1) is $|\\phi| < 1$. What happens when $\\phi = 1$ or $\\phi > 1$?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> When $|\\phi| < 1$, shocks decay over time, keeping variance bounded. When $\\phi = 1$, we have a random walk where shocks persist forever (unit root). When $|\\phi| > 1$, the process explodes exponentially.\n\n<strong>Explanation:</strong>\nThe AR(1) can be written as:\n$$X_t = \\phi^t X_0 + \\sum_{j=0}^{t-1} \\phi^j \\epsilon_{t-j}$$\n\n- If $|\\phi| < 1$: $\\phi^t \\to 0$ and the MA representation converges (bounded variance)\n- If $\\phi = 1$: $X_t = X_0 + \\sum \\epsilon_j$ (random walk, variance $\\to \\infty$)\n- If $|\\phi| > 1$: $\\phi^t \\to \\infty$ (explosive)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Thinking $\\phi = 0.99$ is \"close enough\" to stationary. While technically stationary, near-unit-root processes behave like random walks in finite samples. Predictions degrade quickly.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Why does the PACF of an AR(p) process cut off after lag p while the ACF decays gradually?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> PACF measures direct correlation after controlling for intermediate lags. AR(p) by definition only has direct dependence on $p$ past values, so PACF is zero beyond lag $p$. ACF includes indirect effects through intermediate values, causing gradual decay.\n\n<strong>Explanation:</strong>\nFor AR(2): $X_t = \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + \\epsilon_t$\n- Direct effects: only from $X_{t-1}$ and $X_{t-2}$\n- PACF at lag 3: After controlling for $X_{t-1}, X_{t-2}$, $X_{t-3}$ has no additional predictive power\n- But ACF at lag 3: $X_t$ correlates with $X_{t-3}$ through the chain $X_{t-1} \\to X_{t-2} \\to X_{t-3}$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Expecting perfectly zero PACF beyond lag $p$ in samples. Due to estimation error, you'll see small nonzero values. Use confidence bands to judge significance.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Derive the variance of a stationary AR(1) process: $\\text{Var}(X_t) = \\frac{\\sigma^2}{1-\\phi^2}$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Starting from $X_t = \\phi X_{t-1} + \\epsilon_t$, take variance of both sides and use stationarity.\n\n<strong>Derivation:</strong>\n$$\\text{Var}(X_t) = \\text{Var}(\\phi X_{t-1} + \\epsilon_t)$$\n$$= \\phi^2 \\text{Var}(X_{t-1}) + \\text{Var}(\\epsilon_t) + 2\\phi\\text{Cov}(X_{t-1}, \\epsilon_t)$$\n\nSince $\\epsilon_t$ is independent of $X_{t-1}$:\n$$\\gamma(0) = \\phi^2 \\gamma(0) + \\sigma^2$$\n\nBy stationarity, $\\text{Var}(X_t) = \\text{Var}(X_{t-1}) = \\gamma(0)$:\n$$\\gamma(0) - \\phi^2\\gamma(0) = \\sigma^2$$\n$$\\gamma(0)(1 - \\phi^2) = \\sigma^2$$\n$$\\gamma(0) = \\frac{\\sigma^2}{1-\\phi^2}$$\n\n**Note:** Requires $|\\phi| < 1$ for positive variance.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Forgetting that variance increases as $|\\phi| \\to 1$. Near-unit-root processes have large variance, making them look more volatile than white noise with the same $\\sigma^2$.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> For AR(2), show that the stationarity region in the $(\\phi_1, \\phi_2)$ plane is triangular with vertices at $(2, -1)$, $(-2, -1)$, and $(0, 1)$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The three stationarity conditions $\\phi_1 + \\phi_2 < 1$, $\\phi_2 - \\phi_1 < 1$, and $|\\phi_2| < 1$ define a triangular region.\n\n<strong>Derivation:</strong>\nThe characteristic equation $1 - \\phi_1 z - \\phi_2 z^2 = 0$ must have roots outside unit circle.\n\nSetting $z = 1$: $1 - \\phi_1 - \\phi_2 > 0 \\Rightarrow \\phi_1 + \\phi_2 < 1$\nSetting $z = -1$: $1 + \\phi_1 - \\phi_2 > 0 \\Rightarrow \\phi_2 - \\phi_1 < 1$\n\nFor complex roots, discriminant analysis gives: $|\\phi_2| < 1$\n\nBoundary lines:\n- $\\phi_1 + \\phi_2 = 1$ (passes through $(2, -1)$ and $(0, 1)$)\n- $\\phi_2 - \\phi_1 = 1$ (passes through $(-2, -1)$ and $(0, 1)$)\n- $\\phi_2 = -1$ (connects $(2, -1)$ and $(-2, -1)$)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Checking only one condition. All three must hold simultaneously. A model can satisfy two conditions but fail the third and still be non-stationary.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You fit an AR(3) model and get estimates $\\hat{\\phi}_1 = 0.5$, $\\hat{\\phi}_2 = 0.3$, $\\hat{\\phi}_3 = 0.25$. The residual ACF shows a significant spike at lag 1. What might be wrong?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> A significant residual autocorrelation at lag 1 indicates model misspecification. Possible causes:\n1. MA component is needed (ARMA instead of pure AR)\n2. Structural break in the data\n3. Outliers affecting estimation\n4. Non-stationarity not fully addressed\n\n<strong>Diagnostic steps:</strong>\n1. Perform Ljung-Box test on residuals\n2. Try fitting ARMA(3,1) or ARMA(3,2)\n3. Plot residuals over time to check for patterns\n4. Check for outliers or level shifts\n5. Verify original series was stationary\n\n**Key insight:** Pure AR residuals should be white noise. Significant residual autocorrelation means the model hasn't captured all temporal dependence.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Adding more AR lags to fix residual correlation. Sometimes an MA term is more parsimonious. Compare AIC between AR(4), AR(5) and ARMA(3,1).\n</div>\n</div>\n</details>", "line_start": 159, "level": 2}, {"heading": "References", "content": "1. Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press. Chapter 3.\n2. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis*. Wiley. Chapter 3.\n3. Brockwell, P. J., & Davis, R. A. (2016). *Introduction to Time Series and Forecasting*. Springer. Chapter 3.\n4. Lütkepohl, H. (2005). *New Introduction to Multiple Time Series Analysis*. Springer. Chapter 2.", "line_start": 275, "level": 1}]}, "docs/en/time-domain/arima.md": {"path": "docs/en/time-domain/arima.md", "title": "ARIMA Models", "content": "# ARIMA Models\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> ARIMA(p,d,q) extends ARMA to non-stationary series by including differencing. The \"I\" stands for \"integrated\" — meaning the series needs d differences to become stationary. After differencing d times, fit ARMA(p,q) to the differenced series. Most commonly d=1 (first difference) or d=2 (second difference).\n</div>\n\n## Core Definitions\n\n**ARIMA(p,d,q) Model**:\n\nApply ARMA(p,q) to the d-th difference of $X_t$:\n$$\\Phi(L)(1-L)^d X_t = c + \\Theta(L)\\epsilon_t$$\n\n**Components:**\n- $p$: AR order (autoregressive lags)\n- $d$: Degree of differencing (integration order)\n- $q$: MA order (moving average lags)\n\n**Difference Operator**:\n$$\\nabla X_t = (1-L)X_t = X_t - X_{t-1}$$\n$$\\nabla^2 X_t = X_t - 2X_{t-1} + X_{t-2}$$\n\n**Integrated Process**: A process is integrated of order d, written $I(d)$, if it requires d differences to become stationary.\n\n## Math and Derivations\n\n### ARIMA(0,1,0): Random Walk\n\n$$X_t = X_{t-1} + \\epsilon_t$$\n\nOr equivalently: $(1-L)X_t = \\epsilon_t$\n\nThe first difference $\\nabla X_t = \\epsilon_t$ is white noise (stationary).\n\n### ARIMA(0,1,0) with Drift\n\n$$X_t = c + X_{t-1} + \\epsilon_t$$\n\nThe drift $c$ creates a linear trend in levels:\n$$E[X_t] = X_0 + ct$$\n\n### ARIMA(1,1,0): Differenced AR(1)\n\n$$\\nabla X_t = \\phi \\nabla X_{t-1} + \\epsilon_t$$\n\nExpanding:\n$$(X_t - X_{t-1}) = \\phi(X_{t-1} - X_{t-2}) + \\epsilon_t$$\n$$X_t = (1+\\phi)X_{t-1} - \\phi X_{t-2} + \\epsilon_t$$\n\nThis is AR(2) in levels with a unit root.\n\n### ARIMA(0,1,1): IMA(1,1)\n\n$$\\nabla X_t = \\epsilon_t + \\theta\\epsilon_{t-1}$$\n\nAlso known as an **exponentially weighted moving average (EWMA)** process. Forms the basis for simple exponential smoothing.\n\n### General ARIMA(p,d,q)\n\nIn lag operator notation:\n$$\\Phi(L)(1-L)^d X_t = c + \\Theta(L)\\epsilon_t$$\n\nWhere:\n- $\\Phi(L) = 1 - \\phi_1 L - \\cdots - \\phi_p L^p$ has roots outside unit circle (stationary AR)\n- $\\Theta(L) = 1 + \\theta_1 L + \\cdots + \\theta_q L^q$ has roots outside unit circle (invertible MA)\n- $(1-L)^d$ contributes $d$ unit roots\n\n### Forecasting with ARIMA\n\nFor ARIMA(p,1,q), the h-step forecast:\n$$\\hat{X}_{T+h|T} = E[X_{T+h} | X_T, X_{T-1}, \\ldots]$$\n\nKey property: forecasts revert to a linear trend (if drift) or constant growth for $d \\geq 1$.\n\n**Prediction intervals** widen with horizon due to accumulated uncertainty.\n\n## Algorithm/Model Sketch\n\n**Box-Jenkins Methodology:**\n\n```\n1. IDENTIFICATION\n   - Plot series; check for trend/non-stationarity\n   - Apply ADF/KPSS tests\n   - Difference until stationary (determine d)\n   - Examine ACF/PACF of differenced series\n   - Identify candidate (p, q) orders\n\n2. ESTIMATION\n   - Fit candidate ARIMA models\n   - Use MLE (or CSS for initial values)\n   - Check parameter significance\n\n3. DIAGNOSTICS\n   - Examine residuals: ACF should show no pattern\n   - Ljung-Box test for residual autocorrelation\n   - Check residual normality (Q-Q plot)\n   - Look for outliers\n\n4. FORECASTING\n   - Generate point forecasts\n   - Compute prediction intervals\n   - Validate on holdout data if possible\n```\n\n**Determining d:**\n\n| Symptom | Likely d |\n|---------|----------|\n| Series wanders, slow ACF decay | d = 1 |\n| Trend in differenced series | d = 2 |\n| Seasonal pattern persists | Need seasonal differencing |\n| Already fluctuates around mean | d = 0 |\n\n## Common Pitfalls\n\n1. **Over-differencing**: If the original series is stationary, differencing introduces MA(1) with $\\theta = -1$. Check: if ACF of differenced series has large negative spike at lag 1, you may have over-differenced.\n\n2. **Under-differencing**: ACF that doesn't decay or stays significant at high lags suggests more differencing needed. Also check KPSS test.\n\n3. **Ignoring drift**: ARIMA(0,1,0) without constant is pure random walk. With drift, there's a trend. Misspecifying this affects long-term forecasts.\n\n4. **d > 2 rarely needed**: If you need d > 2, reconsider—series might have other issues (outliers, structural breaks, wrong transformation).\n\n5. **Confusing trend types**: Deterministic trend (detrend with regression) vs. stochastic trend (difference). Using wrong approach gives poor results.\n\n6. **Negative forecasts**: For positive series (prices, counts), ARIMA may forecast negatives. Consider log transform or constrained models.\n\n## Mini Example\n\n```python\nimport numpy as np\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.stattools import adfuller\n\n# Generate ARIMA(1,1,1) process\nnp.random.seed(42)\nn = 300\nphi, theta = 0.5, 0.3\neps = np.random.randn(n + 2)\n\n# First generate the differenced series as ARMA(1,1)\ndX = np.zeros(n)\ndX[0] = eps[1] + theta * eps[0]\nfor t in range(1, n):\n    dX[t] = phi * dX[t-1] + eps[t+1] + theta * eps[t]\n\n# Integrate to get X\nX = np.cumsum(dX)\n\n# Test stationarity\nadf_X = adfuller(X)\nadf_dX = adfuller(np.diff(X))\nprint(f\"ADF p-value (levels): {adf_X[1]:.4f}\")  # Should be high (non-stationary)\nprint(f\"ADF p-value (differenced): {adf_dX[1]:.4f}\")  # Should be low (stationary)\n\n# Fit ARIMA(1,1,1)\nmodel = ARIMA(X, order=(1, 1, 1)).fit()\nprint(f\"\\nTrue: phi={phi}, theta={theta}\")\nprint(f\"Estimated: phi={model.arparams[0]:.3f}, theta={model.maparams[0]:.3f}\")\n\n# Forecast\nforecast = model.forecast(steps=10)\nconf_int = model.get_forecast(10).conf_int()\nprint(f\"\\n10-step forecast: {forecast[-1]:.2f}\")\nprint(f\"95% CI: [{conf_int.iloc[-1, 0]:.2f}, {conf_int.iloc[-1, 1]:.2f}]\")\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What does the \"I\" in ARIMA stand for, and what does it mean for a process to be \"integrated of order d\"?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> \"I\" stands for \"Integrated.\" A process is integrated of order d, denoted I(d), if it requires exactly d differences to become stationary. Integration is the inverse of differencing — if you sum (integrate) a stationary series, you get an I(1) process.\n\n<strong>Explanation:</strong>\n- I(0): Stationary (no differencing needed)\n- I(1): First difference is stationary (e.g., random walk)\n- I(2): Second difference is stationary (e.g., random walk with drift in levels)\n\n**Key insight:** \"Integrated\" comes from continuous-time analogy. In discrete time: $X_t = \\sum_{s=1}^t \\epsilon_s$ (integrated/summed white noise) is I(1).\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Confusing integration order with polynomial degree. I(1) is not about linear trends—it's about the type of non-stationarity (stochastic vs. deterministic).\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> How can you tell if a series has been over-differenced?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Signs of over-differencing:\n1. ACF of differenced series shows large negative spike at lag 1 (often near -0.5)\n2. Variance increases after differencing (should decrease or stay similar)\n3. The differenced series looks \"over-corrected\" with excessive alternation\n\n<strong>Explanation:</strong>\nDifferencing a stationary series adds MA(1) structure with $\\theta \\approx -1$:\n$$(1-L)X_t = X_t - X_{t-1}$$\n\nIf $X_t$ was already stationary, the difference behaves like $\\epsilon_t - \\epsilon_{t-1}$, which is MA(1) with $\\theta = -1$ and $\\rho(1) = -0.5$.\n\n**Test:** If $d=1$ differencing gives ACF(1) ≈ -0.5 and all other ACF ≈ 0, try $d=0$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Automatically differencing because \"everyone does it.\" Always test stationarity first. Many series (especially returns) are already stationary.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Show that ARIMA(0,1,1) with parameter $\\theta$ produces forecasts equivalent to exponential smoothing with $\\alpha = 1/(1+\\theta)$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> ARIMA(0,1,1): $(1-L)X_t = (1+\\theta L)\\epsilon_t$\n\nThe optimal forecast can be written recursively as:\n$$\\hat{X}_{t+1|t} = \\hat{X}_{t|t-1} + (1+\\theta)^{-1}(X_t - \\hat{X}_{t|t-1})$$\n\nThis is exactly exponential smoothing: $\\hat{X}_{t+1} = \\alpha X_t + (1-\\alpha)\\hat{X}_t$ with $\\alpha = \\frac{1}{1+\\theta}$.\n\n<strong>Derivation:</strong>\nFrom ARIMA(0,1,1): $X_t = X_{t-1} + \\epsilon_t + \\theta\\epsilon_{t-1}$\n\nThe forecast error is:\n$$e_t = X_t - \\hat{X}_{t|t-1} = \\epsilon_t$$\n\nThe forecast update:\n$$\\hat{X}_{t+1|t} = X_t + \\theta\\hat{\\epsilon}_t = X_t + \\theta e_t$$\n\nRearranging:\n$$\\hat{X}_{t+1|t} = X_t + \\theta(X_t - \\hat{X}_{t|t-1})/(1+\\theta) \\cdot (1+\\theta)$$\n\nWith $\\alpha = 1/(1+\\theta)$: this gives the exponential smoothing recursion.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> For invertibility, need $|\\theta| < 1$, which means $\\alpha \\in (0.5, 1)$ for IMA(1,1). Values $\\alpha < 0.5$ correspond to non-invertible MA.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Why do prediction intervals for ARIMA models widen as the forecast horizon increases?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Future shocks $\\epsilon_{T+1}, \\epsilon_{T+2}, \\ldots$ are unknown, and their accumulated effect grows with horizon. For I(d) processes, shocks have permanent effects, causing variance to grow without bound.\n\n<strong>Derivation for random walk (ARIMA(0,1,0)):</strong>\n$$X_{T+h} = X_T + \\sum_{j=1}^{h}\\epsilon_{T+j}$$\n\nForecast: $\\hat{X}_{T+h|T} = X_T$\n\nError: $X_{T+h} - \\hat{X}_{T+h|T} = \\sum_{j=1}^{h}\\epsilon_{T+j}$\n\nVariance: $\\text{Var}(X_{T+h} - \\hat{X}_{T+h|T}) = h\\sigma^2$\n\n**95% PI:** $X_T \\pm 1.96\\sigma\\sqrt{h}$\n\nThe interval width grows like $\\sqrt{h}$, becoming arbitrarily wide.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Expecting narrow long-horizon intervals. ARIMA cannot provide tight long-range forecasts—uncertainty is fundamental. This is why judgment and scenarios matter for long-term planning.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You have monthly sales data showing a clear upward trend. After first differencing, the ACF shows significant spikes at lags 1, 12, and 13. What model structure might be appropriate?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The pattern suggests SARIMA with:\n- d=1 (first differencing handles trend)\n- Significant lag 1 suggests AR(1) or MA(1)\n- Significant lag 12 suggests seasonal component (monthly data, annual pattern)\n- Lag 13 = 12+1 is interaction of seasonal and non-seasonal\n\n**Candidate models:**\n- SARIMA(1,1,0)(1,0,0)[12]\n- SARIMA(0,1,1)(0,1,1)[12] (airline model)\n- SARIMA(1,1,1)(1,1,0)[12]\n\n**Next steps:**\n1. Apply seasonal differencing and re-check ACF\n2. Fit candidates and compare AIC\n3. Check residuals for remaining patterns\n4. Validate on holdout data\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Ignoring the seasonal spike and fitting non-seasonal ARIMA. The lag-12 autocorrelation will persist in residuals, degrading forecasts.\n</div>\n</div>\n</details>\n\n## References\n\n1. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis: Forecasting and Control*. Wiley. Chapters 4-6.\n2. Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press. Chapters 15, 17.\n3. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 9.\n4. Brockwell, P. J., & Davis, R. A. (2016). *Introduction to Time Series and Forecasting*. Springer. Chapter 5.\n", "sections": [{"heading": "ARIMA Models", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> ARIMA(p,d,q) extends ARMA to non-stationary series by including differencing. The \"I\" stands for \"integrated\" — meaning the series needs d differences to become stationary. After differencing d times, fit ARMA(p,q) to the differenced series. Most commonly d=1 (first difference) or d=2 (second difference).\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**ARIMA(p,d,q) Model**:\n\nApply ARMA(p,q) to the d-th difference of $X_t$:\n$$\\Phi(L)(1-L)^d X_t = c + \\Theta(L)\\epsilon_t$$\n\n**Components:**\n- $p$: AR order (autoregressive lags)\n- $d$: Degree of differencing (integration order)\n- $q$: MA order (moving average lags)\n\n**Difference Operator**:\n$$\\nabla X_t = (1-L)X_t = X_t - X_{t-1}$$\n$$\\nabla^2 X_t = X_t - 2X_{t-1} + X_{t-2}$$\n\n**Integrated Process**: A process is integrated of order d, written $I(d)$, if it requires d differences to become stationary.", "line_start": 7, "level": 2}, {"heading": "ARIMA(0,1,0): Random Walk", "content": "$$X_t = X_{t-1} + \\epsilon_t$$\n\nOr equivalently: $(1-L)X_t = \\epsilon_t$\n\nThe first difference $\\nabla X_t = \\epsilon_t$ is white noise (stationary).", "line_start": 27, "level": 3}, {"heading": "ARIMA(0,1,0) with Drift", "content": "$$X_t = c + X_{t-1} + \\epsilon_t$$\n\nThe drift $c$ creates a linear trend in levels:\n$$E[X_t] = X_0 + ct$$", "line_start": 35, "level": 3}, {"heading": "ARIMA(1,1,0): Differenced AR(1)", "content": "$$\\nabla X_t = \\phi \\nabla X_{t-1} + \\epsilon_t$$\n\nExpanding:\n$$(X_t - X_{t-1}) = \\phi(X_{t-1} - X_{t-2}) + \\epsilon_t$$\n$$X_t = (1+\\phi)X_{t-1} - \\phi X_{t-2} + \\epsilon_t$$\n\nThis is AR(2) in levels with a unit root.", "line_start": 42, "level": 3}, {"heading": "ARIMA(0,1,1): IMA(1,1)", "content": "$$\\nabla X_t = \\epsilon_t + \\theta\\epsilon_{t-1}$$\n\nAlso known as an **exponentially weighted moving average (EWMA)** process. Forms the basis for simple exponential smoothing.", "line_start": 52, "level": 3}, {"heading": "General ARIMA(p,d,q)", "content": "In lag operator notation:\n$$\\Phi(L)(1-L)^d X_t = c + \\Theta(L)\\epsilon_t$$\n\nWhere:\n- $\\Phi(L) = 1 - \\phi_1 L - \\cdots - \\phi_p L^p$ has roots outside unit circle (stationary AR)\n- $\\Theta(L) = 1 + \\theta_1 L + \\cdots + \\theta_q L^q$ has roots outside unit circle (invertible MA)\n- $(1-L)^d$ contributes $d$ unit roots", "line_start": 58, "level": 3}, {"heading": "Forecasting with ARIMA", "content": "For ARIMA(p,1,q), the h-step forecast:\n$$\\hat{X}_{T+h|T} = E[X_{T+h} | X_T, X_{T-1}, \\ldots]$$\n\nKey property: forecasts revert to a linear trend (if drift) or constant growth for $d \\geq 1$.\n\n**Prediction intervals** widen with horizon due to accumulated uncertainty.", "line_start": 68, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**Box-Jenkins Methodology:**\n\n```\n1. IDENTIFICATION\n   - Plot series; check for trend/non-stationarity\n   - Apply ADF/KPSS tests\n   - Difference until stationary (determine d)\n   - Examine ACF/PACF of differenced series\n   - Identify candidate (p, q) orders\n\n2. ESTIMATION\n   - Fit candidate ARIMA models\n   - Use MLE (or CSS for initial values)\n   - Check parameter significance\n\n3. DIAGNOSTICS\n   - Examine residuals: ACF should show no pattern\n   - Ljung-Box test for residual autocorrelation\n   - Check residual normality (Q-Q plot)\n   - Look for outliers\n\n4. FORECASTING\n   - Generate point forecasts\n   - Compute prediction intervals\n   - Validate on holdout data if possible\n```\n\n**Determining d:**\n\n| Symptom | Likely d |\n|---------|----------|\n| Series wanders, slow ACF decay | d = 1 |\n| Trend in differenced series | d = 2 |\n| Seasonal pattern persists | Need seasonal differencing |\n| Already fluctuates around mean | d = 0 |", "line_start": 77, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Over-differencing**: If the original series is stationary, differencing introduces MA(1) with $\\theta = -1$. Check: if ACF of differenced series has large negative spike at lag 1, you may have over-differenced.\n\n2. **Under-differencing**: ACF that doesn't decay or stays significant at high lags suggests more differencing needed. Also check KPSS test.\n\n3. **Ignoring drift**: ARIMA(0,1,0) without constant is pure random walk. With drift, there's a trend. Misspecifying this affects long-term forecasts.\n\n4. **d > 2 rarely needed**: If you need d > 2, reconsider—series might have other issues (outliers, structural breaks, wrong transformation).\n\n5. **Confusing trend types**: Deterministic trend (detrend with regression) vs. stochastic trend (difference). Using wrong approach gives poor results.\n\n6. **Negative forecasts**: For positive series (prices, counts), ARIMA may forecast negatives. Consider log transform or constrained models.", "line_start": 115, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.stattools import adfuller", "line_start": 129, "level": 1}, {"heading": "Generate ARIMA(1,1,1) process", "content": "np.random.seed(42)\nn = 300\nphi, theta = 0.5, 0.3\neps = np.random.randn(n + 2)", "line_start": 136, "level": 1}, {"heading": "First generate the differenced series as ARMA(1,1)", "content": "dX = np.zeros(n)\ndX[0] = eps[1] + theta * eps[0]\nfor t in range(1, n):\n    dX[t] = phi * dX[t-1] + eps[t+1] + theta * eps[t]", "line_start": 142, "level": 1}, {"heading": "Integrate to get X", "content": "X = np.cumsum(dX)", "line_start": 148, "level": 1}, {"heading": "Test stationarity", "content": "adf_X = adfuller(X)\nadf_dX = adfuller(np.diff(X))\nprint(f\"ADF p-value (levels): {adf_X[1]:.4f}\")  # Should be high (non-stationary)\nprint(f\"ADF p-value (differenced): {adf_dX[1]:.4f}\")  # Should be low (stationary)", "line_start": 151, "level": 1}, {"heading": "Fit ARIMA(1,1,1)", "content": "model = ARIMA(X, order=(1, 1, 1)).fit()\nprint(f\"\\nTrue: phi={phi}, theta={theta}\")\nprint(f\"Estimated: phi={model.arparams[0]:.3f}, theta={model.maparams[0]:.3f}\")", "line_start": 157, "level": 1}, {"heading": "Forecast", "content": "forecast = model.forecast(steps=10)\nconf_int = model.get_forecast(10).conf_int()\nprint(f\"\\n10-step forecast: {forecast[-1]:.2f}\")\nprint(f\"95% CI: [{conf_int.iloc[-1, 0]:.2f}, {conf_int.iloc[-1, 1]:.2f}]\")\n```", "line_start": 162, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What does the \"I\" in ARIMA stand for, and what does it mean for a process to be \"integrated of order d\"?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> \"I\" stands for \"Integrated.\" A process is integrated of order d, denoted I(d), if it requires exactly d differences to become stationary. Integration is the inverse of differencing — if you sum (integrate) a stationary series, you get an I(1) process.\n\n<strong>Explanation:</strong>\n- I(0): Stationary (no differencing needed)\n- I(1): First difference is stationary (e.g., random walk)\n- I(2): Second difference is stationary (e.g., random walk with drift in levels)\n\n**Key insight:** \"Integrated\" comes from continuous-time analogy. In discrete time: $X_t = \\sum_{s=1}^t \\epsilon_s$ (integrated/summed white noise) is I(1).\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Confusing integration order with polynomial degree. I(1) is not about linear trends—it's about the type of non-stationarity (stochastic vs. deterministic).\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> How can you tell if a series has been over-differenced?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Signs of over-differencing:\n1. ACF of differenced series shows large negative spike at lag 1 (often near -0.5)\n2. Variance increases after differencing (should decrease or stay similar)\n3. The differenced series looks \"over-corrected\" with excessive alternation\n\n<strong>Explanation:</strong>\nDifferencing a stationary series adds MA(1) structure with $\\theta \\approx -1$:\n$$(1-L)X_t = X_t - X_{t-1}$$\n\nIf $X_t$ was already stationary, the difference behaves like $\\epsilon_t - \\epsilon_{t-1}$, which is MA(1) with $\\theta = -1$ and $\\rho(1) = -0.5$.\n\n**Test:** If $d=1$ differencing gives ACF(1) ≈ -0.5 and all other ACF ≈ 0, try $d=0$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Automatically differencing because \"everyone does it.\" Always test stationarity first. Many series (especially returns) are already stationary.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Show that ARIMA(0,1,1) with parameter $\\theta$ produces forecasts equivalent to exponential smoothing with $\\alpha = 1/(1+\\theta)$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> ARIMA(0,1,1): $(1-L)X_t = (1+\\theta L)\\epsilon_t$\n\nThe optimal forecast can be written recursively as:\n$$\\hat{X}_{t+1|t} = \\hat{X}_{t|t-1} + (1+\\theta)^{-1}(X_t - \\hat{X}_{t|t-1})$$\n\nThis is exactly exponential smoothing: $\\hat{X}_{t+1} = \\alpha X_t + (1-\\alpha)\\hat{X}_t$ with $\\alpha = \\frac{1}{1+\\theta}$.\n\n<strong>Derivation:</strong>\nFrom ARIMA(0,1,1): $X_t = X_{t-1} + \\epsilon_t + \\theta\\epsilon_{t-1}$\n\nThe forecast error is:\n$$e_t = X_t - \\hat{X}_{t|t-1} = \\epsilon_t$$\n\nThe forecast update:\n$$\\hat{X}_{t+1|t} = X_t + \\theta\\hat{\\epsilon}_t = X_t + \\theta e_t$$\n\nRearranging:\n$$\\hat{X}_{t+1|t} = X_t + \\theta(X_t - \\hat{X}_{t|t-1})/(1+\\theta) \\cdot (1+\\theta)$$\n\nWith $\\alpha = 1/(1+\\theta)$: this gives the exponential smoothing recursion.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> For invertibility, need $|\\theta| < 1$, which means $\\alpha \\in (0.5, 1)$ for IMA(1,1). Values $\\alpha < 0.5$ correspond to non-invertible MA.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Why do prediction intervals for ARIMA models widen as the forecast horizon increases?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Future shocks $\\epsilon_{T+1}, \\epsilon_{T+2}, \\ldots$ are unknown, and their accumulated effect grows with horizon. For I(d) processes, shocks have permanent effects, causing variance to grow without bound.\n\n<strong>Derivation for random walk (ARIMA(0,1,0)):</strong>\n$$X_{T+h} = X_T + \\sum_{j=1}^{h}\\epsilon_{T+j}$$\n\nForecast: $\\hat{X}_{T+h|T} = X_T$\n\nError: $X_{T+h} - \\hat{X}_{T+h|T} = \\sum_{j=1}^{h}\\epsilon_{T+j}$\n\nVariance: $\\text{Var}(X_{T+h} - \\hat{X}_{T+h|T}) = h\\sigma^2$\n\n**95% PI:** $X_T \\pm 1.96\\sigma\\sqrt{h}$\n\nThe interval width grows like $\\sqrt{h}$, becoming arbitrarily wide.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Expecting narrow long-horizon intervals. ARIMA cannot provide tight long-range forecasts—uncertainty is fundamental. This is why judgment and scenarios matter for long-term planning.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You have monthly sales data showing a clear upward trend. After first differencing, the ACF shows significant spikes at lags 1, 12, and 13. What model structure might be appropriate?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The pattern suggests SARIMA with:\n- d=1 (first differencing handles trend)\n- Significant lag 1 suggests AR(1) or MA(1)\n- Significant lag 12 suggests seasonal component (monthly data, annual pattern)\n- Lag 13 = 12+1 is interaction of seasonal and non-seasonal\n\n**Candidate models:**\n- SARIMA(1,1,0)(1,0,0)[12]\n- SARIMA(0,1,1)(0,1,1)[12] (airline model)\n- SARIMA(1,1,1)(1,1,0)[12]\n\n**Next steps:**\n1. Apply seasonal differencing and re-check ACF\n2. Fit candidates and compare AIC\n3. Check residuals for remaining patterns\n4. Validate on holdout data\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Ignoring the seasonal spike and fitting non-seasonal ARIMA. The lag-12 autocorrelation will persist in residuals, degrading forecasts.\n</div>\n</div>\n</details>", "line_start": 169, "level": 2}, {"heading": "References", "content": "1. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis: Forecasting and Control*. Wiley. Chapters 4-6.\n2. Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press. Chapters 15, 17.\n3. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 9.\n4. Brockwell, P. J., & Davis, R. A. (2016). *Introduction to Time Series and Forecasting*. Springer. Chapter 5.", "line_start": 296, "level": 1}]}, "docs/en/time-domain/identification.md": {"path": "docs/en/time-domain/identification.md", "title": "Model Identification", "content": "# Model Identification\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Model identification determines (p,d,q) orders using ACF/PACF patterns, unit root tests, and information criteria. AR(p): PACF cuts off at p. MA(q): ACF cuts off at q. ARMA: both tail off. Use ADF/KPSS for d. For complex cases, use AIC/BIC to compare candidates. Always validate with residual diagnostics.\n</div>\n\n## Core Definitions\n\n**Model Identification**: The process of determining:\n1. Transformation needed (log, Box-Cox)\n2. Order of differencing (d, D)\n3. AR order (p, P)\n4. MA order (q, Q)\n\n**ACF/PACF Patterns for Pure Models:**\n\n| Model | ACF | PACF |\n|-------|-----|------|\n| AR(p) | Tails off (exponential/sinusoidal decay) | Cuts off after lag p |\n| MA(q) | Cuts off after lag q | Tails off (exponential/sinusoidal decay) |\n| ARMA(p,q) | Tails off | Tails off |\n| Non-stationary | Very slow decay | Large spike at lag 1 |\n| White noise | All near zero | All near zero |\n\n**Significance Threshold**: Under white noise, $\\hat{\\rho}(h) \\sim N(0, 1/n)$, so use $\\pm 1.96/\\sqrt{n}$ for 95% bands.\n\n## Math and Derivations\n\n### Information Criteria\n\n**AIC (Akaike Information Criterion)**:\n$$\\text{AIC} = -2\\ln(\\hat{L}) + 2k$$\n\nwhere $\\hat{L}$ is maximum likelihood and $k$ is number of parameters.\n\nFor ARIMA with Gaussian errors:\n$$\\text{AIC} = n\\ln(\\hat{\\sigma}^2) + 2(p+q+1)$$\n\n**BIC (Bayesian/Schwarz Information Criterion)**:\n$$\\text{BIC} = -2\\ln(\\hat{L}) + k\\ln(n)$$\n\nBIC penalizes complexity more heavily than AIC.\n\n**AICc (Corrected AIC)**:\n$$\\text{AICc} = \\text{AIC} + \\frac{2k(k+1)}{n-k-1}$$\n\nPreferred for small samples.\n\n### Extended ACF (EACF)\n\nThe EACF method iteratively removes AR structure to identify MA order:\n\n1. Fit AR(0), AR(1), AR(2), ... up to max\n2. For each AR(j), compute ACF of residuals\n3. Create table: rows = AR order, columns = MA order\n4. \"O\" indicates insignificant, \"X\" indicates significant\n5. Top-left corner of \"O\" triangle suggests (p,q)\n\n### Unit Root Testing Strategy\n\n**Combined ADF + KPSS approach:**\n\n| ADF result | KPSS result | Conclusion |\n|------------|-------------|------------|\n| Reject (p < 0.05) | Fail to reject | Stationary (d=0) |\n| Fail to reject | Reject | Non-stationary (d≥1) |\n| Reject | Reject | Possible structural break |\n| Fail to reject | Fail to reject | Inconclusive; try differencing |\n\n### Ljung-Box Test for Residual Autocorrelation\n\n$$Q(m) = n(n+2)\\sum_{k=1}^{m}\\frac{\\hat{\\rho}_k^2}{n-k}$$\n\nUnder null (white noise residuals): $Q(m) \\sim \\chi^2_{m-p-q}$\n\nReject if Q is large (residuals are not white noise).\n\n## Algorithm/Model Sketch\n\n**Complete Identification Procedure:**\n\n```\nSTEP 1: PRELIMINARY ANALYSIS\n- Plot the series\n- Check for obvious trend, seasonality, outliers\n- Apply transformation if variance changes with level (log, sqrt)\n\nSTEP 2: DETERMINE DIFFERENCING ORDER\n- ADF test: if p > 0.05, difference\n- KPSS test: if p < 0.05, difference\n- After differencing, re-test\n- Usually d ≤ 2; rarely d > 2\n\nSTEP 3: EXAMINE ACF/PACF OF STATIONARY SERIES\n- ACF cuts off at lag q → try MA(q)\n- PACF cuts off at lag p → try AR(p)\n- Both tail off → try ARMA\n\nSTEP 4: FIT CANDIDATE MODELS\n- Start simple: AR(1), MA(1), ARMA(1,1)\n- Add complexity as needed\n- Compare AIC/BIC\n\nSTEP 5: DIAGNOSTIC CHECKING\n- Residual ACF/PACF (should be white noise)\n- Ljung-Box test\n- Residual normality (Q-Q plot)\n- Parameter significance\n\nSTEP 6: SELECT FINAL MODEL\n- Lowest AIC/BIC among adequate models\n- Parsimony when AIC/BIC are close\n- Good residual diagnostics\n```\n\n**Auto-Selection Tools:**\n- `auto.arima()` in R (forecast package)\n- `pmdarima.auto_arima()` in Python\n- Uses stepwise search with AIC\n\n## Common Pitfalls\n\n1. **Mechanical ACF/PACF reading**: Patterns aren't always clean. Real data is noisy. Consider multiple interpretations.\n\n2. **Ignoring parsimony**: If ARMA(1,1) and ARMA(2,1) have similar AIC, prefer simpler model.\n\n3. **Over-relying on automated selection**: `auto.arima` is a good start but may miss important features. Always verify manually.\n\n4. **Forgetting seasonal patterns**: Check ACF at seasonal lags (12, 24, ... for monthly). Standard ARIMA won't capture these.\n\n5. **Ignoring residual diagnostics**: A model with lowest AIC can still have autocorrelated residuals. Always check.\n\n6. **Sample size issues**: With small samples (n < 50), ACF/PACF estimates are unreliable. Use simpler models and be conservative.\n\n## Mini Example\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.tsa.stattools import acf, pacf, adfuller, kpss\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\n\n# Generate ARIMA(2,1,1) data for identification exercise\nnp.random.seed(42)\nn = 250\nphi1, phi2, theta = 0.5, -0.2, 0.3\neps = np.random.randn(n + 3)\n\n# Differenced series as ARMA(2,1)\ndX = np.zeros(n)\nfor t in range(2, n):\n    dX[t] = phi1*dX[t-1] + phi2*dX[t-2] + eps[t+1] + theta*eps[t]\n\nX = np.cumsum(dX) + 50  # Integrate and add level\n\n# Step 1: Test stationarity\nprint(\"Step 1: Stationarity Tests\")\nprint(f\"ADF p-value (levels): {adfuller(X)[1]:.4f}\")\nprint(f\"KPSS p-value (levels): {kpss(X, regression='c')[1]:.4f}\")\n\n# Step 2: Difference and re-test\ndX_obs = np.diff(X)\nprint(f\"\\nADF p-value (differenced): {adfuller(dX_obs)[1]:.4f}\")\nprint(f\"KPSS p-value (differenced): {kpss(dX_obs, regression='c')[1]:.4f}\")\n\n# Step 3: ACF/PACF of differenced series\nprint(\"\\nStep 3: ACF/PACF\")\nacf_vals = acf(dX_obs, nlags=10)\npacf_vals = pacf(dX_obs, nlags=10)\nprint(f\"ACF: {np.round(acf_vals[1:6], 3)}\")\nprint(f\"PACF: {np.round(pacf_vals[1:6], 3)}\")\n\n# Step 4: Compare candidate models\nprint(\"\\nStep 4: Model Comparison\")\nmodels = {\n    'ARIMA(1,1,1)': (1,1,1),\n    'ARIMA(2,1,0)': (2,1,0),\n    'ARIMA(2,1,1)': (2,1,1),\n    'ARIMA(1,1,2)': (1,1,2),\n}\n\nfor name, order in models.items():\n    try:\n        model = ARIMA(X, order=order).fit()\n        lb = acorr_ljungbox(model.resid, lags=[10], return_df=True)\n        print(f\"{name}: AIC={model.aic:.1f}, BIC={model.bic:.1f}, LB p-value={lb['lb_pvalue'].values[0]:.3f}\")\n    except:\n        print(f\"{name}: Failed to converge\")\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why do we say ACF \"cuts off\" for MA(q) but \"tails off\" for AR(p)?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n- MA(q): $X_t$ only involves $\\epsilon_t, \\epsilon_{t-1}, \\ldots, \\epsilon_{t-q}$. Beyond lag q, $X_t$ and $X_{t-h}$ share no common $\\epsilon$ terms, so $\\gamma(h) = 0$ exactly.\n- AR(p): $X_t$ depends on all past values through the recursive structure. Even though only p lags appear explicitly, the chain of dependencies means $X_t$ correlates with $X_{t-h}$ for all $h$.\n\n<strong>Key insight:</strong> AR has infinite memory (gradually decaying); MA has finite memory (exactly q lags).\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> In practice, \"cutoff\" doesn't mean exactly zero — sample ACF will have small nonzero values beyond q due to estimation error. Look for sharp drop versus gradual decay.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> What's the difference between AIC and BIC? When would you prefer one over the other?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n- AIC: $-2\\ln(L) + 2k$; penalizes parameters lightly\n- BIC: $-2\\ln(L) + k\\ln(n)$; penalty grows with sample size\n\n**BIC preference situations:**\n- Large sample sizes (BIC penalty more appropriate)\n- When true model is among candidates (BIC is consistent)\n- For inference/explanation (simpler models)\n\n**AIC preference situations:**\n- Forecasting focus (AIC optimizes prediction)\n- Small samples (use AICc)\n- When true model may be complex\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Blindly using one criterion. For forecasting, cross-validation often beats both AIC and BIC. Use information criteria as guides, not final arbiters.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Show that for a white noise process, the sample ACF has approximate variance $1/n$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> For white noise with $\\rho(h) = 0$ for $h \\neq 0$:\n\n<strong>Derivation (Bartlett's approximation):</strong>\n\nFor large n, sample ACF is approximately normal:\n$$\\hat{\\rho}(h) \\sim N(\\rho(h), V_h/n)$$\n\nwhere $V_h = \\sum_{j=-\\infty}^{\\infty} [\\rho(j)\\rho(j+h) + \\rho(j+h)\\rho(j-h) - 2\\rho(h)\\rho(j)\\rho(j+h)]$\n\nFor white noise ($\\rho(j) = 0$ for $j \\neq 0$):\n$$V_h = 1 \\text{ for all } h \\neq 0$$\n\nTherefore: $\\text{Var}(\\hat{\\rho}(h)) \\approx 1/n$\n\n**95% confidence interval:** $\\hat{\\rho}(h) \\pm 1.96/\\sqrt{n}$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> This formula only holds under white noise. For non-white-noise series, the variance is different (Bartlett's formula gives different expressions).\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> The Ljung-Box test statistic is $Q(m) = n(n+2)\\sum_{k=1}^{m}\\frac{\\hat{\\rho}_k^2}{n-k}$. Why is there a $(n-k)$ denominator?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The $(n-k)$ term is a small-sample correction. At lag $k$, only $n-k$ pairs of observations contribute to $\\hat{\\rho}(k)$, making the estimate less precise.\n\n<strong>Explanation:</strong>\nThe original Box-Pierce statistic used $Q' = n\\sum \\hat{\\rho}_k^2$. Ljung and Box modified it because:\n1. $\\text{Var}(\\hat{\\rho}(k)) \\approx (n-k)/n^2$ rather than $1/n$\n2. The correction $(n+2)/(n-k)$ improves the chi-squared approximation in finite samples\n\nWithout correction, the test is undersized (rejects less than it should), missing autocorrelation.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Choosing m too large. If $m > n/4$, the test loses power. Common choices: $m = 10$ for non-seasonal, $m = 2s$ for seasonal data.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You examine ACF/PACF and see: ACF decays slowly over first 3 lags then drops; PACF has spikes at lags 1, 2, 3 then drops. AIC favors ARIMA(3,0,0) but residuals show significant ACF at lag 1. What's your next step?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The significant residual ACF at lag 1 indicates model inadequacy despite good AIC. Try:\n\n1. **Add MA(1)**: Try ARIMA(3,0,1) — MA term may capture the remaining lag-1 correlation\n2. **Check for near-redundancy**: If AR(3) coefficients are close to forming MA factor, simplify\n3. **Consider ARIMA(2,0,1)**: Sometimes mixed model is more parsimonious\n4. **Check for outliers**: Single outliers can cause lag-1 residual correlation\n5. **Re-examine stationarity**: Maybe $d=1$ differencing is needed\n\n**Key principle:** A model isn't adequate until residuals are white noise. AIC is necessary but not sufficient — must pass diagnostic checks.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Accepting a model just because it has lowest AIC. Always verify residual ACF is within bands at all lags, especially early ones.\n</div>\n</div>\n</details>\n\n## References\n\n1. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis*. Wiley. Chapters 6-8.\n2. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 9.\n3. Tsay, R. S. (2010). *Analysis of Financial Time Series*. Wiley. Chapter 2.\n4. Ljung, G. M., & Box, G. E. P. (1978). On a measure of lack of fit in time series models. *Biometrika*, 65(2), 297-303.\n", "sections": [{"heading": "Model Identification", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Model identification determines (p,d,q) orders using ACF/PACF patterns, unit root tests, and information criteria. AR(p): PACF cuts off at p. MA(q): ACF cuts off at q. ARMA: both tail off. Use ADF/KPSS for d. For complex cases, use AIC/BIC to compare candidates. Always validate with residual diagnostics.\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**Model Identification**: The process of determining:\n1. Transformation needed (log, Box-Cox)\n2. Order of differencing (d, D)\n3. AR order (p, P)\n4. MA order (q, Q)\n\n**ACF/PACF Patterns for Pure Models:**\n\n| Model | ACF | PACF |\n|-------|-----|------|\n| AR(p) | Tails off (exponential/sinusoidal decay) | Cuts off after lag p |\n| MA(q) | Cuts off after lag q | Tails off (exponential/sinusoidal decay) |\n| ARMA(p,q) | Tails off | Tails off |\n| Non-stationary | Very slow decay | Large spike at lag 1 |\n| White noise | All near zero | All near zero |\n\n**Significance Threshold**: Under white noise, $\\hat{\\rho}(h) \\sim N(0, 1/n)$, so use $\\pm 1.96/\\sqrt{n}$ for 95% bands.", "line_start": 7, "level": 2}, {"heading": "Information Criteria", "content": "**AIC (Akaike Information Criterion)**:\n$$\\text{AIC} = -2\\ln(\\hat{L}) + 2k$$\n\nwhere $\\hat{L}$ is maximum likelihood and $k$ is number of parameters.\n\nFor ARIMA with Gaussian errors:\n$$\\text{AIC} = n\\ln(\\hat{\\sigma}^2) + 2(p+q+1)$$\n\n**BIC (Bayesian/Schwarz Information Criterion)**:\n$$\\text{BIC} = -2\\ln(\\hat{L}) + k\\ln(n)$$\n\nBIC penalizes complexity more heavily than AIC.\n\n**AICc (Corrected AIC)**:\n$$\\text{AICc} = \\text{AIC} + \\frac{2k(k+1)}{n-k-1}$$\n\nPreferred for small samples.", "line_start": 29, "level": 3}, {"heading": "Extended ACF (EACF)", "content": "The EACF method iteratively removes AR structure to identify MA order:\n\n1. Fit AR(0), AR(1), AR(2), ... up to max\n2. For each AR(j), compute ACF of residuals\n3. Create table: rows = AR order, columns = MA order\n4. \"O\" indicates insignificant, \"X\" indicates significant\n5. Top-left corner of \"O\" triangle suggests (p,q)", "line_start": 49, "level": 3}, {"heading": "Unit Root Testing Strategy", "content": "**Combined ADF + KPSS approach:**\n\n| ADF result | KPSS result | Conclusion |\n|------------|-------------|------------|\n| Reject (p < 0.05) | Fail to reject | Stationary (d=0) |\n| Fail to reject | Reject | Non-stationary (d≥1) |\n| Reject | Reject | Possible structural break |\n| Fail to reject | Fail to reject | Inconclusive; try differencing |", "line_start": 59, "level": 3}, {"heading": "Ljung-Box Test for Residual Autocorrelation", "content": "$$Q(m) = n(n+2)\\sum_{k=1}^{m}\\frac{\\hat{\\rho}_k^2}{n-k}$$\n\nUnder null (white noise residuals): $Q(m) \\sim \\chi^2_{m-p-q}$\n\nReject if Q is large (residuals are not white noise).", "line_start": 70, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**Complete Identification Procedure:**\n\n```\nSTEP 1: PRELIMINARY ANALYSIS\n- Plot the series\n- Check for obvious trend, seasonality, outliers\n- Apply transformation if variance changes with level (log, sqrt)\n\nSTEP 2: DETERMINE DIFFERENCING ORDER\n- ADF test: if p > 0.05, difference\n- KPSS test: if p < 0.05, difference\n- After differencing, re-test\n- Usually d ≤ 2; rarely d > 2\n\nSTEP 3: EXAMINE ACF/PACF OF STATIONARY SERIES\n- ACF cuts off at lag q → try MA(q)\n- PACF cuts off at lag p → try AR(p)\n- Both tail off → try ARMA\n\nSTEP 4: FIT CANDIDATE MODELS\n- Start simple: AR(1), MA(1), ARMA(1,1)\n- Add complexity as needed\n- Compare AIC/BIC\n\nSTEP 5: DIAGNOSTIC CHECKING\n- Residual ACF/PACF (should be white noise)\n- Ljung-Box test\n- Residual normality (Q-Q plot)\n- Parameter significance\n\nSTEP 6: SELECT FINAL MODEL\n- Lowest AIC/BIC among adequate models\n- Parsimony when AIC/BIC are close\n- Good residual diagnostics\n```\n\n**Auto-Selection Tools:**\n- `auto.arima()` in R (forecast package)\n- `pmdarima.auto_arima()` in Python\n- Uses stepwise search with AIC", "line_start": 78, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Mechanical ACF/PACF reading**: Patterns aren't always clean. Real data is noisy. Consider multiple interpretations.\n\n2. **Ignoring parsimony**: If ARMA(1,1) and ARMA(2,1) have similar AIC, prefer simpler model.\n\n3. **Over-relying on automated selection**: `auto.arima` is a good start but may miss important features. Always verify manually.\n\n4. **Forgetting seasonal patterns**: Check ACF at seasonal lags (12, 24, ... for monthly). Standard ARIMA won't capture these.\n\n5. **Ignoring residual diagnostics**: A model with lowest AIC can still have autocorrelated residuals. Always check.\n\n6. **Sample size issues**: With small samples (n < 50), ACF/PACF estimates are unreliable. Use simpler models and be conservative.", "line_start": 121, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.tsa.stattools import acf, pacf, adfuller, kpss\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.stats.diagnostic import acorr_ljungbox", "line_start": 135, "level": 1}, {"heading": "Generate ARIMA(2,1,1) data for identification exercise", "content": "np.random.seed(42)\nn = 250\nphi1, phi2, theta = 0.5, -0.2, 0.3\neps = np.random.randn(n + 3)", "line_start": 144, "level": 1}, {"heading": "Differenced series as ARMA(2,1)", "content": "dX = np.zeros(n)\nfor t in range(2, n):\n    dX[t] = phi1*dX[t-1] + phi2*dX[t-2] + eps[t+1] + theta*eps[t]\n\nX = np.cumsum(dX) + 50  # Integrate and add level", "line_start": 150, "level": 1}, {"heading": "Step 1: Test stationarity", "content": "print(\"Step 1: Stationarity Tests\")\nprint(f\"ADF p-value (levels): {adfuller(X)[1]:.4f}\")\nprint(f\"KPSS p-value (levels): {kpss(X, regression='c')[1]:.4f}\")", "line_start": 157, "level": 1}, {"heading": "Step 2: Difference and re-test", "content": "dX_obs = np.diff(X)\nprint(f\"\\nADF p-value (differenced): {adfuller(dX_obs)[1]:.4f}\")\nprint(f\"KPSS p-value (differenced): {kpss(dX_obs, regression='c')[1]:.4f}\")", "line_start": 162, "level": 1}, {"heading": "Step 3: ACF/PACF of differenced series", "content": "print(\"\\nStep 3: ACF/PACF\")\nacf_vals = acf(dX_obs, nlags=10)\npacf_vals = pacf(dX_obs, nlags=10)\nprint(f\"ACF: {np.round(acf_vals[1:6], 3)}\")\nprint(f\"PACF: {np.round(pacf_vals[1:6], 3)}\")", "line_start": 167, "level": 1}, {"heading": "Step 4: Compare candidate models", "content": "print(\"\\nStep 4: Model Comparison\")\nmodels = {\n    'ARIMA(1,1,1)': (1,1,1),\n    'ARIMA(2,1,0)': (2,1,0),\n    'ARIMA(2,1,1)': (2,1,1),\n    'ARIMA(1,1,2)': (1,1,2),\n}\n\nfor name, order in models.items():\n    try:\n        model = ARIMA(X, order=order).fit()\n        lb = acorr_ljungbox(model.resid, lags=[10], return_df=True)\n        print(f\"{name}: AIC={model.aic:.1f}, BIC={model.bic:.1f}, LB p-value={lb['lb_pvalue'].values[0]:.3f}\")\n    except:\n        print(f\"{name}: Failed to converge\")\n```", "line_start": 174, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why do we say ACF \"cuts off\" for MA(q) but \"tails off\" for AR(p)?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n- MA(q): $X_t$ only involves $\\epsilon_t, \\epsilon_{t-1}, \\ldots, \\epsilon_{t-q}$. Beyond lag q, $X_t$ and $X_{t-h}$ share no common $\\epsilon$ terms, so $\\gamma(h) = 0$ exactly.\n- AR(p): $X_t$ depends on all past values through the recursive structure. Even though only p lags appear explicitly, the chain of dependencies means $X_t$ correlates with $X_{t-h}$ for all $h$.\n\n<strong>Key insight:</strong> AR has infinite memory (gradually decaying); MA has finite memory (exactly q lags).\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> In practice, \"cutoff\" doesn't mean exactly zero — sample ACF will have small nonzero values beyond q due to estimation error. Look for sharp drop versus gradual decay.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> What's the difference between AIC and BIC? When would you prefer one over the other?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n- AIC: $-2\\ln(L) + 2k$; penalizes parameters lightly\n- BIC: $-2\\ln(L) + k\\ln(n)$; penalty grows with sample size\n\n**BIC preference situations:**\n- Large sample sizes (BIC penalty more appropriate)\n- When true model is among candidates (BIC is consistent)\n- For inference/explanation (simpler models)\n\n**AIC preference situations:**\n- Forecasting focus (AIC optimizes prediction)\n- Small samples (use AICc)\n- When true model may be complex\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Blindly using one criterion. For forecasting, cross-validation often beats both AIC and BIC. Use information criteria as guides, not final arbiters.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Show that for a white noise process, the sample ACF has approximate variance $1/n$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> For white noise with $\\rho(h) = 0$ for $h \\neq 0$:\n\n<strong>Derivation (Bartlett's approximation):</strong>\n\nFor large n, sample ACF is approximately normal:\n$$\\hat{\\rho}(h) \\sim N(\\rho(h), V_h/n)$$\n\nwhere $V_h = \\sum_{j=-\\infty}^{\\infty} [\\rho(j)\\rho(j+h) + \\rho(j+h)\\rho(j-h) - 2\\rho(h)\\rho(j)\\rho(j+h)]$\n\nFor white noise ($\\rho(j) = 0$ for $j \\neq 0$):\n$$V_h = 1 \\text{ for all } h \\neq 0$$\n\nTherefore: $\\text{Var}(\\hat{\\rho}(h)) \\approx 1/n$\n\n**95% confidence interval:** $\\hat{\\rho}(h) \\pm 1.96/\\sqrt{n}$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> This formula only holds under white noise. For non-white-noise series, the variance is different (Bartlett's formula gives different expressions).\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> The Ljung-Box test statistic is $Q(m) = n(n+2)\\sum_{k=1}^{m}\\frac{\\hat{\\rho}_k^2}{n-k}$. Why is there a $(n-k)$ denominator?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The $(n-k)$ term is a small-sample correction. At lag $k$, only $n-k$ pairs of observations contribute to $\\hat{\\rho}(k)$, making the estimate less precise.\n\n<strong>Explanation:</strong>\nThe original Box-Pierce statistic used $Q' = n\\sum \\hat{\\rho}_k^2$. Ljung and Box modified it because:\n1. $\\text{Var}(\\hat{\\rho}(k)) \\approx (n-k)/n^2$ rather than $1/n$\n2. The correction $(n+2)/(n-k)$ improves the chi-squared approximation in finite samples\n\nWithout correction, the test is undersized (rejects less than it should), missing autocorrelation.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Choosing m too large. If $m > n/4$, the test loses power. Common choices: $m = 10$ for non-seasonal, $m = 2s$ for seasonal data.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You examine ACF/PACF and see: ACF decays slowly over first 3 lags then drops; PACF has spikes at lags 1, 2, 3 then drops. AIC favors ARIMA(3,0,0) but residuals show significant ACF at lag 1. What's your next step?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The significant residual ACF at lag 1 indicates model inadequacy despite good AIC. Try:\n\n1. **Add MA(1)**: Try ARIMA(3,0,1) — MA term may capture the remaining lag-1 correlation\n2. **Check for near-redundancy**: If AR(3) coefficients are close to forming MA factor, simplify\n3. **Consider ARIMA(2,0,1)**: Sometimes mixed model is more parsimonious\n4. **Check for outliers**: Single outliers can cause lag-1 residual correlation\n5. **Re-examine stationarity**: Maybe $d=1$ differencing is needed\n\n**Key principle:** A model isn't adequate until residuals are white noise. AIC is necessary but not sufficient — must pass diagnostic checks.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Accepting a model just because it has lowest AIC. Always verify residual ACF is within bands at all lags, especially early ones.\n</div>\n</div>\n</details>", "line_start": 192, "level": 2}, {"heading": "References", "content": "1. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis*. Wiley. Chapters 6-8.\n2. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 9.\n3. Tsay, R. S. (2010). *Analysis of Financial Time Series*. Wiley. Chapter 2.\n4. Ljung, G. M., & Box, G. E. P. (1978). On a measure of lack of fit in time series models. *Biometrika*, 65(2), 297-303.", "line_start": 299, "level": 1}]}, "docs/en/time-domain/sarima.md": {"path": "docs/en/time-domain/sarima.md", "title": "Seasonal ARIMA (SARIMA) Models", "content": "# Seasonal ARIMA (SARIMA) Models\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> SARIMA(p,d,q)(P,D,Q)[s] adds seasonal AR and MA terms at lag s (e.g., s=12 for monthly). Seasonal differencing $(1-L^s)$ removes seasonal unit roots. The \"airline model\" SARIMA(0,1,1)(0,1,1)[12] is a benchmark for seasonal data. Identification uses ACF/PACF at both regular and seasonal lags.\n</div>\n\n## Core Definitions\n\n**SARIMA(p,d,q)(P,D,Q)[s] Model**:\n$$\\Phi(L)\\Phi_s(L^s)(1-L)^d(1-L^s)^D X_t = c + \\Theta(L)\\Theta_s(L^s)\\epsilon_t$$\n\n**Components:**\n- $(p, d, q)$: Non-seasonal AR order, differencing, MA order\n- $(P, D, Q)$: Seasonal AR order, differencing, MA order\n- $s$: Seasonal period (e.g., 12 for monthly, 4 for quarterly)\n\n**Polynomials:**\n- $\\Phi(L) = 1 - \\phi_1 L - \\cdots - \\phi_p L^p$ (non-seasonal AR)\n- $\\Theta(L) = 1 + \\theta_1 L + \\cdots + \\theta_q L^q$ (non-seasonal MA)\n- $\\Phi_s(L^s) = 1 - \\Phi_1 L^s - \\cdots - \\Phi_P L^{Ps}$ (seasonal AR)\n- $\\Theta_s(L^s) = 1 + \\Theta_1 L^s + \\cdots + \\Theta_Q L^{Qs}$ (seasonal MA)\n\n**Seasonal Difference Operator**:\n$$\\nabla_s X_t = (1 - L^s)X_t = X_t - X_{t-s}$$\n\n## Math and Derivations\n\n### SARIMA(0,0,0)(1,0,0)[12]: Seasonal AR(1)\n\n$$X_t = \\Phi_1 X_{t-12} + \\epsilon_t$$\n\nACF significant only at lags 12, 24, 36, ... with exponential decay.\n\n### SARIMA(0,0,0)(0,0,1)[12]: Seasonal MA(1)\n\n$$X_t = \\epsilon_t + \\Theta_1\\epsilon_{t-12}$$\n\nACF significant only at lag 12, zero elsewhere.\n\n### SARIMA(0,1,1)(0,1,1)[12]: The Airline Model\n\nThe classic Box-Jenkins airline passenger model:\n$$(1-L)(1-L^{12})X_t = (1+\\theta L)(1+\\Theta L^{12})\\epsilon_t$$\n\nExpanding:\n$$X_t - X_{t-1} - X_{t-12} + X_{t-13} = \\epsilon_t + \\theta\\epsilon_{t-1} + \\Theta\\epsilon_{t-12} + \\theta\\Theta\\epsilon_{t-13}$$\n\n**Key properties:**\n- First difference handles trend\n- Seasonal difference handles annual pattern\n- MA(1) smooths non-seasonal noise\n- Seasonal MA(1) smooths annual noise\n- Cross-term $\\theta\\Theta$ at lag 13\n\n### ACF/PACF Patterns for SARIMA\n\n**Pure seasonal AR (0,0,0)(P,0,0)[s]:**\n- ACF: Exponential decay at seasonal lags (s, 2s, 3s, ...)\n- PACF: Cuts off at lag Ps\n\n**Pure seasonal MA (0,0,0)(0,0,Q)[s]:**\n- ACF: Cuts off at lag Qs\n- PACF: Exponential decay at seasonal lags\n\n**Mixed SARIMA:**\n- Non-seasonal patterns at lags 1, 2, 3, ...\n- Seasonal patterns at lags s, 2s, 3s, ...\n- Interaction patterns at lags s±1, s±2, ...\n\n### Multiplicative Model Structure\n\nThe multiplicative form means:\n$$\\Phi(L)\\Phi_s(L^s) = (1-\\phi_1 L)(1 - \\Phi_1 L^s) = 1 - \\phi_1 L - \\Phi_1 L^s + \\phi_1\\Phi_1 L^{s+1}$$\n\nThis creates interaction terms (e.g., coefficient at lag 13 for monthly data with AR(1) × SAR(1)).\n\n## Algorithm/Model Sketch\n\n**Identification Procedure:**\n\n```\n1. Plot series; identify seasonal period s\n2. Check for trend → apply regular differencing (d)\n3. Check for seasonal pattern → apply seasonal differencing (D)\n4. Usually D ≤ 1, d ≤ 2\n\n5. Examine ACF/PACF of stationary series:\n   - At lags 1, 2, ..., s-1: determine p, q\n   - At lags s, 2s, 3s: determine P, Q\n   - Spikes at s±k: interaction effects\n\n6. Fit candidate models\n7. Compare AIC/BIC\n8. Check residuals at both regular and seasonal lags\n```\n\n**Common Seasonal Periods:**\n\n| Data Frequency | Period s |\n|----------------|----------|\n| Monthly | 12 |\n| Quarterly | 4 |\n| Weekly (annual) | 52 |\n| Daily (weekly) | 7 |\n| Hourly (daily) | 24 |\n\n## Common Pitfalls\n\n1. **Double seasonal patterns**: Some data has multiple seasonalities (daily + weekly). Standard SARIMA handles one period. Consider multiple seasonal models or alternative methods.\n\n2. **Large s causes issues**: For s=52 or s=365, estimation is difficult. Consider Fourier terms or alternative decomposition methods.\n\n3. **Seasonal differencing with D > 1**: Rarely needed and often causes over-differencing. Check if D=1 suffices.\n\n4. **Ignoring multiplicative structure**: The model is multiplicative, so lag s+1 effects exist when both $\\phi$ and $\\Phi$ are non-zero.\n\n5. **Non-integer periods**: If seasonality isn't at integer lags (e.g., 365.25 days/year), SARIMA doesn't apply directly. Use trigonometric seasonality.\n\n6. **Forgetting trend after seasonal differencing**: Seasonal differencing $(1-L^{12})$ doesn't remove linear trend. May still need $d=1$.\n\n## Mini Example\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# Generate SARIMA(1,1,1)(1,1,1)[12] data\nnp.random.seed(42)\nn = 200\ns = 12\n\n# Create seasonal + trend + noise\nt = np.arange(n)\nseasonal = 10 * np.sin(2 * np.pi * t / s)\ntrend = 0.1 * t\nnoise = np.random.randn(n) * 2\nX = trend + seasonal + np.cumsum(noise)\n\n# Fit SARIMA\nmodel = SARIMAX(X, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\nresults = model.fit(disp=False)\nprint(results.summary().tables[1])\n\n# Check residuals at seasonal lags\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nlb_test = acorr_ljungbox(results.resid, lags=[12, 24], return_df=True)\nprint(\"\\nLjung-Box test at seasonal lags:\")\nprint(lb_test)\n\n# Forecast\nforecast = results.get_forecast(steps=12)\nprint(f\"\\n12-month forecast mean: {forecast.predicted_mean.values}\")\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why is the SARIMA model called \"multiplicative\"? How does this affect the lag structure?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> \"Multiplicative\" refers to multiplying AR and MA polynomials: $\\Phi(L) \\times \\Phi_s(L^s)$. This creates interaction terms at combined lags.\n\n<strong>Example:</strong> SARIMA(1,0,0)(1,0,0)[12]:\n$$(1-\\phi L)(1-\\Phi L^{12})X_t = \\epsilon_t$$\n$$X_t - \\phi X_{t-1} - \\Phi X_{t-12} + \\phi\\Phi X_{t-13} = \\epsilon_t$$\n\nThe $\\phi\\Phi$ term at lag 13 is an interaction effect—it wouldn't exist in an additive model.\n\n**Implication:** Check ACF/PACF at lags like 11, 13 (not just 12) for monthly data.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Expecting clean separation of seasonal and non-seasonal effects. The multiplicative structure blends them, which can confuse identification.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> What is the \"airline model\" and why is it a useful benchmark?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The airline model is SARIMA(0,1,1)(0,1,1)[12], originally fit to airline passenger data by Box and Jenkins. It's useful because:\n\n1. Handles trend via first differencing\n2. Handles annual seasonality via seasonal differencing\n3. Uses only 2 parameters ($\\theta$, $\\Theta$) yet fits many seasonal series well\n4. Equivalent to Holt-Winters exponential smoothing\n\n**Model:**\n$$(1-L)(1-L^{12})X_t = (1+\\theta L)(1+\\Theta L^{12})\\epsilon_t$$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using the airline model as the default without checking fit. For some data, AR terms or different differencing may be needed. Always verify with residual diagnostics.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Derive the ACF at lag 12 for the seasonal MA(1) model: $X_t = \\epsilon_t + \\Theta\\epsilon_{t-12}$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> $\\rho(12) = \\frac{\\Theta}{1+\\Theta^2}$, and $\\rho(h) = 0$ for $h \\neq 0, 12$.\n\n<strong>Derivation:</strong>\n\n**Variance:**\n$$\\gamma(0) = \\text{Var}(\\epsilon_t + \\Theta\\epsilon_{t-12}) = \\sigma^2(1 + \\Theta^2)$$\n\n**Autocovariance at lag 12:**\n$$\\gamma(12) = E[(\\epsilon_t + \\Theta\\epsilon_{t-12})(\\epsilon_{t-12} + \\Theta\\epsilon_{t-24})]$$\n$$= E[\\Theta\\epsilon_{t-12}^2] = \\Theta\\sigma^2$$\n\n**ACF:**\n$$\\rho(12) = \\frac{\\gamma(12)}{\\gamma(0)} = \\frac{\\Theta\\sigma^2}{\\sigma^2(1+\\Theta^2)} = \\frac{\\Theta}{1+\\Theta^2}$$\n\nFor other lags, there's no overlap of $\\epsilon$ terms, so $\\gamma(h) = 0$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Note $|\\rho(12)| \\leq 0.5$, same constraint as non-seasonal MA(1). Larger observed seasonal correlations suggest seasonal AR or combined model.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> For SARIMA(0,1,0)(0,1,0)[12], write out the model equation and explain what it represents.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> This is a \"seasonal random walk\":\n$$(1-L)(1-L^{12})X_t = \\epsilon_t$$\n\nExpanding:\n$$X_t - X_{t-1} - X_{t-12} + X_{t-13} = \\epsilon_t$$\n$$X_t = X_{t-1} + X_{t-12} - X_{t-13} + \\epsilon_t$$\n\n**Interpretation:** Today's value = yesterday's value + this month last year − same day last year + noise.\n\nThis is the \"naïve seasonal\" forecast: $\\hat{X}_{t+1} = X_t + (X_{t+1-12} - X_{t-12})$.\n\nIt says: repeat last year's seasonal pattern while continuing yesterday's level.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> This model is a useful benchmark but often too simplistic. Real data usually benefits from MA terms to smooth noise.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You're modeling hourly electricity demand with clear daily (24h) and weekly (168h) patterns. Can you use standard SARIMA? What alternatives exist?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Standard SARIMA handles only one seasonal period. For multiple seasonalities, alternatives include:\n\n1. **Double seasonal models**: SARIMA with s=24 plus external regressors for weekly pattern\n2. **TBATS**: Exponential smoothing with multiple seasonal periods\n3. **Fourier terms**: Include sin/cos terms at both frequencies\n4. **Prophet**: Handles multiple seasonalities via additive decomposition\n5. **Neural approaches**: LSTM or Transformers can learn complex patterns\n\n**Practical approach:**\n- Use s=24 (dominant pattern)\n- Add day-of-week dummies or Fourier terms for weekly pattern\n- Consider: `SARIMAX(p,d,q)(P,D,Q)[24]` with `exog=weekly_dummies`\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Trying to use s=168 for weekly seasonality causes estimation problems (168 is large). Use hierarchical or additive approaches instead.\n</div>\n</div>\n</details>\n\n## References\n\n1. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis*. Wiley. Chapter 9.\n2. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 9.\n3. Shumway, R. H., & Stoffer, D. S. (2017). *Time Series Analysis and Its Applications*. Springer. Chapter 3.\n4. De Livera, A. M., Hyndman, R. J., & Snyder, R. D. (2011). Forecasting time series with complex seasonal patterns using exponential smoothing. *JASA*, 106(496), 1513-1527.\n", "sections": [{"heading": "Seasonal ARIMA (SARIMA) Models", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> SARIMA(p,d,q)(P,D,Q)[s] adds seasonal AR and MA terms at lag s (e.g., s=12 for monthly). Seasonal differencing $(1-L^s)$ removes seasonal unit roots. The \"airline model\" SARIMA(0,1,1)(0,1,1)[12] is a benchmark for seasonal data. Identification uses ACF/PACF at both regular and seasonal lags.\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**SARIMA(p,d,q)(P,D,Q)[s] Model**:\n$$\\Phi(L)\\Phi_s(L^s)(1-L)^d(1-L^s)^D X_t = c + \\Theta(L)\\Theta_s(L^s)\\epsilon_t$$\n\n**Components:**\n- $(p, d, q)$: Non-seasonal AR order, differencing, MA order\n- $(P, D, Q)$: Seasonal AR order, differencing, MA order\n- $s$: Seasonal period (e.g., 12 for monthly, 4 for quarterly)\n\n**Polynomials:**\n- $\\Phi(L) = 1 - \\phi_1 L - \\cdots - \\phi_p L^p$ (non-seasonal AR)\n- $\\Theta(L) = 1 + \\theta_1 L + \\cdots + \\theta_q L^q$ (non-seasonal MA)\n- $\\Phi_s(L^s) = 1 - \\Phi_1 L^s - \\cdots - \\Phi_P L^{Ps}$ (seasonal AR)\n- $\\Theta_s(L^s) = 1 + \\Theta_1 L^s + \\cdots + \\Theta_Q L^{Qs}$ (seasonal MA)\n\n**Seasonal Difference Operator**:\n$$\\nabla_s X_t = (1 - L^s)X_t = X_t - X_{t-s}$$", "line_start": 7, "level": 2}, {"heading": "SARIMA(0,0,0)(1,0,0)[12]: Seasonal AR(1)", "content": "$$X_t = \\Phi_1 X_{t-12} + \\epsilon_t$$\n\nACF significant only at lags 12, 24, 36, ... with exponential decay.", "line_start": 28, "level": 3}, {"heading": "SARIMA(0,0,0)(0,0,1)[12]: Seasonal MA(1)", "content": "$$X_t = \\epsilon_t + \\Theta_1\\epsilon_{t-12}$$\n\nACF significant only at lag 12, zero elsewhere.", "line_start": 34, "level": 3}, {"heading": "SARIMA(0,1,1)(0,1,1)[12]: The Airline Model", "content": "The classic Box-Jenkins airline passenger model:\n$$(1-L)(1-L^{12})X_t = (1+\\theta L)(1+\\Theta L^{12})\\epsilon_t$$\n\nExpanding:\n$$X_t - X_{t-1} - X_{t-12} + X_{t-13} = \\epsilon_t + \\theta\\epsilon_{t-1} + \\Theta\\epsilon_{t-12} + \\theta\\Theta\\epsilon_{t-13}$$\n\n**Key properties:**\n- First difference handles trend\n- Seasonal difference handles annual pattern\n- MA(1) smooths non-seasonal noise\n- Seasonal MA(1) smooths annual noise\n- Cross-term $\\theta\\Theta$ at lag 13", "line_start": 40, "level": 3}, {"heading": "ACF/PACF Patterns for SARIMA", "content": "**Pure seasonal AR (0,0,0)(P,0,0)[s]:**\n- ACF: Exponential decay at seasonal lags (s, 2s, 3s, ...)\n- PACF: Cuts off at lag Ps\n\n**Pure seasonal MA (0,0,0)(0,0,Q)[s]:**\n- ACF: Cuts off at lag Qs\n- PACF: Exponential decay at seasonal lags\n\n**Mixed SARIMA:**\n- Non-seasonal patterns at lags 1, 2, 3, ...\n- Seasonal patterns at lags s, 2s, 3s, ...\n- Interaction patterns at lags s±1, s±2, ...", "line_start": 55, "level": 3}, {"heading": "Multiplicative Model Structure", "content": "The multiplicative form means:\n$$\\Phi(L)\\Phi_s(L^s) = (1-\\phi_1 L)(1 - \\Phi_1 L^s) = 1 - \\phi_1 L - \\Phi_1 L^s + \\phi_1\\Phi_1 L^{s+1}$$\n\nThis creates interaction terms (e.g., coefficient at lag 13 for monthly data with AR(1) × SAR(1)).", "line_start": 70, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**Identification Procedure:**\n\n```\n1. Plot series; identify seasonal period s\n2. Check for trend → apply regular differencing (d)\n3. Check for seasonal pattern → apply seasonal differencing (D)\n4. Usually D ≤ 1, d ≤ 2\n\n5. Examine ACF/PACF of stationary series:\n   - At lags 1, 2, ..., s-1: determine p, q\n   - At lags s, 2s, 3s: determine P, Q\n   - Spikes at s±k: interaction effects\n\n6. Fit candidate models\n7. Compare AIC/BIC\n8. Check residuals at both regular and seasonal lags\n```\n\n**Common Seasonal Periods:**\n\n| Data Frequency | Period s |\n|----------------|----------|\n| Monthly | 12 |\n| Quarterly | 4 |\n| Weekly (annual) | 52 |\n| Daily (weekly) | 7 |\n| Hourly (daily) | 24 |", "line_start": 77, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Double seasonal patterns**: Some data has multiple seasonalities (daily + weekly). Standard SARIMA handles one period. Consider multiple seasonal models or alternative methods.\n\n2. **Large s causes issues**: For s=52 or s=365, estimation is difficult. Consider Fourier terms or alternative decomposition methods.\n\n3. **Seasonal differencing with D > 1**: Rarely needed and often causes over-differencing. Check if D=1 suffices.\n\n4. **Ignoring multiplicative structure**: The model is multiplicative, so lag s+1 effects exist when both $\\phi$ and $\\Phi$ are non-zero.\n\n5. **Non-integer periods**: If seasonality isn't at integer lags (e.g., 365.25 days/year), SARIMA doesn't apply directly. Use trigonometric seasonality.\n\n6. **Forgetting trend after seasonal differencing**: Seasonal differencing $(1-L^{12})$ doesn't remove linear trend. May still need $d=1$.", "line_start": 107, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.seasonal import seasonal_decompose", "line_start": 121, "level": 1}, {"heading": "Generate SARIMA(1,1,1)(1,1,1)[12] data", "content": "np.random.seed(42)\nn = 200\ns = 12", "line_start": 129, "level": 1}, {"heading": "Create seasonal + trend + noise", "content": "t = np.arange(n)\nseasonal = 10 * np.sin(2 * np.pi * t / s)\ntrend = 0.1 * t\nnoise = np.random.randn(n) * 2\nX = trend + seasonal + np.cumsum(noise)", "line_start": 134, "level": 1}, {"heading": "Fit SARIMA", "content": "model = SARIMAX(X, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\nresults = model.fit(disp=False)\nprint(results.summary().tables[1])", "line_start": 141, "level": 1}, {"heading": "Check residuals at seasonal lags", "content": "from statsmodels.stats.diagnostic import acorr_ljungbox\nlb_test = acorr_ljungbox(results.resid, lags=[12, 24], return_df=True)\nprint(\"\\nLjung-Box test at seasonal lags:\")\nprint(lb_test)", "line_start": 146, "level": 1}, {"heading": "Forecast", "content": "forecast = results.get_forecast(steps=12)\nprint(f\"\\n12-month forecast mean: {forecast.predicted_mean.values}\")\n```", "line_start": 152, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why is the SARIMA model called \"multiplicative\"? How does this affect the lag structure?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> \"Multiplicative\" refers to multiplying AR and MA polynomials: $\\Phi(L) \\times \\Phi_s(L^s)$. This creates interaction terms at combined lags.\n\n<strong>Example:</strong> SARIMA(1,0,0)(1,0,0)[12]:\n$$(1-\\phi L)(1-\\Phi L^{12})X_t = \\epsilon_t$$\n$$X_t - \\phi X_{t-1} - \\Phi X_{t-12} + \\phi\\Phi X_{t-13} = \\epsilon_t$$\n\nThe $\\phi\\Phi$ term at lag 13 is an interaction effect—it wouldn't exist in an additive model.\n\n**Implication:** Check ACF/PACF at lags like 11, 13 (not just 12) for monthly data.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Expecting clean separation of seasonal and non-seasonal effects. The multiplicative structure blends them, which can confuse identification.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> What is the \"airline model\" and why is it a useful benchmark?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The airline model is SARIMA(0,1,1)(0,1,1)[12], originally fit to airline passenger data by Box and Jenkins. It's useful because:\n\n1. Handles trend via first differencing\n2. Handles annual seasonality via seasonal differencing\n3. Uses only 2 parameters ($\\theta$, $\\Theta$) yet fits many seasonal series well\n4. Equivalent to Holt-Winters exponential smoothing\n\n**Model:**\n$$(1-L)(1-L^{12})X_t = (1+\\theta L)(1+\\Theta L^{12})\\epsilon_t$$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using the airline model as the default without checking fit. For some data, AR terms or different differencing may be needed. Always verify with residual diagnostics.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Derive the ACF at lag 12 for the seasonal MA(1) model: $X_t = \\epsilon_t + \\Theta\\epsilon_{t-12}$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> $\\rho(12) = \\frac{\\Theta}{1+\\Theta^2}$, and $\\rho(h) = 0$ for $h \\neq 0, 12$.\n\n<strong>Derivation:</strong>\n\n**Variance:**\n$$\\gamma(0) = \\text{Var}(\\epsilon_t + \\Theta\\epsilon_{t-12}) = \\sigma^2(1 + \\Theta^2)$$\n\n**Autocovariance at lag 12:**\n$$\\gamma(12) = E[(\\epsilon_t + \\Theta\\epsilon_{t-12})(\\epsilon_{t-12} + \\Theta\\epsilon_{t-24})]$$\n$$= E[\\Theta\\epsilon_{t-12}^2] = \\Theta\\sigma^2$$\n\n**ACF:**\n$$\\rho(12) = \\frac{\\gamma(12)}{\\gamma(0)} = \\frac{\\Theta\\sigma^2}{\\sigma^2(1+\\Theta^2)} = \\frac{\\Theta}{1+\\Theta^2}$$\n\nFor other lags, there's no overlap of $\\epsilon$ terms, so $\\gamma(h) = 0$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Note $|\\rho(12)| \\leq 0.5$, same constraint as non-seasonal MA(1). Larger observed seasonal correlations suggest seasonal AR or combined model.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> For SARIMA(0,1,0)(0,1,0)[12], write out the model equation and explain what it represents.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> This is a \"seasonal random walk\":\n$$(1-L)(1-L^{12})X_t = \\epsilon_t$$\n\nExpanding:\n$$X_t - X_{t-1} - X_{t-12} + X_{t-13} = \\epsilon_t$$\n$$X_t = X_{t-1} + X_{t-12} - X_{t-13} + \\epsilon_t$$\n\n**Interpretation:** Today's value = yesterday's value + this month last year − same day last year + noise.\n\nThis is the \"naïve seasonal\" forecast: $\\hat{X}_{t+1} = X_t + (X_{t+1-12} - X_{t-12})$.\n\nIt says: repeat last year's seasonal pattern while continuing yesterday's level.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> This model is a useful benchmark but often too simplistic. Real data usually benefits from MA terms to smooth noise.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You're modeling hourly electricity demand with clear daily (24h) and weekly (168h) patterns. Can you use standard SARIMA? What alternatives exist?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Standard SARIMA handles only one seasonal period. For multiple seasonalities, alternatives include:\n\n1. **Double seasonal models**: SARIMA with s=24 plus external regressors for weekly pattern\n2. **TBATS**: Exponential smoothing with multiple seasonal periods\n3. **Fourier terms**: Include sin/cos terms at both frequencies\n4. **Prophet**: Handles multiple seasonalities via additive decomposition\n5. **Neural approaches**: LSTM or Transformers can learn complex patterns\n\n**Practical approach:**\n- Use s=24 (dominant pattern)\n- Add day-of-week dummies or Fourier terms for weekly pattern\n- Consider: `SARIMAX(p,d,q)(P,D,Q)[24]` with `exog=weekly_dummies`\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Trying to use s=168 for weekly seasonality causes estimation problems (168 is large). Use hierarchical or additive approaches instead.\n</div>\n</div>\n</details>", "line_start": 157, "level": 2}, {"heading": "References", "content": "1. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis*. Wiley. Chapter 9.\n2. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 9.\n3. Shumway, R. H., & Stoffer, D. S. (2017). *Time Series Analysis and Its Applications*. Springer. Chapter 3.\n4. De Livera, A. M., Hyndman, R. J., & Snyder, R. D. (2011). Forecasting time series with complex seasonal patterns using exponential smoothing. *JASA*, 106(496), 1513-1527.", "line_start": 271, "level": 1}]}, "docs/en/deep-learning/deep-learning-ts.md": {"path": "docs/en/deep-learning/deep-learning-ts.md", "title": "Deep Learning for Time Series", "content": "# Deep Learning for Time Series\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Deep learning models (RNN, LSTM, TCN, Transformer) capture complex temporal patterns. LSTM addresses vanishing gradients via gates. TCN uses dilated causal convolutions for long-range dependencies. Transformers use attention mechanisms. DL shines with: large data, multiple series, complex patterns. May underperform classical methods on small data or simple patterns.\n</div>\n\n## Core Definitions\n\n**RNN (Recurrent Neural Network):**\n$$h_t = \\tanh(W_h h_{t-1} + W_x x_t + b)$$\n$$\\hat{y}_t = W_y h_t$$\n\n**LSTM (Long Short-Term Memory):**\n- Forget gate: $f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$\n- Input gate: $i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$\n- Cell update: $\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$\n- Cell state: $C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$\n- Output gate: $o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$\n- Hidden state: $h_t = o_t * \\tanh(C_t)$\n\n**TCN (Temporal Convolutional Network):**\nDilated causal convolutions with residual connections.\n\n**Transformer:** Self-attention mechanism for sequence modeling.\n\n## Math and Derivations\n\n### Vanishing Gradient Problem in RNN\n\nGradient of loss with respect to early hidden state:\n$$\\frac{\\partial L}{\\partial h_t} = \\frac{\\partial L}{\\partial h_T}\\prod_{k=t}^{T-1}\\frac{\\partial h_{k+1}}{\\partial h_k}$$\n\nFor tanh activation: $|\\frac{\\partial h_{k+1}}{\\partial h_k}| < 1$ typically\n\nProduct of many small numbers → gradient vanishes.\n\n**LSTM solution:** Cell state path has additive updates (not multiplicative), preserving gradients over long sequences.\n\n### TCN Dilated Convolutions\n\nFor dilation factor $d$ and filter size $k$:\n$$(F *_d x)_t = \\sum_{i=0}^{k-1} f_i \\cdot x_{t-d \\cdot i}$$\n\n**Receptive field with L layers:**\n$$R = 1 + (k-1)\\sum_{l=0}^{L-1}d_l$$\n\nWith exponential dilation ($d_l = 2^l$): $R = 1 + (k-1)(2^L - 1)$\n\n### Transformer Self-Attention\n\n**Attention:**\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n\nFor time series:\n- Q, K, V derived from input sequence\n- Attention weights show which past timesteps are relevant\n- Position encoding added to maintain temporal order\n\n### Training Strategies\n\n**Teacher forcing:** During training, use actual values (not predictions) as input.\n\n**Multi-step loss:** Optimize over multiple forecast horizons:\n$$L = \\sum_{h=1}^{H}w_h \\cdot L_h(\\hat{y}_{t+h}, y_{t+h})$$\n\n## Algorithm/Model Sketch\n\n**LSTM for Forecasting:**\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass LSTMForecaster(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        # x: (batch, seq_len, input_size)\n        lstm_out, _ = self.lstm(x)\n        # Take last hidden state\n        out = self.fc(lstm_out[:, -1, :])\n        return out\n\n# Training loop\nmodel = LSTMForecaster(input_size=1, hidden_size=64, num_layers=2, output_size=1)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.MSELoss()\n\nfor epoch in range(100):\n    for X_batch, y_batch in dataloader:\n        optimizer.zero_grad()\n        y_pred = model(X_batch)\n        loss = criterion(y_pred, y_batch)\n        loss.backward()\n        optimizer.step()\n```\n\n**When to Use Deep Learning:**\n\n| Scenario | Recommendation |\n|----------|---------------|\n| Small data (<1000) | Classical (ARIMA, ETS) |\n| Medium data, simple patterns | Classical or simple NN |\n| Large data, complex patterns | Deep learning |\n| Many related series | Deep learning (transfer) |\n| Real-time, low latency | TCN (parallelizable) |\n\n## Common Pitfalls\n\n1. **Too little data:** DL needs thousands+ of observations. With small data, ARIMA often wins.\n\n2. **Over-complicated architecture:** Simple LSTM often beats complex Transformer on univariate forecasting.\n\n3. **Ignoring baselines:** Always compare to naive, seasonal naive, and ARIMA before claiming DL success.\n\n4. **Lookback window too short:** LSTM can learn long patterns only if lookback is long enough.\n\n5. **Not using validation properly:** Use time-aware validation (rolling origin), not random split.\n\n6. **Training instability:** Gradient clipping, learning rate scheduling, and careful initialization matter.\n\n## Mini Example\n\n```python\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Generate data\nnp.random.seed(42)\nn = 2000\nt = np.arange(n)\ny = np.sin(2 * np.pi * t / 50) + 0.5 * np.sin(2 * np.pi * t / 10) + np.random.randn(n) * 0.3\n\n# Create sequences\ndef create_sequences(data, seq_length):\n    X, Y = [], []\n    for i in range(len(data) - seq_length):\n        X.append(data[i:i+seq_length])\n        Y.append(data[i+seq_length])\n    return np.array(X), np.array(Y)\n\nseq_length = 50\nX, Y = create_sequences(y, seq_length)\nX = torch.FloatTensor(X).unsqueeze(-1)  # (N, seq_len, 1)\nY = torch.FloatTensor(Y).unsqueeze(-1)  # (N, 1)\n\n# Train-test split\ntrain_size = int(len(X) * 0.8)\nX_train, Y_train = X[:train_size], Y[:train_size]\nX_test, Y_test = X[train_size:], Y[train_size:]\n\n# Simple LSTM model\nclass SimpleLSTM(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lstm = nn.LSTM(1, 32, 1, batch_first=True)\n        self.fc = nn.Linear(32, 1)\n\n    def forward(self, x):\n        out, _ = self.lstm(x)\n        return self.fc(out[:, -1, :])\n\nmodel = SimpleLSTM()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\ncriterion = nn.MSELoss()\n\n# Train\ntrain_loader = DataLoader(TensorDataset(X_train, Y_train), batch_size=32, shuffle=True)\nfor epoch in range(20):\n    for X_batch, Y_batch in train_loader:\n        optimizer.zero_grad()\n        loss = criterion(model(X_batch), Y_batch)\n        loss.backward()\n        optimizer.step()\n\n# Evaluate\nmodel.eval()\nwith torch.no_grad():\n    y_pred = model(X_test)\n    test_rmse = torch.sqrt(criterion(y_pred, Y_test))\n    print(f\"Test RMSE: {test_rmse:.4f}\")\n\n# Compare to naive\nnaive_rmse = np.sqrt(np.mean((Y_test.numpy() - X_test[:, -1, :].numpy())**2))\nprint(f\"Naive RMSE: {naive_rmse:.4f}\")\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What is the vanishing gradient problem and how does LSTM address it?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Problem:** In vanilla RNN, gradients are multiplied through each timestep. With many steps, gradients become exponentially small (vanish), preventing learning of long-range dependencies.\n\n**LSTM Solution:**\n- Cell state $C_t$ is updated additively, not multiplicatively\n- Forget gate controls what to keep: $C_t = f_t * C_{t-1} + ...$\n- When $f_t \\approx 1$, gradient flows unchanged\n- Information can persist over hundreds of timesteps\n\n**Key insight:** The cell state acts as a \"highway\" for gradients, bypassing the vanishing problem.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Thinking LSTM completely solves long-range dependencies. Very long sequences (1000+) may still need attention mechanisms or hierarchical structures.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> When would you choose TCN over LSTM for time series?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Prefer TCN when:**\n1. **Parallelization needed:** TCN processes all timesteps simultaneously; LSTM is sequential\n2. **Long sequences:** Dilated convolutions efficiently handle very long range\n3. **Training stability:** TCN gradients don't explode/vanish as easily\n4. **Inference speed:** No hidden state to maintain\n5. **Variable length at inference:** Can process any length\n\n**Prefer LSTM when:**\n1. **Truly sequential processing:** Online/streaming data\n2. **Variable length training:** LSTM naturally handles different lengths\n3. **State tracking needed:** Hidden state captures \"memory\"\n4. **Smaller receptive field sufficient:** LSTM may use parameters more efficiently\n\n**Research finding:** TCN often matches or beats LSTM on standard benchmarks with faster training.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Defaulting to LSTM because it's \"the standard.\" TCN is often simpler and faster with comparable accuracy.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> How does dilated convolution increase receptive field without increasing parameters?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Dilation introduces gaps in the convolution, sampling inputs at regular intervals.\n\n**Standard convolution (dilation=1):**\n$$y_t = \\sum_{i=0}^{k-1} w_i \\cdot x_{t-i}$$\nReceptive field = k\n\n**Dilated convolution (dilation=d):**\n$$y_t = \\sum_{i=0}^{k-1} w_i \\cdot x_{t-d \\cdot i}$$\nReceptive field = 1 + (k-1) × d\n\n**With exponential dilation (d = 2^l):**\n- Layer 0: RF = k\n- Layer 1: RF = k + (k-1)×2\n- Layer L-1: RF = 1 + (k-1)(2^L - 1)\n\n**Example:** k=3, L=8 → RF = 1 + 2×255 = 511\n\nSame number of parameters (k weights per layer), but 500+ timestep receptive field!\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using too many layers. With k=3 and L=10, RF ≈ 2000. Check if you actually need that range.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Explain why Transformers need positional encoding for time series.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Self-attention is permutation invariant—it treats input as a set, not a sequence.\n\n**Problem:**\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n\nThis doesn't change if we permute the input order—attention weights depend only on pairwise similarities.\n\n**Solution: Positional encoding**\nAdd position information to embeddings:\n$$x'_t = x_t + PE(t)$$\n\nCommon encoding:\n$$PE(t, 2i) = \\sin(t / 10000^{2i/d})$$\n$$PE(t, 2i+1) = \\cos(t / 10000^{2i/d})$$\n\nNow the model can distinguish $x_5$ from $x_{50}$ even if content is identical.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Forgetting positional encoding → Transformer treats sequence as bag of vectors, losing temporal structure entirely.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You train an LSTM on energy demand data and it predicts flat lines (always the mean). What's wrong?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Several possible causes:\n\n1. **Learning rate too high:** Weights oscillate, model defaults to mean\n   - Fix: Reduce LR, use scheduler\n\n2. **Vanishing gradients:** Despite LSTM, can still occur\n   - Fix: Gradient clipping, check gradient norms\n\n3. **Data not scaled:** Large values cause saturation\n   - Fix: Standardize inputs and targets\n\n4. **Lookback too short:** Model can't see useful patterns\n   - Fix: Increase sequence length\n\n5. **Too few epochs:** Model hasn't learned yet\n   - Fix: Train longer, check loss curve\n\n6. **Wrong loss function:** MSE on non-stationary data dominated by trend\n   - Fix: Use differenced data or relative errors\n\n7. **Model too small:** Can't capture complexity\n   - Fix: Increase hidden size / layers\n\n**Diagnosis:**\n- Plot training loss: decreasing or flat?\n- Check gradient magnitudes\n- Visualize predictions vs. actuals over training\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Assuming mean prediction is \"failure.\" For high-noise data with no pattern, mean IS optimal. Compare to naive baselines first.\n</div>\n</div>\n</details>\n\n## References\n\n1. Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. *Neural Computation*, 9(8), 1735-1780.\n2. Bai, S., Kolter, J. Z., & Koltun, V. (2018). An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. *arXiv:1803.01271*.\n3. Vaswani, A., et al. (2017). Attention is all you need. *NeurIPS*.\n4. Lim, B., & Zohren, S. (2021). Time-series forecasting with deep learning: a survey. *Philosophical Transactions A*, 379(2194).\n", "sections": [{"heading": "Deep Learning for Time Series", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Deep learning models (RNN, LSTM, TCN, Transformer) capture complex temporal patterns. LSTM addresses vanishing gradients via gates. TCN uses dilated causal convolutions for long-range dependencies. Transformers use attention mechanisms. DL shines with: large data, multiple series, complex patterns. May underperform classical methods on small data or simple patterns.\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**RNN (Recurrent Neural Network):**\n$$h_t = \\tanh(W_h h_{t-1} + W_x x_t + b)$$\n$$\\hat{y}_t = W_y h_t$$\n\n**LSTM (Long Short-Term Memory):**\n- Forget gate: $f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$\n- Input gate: $i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$\n- Cell update: $\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$\n- Cell state: $C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$\n- Output gate: $o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$\n- Hidden state: $h_t = o_t * \\tanh(C_t)$\n\n**TCN (Temporal Convolutional Network):**\nDilated causal convolutions with residual connections.\n\n**Transformer:** Self-attention mechanism for sequence modeling.", "line_start": 7, "level": 2}, {"heading": "Vanishing Gradient Problem in RNN", "content": "Gradient of loss with respect to early hidden state:\n$$\\frac{\\partial L}{\\partial h_t} = \\frac{\\partial L}{\\partial h_T}\\prod_{k=t}^{T-1}\\frac{\\partial h_{k+1}}{\\partial h_k}$$\n\nFor tanh activation: $|\\frac{\\partial h_{k+1}}{\\partial h_k}| < 1$ typically\n\nProduct of many small numbers → gradient vanishes.\n\n**LSTM solution:** Cell state path has additive updates (not multiplicative), preserving gradients over long sequences.", "line_start": 28, "level": 3}, {"heading": "TCN Dilated Convolutions", "content": "For dilation factor $d$ and filter size $k$:\n$$(F *_d x)_t = \\sum_{i=0}^{k-1} f_i \\cdot x_{t-d \\cdot i}$$\n\n**Receptive field with L layers:**\n$$R = 1 + (k-1)\\sum_{l=0}^{L-1}d_l$$\n\nWith exponential dilation ($d_l = 2^l$): $R = 1 + (k-1)(2^L - 1)$", "line_start": 39, "level": 3}, {"heading": "Transformer Self-Attention", "content": "**Attention:**\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n\nFor time series:\n- Q, K, V derived from input sequence\n- Attention weights show which past timesteps are relevant\n- Position encoding added to maintain temporal order", "line_start": 49, "level": 3}, {"heading": "Training Strategies", "content": "**Teacher forcing:** During training, use actual values (not predictions) as input.\n\n**Multi-step loss:** Optimize over multiple forecast horizons:\n$$L = \\sum_{h=1}^{H}w_h \\cdot L_h(\\hat{y}_{t+h}, y_{t+h})$$", "line_start": 59, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**LSTM for Forecasting:**\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass LSTMForecaster(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        # x: (batch, seq_len, input_size)\n        lstm_out, _ = self.lstm(x)\n        # Take last hidden state\n        out = self.fc(lstm_out[:, -1, :])\n        return out", "line_start": 66, "level": 1}, {"heading": "Training loop", "content": "model = LSTMForecaster(input_size=1, hidden_size=64, num_layers=2, output_size=1)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.MSELoss()\n\nfor epoch in range(100):\n    for X_batch, y_batch in dataloader:\n        optimizer.zero_grad()\n        y_pred = model(X_batch)\n        loss = criterion(y_pred, y_batch)\n        loss.backward()\n        optimizer.step()\n```\n\n**When to Use Deep Learning:**\n\n| Scenario | Recommendation |\n|----------|---------------|\n| Small data (<1000) | Classical (ARIMA, ETS) |\n| Medium data, simple patterns | Classical or simple NN |\n| Large data, complex patterns | Deep learning |\n| Many related series | Deep learning (transfer) |\n| Real-time, low latency | TCN (parallelizable) |", "line_start": 87, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Too little data:** DL needs thousands+ of observations. With small data, ARIMA often wins.\n\n2. **Over-complicated architecture:** Simple LSTM often beats complex Transformer on univariate forecasting.\n\n3. **Ignoring baselines:** Always compare to naive, seasonal naive, and ARIMA before claiming DL success.\n\n4. **Lookback window too short:** LSTM can learn long patterns only if lookback is long enough.\n\n5. **Not using validation properly:** Use time-aware validation (rolling origin), not random split.\n\n6. **Training instability:** Gradient clipping, learning rate scheduling, and careful initialization matter.", "line_start": 111, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset", "line_start": 125, "level": 1}, {"heading": "Generate data", "content": "np.random.seed(42)\nn = 2000\nt = np.arange(n)\ny = np.sin(2 * np.pi * t / 50) + 0.5 * np.sin(2 * np.pi * t / 10) + np.random.randn(n) * 0.3", "line_start": 133, "level": 1}, {"heading": "Create sequences", "content": "def create_sequences(data, seq_length):\n    X, Y = [], []\n    for i in range(len(data) - seq_length):\n        X.append(data[i:i+seq_length])\n        Y.append(data[i+seq_length])\n    return np.array(X), np.array(Y)\n\nseq_length = 50\nX, Y = create_sequences(y, seq_length)\nX = torch.FloatTensor(X).unsqueeze(-1)  # (N, seq_len, 1)\nY = torch.FloatTensor(Y).unsqueeze(-1)  # (N, 1)", "line_start": 139, "level": 1}, {"heading": "Train-test split", "content": "train_size = int(len(X) * 0.8)\nX_train, Y_train = X[:train_size], Y[:train_size]\nX_test, Y_test = X[train_size:], Y[train_size:]", "line_start": 152, "level": 1}, {"heading": "Simple LSTM model", "content": "class SimpleLSTM(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lstm = nn.LSTM(1, 32, 1, batch_first=True)\n        self.fc = nn.Linear(32, 1)\n\n    def forward(self, x):\n        out, _ = self.lstm(x)\n        return self.fc(out[:, -1, :])\n\nmodel = SimpleLSTM()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\ncriterion = nn.MSELoss()", "line_start": 157, "level": 1}, {"heading": "Train", "content": "train_loader = DataLoader(TensorDataset(X_train, Y_train), batch_size=32, shuffle=True)\nfor epoch in range(20):\n    for X_batch, Y_batch in train_loader:\n        optimizer.zero_grad()\n        loss = criterion(model(X_batch), Y_batch)\n        loss.backward()\n        optimizer.step()", "line_start": 172, "level": 1}, {"heading": "Evaluate", "content": "model.eval()\nwith torch.no_grad():\n    y_pred = model(X_test)\n    test_rmse = torch.sqrt(criterion(y_pred, Y_test))\n    print(f\"Test RMSE: {test_rmse:.4f}\")", "line_start": 181, "level": 1}, {"heading": "Compare to naive", "content": "naive_rmse = np.sqrt(np.mean((Y_test.numpy() - X_test[:, -1, :].numpy())**2))\nprint(f\"Naive RMSE: {naive_rmse:.4f}\")\n```", "line_start": 188, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What is the vanishing gradient problem and how does LSTM address it?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Problem:** In vanilla RNN, gradients are multiplied through each timestep. With many steps, gradients become exponentially small (vanish), preventing learning of long-range dependencies.\n\n**LSTM Solution:**\n- Cell state $C_t$ is updated additively, not multiplicatively\n- Forget gate controls what to keep: $C_t = f_t * C_{t-1} + ...$\n- When $f_t \\approx 1$, gradient flows unchanged\n- Information can persist over hundreds of timesteps\n\n**Key insight:** The cell state acts as a \"highway\" for gradients, bypassing the vanishing problem.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Thinking LSTM completely solves long-range dependencies. Very long sequences (1000+) may still need attention mechanisms or hierarchical structures.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> When would you choose TCN over LSTM for time series?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Prefer TCN when:**\n1. **Parallelization needed:** TCN processes all timesteps simultaneously; LSTM is sequential\n2. **Long sequences:** Dilated convolutions efficiently handle very long range\n3. **Training stability:** TCN gradients don't explode/vanish as easily\n4. **Inference speed:** No hidden state to maintain\n5. **Variable length at inference:** Can process any length\n\n**Prefer LSTM when:**\n1. **Truly sequential processing:** Online/streaming data\n2. **Variable length training:** LSTM naturally handles different lengths\n3. **State tracking needed:** Hidden state captures \"memory\"\n4. **Smaller receptive field sufficient:** LSTM may use parameters more efficiently\n\n**Research finding:** TCN often matches or beats LSTM on standard benchmarks with faster training.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Defaulting to LSTM because it's \"the standard.\" TCN is often simpler and faster with comparable accuracy.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> How does dilated convolution increase receptive field without increasing parameters?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Dilation introduces gaps in the convolution, sampling inputs at regular intervals.\n\n**Standard convolution (dilation=1):**\n$$y_t = \\sum_{i=0}^{k-1} w_i \\cdot x_{t-i}$$\nReceptive field = k\n\n**Dilated convolution (dilation=d):**\n$$y_t = \\sum_{i=0}^{k-1} w_i \\cdot x_{t-d \\cdot i}$$\nReceptive field = 1 + (k-1) × d\n\n**With exponential dilation (d = 2^l):**\n- Layer 0: RF = k\n- Layer 1: RF = k + (k-1)×2\n- Layer L-1: RF = 1 + (k-1)(2^L - 1)\n\n**Example:** k=3, L=8 → RF = 1 + 2×255 = 511\n\nSame number of parameters (k weights per layer), but 500+ timestep receptive field!\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using too many layers. With k=3 and L=10, RF ≈ 2000. Check if you actually need that range.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Explain why Transformers need positional encoding for time series.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Self-attention is permutation invariant—it treats input as a set, not a sequence.\n\n**Problem:**\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n\nThis doesn't change if we permute the input order—attention weights depend only on pairwise similarities.\n\n**Solution: Positional encoding**\nAdd position information to embeddings:\n$$x'_t = x_t + PE(t)$$\n\nCommon encoding:\n$$PE(t, 2i) = \\sin(t / 10000^{2i/d})$$\n$$PE(t, 2i+1) = \\cos(t / 10000^{2i/d})$$\n\nNow the model can distinguish $x_5$ from $x_{50}$ even if content is identical.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Forgetting positional encoding → Transformer treats sequence as bag of vectors, losing temporal structure entirely.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You train an LSTM on energy demand data and it predicts flat lines (always the mean). What's wrong?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Several possible causes:\n\n1. **Learning rate too high:** Weights oscillate, model defaults to mean\n   - Fix: Reduce LR, use scheduler\n\n2. **Vanishing gradients:** Despite LSTM, can still occur\n   - Fix: Gradient clipping, check gradient norms\n\n3. **Data not scaled:** Large values cause saturation\n   - Fix: Standardize inputs and targets\n\n4. **Lookback too short:** Model can't see useful patterns\n   - Fix: Increase sequence length\n\n5. **Too few epochs:** Model hasn't learned yet\n   - Fix: Train longer, check loss curve\n\n6. **Wrong loss function:** MSE on non-stationary data dominated by trend\n   - Fix: Use differenced data or relative errors\n\n7. **Model too small:** Can't capture complexity\n   - Fix: Increase hidden size / layers\n\n**Diagnosis:**\n- Plot training loss: decreasing or flat?\n- Check gradient magnitudes\n- Visualize predictions vs. actuals over training\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Assuming mean prediction is \"failure.\" For high-noise data with no pattern, mean IS optimal. Compare to naive baselines first.\n</div>\n</div>\n</details>", "line_start": 193, "level": 2}, {"heading": "References", "content": "1. Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. *Neural Computation*, 9(8), 1735-1780.\n2. Bai, S., Kolter, J. Z., & Koltun, V. (2018). An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. *arXiv:1803.01271*.\n3. Vaswani, A., et al. (2017). Attention is all you need. *NeurIPS*.\n4. Lim, B., & Zohren, S. (2021). Time-series forecasting with deep learning: a survey. *Philosophical Transactions A*, 379(2194).", "line_start": 338, "level": 1}]}, "docs/en/exponential-smoothing/ets.md": {"path": "docs/en/exponential-smoothing/ets.md", "title": "ETS Framework", "content": "# ETS Framework\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> ETS (Error-Trend-Seasonal) is a state space framework unifying exponential smoothing methods. Named by three components: Error (A/M), Trend (N/A/Ad/M/Md), Seasonal (N/A/M). Provides likelihood-based estimation and proper prediction intervals. ETS(A,A,A) = additive Holt-Winters. Total of 30 model variants.\n</div>\n\n## Core Definitions\n\n**ETS Taxonomy:**\n\n- **E (Error)**: Additive (A) or Multiplicative (M)\n- **T (Trend)**: None (N), Additive (A), Additive damped (Ad), Multiplicative (M), Multiplicative damped (Md)\n- **S (Seasonal)**: None (N), Additive (A), Multiplicative (M)\n\n**Notation:** ETS(E,T,S)\n\n**Examples:**\n- ETS(A,N,N) = Simple Exponential Smoothing\n- ETS(A,A,N) = Holt's linear\n- ETS(A,Ad,N) = Damped trend\n- ETS(A,A,A) = Additive Holt-Winters\n- ETS(M,A,M) = Multiplicative error, additive trend, multiplicative seasonal\n\n## Math and Derivations\n\n### State Space Form\n\n**General form:**\n$$y_t = w(\\mathbf{x}_{t-1}) + r(\\mathbf{x}_{t-1})\\epsilon_t$$\n$$\\mathbf{x}_t = f(\\mathbf{x}_{t-1}) + g(\\mathbf{x}_{t-1})\\epsilon_t$$\n\nwhere $\\mathbf{x}_t$ is the state vector (level, trend, seasonal components).\n\n### ETS(A,A,N): Additive Error, Additive Trend, No Seasonal\n\nMeasurement: $y_t = \\ell_{t-1} + b_{t-1} + \\epsilon_t$\n\nState transitions:\n$$\\ell_t = \\ell_{t-1} + b_{t-1} + \\alpha\\epsilon_t$$\n$$b_t = b_{t-1} + \\beta\\epsilon_t$$\n\nMatrix form:\n$$\\begin{pmatrix} \\ell_t \\\\ b_t \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} \\ell_{t-1} \\\\ b_{t-1} \\end{pmatrix} + \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix}\\epsilon_t$$\n\n### ETS(M,A,M): Multiplicative Error and Seasonal\n\nMeasurement: $y_t = (\\ell_{t-1} + b_{t-1})s_{t-m}(1 + \\epsilon_t)$\n\nThis means: $\\epsilon_t = \\frac{y_t - (\\ell_{t-1} + b_{t-1})s_{t-m}}{(\\ell_{t-1} + b_{t-1})s_{t-m}}$\n\nState transitions:\n$$\\ell_t = (\\ell_{t-1} + b_{t-1})(1 + \\alpha\\epsilon_t)$$\n$$b_t = b_{t-1} + \\beta(\\ell_{t-1} + b_{t-1})\\epsilon_t$$\n$$s_t = s_{t-m}(1 + \\gamma\\epsilon_t)$$\n\n### Likelihood Function\n\nFor additive errors:\n$$L(\\boldsymbol{\\theta}|\\mathbf{y}) = \\prod_{t=1}^{n}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{\\epsilon_t^2}{2\\sigma^2}\\right)$$\n\nFor multiplicative errors:\n$$L(\\boldsymbol{\\theta}|\\mathbf{y}) = \\prod_{t=1}^{n}\\frac{1}{\\sqrt{2\\pi\\sigma^2}\\mu_t}\\exp\\left(-\\frac{\\epsilon_t^2}{2\\sigma^2}\\right)$$\n\nwhere $\\mu_t$ is the one-step-ahead forecast.\n\n### Prediction Intervals\n\nState space formulation enables analytical or simulation-based prediction intervals:\n\n**Analytical** (for some models):\n$$\\text{Var}(y_{T+h}|y_{1:T}) = \\sigma^2 \\sum_{j=0}^{h-1}c_j^2$$\n\n**Simulation** (general):\n1. Sample future errors $\\epsilon_{T+1}, \\ldots, \\epsilon_{T+h}$\n2. Generate sample paths using state equations\n3. Compute percentiles of forecast distribution\n\n## Algorithm/Model Sketch\n\n**Model Selection with ETS:**\n\n```\n1. Consider all valid ETS combinations:\n   - 30 models total (some multiplicative error combinations unstable)\n   - Stable models: about 15-20 depending on data\n\n2. For each model:\n   - Estimate parameters by MLE\n   - Compute AIC/BIC\n\n3. Select model with lowest AIC (or BIC)\n\n4. Validate:\n   - Check residual diagnostics\n   - Compare forecast accuracy on holdout\n\n5. Generate forecasts with prediction intervals\n```\n\n**Valid/Stable Models:**\n- Multiplicative error requires positive data\n- Some combinations are unstable (e.g., M,Md,M with certain parameters)\n- ETS implementations typically restrict to admissible parameter space\n\n## Common Pitfalls\n\n1. **Assuming ETS = Holt-Winters**: ETS is broader—includes multiplicative error variants and provides proper statistical framework.\n\n2. **Ignoring multiplicative error**: For positive data with variance proportional to level, multiplicative error often fits better.\n\n3. **Model averaging**: Instead of selecting one model, averaging forecasts from multiple ETS models can improve accuracy.\n\n4. **Large seasonal period**: ETS with $m > 24$ is often impractical. Use Fourier terms or TBATS instead.\n\n5. **Negative forecasts**: Additive models can forecast negatives. For positive data, prefer multiplicative components.\n\n6. **Prediction interval coverage**: Check that actual coverage matches nominal (e.g., 95% intervals should contain ~95% of observations).\n\n## Mini Example\n\n```python\nimport numpy as np\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\n# Generate data with trend and multiplicative seasonality\nnp.random.seed(42)\nn = 120\nt = np.arange(n)\nlevel = 100 + 0.5 * t\nseasonal = 1 + 0.2 * np.sin(2 * np.pi * t / 12)\ny = level * seasonal * (1 + 0.05 * np.random.randn(n))\n\n# Fit various ETS models\nmodels = {\n    'ETS(A,A,A)': {'trend': 'add', 'seasonal': 'add'},\n    'ETS(A,A,M)': {'trend': 'add', 'seasonal': 'mul'},\n    'ETS(A,Ad,A)': {'trend': 'add', 'seasonal': 'add', 'damped_trend': True},\n    'ETS(A,Ad,M)': {'trend': 'add', 'seasonal': 'mul', 'damped_trend': True},\n}\n\nresults = {}\nfor name, params in models.items():\n    try:\n        model = ExponentialSmoothing(\n            y,\n            seasonal_periods=12,\n            **params\n        ).fit()\n        results[name] = {'AIC': model.aic, 'BIC': model.bic}\n    except:\n        results[name] = {'AIC': np.inf, 'BIC': np.inf}\n\nprint(\"Model Comparison:\")\nfor name, metrics in sorted(results.items(), key=lambda x: x[1]['AIC']):\n    print(f\"  {name}: AIC={metrics['AIC']:.1f}, BIC={metrics['BIC']:.1f}\")\n\n# Best model\nbest_model = min(results.items(), key=lambda x: x[1]['AIC'])[0]\nprint(f\"\\nBest model by AIC: {best_model}\")\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What is the advantage of the ETS framework over traditional exponential smoothing formulations?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> ETS provides:\n\n1. **Statistical foundation**: State space formulation enables proper likelihood inference\n2. **Model selection**: AIC/BIC for comparing all 30 variants systematically\n3. **Proper prediction intervals**: Based on forecast error distribution, not ad-hoc formulas\n4. **Unified framework**: All exponential smoothing methods in one consistent notation\n5. **Automatic selection**: Can search over models algorithmically\n\nTraditional formulations gave point forecasts but lacked principled interval estimation and model comparison.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using traditional Holt-Winters formulas then computing intervals with ETS assumptions. The interval formulas depend on the error structure — must be consistent.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> When would you prefer multiplicative error (M) over additive error (A)?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Use multiplicative error when:\n\n1. **Variance scales with level**: Higher values have larger absolute errors but similar percentage errors\n2. **Positive data only**: Multiplicative error requires $y_t > 0$\n3. **Percentage errors are meaningful**: Business contexts where 10% error is similar regardless of level\n4. **Heteroskedasticity**: Variance is not constant over time\n\n**Diagnostic:** Plot residuals vs. fitted values. If variance increases with fitted values → multiplicative error.\n\n**Mathematical interpretation:**\n- Additive: $y_t = \\mu_t + \\epsilon_t$ (constant variance)\n- Multiplicative: $y_t = \\mu_t(1 + \\epsilon_t)$ (variance proportional to $\\mu_t^2$)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using additive error for sales/financial data where percentage errors are natural. This underestimates uncertainty at high values.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Write the state space equations for ETS(A,N,N) — simple exponential smoothing.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Measurement equation:**\n$$y_t = \\ell_{t-1} + \\epsilon_t$$\n\n**State transition:**\n$$\\ell_t = \\ell_{t-1} + \\alpha\\epsilon_t$$\n\n**Or equivalently:**\n$$\\ell_t = \\alpha y_t + (1-\\alpha)\\ell_{t-1}$$\n\nThis is exactly SES. The state is just the level $\\ell_t$. Forecast: $\\hat{y}_{t+h|t} = \\ell_t$ for all $h$.\n\n**Variance of h-step forecast error:**\n$$\\text{Var}(y_{t+h} - \\hat{y}_{t+h|t}) = \\sigma^2[1 + (h-1)\\alpha^2]$$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Forgetting that ETS(A,N,N) prediction intervals widen with horizon. Flat forecasts don't mean constant uncertainty.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Why are some ETS model combinations inadmissible or unstable?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Certain combinations lead to:\n\n1. **Negative components**: Multiplicative seasonal with additive trend can produce $\\ell_t + b_t < 0$, making $y_t = (\\ell_t + b_t) \\times s_t$ negative even with positive seasonals.\n\n2. **Explosive variance**: Some multiplicative error combinations have variance that grows exponentially with horizon.\n\n3. **Non-identifiability**: Parameter combinations that produce identical forecasts.\n\n**Specifically problematic:**\n- ETS(M,M,*) — multiplicative trend with multiplicative error can explode\n- ETS(M,*,M) — can give negative forecasts or infinite variance\n\n**Admissible region:** Parameters must satisfy constraints to ensure positive forecasts and bounded variance. Software enforces these.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Manually setting parameters outside admissible bounds. Always use constrained optimization or let software handle admissibility.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You run automatic ETS selection and get ETS(M,Ad,M) with AIC much lower than alternatives. What checks should you perform before accepting this model?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Before accepting:\n\n1. **Check residuals:**\n   - Plot standardized residuals — should look like white noise\n   - ACF of residuals — no significant spikes\n   - Ljung-Box test — fail to reject white noise\n\n2. **Verify assumptions:**\n   - Data is positive (required for multiplicative)\n   - Variance scales with level (justifies M error)\n   - Seasonal pattern is proportional (justifies M seasonal)\n\n3. **Compare forecasts:**\n   - Out-of-sample validation if possible\n   - Do forecasts look reasonable?\n   - Check prediction interval coverage\n\n4. **Parameter reasonableness:**\n   - Damping parameter φ — should be 0.8-0.98\n   - α, β, γ — not at boundaries\n\n5. **Compare to simpler models:**\n   - If ETS(A,A,M) is close, prefer simpler\n   - Log-transform + ETS(A,*,A) might be equivalent\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Accepting complex model without validation. ETS(M,Ad,M) has many parameters — risk of overfitting. Always validate on holdout data.\n</div>\n</div>\n</details>\n\n## References\n\n1. Hyndman, R. J., Koehler, A. B., Snyder, R. D., & Grose, S. (2002). A state space framework for automatic forecasting using exponential smoothing methods. *IJF*, 18(3), 439-454.\n2. Hyndman, R. J., Koehler, A. B., Ord, J. K., & Snyder, R. D. (2008). *Forecasting with Exponential Smoothing: The State Space Approach*. Springer.\n3. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 8.\n4. De Livera, A. M., Hyndman, R. J., & Snyder, R. D. (2011). Forecasting time series with complex seasonal patterns using exponential smoothing. *JASA*, 106(496), 1513-1527.\n", "sections": [{"heading": "ETS Framework", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> ETS (Error-Trend-Seasonal) is a state space framework unifying exponential smoothing methods. Named by three components: Error (A/M), Trend (N/A/Ad/M/Md), Seasonal (N/A/M). Provides likelihood-based estimation and proper prediction intervals. ETS(A,A,A) = additive Holt-Winters. Total of 30 model variants.\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**ETS Taxonomy:**\n\n- **E (Error)**: Additive (A) or Multiplicative (M)\n- **T (Trend)**: None (N), Additive (A), Additive damped (Ad), Multiplicative (M), Multiplicative damped (Md)\n- **S (Seasonal)**: None (N), Additive (A), Multiplicative (M)\n\n**Notation:** ETS(E,T,S)\n\n**Examples:**\n- ETS(A,N,N) = Simple Exponential Smoothing\n- ETS(A,A,N) = Holt's linear\n- ETS(A,Ad,N) = Damped trend\n- ETS(A,A,A) = Additive Holt-Winters\n- ETS(M,A,M) = Multiplicative error, additive trend, multiplicative seasonal", "line_start": 7, "level": 2}, {"heading": "State Space Form", "content": "**General form:**\n$$y_t = w(\\mathbf{x}_{t-1}) + r(\\mathbf{x}_{t-1})\\epsilon_t$$\n$$\\mathbf{x}_t = f(\\mathbf{x}_{t-1}) + g(\\mathbf{x}_{t-1})\\epsilon_t$$\n\nwhere $\\mathbf{x}_t$ is the state vector (level, trend, seasonal components).", "line_start": 26, "level": 3}, {"heading": "ETS(A,A,N): Additive Error, Additive Trend, No Seasonal", "content": "Measurement: $y_t = \\ell_{t-1} + b_{t-1} + \\epsilon_t$\n\nState transitions:\n$$\\ell_t = \\ell_{t-1} + b_{t-1} + \\alpha\\epsilon_t$$\n$$b_t = b_{t-1} + \\beta\\epsilon_t$$\n\nMatrix form:\n$$\\begin{pmatrix} \\ell_t \\\\ b_t \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} \\ell_{t-1} \\\\ b_{t-1} \\end{pmatrix} + \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix}\\epsilon_t$$", "line_start": 34, "level": 3}, {"heading": "ETS(M,A,M): Multiplicative Error and Seasonal", "content": "Measurement: $y_t = (\\ell_{t-1} + b_{t-1})s_{t-m}(1 + \\epsilon_t)$\n\nThis means: $\\epsilon_t = \\frac{y_t - (\\ell_{t-1} + b_{t-1})s_{t-m}}{(\\ell_{t-1} + b_{t-1})s_{t-m}}$\n\nState transitions:\n$$\\ell_t = (\\ell_{t-1} + b_{t-1})(1 + \\alpha\\epsilon_t)$$\n$$b_t = b_{t-1} + \\beta(\\ell_{t-1} + b_{t-1})\\epsilon_t$$\n$$s_t = s_{t-m}(1 + \\gamma\\epsilon_t)$$", "line_start": 45, "level": 3}, {"heading": "Likelihood Function", "content": "For additive errors:\n$$L(\\boldsymbol{\\theta}|\\mathbf{y}) = \\prod_{t=1}^{n}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{\\epsilon_t^2}{2\\sigma^2}\\right)$$\n\nFor multiplicative errors:\n$$L(\\boldsymbol{\\theta}|\\mathbf{y}) = \\prod_{t=1}^{n}\\frac{1}{\\sqrt{2\\pi\\sigma^2}\\mu_t}\\exp\\left(-\\frac{\\epsilon_t^2}{2\\sigma^2}\\right)$$\n\nwhere $\\mu_t$ is the one-step-ahead forecast.", "line_start": 56, "level": 3}, {"heading": "Prediction Intervals", "content": "State space formulation enables analytical or simulation-based prediction intervals:\n\n**Analytical** (for some models):\n$$\\text{Var}(y_{T+h}|y_{1:T}) = \\sigma^2 \\sum_{j=0}^{h-1}c_j^2$$\n\n**Simulation** (general):\n1. Sample future errors $\\epsilon_{T+1}, \\ldots, \\epsilon_{T+h}$\n2. Generate sample paths using state equations\n3. Compute percentiles of forecast distribution", "line_start": 66, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**Model Selection with ETS:**\n\n```\n1. Consider all valid ETS combinations:\n   - 30 models total (some multiplicative error combinations unstable)\n   - Stable models: about 15-20 depending on data\n\n2. For each model:\n   - Estimate parameters by MLE\n   - Compute AIC/BIC\n\n3. Select model with lowest AIC (or BIC)\n\n4. Validate:\n   - Check residual diagnostics\n   - Compare forecast accuracy on holdout\n\n5. Generate forecasts with prediction intervals\n```\n\n**Valid/Stable Models:**\n- Multiplicative error requires positive data\n- Some combinations are unstable (e.g., M,Md,M with certain parameters)\n- ETS implementations typically restrict to admissible parameter space", "line_start": 78, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Assuming ETS = Holt-Winters**: ETS is broader—includes multiplicative error variants and provides proper statistical framework.\n\n2. **Ignoring multiplicative error**: For positive data with variance proportional to level, multiplicative error often fits better.\n\n3. **Model averaging**: Instead of selecting one model, averaging forecasts from multiple ETS models can improve accuracy.\n\n4. **Large seasonal period**: ETS with $m > 24$ is often impractical. Use Fourier terms or TBATS instead.\n\n5. **Negative forecasts**: Additive models can forecast negatives. For positive data, prefer multiplicative components.\n\n6. **Prediction interval coverage**: Check that actual coverage matches nominal (e.g., 95% intervals should contain ~95% of observations).", "line_start": 105, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing", "line_start": 119, "level": 1}, {"heading": "Generate data with trend and multiplicative seasonality", "content": "np.random.seed(42)\nn = 120\nt = np.arange(n)\nlevel = 100 + 0.5 * t\nseasonal = 1 + 0.2 * np.sin(2 * np.pi * t / 12)\ny = level * seasonal * (1 + 0.05 * np.random.randn(n))", "line_start": 125, "level": 1}, {"heading": "Fit various ETS models", "content": "models = {\n    'ETS(A,A,A)': {'trend': 'add', 'seasonal': 'add'},\n    'ETS(A,A,M)': {'trend': 'add', 'seasonal': 'mul'},\n    'ETS(A,Ad,A)': {'trend': 'add', 'seasonal': 'add', 'damped_trend': True},\n    'ETS(A,Ad,M)': {'trend': 'add', 'seasonal': 'mul', 'damped_trend': True},\n}\n\nresults = {}\nfor name, params in models.items():\n    try:\n        model = ExponentialSmoothing(\n            y,\n            seasonal_periods=12,\n            **params\n        ).fit()\n        results[name] = {'AIC': model.aic, 'BIC': model.bic}\n    except:\n        results[name] = {'AIC': np.inf, 'BIC': np.inf}\n\nprint(\"Model Comparison:\")\nfor name, metrics in sorted(results.items(), key=lambda x: x[1]['AIC']):\n    print(f\"  {name}: AIC={metrics['AIC']:.1f}, BIC={metrics['BIC']:.1f}\")", "line_start": 133, "level": 1}, {"heading": "Best model", "content": "best_model = min(results.items(), key=lambda x: x[1]['AIC'])[0]\nprint(f\"\\nBest model by AIC: {best_model}\")\n```", "line_start": 157, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What is the advantage of the ETS framework over traditional exponential smoothing formulations?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> ETS provides:\n\n1. **Statistical foundation**: State space formulation enables proper likelihood inference\n2. **Model selection**: AIC/BIC for comparing all 30 variants systematically\n3. **Proper prediction intervals**: Based on forecast error distribution, not ad-hoc formulas\n4. **Unified framework**: All exponential smoothing methods in one consistent notation\n5. **Automatic selection**: Can search over models algorithmically\n\nTraditional formulations gave point forecasts but lacked principled interval estimation and model comparison.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using traditional Holt-Winters formulas then computing intervals with ETS assumptions. The interval formulas depend on the error structure — must be consistent.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> When would you prefer multiplicative error (M) over additive error (A)?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Use multiplicative error when:\n\n1. **Variance scales with level**: Higher values have larger absolute errors but similar percentage errors\n2. **Positive data only**: Multiplicative error requires $y_t > 0$\n3. **Percentage errors are meaningful**: Business contexts where 10% error is similar regardless of level\n4. **Heteroskedasticity**: Variance is not constant over time\n\n**Diagnostic:** Plot residuals vs. fitted values. If variance increases with fitted values → multiplicative error.\n\n**Mathematical interpretation:**\n- Additive: $y_t = \\mu_t + \\epsilon_t$ (constant variance)\n- Multiplicative: $y_t = \\mu_t(1 + \\epsilon_t)$ (variance proportional to $\\mu_t^2$)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using additive error for sales/financial data where percentage errors are natural. This underestimates uncertainty at high values.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Write the state space equations for ETS(A,N,N) — simple exponential smoothing.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Measurement equation:**\n$$y_t = \\ell_{t-1} + \\epsilon_t$$\n\n**State transition:**\n$$\\ell_t = \\ell_{t-1} + \\alpha\\epsilon_t$$\n\n**Or equivalently:**\n$$\\ell_t = \\alpha y_t + (1-\\alpha)\\ell_{t-1}$$\n\nThis is exactly SES. The state is just the level $\\ell_t$. Forecast: $\\hat{y}_{t+h|t} = \\ell_t$ for all $h$.\n\n**Variance of h-step forecast error:**\n$$\\text{Var}(y_{t+h} - \\hat{y}_{t+h|t}) = \\sigma^2[1 + (h-1)\\alpha^2]$$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Forgetting that ETS(A,N,N) prediction intervals widen with horizon. Flat forecasts don't mean constant uncertainty.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Why are some ETS model combinations inadmissible or unstable?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Certain combinations lead to:\n\n1. **Negative components**: Multiplicative seasonal with additive trend can produce $\\ell_t + b_t < 0$, making $y_t = (\\ell_t + b_t) \\times s_t$ negative even with positive seasonals.\n\n2. **Explosive variance**: Some multiplicative error combinations have variance that grows exponentially with horizon.\n\n3. **Non-identifiability**: Parameter combinations that produce identical forecasts.\n\n**Specifically problematic:**\n- ETS(M,M,*) — multiplicative trend with multiplicative error can explode\n- ETS(M,*,M) — can give negative forecasts or infinite variance\n\n**Admissible region:** Parameters must satisfy constraints to ensure positive forecasts and bounded variance. Software enforces these.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Manually setting parameters outside admissible bounds. Always use constrained optimization or let software handle admissibility.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You run automatic ETS selection and get ETS(M,Ad,M) with AIC much lower than alternatives. What checks should you perform before accepting this model?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Before accepting:\n\n1. **Check residuals:**\n   - Plot standardized residuals — should look like white noise\n   - ACF of residuals — no significant spikes\n   - Ljung-Box test — fail to reject white noise\n\n2. **Verify assumptions:**\n   - Data is positive (required for multiplicative)\n   - Variance scales with level (justifies M error)\n   - Seasonal pattern is proportional (justifies M seasonal)\n\n3. **Compare forecasts:**\n   - Out-of-sample validation if possible\n   - Do forecasts look reasonable?\n   - Check prediction interval coverage\n\n4. **Parameter reasonableness:**\n   - Damping parameter φ — should be 0.8-0.98\n   - α, β, γ — not at boundaries\n\n5. **Compare to simpler models:**\n   - If ETS(A,A,M) is close, prefer simpler\n   - Log-transform + ETS(A,*,A) might be equivalent\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Accepting complex model without validation. ETS(M,Ad,M) has many parameters — risk of overfitting. Always validate on holdout data.\n</div>\n</div>\n</details>", "line_start": 162, "level": 2}, {"heading": "References", "content": "1. Hyndman, R. J., Koehler, A. B., Snyder, R. D., & Grose, S. (2002). A state space framework for automatic forecasting using exponential smoothing methods. *IJF*, 18(3), 439-454.\n2. Hyndman, R. J., Koehler, A. B., Ord, J. K., & Snyder, R. D. (2008). *Forecasting with Exponential Smoothing: The State Space Approach*. Springer.\n3. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 8.\n4. De Livera, A. M., Hyndman, R. J., & Snyder, R. D. (2011). Forecasting time series with complex seasonal patterns using exponential smoothing. *JASA*, 106(496), 1513-1527.", "line_start": 292, "level": 1}]}, "docs/en/exponential-smoothing/holt.md": {"path": "docs/en/exponential-smoothing/holt.md", "title": "Holt's Linear Method", "content": "# Holt's Linear Method\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Holt's method extends SES to capture linear trends using two equations: one for level, one for trend. Forecasts follow a linear trajectory. Two parameters: α (level smoothing) and β (trend smoothing). Equivalent to ARIMA(0,2,2). Use when data shows persistent trend but no seasonality.\n</div>\n\n## Core Definitions\n\n**Holt's Linear Method**:\n\nLevel equation:\n$$\\ell_t = \\alpha y_t + (1-\\alpha)(\\ell_{t-1} + b_{t-1})$$\n\nTrend equation:\n$$b_t = \\beta(\\ell_t - \\ell_{t-1}) + (1-\\beta)b_{t-1}$$\n\nForecast equation:\n$$\\hat{y}_{t+h|t} = \\ell_t + hb_t$$\n\n**Parameters:**\n- $\\alpha \\in (0,1)$: Level smoothing\n- $\\beta \\in (0,1)$: Trend smoothing\n- $\\ell_0$: Initial level\n- $b_0$: Initial trend\n\n**Damped Trend Variant:**\n$$\\hat{y}_{t+h|t} = \\ell_t + (\\phi + \\phi^2 + \\cdots + \\phi^h)b_t$$\n\nwhere $\\phi \\in (0,1)$ dampens the trend for long horizons.\n\n## Math and Derivations\n\n### Forecast Trajectory\n\nFor standard Holt:\n$$\\hat{y}_{t+h|t} = \\ell_t + hb_t$$\n\nThis is linear in $h$ with:\n- Intercept: $\\ell_t$ (current level)\n- Slope: $b_t$ (current trend)\n\n### Connection to ARIMA(0,2,2)\n\nARIMA(0,2,2): $(1-L)^2 y_t = (1+\\theta_1 L + \\theta_2 L^2)\\epsilon_t$\n\nThe relationship:\n$$\\alpha = 1 - \\theta_1 - \\theta_2$$\n$$\\beta = \\frac{-\\theta_2}{1-\\theta_1-\\theta_2}$$\n\n### Damped Trend Forecast\n\n$$\\hat{y}_{t+h|t} = \\ell_t + \\sum_{j=1}^{h}\\phi^j b_t = \\ell_t + \\frac{\\phi(1-\\phi^h)}{1-\\phi}b_t$$\n\nAs $h \\to \\infty$:\n$$\\hat{y}_{t+h|t} \\to \\ell_t + \\frac{\\phi}{1-\\phi}b_t$$\n\nForecasts asymptote to a constant (trend dies out).\n\n### Prediction Intervals\n\nApproximate variance for Holt's method:\n$$\\text{Var}(\\hat{e}_{t+h|t}) \\approx \\sigma^2[1 + (h-1)(\\alpha^2 + \\alpha\\beta h + \\frac{\\beta^2 h(2h-1)}{6})]$$\n\nThis grows faster than SES because trend adds uncertainty.\n\n## Algorithm/Model Sketch\n\n**Holt's Method Algorithm:**\n\n```\nInput: y[1:n], α, β (or optimize)\nOutput: level, trend, forecasts\n\n1. Initialize:\n   ℓ[0] = y[1]\n   b[0] = y[2] - y[1]  (or use regression on first few points)\n\n2. For t = 1 to n:\n   ℓ[t] = α * y[t] + (1-α) * (ℓ[t-1] + b[t-1])\n   b[t] = β * (ℓ[t] - ℓ[t-1]) + (1-β) * b[t-1]\n\n3. For h = 1 to H:\n   forecast[n+h] = ℓ[n] + h * b[n]\n\nReturn forecasts\n```\n\n**When to Use Damped Trend:**\n- Long forecast horizons\n- Trend expected to flatten\n- Historical trend reversals\n- Generally safer for production\n\n## Common Pitfalls\n\n1. **Extrapolating linear trend too far**: Linear trends rarely continue indefinitely. Use damped trend for long horizons.\n\n2. **Using when trend changes sign**: Holt assumes consistent trend direction. Frequent trend reversals confuse the method.\n\n3. **Over-smoothing trend (low β)**: Makes trend too sticky; slow to recognize trend changes.\n\n4. **Under-smoothing trend (high β)**: Makes trend too volatile; noisy trend estimates.\n\n5. **Ignoring negative forecasts**: For positive series, linear extrapolation can predict negatives. Apply transforms or bounds.\n\n6. **Not comparing to damped**: Damped trend often outperforms linear Holt, especially for h > 4.\n\n## Mini Example\n\n```python\nimport numpy as np\nfrom statsmodels.tsa.holtwinters import Holt\n\n# Generate trend + noise data\nnp.random.seed(42)\nn = 100\ntrend = 0.5\ny = 10 + trend * np.arange(n) + np.random.randn(n) * 3\n\n# Fit Holt's linear method\nmodel = Holt(y, initialization_method='estimated')\nfit = model.fit(optimized=True)\n\nprint(f\"Optimal alpha: {fit.params['smoothing_level']:.3f}\")\nprint(f\"Optimal beta: {fit.params['smoothing_trend']:.3f}\")\n\n# Forecast\nforecast = fit.forecast(20)\nprint(f\"Forecast at h=10: {forecast.iloc[9]:.2f}\")\nprint(f\"Forecast at h=20: {forecast.iloc[19]:.2f}\")\n\n# Compare with damped trend\nfit_damped = Holt(y, damped_trend=True, initialization_method='estimated').fit()\nforecast_damped = fit_damped.forecast(20)\nprint(f\"\\nDamped phi: {fit_damped.params['damping_trend']:.3f}\")\nprint(f\"Damped forecast at h=20: {forecast_damped.iloc[19]:.2f}\")\n\n# Note: damped forecast will be lower than linear at h=20\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why does Holt's method need two smoothing parameters while SES only needs one?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> SES models only the level. Holt's models both level and trend, which are separate components that may need different degrees of smoothing.\n\n<strong>Explanation:</strong>\n- Level may change frequently → need responsive α\n- Trend may be stable → need smooth β (or vice versa)\n\nSeparating the parameters allows:\n- Responsive level tracking (high α) + stable trend (low β)\n- Or stable level (low α) + responsive trend (high β)\n\nOne parameter couldn't capture both behaviors.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Setting α = β. These control different aspects; optimizing them independently usually improves forecasts.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Why is damped trend often preferred in practice?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Linear trends rarely persist indefinitely. Damped trend is more realistic because:\n\n1. **Bounded growth**: Real quantities (sales, populations) don't grow linearly forever\n2. **Mean reversion**: Many series return toward long-run average\n3. **Forecast safety**: Prevents extreme predictions at long horizons\n4. **Empirical success**: Often wins forecasting competitions\n\n**Key insight:** Damped trend hedges between \"trend continues\" and \"trend stops,\" which is often closer to reality.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using φ = 1 (no damping) as default. Research shows φ ≈ 0.8-0.98 often optimal. Let optimization choose φ.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> For damped trend with φ = 0.9, derive the long-run forecast limit.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> As h → ∞, the forecast approaches $\\ell_T + \\frac{\\phi}{1-\\phi}b_T = \\ell_T + 9b_T$.\n\n<strong>Derivation:</strong>\n$$\\hat{y}_{T+h|T} = \\ell_T + \\sum_{j=1}^{h}\\phi^j b_T = \\ell_T + b_T\\sum_{j=1}^{h}\\phi^j$$\n\n$$\\sum_{j=1}^{h}\\phi^j = \\phi\\frac{1-\\phi^h}{1-\\phi}$$\n\nAs $h \\to \\infty$ with $|\\phi| < 1$:\n$$\\sum_{j=1}^{\\infty}\\phi^j = \\frac{\\phi}{1-\\phi}$$\n\nFor $\\phi = 0.9$:\n$$\\frac{0.9}{0.1} = 9$$\n\nSo forecast asymptotes to $\\ell_T + 9b_T$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Thinking damped trend means no trend. The trend still contributes; it just doesn't compound indefinitely.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Show that with α = 1 and β = 0, Holt's method reduces to the naive forecast.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> With these parameters:\n\n**Level equation:** $\\ell_t = y_t$ (level = most recent observation)\n\n**Trend equation:** $b_t = b_{t-1}$ (trend never updates from initial value)\n\nIf $b_0 = 0$:\n$$\\hat{y}_{t+h|t} = \\ell_t + h \\cdot 0 = y_t$$\n\nThis is the naive forecast: predict the most recent value.\n\nIf $b_0 \\neq 0$: Still get naive plus a fixed linear trend from initialization.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Extreme parameter values (0 or 1) often indicate model problems. If optimization pushes toward boundaries, reconsider the model or check data quality.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You fit Holt's method and get α = 0.8, β = 0.01. What does this suggest about your data?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n- **High α (0.8)**: Level is volatile; forecasts track recent values closely\n- **Very low β (0.01)**: Trend is very stable; slow to change from initial estimate\n\n**Interpretation:**\nThe data has a persistent, stable trend with volatile fluctuations around it. The model:\n- Quickly adapts level to recent observations\n- Keeps trend nearly constant (essentially using initial trend throughout)\n\n**Consider:**\n1. Is the trend actually constant? Maybe SES + deterministic trend is better\n2. Check if β = 0.01 is at/near boundary → might indicate trend isn't needed\n3. Compare to SES → if similar forecast accuracy, use simpler model\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Very low β might mean Holt's is overfitting — the trend component adds little beyond SES. Compare models with AIC or holdout validation.\n</div>\n</div>\n</details>\n\n## References\n\n1. Holt, C. C. (1957). Forecasting seasonals and trends by exponentially weighted moving averages. ONR Research Memorandum 52.\n2. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 8.\n3. Gardner, E. S., & McKenzie, E. (1985). Forecasting trends in time series. *Management Science*, 31(10), 1237-1246.\n4. Makridakis, S., & Hibon, M. (2000). The M3-Competition. *IJF*, 16(4), 451-476.\n", "sections": [{"heading": "Holt's Linear Method", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Holt's method extends SES to capture linear trends using two equations: one for level, one for trend. Forecasts follow a linear trajectory. Two parameters: α (level smoothing) and β (trend smoothing). Equivalent to ARIMA(0,2,2). Use when data shows persistent trend but no seasonality.\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**Holt's Linear Method**:\n\nLevel equation:\n$$\\ell_t = \\alpha y_t + (1-\\alpha)(\\ell_{t-1} + b_{t-1})$$\n\nTrend equation:\n$$b_t = \\beta(\\ell_t - \\ell_{t-1}) + (1-\\beta)b_{t-1}$$\n\nForecast equation:\n$$\\hat{y}_{t+h|t} = \\ell_t + hb_t$$\n\n**Parameters:**\n- $\\alpha \\in (0,1)$: Level smoothing\n- $\\beta \\in (0,1)$: Trend smoothing\n- $\\ell_0$: Initial level\n- $b_0$: Initial trend\n\n**Damped Trend Variant:**\n$$\\hat{y}_{t+h|t} = \\ell_t + (\\phi + \\phi^2 + \\cdots + \\phi^h)b_t$$\n\nwhere $\\phi \\in (0,1)$ dampens the trend for long horizons.", "line_start": 7, "level": 2}, {"heading": "Forecast Trajectory", "content": "For standard Holt:\n$$\\hat{y}_{t+h|t} = \\ell_t + hb_t$$\n\nThis is linear in $h$ with:\n- Intercept: $\\ell_t$ (current level)\n- Slope: $b_t$ (current trend)", "line_start": 33, "level": 3}, {"heading": "Connection to ARIMA(0,2,2)", "content": "ARIMA(0,2,2): $(1-L)^2 y_t = (1+\\theta_1 L + \\theta_2 L^2)\\epsilon_t$\n\nThe relationship:\n$$\\alpha = 1 - \\theta_1 - \\theta_2$$\n$$\\beta = \\frac{-\\theta_2}{1-\\theta_1-\\theta_2}$$", "line_start": 42, "level": 3}, {"heading": "Damped Trend Forecast", "content": "$$\\hat{y}_{t+h|t} = \\ell_t + \\sum_{j=1}^{h}\\phi^j b_t = \\ell_t + \\frac{\\phi(1-\\phi^h)}{1-\\phi}b_t$$\n\nAs $h \\to \\infty$:\n$$\\hat{y}_{t+h|t} \\to \\ell_t + \\frac{\\phi}{1-\\phi}b_t$$\n\nForecasts asymptote to a constant (trend dies out).", "line_start": 50, "level": 3}, {"heading": "Prediction Intervals", "content": "Approximate variance for Holt's method:\n$$\\text{Var}(\\hat{e}_{t+h|t}) \\approx \\sigma^2[1 + (h-1)(\\alpha^2 + \\alpha\\beta h + \\frac{\\beta^2 h(2h-1)}{6})]$$\n\nThis grows faster than SES because trend adds uncertainty.", "line_start": 59, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**Holt's Method Algorithm:**\n\n```\nInput: y[1:n], α, β (or optimize)\nOutput: level, trend, forecasts\n\n1. Initialize:\n   ℓ[0] = y[1]\n   b[0] = y[2] - y[1]  (or use regression on first few points)\n\n2. For t = 1 to n:\n   ℓ[t] = α * y[t] + (1-α) * (ℓ[t-1] + b[t-1])\n   b[t] = β * (ℓ[t] - ℓ[t-1]) + (1-β) * b[t-1]\n\n3. For h = 1 to H:\n   forecast[n+h] = ℓ[n] + h * b[n]\n\nReturn forecasts\n```\n\n**When to Use Damped Trend:**\n- Long forecast horizons\n- Trend expected to flatten\n- Historical trend reversals\n- Generally safer for production", "line_start": 66, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Extrapolating linear trend too far**: Linear trends rarely continue indefinitely. Use damped trend for long horizons.\n\n2. **Using when trend changes sign**: Holt assumes consistent trend direction. Frequent trend reversals confuse the method.\n\n3. **Over-smoothing trend (low β)**: Makes trend too sticky; slow to recognize trend changes.\n\n4. **Under-smoothing trend (high β)**: Makes trend too volatile; noisy trend estimates.\n\n5. **Ignoring negative forecasts**: For positive series, linear extrapolation can predict negatives. Apply transforms or bounds.\n\n6. **Not comparing to damped**: Damped trend often outperforms linear Holt, especially for h > 4.", "line_start": 94, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.holtwinters import Holt", "line_start": 108, "level": 1}, {"heading": "Generate trend + noise data", "content": "np.random.seed(42)\nn = 100\ntrend = 0.5\ny = 10 + trend * np.arange(n) + np.random.randn(n) * 3", "line_start": 114, "level": 1}, {"heading": "Fit Holt's linear method", "content": "model = Holt(y, initialization_method='estimated')\nfit = model.fit(optimized=True)\n\nprint(f\"Optimal alpha: {fit.params['smoothing_level']:.3f}\")\nprint(f\"Optimal beta: {fit.params['smoothing_trend']:.3f}\")", "line_start": 120, "level": 1}, {"heading": "Forecast", "content": "forecast = fit.forecast(20)\nprint(f\"Forecast at h=10: {forecast.iloc[9]:.2f}\")\nprint(f\"Forecast at h=20: {forecast.iloc[19]:.2f}\")", "line_start": 127, "level": 1}, {"heading": "Compare with damped trend", "content": "fit_damped = Holt(y, damped_trend=True, initialization_method='estimated').fit()\nforecast_damped = fit_damped.forecast(20)\nprint(f\"\\nDamped phi: {fit_damped.params['damping_trend']:.3f}\")\nprint(f\"Damped forecast at h=20: {forecast_damped.iloc[19]:.2f}\")", "line_start": 132, "level": 1}, {"heading": "Note: damped forecast will be lower than linear at h=20", "content": "```", "line_start": 138, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why does Holt's method need two smoothing parameters while SES only needs one?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> SES models only the level. Holt's models both level and trend, which are separate components that may need different degrees of smoothing.\n\n<strong>Explanation:</strong>\n- Level may change frequently → need responsive α\n- Trend may be stable → need smooth β (or vice versa)\n\nSeparating the parameters allows:\n- Responsive level tracking (high α) + stable trend (low β)\n- Or stable level (low α) + responsive trend (high β)\n\nOne parameter couldn't capture both behaviors.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Setting α = β. These control different aspects; optimizing them independently usually improves forecasts.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Why is damped trend often preferred in practice?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Linear trends rarely persist indefinitely. Damped trend is more realistic because:\n\n1. **Bounded growth**: Real quantities (sales, populations) don't grow linearly forever\n2. **Mean reversion**: Many series return toward long-run average\n3. **Forecast safety**: Prevents extreme predictions at long horizons\n4. **Empirical success**: Often wins forecasting competitions\n\n**Key insight:** Damped trend hedges between \"trend continues\" and \"trend stops,\" which is often closer to reality.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using φ = 1 (no damping) as default. Research shows φ ≈ 0.8-0.98 often optimal. Let optimization choose φ.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> For damped trend with φ = 0.9, derive the long-run forecast limit.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> As h → ∞, the forecast approaches $\\ell_T + \\frac{\\phi}{1-\\phi}b_T = \\ell_T + 9b_T$.\n\n<strong>Derivation:</strong>\n$$\\hat{y}_{T+h|T} = \\ell_T + \\sum_{j=1}^{h}\\phi^j b_T = \\ell_T + b_T\\sum_{j=1}^{h}\\phi^j$$\n\n$$\\sum_{j=1}^{h}\\phi^j = \\phi\\frac{1-\\phi^h}{1-\\phi}$$\n\nAs $h \\to \\infty$ with $|\\phi| < 1$:\n$$\\sum_{j=1}^{\\infty}\\phi^j = \\frac{\\phi}{1-\\phi}$$\n\nFor $\\phi = 0.9$:\n$$\\frac{0.9}{0.1} = 9$$\n\nSo forecast asymptotes to $\\ell_T + 9b_T$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Thinking damped trend means no trend. The trend still contributes; it just doesn't compound indefinitely.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Show that with α = 1 and β = 0, Holt's method reduces to the naive forecast.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> With these parameters:\n\n**Level equation:** $\\ell_t = y_t$ (level = most recent observation)\n\n**Trend equation:** $b_t = b_{t-1}$ (trend never updates from initial value)\n\nIf $b_0 = 0$:\n$$\\hat{y}_{t+h|t} = \\ell_t + h \\cdot 0 = y_t$$\n\nThis is the naive forecast: predict the most recent value.\n\nIf $b_0 \\neq 0$: Still get naive plus a fixed linear trend from initialization.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Extreme parameter values (0 or 1) often indicate model problems. If optimization pushes toward boundaries, reconsider the model or check data quality.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You fit Holt's method and get α = 0.8, β = 0.01. What does this suggest about your data?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n- **High α (0.8)**: Level is volatile; forecasts track recent values closely\n- **Very low β (0.01)**: Trend is very stable; slow to change from initial estimate\n\n**Interpretation:**\nThe data has a persistent, stable trend with volatile fluctuations around it. The model:\n- Quickly adapts level to recent observations\n- Keeps trend nearly constant (essentially using initial trend throughout)\n\n**Consider:**\n1. Is the trend actually constant? Maybe SES + deterministic trend is better\n2. Check if β = 0.01 is at/near boundary → might indicate trend isn't needed\n3. Compare to SES → if similar forecast accuracy, use simpler model\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Very low β might mean Holt's is overfitting — the trend component adds little beyond SES. Compare models with AIC or holdout validation.\n</div>\n</div>\n</details>", "line_start": 141, "level": 2}, {"heading": "References", "content": "1. Holt, C. C. (1957). Forecasting seasonals and trends by exponentially weighted moving averages. ONR Research Memorandum 52.\n2. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 8.\n3. Gardner, E. S., & McKenzie, E. (1985). Forecasting trends in time series. *Management Science*, 31(10), 1237-1246.\n4. Makridakis, S., & Hibon, M. (2000). The M3-Competition. *IJF*, 16(4), 451-476.", "line_start": 256, "level": 1}]}, "docs/en/exponential-smoothing/holt-winters.md": {"path": "docs/en/exponential-smoothing/holt-winters.md", "title": "Holt-Winters Method", "content": "# Holt-Winters Method\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Holt-Winters extends Holt's method to handle seasonality. Additive version: $\\hat{y} = \\ell + hb + s_{t+h-m}$. Multiplicative version: $\\hat{y} = (\\ell + hb) \\times s_{t+h-m}$. Three parameters: α (level), β (trend), γ (seasonal). Choose additive when seasonal variation is constant; multiplicative when it scales with level.\n</div>\n\n## Core Definitions\n\n**Holt-Winters Additive Method:**\n\nLevel: $\\ell_t = \\alpha(y_t - s_{t-m}) + (1-\\alpha)(\\ell_{t-1} + b_{t-1})$\n\nTrend: $b_t = \\beta(\\ell_t - \\ell_{t-1}) + (1-\\beta)b_{t-1}$\n\nSeasonal: $s_t = \\gamma(y_t - \\ell_{t-1} - b_{t-1}) + (1-\\gamma)s_{t-m}$\n\nForecast: $\\hat{y}_{t+h|t} = \\ell_t + hb_t + s_{t+h-m(k+1)}$\n\nwhere $k = \\lfloor(h-1)/m\\rfloor$ and $m$ is the seasonal period.\n\n**Holt-Winters Multiplicative Method:**\n\nLevel: $\\ell_t = \\alpha\\frac{y_t}{s_{t-m}} + (1-\\alpha)(\\ell_{t-1} + b_{t-1})$\n\nTrend: $b_t = \\beta(\\ell_t - \\ell_{t-1}) + (1-\\beta)b_{t-1}$\n\nSeasonal: $s_t = \\gamma\\frac{y_t}{\\ell_{t-1} + b_{t-1}} + (1-\\gamma)s_{t-m}$\n\nForecast: $\\hat{y}_{t+h|t} = (\\ell_t + hb_t) \\times s_{t+h-m(k+1)}$\n\n## Math and Derivations\n\n### Additive vs. Multiplicative Seasonality\n\n**Additive**: Seasonal effect is constant amount\n$$y_t = \\ell_t + b_t + s_t + \\epsilon_t$$\n\n**Multiplicative**: Seasonal effect is proportional to level\n$$y_t = (\\ell_t + b_t) \\times s_t \\times \\epsilon_t$$\n\n**Decision rule:**\n- Plot the series: if seasonal swings grow with level → multiplicative\n- If seasonal swings are constant → additive\n- Ratio test: if std(seasonal) / mean(level) is constant → multiplicative\n\n### Seasonal Indices\n\nFor a complete seasonal cycle, indices should:\n- Additive: sum to zero ($\\sum_{j=1}^{m} s_j = 0$)\n- Multiplicative: sum to m ($\\sum_{j=1}^{m} s_j = m$)\n\nNormalization is applied after each update.\n\n### Connection to SARIMA\n\nHolt-Winters additive with no trend is similar to SARIMA(0,1,m+1)(0,1,0)[m].\n\nThe exact equivalence is:\n$$\\text{ARIMA}(0,1,m+1)(0,1,0)_m: (1-L)(1-L^m)y_t = (1+\\theta_1 L + \\cdots + \\theta_{m+1}L^{m+1})\\epsilon_t$$\n\n### Damped Seasonal Variants\n\nCan combine damped trend with seasonal:\n$$\\hat{y}_{t+h|t} = \\ell_t + \\sum_{j=1}^{h}\\phi^j b_t + s_{t+h-m(k+1)}$$\n\nTrend flattens while seasonal pattern continues.\n\n## Algorithm/Model Sketch\n\n**Initialization:**\n\n```\nFor first m observations:\n1. Level: ℓ[0] = average of first seasonal cycle\n2. Trend: b[0] = (average of 2nd cycle - average of 1st cycle) / m\n3. Seasonal indices:\n   - Additive: s[j] = y[j] - ℓ[0] for j = 1,...,m\n   - Multiplicative: s[j] = y[j] / ℓ[0] for j = 1,...,m\n4. Normalize seasonal indices\n```\n\n**Parameter Selection:**\n- Start with α = β = γ = 0.2\n- Optimize to minimize SSE or MAE\n- Typical ranges: α ∈ [0.1, 0.5], β ∈ [0, 0.3], γ ∈ [0, 0.5]\n\n## Common Pitfalls\n\n1. **Wrong seasonality type**: Using additive when multiplicative is appropriate (or vice versa) degrades forecasts significantly.\n\n2. **Insufficient history**: Need at least 2 full seasonal cycles for reliable initialization. More is better.\n\n3. **Multiple seasonalities**: Holt-Winters handles one seasonal period. For multiple (e.g., daily + weekly), consider TBATS or decomposition.\n\n4. **Non-integer periods**: Seasonal period must be integer. For non-integer (e.g., 365.25 days/year), use Fourier terms.\n\n5. **Over-fitting γ**: High γ makes seasonal indices volatile. If the seasonal pattern is stable, use lower γ.\n\n6. **Forgetting normalization**: Seasonal indices can drift without normalization, causing forecast bias.\n\n## Mini Example\n\n```python\nimport numpy as np\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\n# Generate seasonal data with trend\nnp.random.seed(42)\nn = 96  # 8 years of monthly data\nt = np.arange(n)\ntrend = 0.1 * t\nseasonal = 10 * np.sin(2 * np.pi * t / 12)  # Annual seasonality\nnoise = np.random.randn(n) * 2\ny = 50 + trend + seasonal + noise\n\n# Fit additive Holt-Winters\nhw_add = ExponentialSmoothing(\n    y,\n    trend='add',\n    seasonal='add',\n    seasonal_periods=12\n).fit()\n\n# Fit multiplicative Holt-Winters\nhw_mul = ExponentialSmoothing(\n    y,\n    trend='add',\n    seasonal='mul',\n    seasonal_periods=12\n).fit()\n\nprint(\"Additive HW:\")\nprint(f\"  α={hw_add.params['smoothing_level']:.3f}\")\nprint(f\"  β={hw_add.params['smoothing_trend']:.3f}\")\nprint(f\"  γ={hw_add.params['smoothing_seasonal']:.3f}\")\nprint(f\"  AIC={hw_add.aic:.1f}\")\n\nprint(\"\\nMultiplicative HW:\")\nprint(f\"  AIC={hw_mul.aic:.1f}\")\n\n# Forecast\nforecast = hw_add.forecast(12)\nprint(f\"\\n12-month forecast range: [{forecast.min():.1f}, {forecast.max():.1f}]\")\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> How do you decide between additive and multiplicative seasonality?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Examine how seasonal variation changes with the level of the series:\n\n**Additive** if:\n- Seasonal swings (peak-to-trough) are roughly constant over time\n- Percentage variation decreases as level increases\n- Log transformation makes pattern multiplicative\n\n**Multiplicative** if:\n- Seasonal swings grow proportionally with level\n- Percentage variation is constant\n- Log transformation makes pattern additive\n\n**Practical test:**\n1. Plot the series — visual inspection often sufficient\n2. Compute seasonal variation in subperiods — if it grows with mean, use multiplicative\n3. Fit both and compare AIC/BIC\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using additive by default. Many business/economic series have multiplicative seasonality (higher sales → larger seasonal swings).\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Why do seasonal indices need to be normalized?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Without normalization, seasonal indices can drift, causing:\n1. Bias in level estimates\n2. Systematic over/under-forecasting\n3. Indices that no longer represent pure seasonal effects\n\n**Normalization constraints:**\n- Additive: $\\sum s_j = 0$ (seasonal effects cancel over a full cycle)\n- Multiplicative: $\\sum s_j = m$ (average seasonal factor is 1)\n\n**When drift occurs:**\nEach update $s_t = \\gamma(\\cdot) + (1-\\gamma)s_{t-m}$ can gradually shift the indices if the constraint isn't enforced, especially with estimation error.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Most software handles normalization automatically. If implementing manually, forget to normalize after updates → forecasts develop systematic bias.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> For additive Holt-Winters, show that with α = γ = 1 and β = 0, the seasonal index becomes $s_t = y_t - y_{t-m}$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> With these parameters:\n\n**Level equation:**\n$$\\ell_t = (y_t - s_{t-m}) + 0 \\cdot (\\ell_{t-1} + b_{t-1}) = y_t - s_{t-m}$$\n\n**Trend equation:** $b_t = b_{t-1}$ (constant, assume $b_0 = 0$)\n\n**Seasonal equation:**\n$$s_t = 1 \\cdot (y_t - \\ell_{t-1} - b_{t-1}) + 0 \\cdot s_{t-m}$$\n$$= y_t - \\ell_{t-1}$$\n\nSince $\\ell_{t-1} = y_{t-1} - s_{t-1-m}$ and with $s_{t-m} = y_{t-m} - \\ell_{t-m-1}$...\n\nAfter simplification:\n$$s_t = y_t - y_{t-m} + s_{t-m} - s_{t-2m} + \\cdots$$\n\nFor the simple case starting from initialization, this reduces to $s_t \\approx y_t - y_{t-m}$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Extreme parameters (α = 1, γ = 1) lead to overfitting — forecasts chase noise. Optimal parameters are usually well inside (0, 1).\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Write the h-step prediction interval formula for additive Holt-Winters (approximate).</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Approximate 95% prediction interval:\n\n$$\\hat{y}_{t+h|t} \\pm 1.96 \\cdot \\hat{\\sigma} \\cdot \\sqrt{1 + \\sum_{j=1}^{h-1}c_j^2}$$\n\nwhere $c_j$ are coefficients from the MA(∞) representation.\n\n**Simplified approximation:**\n$$\\hat{y}_{t+h|t} \\pm 1.96\\hat{\\sigma}\\sqrt{h + \\text{(trend and seasonal variance terms)}}$$\n\nFor practical use, the variance grows approximately linearly with $h$ for short horizons, then the seasonal component adds periodic variation.\n\n**Software approach:** Most implementations use simulation or state-space model variance formulas for accurate intervals.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Assuming constant prediction interval width. Holt-Winters intervals grow with horizon, though seasonal patterns create periodic widening/narrowing within each cycle.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You're forecasting monthly retail sales with clear December peaks. The peak has grown from $100K above average to $200K above average as total sales doubled. Which version of Holt-Winters should you use?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> **Multiplicative** seasonality, because:\n\n- December peak grew proportionally with overall sales level\n- $100K peak when average was (say) $200K → 50% above average\n- $200K peak when average was $400K → still 50% above average\n- The percentage deviation is constant → multiplicative\n\nWith **additive**, you'd assume December is always \"$150K above average\" (or some fixed amount), which doesn't match the pattern.\n\n**Model:** `ExponentialSmoothing(y, trend='add', seasonal='mul', seasonal_periods=12)`\n\n**Verification:** After fitting, check that multiplicative seasonal indices (as percentages) are roughly stable over time, while additive would show growing indices.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Looking only at absolute seasonal deviations. The key question is whether deviations scale with level. Plot deviations/level ratio over time to check.\n</div>\n</div>\n</details>\n\n## References\n\n1. Winters, P. R. (1960). Forecasting sales by exponentially weighted moving averages. *Management Science*, 6(3), 324-342.\n2. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 8.\n3. Chatfield, C., & Yar, M. (1988). Holt-Winters forecasting: Some practical issues. *The Statistician*, 129-140.\n4. Gardner, E. S. (2006). Exponential smoothing: The state of the art—Part II. *IJF*, 22(4), 637-666.\n", "sections": [{"heading": "Holt-Winters Method", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Holt-Winters extends Holt's method to handle seasonality. Additive version: $\\hat{y} = \\ell + hb + s_{t+h-m}$. Multiplicative version: $\\hat{y} = (\\ell + hb) \\times s_{t+h-m}$. Three parameters: α (level), β (trend), γ (seasonal). Choose additive when seasonal variation is constant; multiplicative when it scales with level.\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**Holt-Winters Additive Method:**\n\nLevel: $\\ell_t = \\alpha(y_t - s_{t-m}) + (1-\\alpha)(\\ell_{t-1} + b_{t-1})$\n\nTrend: $b_t = \\beta(\\ell_t - \\ell_{t-1}) + (1-\\beta)b_{t-1}$\n\nSeasonal: $s_t = \\gamma(y_t - \\ell_{t-1} - b_{t-1}) + (1-\\gamma)s_{t-m}$\n\nForecast: $\\hat{y}_{t+h|t} = \\ell_t + hb_t + s_{t+h-m(k+1)}$\n\nwhere $k = \\lfloor(h-1)/m\\rfloor$ and $m$ is the seasonal period.\n\n**Holt-Winters Multiplicative Method:**\n\nLevel: $\\ell_t = \\alpha\\frac{y_t}{s_{t-m}} + (1-\\alpha)(\\ell_{t-1} + b_{t-1})$\n\nTrend: $b_t = \\beta(\\ell_t - \\ell_{t-1}) + (1-\\beta)b_{t-1}$\n\nSeasonal: $s_t = \\gamma\\frac{y_t}{\\ell_{t-1} + b_{t-1}} + (1-\\gamma)s_{t-m}$\n\nForecast: $\\hat{y}_{t+h|t} = (\\ell_t + hb_t) \\times s_{t+h-m(k+1)}$", "line_start": 7, "level": 2}, {"heading": "Additive vs. Multiplicative Seasonality", "content": "**Additive**: Seasonal effect is constant amount\n$$y_t = \\ell_t + b_t + s_t + \\epsilon_t$$\n\n**Multiplicative**: Seasonal effect is proportional to level\n$$y_t = (\\ell_t + b_t) \\times s_t \\times \\epsilon_t$$\n\n**Decision rule:**\n- Plot the series: if seasonal swings grow with level → multiplicative\n- If seasonal swings are constant → additive\n- Ratio test: if std(seasonal) / mean(level) is constant → multiplicative", "line_start": 33, "level": 3}, {"heading": "Seasonal Indices", "content": "For a complete seasonal cycle, indices should:\n- Additive: sum to zero ($\\sum_{j=1}^{m} s_j = 0$)\n- Multiplicative: sum to m ($\\sum_{j=1}^{m} s_j = m$)\n\nNormalization is applied after each update.", "line_start": 46, "level": 3}, {"heading": "Connection to SARIMA", "content": "Holt-Winters additive with no trend is similar to SARIMA(0,1,m+1)(0,1,0)[m].\n\nThe exact equivalence is:\n$$\\text{ARIMA}(0,1,m+1)(0,1,0)_m: (1-L)(1-L^m)y_t = (1+\\theta_1 L + \\cdots + \\theta_{m+1}L^{m+1})\\epsilon_t$$", "line_start": 54, "level": 3}, {"heading": "Damped Seasonal Variants", "content": "Can combine damped trend with seasonal:\n$$\\hat{y}_{t+h|t} = \\ell_t + \\sum_{j=1}^{h}\\phi^j b_t + s_{t+h-m(k+1)}$$\n\nTrend flattens while seasonal pattern continues.", "line_start": 61, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**Initialization:**\n\n```\nFor first m observations:\n1. Level: ℓ[0] = average of first seasonal cycle\n2. Trend: b[0] = (average of 2nd cycle - average of 1st cycle) / m\n3. Seasonal indices:\n   - Additive: s[j] = y[j] - ℓ[0] for j = 1,...,m\n   - Multiplicative: s[j] = y[j] / ℓ[0] for j = 1,...,m\n4. Normalize seasonal indices\n```\n\n**Parameter Selection:**\n- Start with α = β = γ = 0.2\n- Optimize to minimize SSE or MAE\n- Typical ranges: α ∈ [0.1, 0.5], β ∈ [0, 0.3], γ ∈ [0, 0.5]", "line_start": 68, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Wrong seasonality type**: Using additive when multiplicative is appropriate (or vice versa) degrades forecasts significantly.\n\n2. **Insufficient history**: Need at least 2 full seasonal cycles for reliable initialization. More is better.\n\n3. **Multiple seasonalities**: Holt-Winters handles one seasonal period. For multiple (e.g., daily + weekly), consider TBATS or decomposition.\n\n4. **Non-integer periods**: Seasonal period must be integer. For non-integer (e.g., 365.25 days/year), use Fourier terms.\n\n5. **Over-fitting γ**: High γ makes seasonal indices volatile. If the seasonal pattern is stable, use lower γ.\n\n6. **Forgetting normalization**: Seasonal indices can drift without normalization, causing forecast bias.", "line_start": 87, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing", "line_start": 101, "level": 1}, {"heading": "Generate seasonal data with trend", "content": "np.random.seed(42)\nn = 96  # 8 years of monthly data\nt = np.arange(n)\ntrend = 0.1 * t\nseasonal = 10 * np.sin(2 * np.pi * t / 12)  # Annual seasonality\nnoise = np.random.randn(n) * 2\ny = 50 + trend + seasonal + noise", "line_start": 107, "level": 1}, {"heading": "Fit additive Holt-Winters", "content": "hw_add = ExponentialSmoothing(\n    y,\n    trend='add',\n    seasonal='add',\n    seasonal_periods=12\n).fit()", "line_start": 116, "level": 1}, {"heading": "Fit multiplicative Holt-Winters", "content": "hw_mul = ExponentialSmoothing(\n    y,\n    trend='add',\n    seasonal='mul',\n    seasonal_periods=12\n).fit()\n\nprint(\"Additive HW:\")\nprint(f\"  α={hw_add.params['smoothing_level']:.3f}\")\nprint(f\"  β={hw_add.params['smoothing_trend']:.3f}\")\nprint(f\"  γ={hw_add.params['smoothing_seasonal']:.3f}\")\nprint(f\"  AIC={hw_add.aic:.1f}\")\n\nprint(\"\\nMultiplicative HW:\")\nprint(f\"  AIC={hw_mul.aic:.1f}\")", "line_start": 124, "level": 1}, {"heading": "Forecast", "content": "forecast = hw_add.forecast(12)\nprint(f\"\\n12-month forecast range: [{forecast.min():.1f}, {forecast.max():.1f}]\")\n```", "line_start": 141, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> How do you decide between additive and multiplicative seasonality?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Examine how seasonal variation changes with the level of the series:\n\n**Additive** if:\n- Seasonal swings (peak-to-trough) are roughly constant over time\n- Percentage variation decreases as level increases\n- Log transformation makes pattern multiplicative\n\n**Multiplicative** if:\n- Seasonal swings grow proportionally with level\n- Percentage variation is constant\n- Log transformation makes pattern additive\n\n**Practical test:**\n1. Plot the series — visual inspection often sufficient\n2. Compute seasonal variation in subperiods — if it grows with mean, use multiplicative\n3. Fit both and compare AIC/BIC\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using additive by default. Many business/economic series have multiplicative seasonality (higher sales → larger seasonal swings).\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Why do seasonal indices need to be normalized?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Without normalization, seasonal indices can drift, causing:\n1. Bias in level estimates\n2. Systematic over/under-forecasting\n3. Indices that no longer represent pure seasonal effects\n\n**Normalization constraints:**\n- Additive: $\\sum s_j = 0$ (seasonal effects cancel over a full cycle)\n- Multiplicative: $\\sum s_j = m$ (average seasonal factor is 1)\n\n**When drift occurs:**\nEach update $s_t = \\gamma(\\cdot) + (1-\\gamma)s_{t-m}$ can gradually shift the indices if the constraint isn't enforced, especially with estimation error.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Most software handles normalization automatically. If implementing manually, forget to normalize after updates → forecasts develop systematic bias.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> For additive Holt-Winters, show that with α = γ = 1 and β = 0, the seasonal index becomes $s_t = y_t - y_{t-m}$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> With these parameters:\n\n**Level equation:**\n$$\\ell_t = (y_t - s_{t-m}) + 0 \\cdot (\\ell_{t-1} + b_{t-1}) = y_t - s_{t-m}$$\n\n**Trend equation:** $b_t = b_{t-1}$ (constant, assume $b_0 = 0$)\n\n**Seasonal equation:**\n$$s_t = 1 \\cdot (y_t - \\ell_{t-1} - b_{t-1}) + 0 \\cdot s_{t-m}$$\n$$= y_t - \\ell_{t-1}$$\n\nSince $\\ell_{t-1} = y_{t-1} - s_{t-1-m}$ and with $s_{t-m} = y_{t-m} - \\ell_{t-m-1}$...\n\nAfter simplification:\n$$s_t = y_t - y_{t-m} + s_{t-m} - s_{t-2m} + \\cdots$$\n\nFor the simple case starting from initialization, this reduces to $s_t \\approx y_t - y_{t-m}$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Extreme parameters (α = 1, γ = 1) lead to overfitting — forecasts chase noise. Optimal parameters are usually well inside (0, 1).\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Write the h-step prediction interval formula for additive Holt-Winters (approximate).</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Approximate 95% prediction interval:\n\n$$\\hat{y}_{t+h|t} \\pm 1.96 \\cdot \\hat{\\sigma} \\cdot \\sqrt{1 + \\sum_{j=1}^{h-1}c_j^2}$$\n\nwhere $c_j$ are coefficients from the MA(∞) representation.\n\n**Simplified approximation:**\n$$\\hat{y}_{t+h|t} \\pm 1.96\\hat{\\sigma}\\sqrt{h + \\text{(trend and seasonal variance terms)}}$$\n\nFor practical use, the variance grows approximately linearly with $h$ for short horizons, then the seasonal component adds periodic variation.\n\n**Software approach:** Most implementations use simulation or state-space model variance formulas for accurate intervals.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Assuming constant prediction interval width. Holt-Winters intervals grow with horizon, though seasonal patterns create periodic widening/narrowing within each cycle.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You're forecasting monthly retail sales with clear December peaks. The peak has grown from $100K above average to $200K above average as total sales doubled. Which version of Holt-Winters should you use?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> **Multiplicative** seasonality, because:\n\n- December peak grew proportionally with overall sales level\n- $100K peak when average was (say) $200K → 50% above average\n- $200K peak when average was $400K → still 50% above average\n- The percentage deviation is constant → multiplicative\n\nWith **additive**, you'd assume December is always \"$150K above average\" (or some fixed amount), which doesn't match the pattern.\n\n**Model:** `ExponentialSmoothing(y, trend='add', seasonal='mul', seasonal_periods=12)`\n\n**Verification:** After fitting, check that multiplicative seasonal indices (as percentages) are roughly stable over time, while additive would show growing indices.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Looking only at absolute seasonal deviations. The key question is whether deviations scale with level. Plot deviations/level ratio over time to check.\n</div>\n</div>\n</details>", "line_start": 146, "level": 2}, {"heading": "References", "content": "1. Winters, P. R. (1960). Forecasting sales by exponentially weighted moving averages. *Management Science*, 6(3), 324-342.\n2. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 8.\n3. Chatfield, C., & Yar, M. (1988). Holt-Winters forecasting: Some practical issues. *The Statistician*, 129-140.\n4. Gardner, E. S. (2006). Exponential smoothing: The state of the art—Part II. *IJF*, 22(4), 637-666.", "line_start": 271, "level": 1}]}, "docs/en/exponential-smoothing/ses.md": {"path": "docs/en/exponential-smoothing/ses.md", "title": "Simple Exponential Smoothing (SES)", "content": "# Simple Exponential Smoothing (SES)\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> SES forecasts using weighted average of past observations with exponentially decaying weights. Formula: $\\hat{y}_{t+1} = \\alpha y_t + (1-\\alpha)\\hat{y}_t$. Parameter $\\alpha \\in (0,1)$ controls responsiveness. Equivalent to ARIMA(0,1,1). Best for series with no trend or seasonality. Point forecasts are flat (constant) for all horizons.\n</div>\n\n## Core Definitions\n\n**Simple Exponential Smoothing**:\n$$\\hat{y}_{t+1|t} = \\alpha y_t + (1-\\alpha)\\hat{y}_{t|t-1}$$\n\n**Alternative Forms:**\n\nComponent form:\n$$\\ell_t = \\alpha y_t + (1-\\alpha)\\ell_{t-1}$$\n$$\\hat{y}_{t+h|t} = \\ell_t$$\n\nWeighted average form:\n$$\\hat{y}_{t+1|t} = \\alpha \\sum_{j=0}^{t-1}(1-\\alpha)^j y_{t-j} + (1-\\alpha)^t \\ell_0$$\n\n**Parameters:**\n- $\\alpha \\in (0,1)$: Smoothing parameter\n- $\\ell_0$: Initial level\n\n## Math and Derivations\n\n### Exponential Weights\n\nExpanding the recursion:\n$$\\hat{y}_{t+1|t} = \\alpha y_t + \\alpha(1-\\alpha)y_{t-1} + \\alpha(1-\\alpha)^2 y_{t-2} + \\cdots$$\n\nWeights: $\\alpha, \\alpha(1-\\alpha), \\alpha(1-\\alpha)^2, \\ldots$\n\nThese sum to 1: $\\alpha \\sum_{j=0}^{\\infty}(1-\\alpha)^j = \\alpha \\cdot \\frac{1}{1-(1-\\alpha)} = 1$\n\n### Connection to ARIMA(0,1,1)\n\nARIMA(0,1,1): $(1-L)y_t = (1+\\theta L)\\epsilon_t$\n\nThe optimal forecast is:\n$$\\hat{y}_{t+1|t} = \\hat{y}_{t|t-1} + \\frac{1}{1+\\theta}(y_t - \\hat{y}_{t|t-1})$$\n\nWith $\\alpha = \\frac{1}{1+\\theta}$, this is identical to SES.\n\nFor invertibility ($|\\theta| < 1$): $\\alpha \\in (0.5, 1)$\n\n### Forecast Error Variance\n\nFor ARIMA(0,1,1):\n$$\\text{Var}(y_{t+h} - \\hat{y}_{t+h|t}) = \\sigma^2[1 + (h-1)(1-\\alpha)^2]$$\n\nPrediction interval:\n$$\\hat{y}_{t+h|t} \\pm z_{\\alpha/2}\\sigma\\sqrt{1 + (h-1)(1-\\alpha)^2}$$\n\n### Optimal Smoothing Parameter\n\nChoose $\\alpha$ to minimize sum of squared one-step forecast errors:\n$$\\text{SSE} = \\sum_{t=1}^{n}(y_t - \\hat{y}_{t|t-1})^2$$\n\nNo closed form; use numerical optimization.\n\n## Algorithm/Model Sketch\n\n**SES Algorithm:**\n\n```\nInput: time series y[1:n], smoothing parameter α\nOutput: forecasts\n\n1. Initialize: ℓ[0] = y[1] (or average of first few values)\n\n2. For t = 1 to n:\n   ℓ[t] = α * y[t] + (1-α) * ℓ[t-1]\n   fitted[t] = ℓ[t-1]  # one-step-ahead\n\n3. For h = 1 to H:\n   forecast[n+h] = ℓ[n]  # flat forecast\n\nReturn forecasts\n```\n\n**Selecting α:**\n- $\\alpha \\to 0$: Heavy smoothing, slow response to changes\n- $\\alpha \\to 1$: Light smoothing, forecasts close to most recent observation\n- Typical range: 0.1 to 0.3\n\n## Common Pitfalls\n\n1. **Using SES with trend**: SES produces flat forecasts. For trending data, use Holt's method.\n\n2. **Using SES with seasonality**: SES doesn't capture seasonal patterns. Use Holt-Winters or seasonal decomposition first.\n\n3. **Choosing α arbitrarily**: Always optimize α using historical data or use cross-validation.\n\n4. **Ignoring initialization**: The choice of $\\ell_0$ affects early forecasts. Common choices: $\\ell_0 = y_1$ or $\\ell_0 = \\bar{y}$.\n\n5. **Expecting decreasing forecast intervals**: For SES, prediction intervals grow with horizon (like random walk).\n\n6. **Confusing α interpretation**: High α = less smoothing (more weight on recent data). Some practitioners expect the opposite.\n\n## Mini Example\n\n```python\nimport numpy as np\nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing\n\n# Generate level + noise data (appropriate for SES)\nnp.random.seed(42)\nn = 100\nlevel = 50\ny = level + np.random.randn(n) * 5\n\n# Fit SES with optimization\nmodel = SimpleExpSmoothing(y, initialization_method='estimated')\nfit = model.fit(optimized=True)\n\nprint(f\"Optimal alpha: {fit.params['smoothing_level']:.3f}\")\nprint(f\"Initial level: {fit.params['initial_level']:.2f}\")\n\n# Compare different alpha values\nalphas = [0.1, 0.3, 0.5, 0.9]\nfor alpha in alphas:\n    fit_alpha = model.fit(smoothing_level=alpha, optimized=False)\n    sse = np.sum((y - fit_alpha.fittedvalues)**2)\n    print(f\"Alpha={alpha}: SSE={sse:.1f}\")\n\n# Forecast\nforecast = fit.forecast(10)\nprint(f\"\\n10-step forecast (all same): {forecast.values}\")\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why are SES forecasts constant (flat) for all future horizons?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> SES models the series as a local level plus noise: $y_t = \\ell_t + \\epsilon_t$. The best estimate of future level is the current level $\\ell_T$. With no trend or seasonality modeled, there's no reason to predict change.\n\n<strong>Explanation:</strong>\nThe forecast equation:\n$$\\hat{y}_{T+h|T} = \\ell_T \\text{ for all } h \\geq 1$$\n\nThis assumes the level stays constant. The uncertainty (prediction interval) grows with h, but the point forecast doesn't change.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using flat forecasts for trending data leads to systematic under/over-prediction. Always check if the series has a trend before choosing SES.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Explain the trade-off when choosing the smoothing parameter α.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n- **High α (close to 1)**: More weight on recent observations. Fast response to changes but noisy forecasts. Good for series with frequent level shifts.\n- **Low α (close to 0)**: More weight on distant observations. Smooth forecasts but slow to adapt. Good for stable series with much noise.\n\n**The trade-off:**\n- Responsiveness vs. stability\n- Bias vs. variance (high α = high variance; low α = potential bias if level changes)\n\n**Optimal α:** Balances these considerations; found by minimizing forecast error on historical data.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Assuming lower α is always \"smoother\" and better. In volatile series, low α causes the forecast to lag behind actual level shifts.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Show that the weights in SES sum to 1.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The forecast is $\\hat{y} = \\alpha \\sum_{j=0}^{\\infty}(1-\\alpha)^j y_{t-j}$.\n\n<strong>Derivation:</strong>\nSum of weights:\n$$\\sum_{j=0}^{\\infty}\\alpha(1-\\alpha)^j = \\alpha \\sum_{j=0}^{\\infty}(1-\\alpha)^j$$\n\nThis is a geometric series with ratio $(1-\\alpha)$, where $|1-\\alpha| < 1$:\n$$= \\alpha \\cdot \\frac{1}{1-(1-\\alpha)} = \\alpha \\cdot \\frac{1}{\\alpha} = 1$$\n\n**Key equation:** $\\sum_{j=0}^{\\infty}r^j = \\frac{1}{1-r}$ for $|r| < 1$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> In practice, we don't have infinite history. The \"missing\" weight goes to the initialization $\\ell_0$, which is why initialization matters for short series.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Derive the relationship between SES parameter α and ARIMA(0,1,1) parameter θ.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> $\\alpha = \\frac{1}{1+\\theta}$ or equivalently $\\theta = \\frac{1-\\alpha}{\\alpha}$.\n\n<strong>Derivation:</strong>\nARIMA(0,1,1): $y_t - y_{t-1} = \\epsilon_t + \\theta\\epsilon_{t-1}$\n\nThe optimal h-step forecast:\n$$\\hat{y}_{t+1|t} = y_t + \\theta\\hat{\\epsilon}_t$$\n\nwhere $\\hat{\\epsilon}_t = y_t - \\hat{y}_{t|t-1}$\n\nThis gives:\n$$\\hat{y}_{t+1|t} = y_t + \\theta(y_t - \\hat{y}_{t|t-1}) = (1+\\theta)y_t - \\theta\\hat{y}_{t|t-1}$$\n\nRearranging:\n$$\\hat{y}_{t+1|t} = \\frac{1}{1+\\theta}(1+\\theta)y_t + \\frac{\\theta}{1+\\theta}\\hat{y}_{t|t-1}$$\n\nWith $\\alpha = \\frac{1}{1+\\theta}$ and $1-\\alpha = \\frac{\\theta}{1+\\theta}$, this matches SES.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> ARIMA(0,1,1) with $\\theta > 0$ gives $\\alpha < 0.5$, which is non-invertible. Standard SES with optimized α usually gives $\\alpha > 0.5$ (invertible range).\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You apply SES to monthly sales data and get optimal α = 0.95. What does this suggest about your data? What should you consider?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> α = 0.95 means almost all weight on the most recent observation. This suggests:\n\n1. **High volatility or frequent level shifts** in the data\n2. **Possible trend** that SES is trying to track by being very responsive\n3. **Potential outliers** pulling the optimization toward high α\n4. **Near-random-walk behavior**\n\n**What to consider:**\n1. Check for trend → use Holt's method instead\n2. Look for outliers → they inflate optimal α\n3. Plot the series and fitted values → see if SES is \"chasing\" the data\n4. Try Holt's or Holt-Winters → may give better forecasts with lower α\n5. Consider ARIMA(0,1,1) → the θ would be near 0, confirming random walk\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Very high α often indicates model misspecification (missing trend or seasonality), not that SES is appropriate. Investigate before accepting.\n</div>\n</div>\n</details>\n\n## References\n\n1. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 8.\n2. Hyndman, R. J., Koehler, A. B., Ord, J. K., & Snyder, R. D. (2008). *Forecasting with Exponential Smoothing*. Springer.\n3. Gardner, E. S. (1985). Exponential smoothing: The state of the art. *Journal of Forecasting*, 4(1), 1-28.\n4. Brown, R. G. (1959). *Statistical Forecasting for Inventory Control*. McGraw-Hill.\n", "sections": [{"heading": "Simple Exponential Smoothing (SES)", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> SES forecasts using weighted average of past observations with exponentially decaying weights. Formula: $\\hat{y}_{t+1} = \\alpha y_t + (1-\\alpha)\\hat{y}_t$. Parameter $\\alpha \\in (0,1)$ controls responsiveness. Equivalent to ARIMA(0,1,1). Best for series with no trend or seasonality. Point forecasts are flat (constant) for all horizons.\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**Simple Exponential Smoothing**:\n$$\\hat{y}_{t+1|t} = \\alpha y_t + (1-\\alpha)\\hat{y}_{t|t-1}$$\n\n**Alternative Forms:**\n\nComponent form:\n$$\\ell_t = \\alpha y_t + (1-\\alpha)\\ell_{t-1}$$\n$$\\hat{y}_{t+h|t} = \\ell_t$$\n\nWeighted average form:\n$$\\hat{y}_{t+1|t} = \\alpha \\sum_{j=0}^{t-1}(1-\\alpha)^j y_{t-j} + (1-\\alpha)^t \\ell_0$$\n\n**Parameters:**\n- $\\alpha \\in (0,1)$: Smoothing parameter\n- $\\ell_0$: Initial level", "line_start": 7, "level": 2}, {"heading": "Exponential Weights", "content": "Expanding the recursion:\n$$\\hat{y}_{t+1|t} = \\alpha y_t + \\alpha(1-\\alpha)y_{t-1} + \\alpha(1-\\alpha)^2 y_{t-2} + \\cdots$$\n\nWeights: $\\alpha, \\alpha(1-\\alpha), \\alpha(1-\\alpha)^2, \\ldots$\n\nThese sum to 1: $\\alpha \\sum_{j=0}^{\\infty}(1-\\alpha)^j = \\alpha \\cdot \\frac{1}{1-(1-\\alpha)} = 1$", "line_start": 27, "level": 3}, {"heading": "Connection to ARIMA(0,1,1)", "content": "ARIMA(0,1,1): $(1-L)y_t = (1+\\theta L)\\epsilon_t$\n\nThe optimal forecast is:\n$$\\hat{y}_{t+1|t} = \\hat{y}_{t|t-1} + \\frac{1}{1+\\theta}(y_t - \\hat{y}_{t|t-1})$$\n\nWith $\\alpha = \\frac{1}{1+\\theta}$, this is identical to SES.\n\nFor invertibility ($|\\theta| < 1$): $\\alpha \\in (0.5, 1)$", "line_start": 36, "level": 3}, {"heading": "Forecast Error Variance", "content": "For ARIMA(0,1,1):\n$$\\text{Var}(y_{t+h} - \\hat{y}_{t+h|t}) = \\sigma^2[1 + (h-1)(1-\\alpha)^2]$$\n\nPrediction interval:\n$$\\hat{y}_{t+h|t} \\pm z_{\\alpha/2}\\sigma\\sqrt{1 + (h-1)(1-\\alpha)^2}$$", "line_start": 47, "level": 3}, {"heading": "Optimal Smoothing Parameter", "content": "Choose $\\alpha$ to minimize sum of squared one-step forecast errors:\n$$\\text{SSE} = \\sum_{t=1}^{n}(y_t - \\hat{y}_{t|t-1})^2$$\n\nNo closed form; use numerical optimization.", "line_start": 55, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**SES Algorithm:**\n\n```\nInput: time series y[1:n], smoothing parameter α\nOutput: forecasts\n\n1. Initialize: ℓ[0] = y[1] (or average of first few values)\n\n2. For t = 1 to n:\n   ℓ[t] = α * y[t] + (1-α) * ℓ[t-1]\n   fitted[t] = ℓ[t-1]  # one-step-ahead\n\n3. For h = 1 to H:\n   forecast[n+h] = ℓ[n]  # flat forecast\n\nReturn forecasts\n```\n\n**Selecting α:**\n- $\\alpha \\to 0$: Heavy smoothing, slow response to changes\n- $\\alpha \\to 1$: Light smoothing, forecasts close to most recent observation\n- Typical range: 0.1 to 0.3", "line_start": 62, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Using SES with trend**: SES produces flat forecasts. For trending data, use Holt's method.\n\n2. **Using SES with seasonality**: SES doesn't capture seasonal patterns. Use Holt-Winters or seasonal decomposition first.\n\n3. **Choosing α arbitrarily**: Always optimize α using historical data or use cross-validation.\n\n4. **Ignoring initialization**: The choice of $\\ell_0$ affects early forecasts. Common choices: $\\ell_0 = y_1$ or $\\ell_0 = \\bar{y}$.\n\n5. **Expecting decreasing forecast intervals**: For SES, prediction intervals grow with horizon (like random walk).\n\n6. **Confusing α interpretation**: High α = less smoothing (more weight on recent data). Some practitioners expect the opposite.", "line_start": 87, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing", "line_start": 101, "level": 1}, {"heading": "Generate level + noise data (appropriate for SES)", "content": "np.random.seed(42)\nn = 100\nlevel = 50\ny = level + np.random.randn(n) * 5", "line_start": 107, "level": 1}, {"heading": "Fit SES with optimization", "content": "model = SimpleExpSmoothing(y, initialization_method='estimated')\nfit = model.fit(optimized=True)\n\nprint(f\"Optimal alpha: {fit.params['smoothing_level']:.3f}\")\nprint(f\"Initial level: {fit.params['initial_level']:.2f}\")", "line_start": 113, "level": 1}, {"heading": "Compare different alpha values", "content": "alphas = [0.1, 0.3, 0.5, 0.9]\nfor alpha in alphas:\n    fit_alpha = model.fit(smoothing_level=alpha, optimized=False)\n    sse = np.sum((y - fit_alpha.fittedvalues)**2)\n    print(f\"Alpha={alpha}: SSE={sse:.1f}\")", "line_start": 120, "level": 1}, {"heading": "Forecast", "content": "forecast = fit.forecast(10)\nprint(f\"\\n10-step forecast (all same): {forecast.values}\")\n```", "line_start": 127, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why are SES forecasts constant (flat) for all future horizons?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> SES models the series as a local level plus noise: $y_t = \\ell_t + \\epsilon_t$. The best estimate of future level is the current level $\\ell_T$. With no trend or seasonality modeled, there's no reason to predict change.\n\n<strong>Explanation:</strong>\nThe forecast equation:\n$$\\hat{y}_{T+h|T} = \\ell_T \\text{ for all } h \\geq 1$$\n\nThis assumes the level stays constant. The uncertainty (prediction interval) grows with h, but the point forecast doesn't change.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using flat forecasts for trending data leads to systematic under/over-prediction. Always check if the series has a trend before choosing SES.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Explain the trade-off when choosing the smoothing parameter α.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n- **High α (close to 1)**: More weight on recent observations. Fast response to changes but noisy forecasts. Good for series with frequent level shifts.\n- **Low α (close to 0)**: More weight on distant observations. Smooth forecasts but slow to adapt. Good for stable series with much noise.\n\n**The trade-off:**\n- Responsiveness vs. stability\n- Bias vs. variance (high α = high variance; low α = potential bias if level changes)\n\n**Optimal α:** Balances these considerations; found by minimizing forecast error on historical data.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Assuming lower α is always \"smoother\" and better. In volatile series, low α causes the forecast to lag behind actual level shifts.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Show that the weights in SES sum to 1.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The forecast is $\\hat{y} = \\alpha \\sum_{j=0}^{\\infty}(1-\\alpha)^j y_{t-j}$.\n\n<strong>Derivation:</strong>\nSum of weights:\n$$\\sum_{j=0}^{\\infty}\\alpha(1-\\alpha)^j = \\alpha \\sum_{j=0}^{\\infty}(1-\\alpha)^j$$\n\nThis is a geometric series with ratio $(1-\\alpha)$, where $|1-\\alpha| < 1$:\n$$= \\alpha \\cdot \\frac{1}{1-(1-\\alpha)} = \\alpha \\cdot \\frac{1}{\\alpha} = 1$$\n\n**Key equation:** $\\sum_{j=0}^{\\infty}r^j = \\frac{1}{1-r}$ for $|r| < 1$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> In practice, we don't have infinite history. The \"missing\" weight goes to the initialization $\\ell_0$, which is why initialization matters for short series.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Derive the relationship between SES parameter α and ARIMA(0,1,1) parameter θ.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> $\\alpha = \\frac{1}{1+\\theta}$ or equivalently $\\theta = \\frac{1-\\alpha}{\\alpha}$.\n\n<strong>Derivation:</strong>\nARIMA(0,1,1): $y_t - y_{t-1} = \\epsilon_t + \\theta\\epsilon_{t-1}$\n\nThe optimal h-step forecast:\n$$\\hat{y}_{t+1|t} = y_t + \\theta\\hat{\\epsilon}_t$$\n\nwhere $\\hat{\\epsilon}_t = y_t - \\hat{y}_{t|t-1}$\n\nThis gives:\n$$\\hat{y}_{t+1|t} = y_t + \\theta(y_t - \\hat{y}_{t|t-1}) = (1+\\theta)y_t - \\theta\\hat{y}_{t|t-1}$$\n\nRearranging:\n$$\\hat{y}_{t+1|t} = \\frac{1}{1+\\theta}(1+\\theta)y_t + \\frac{\\theta}{1+\\theta}\\hat{y}_{t|t-1}$$\n\nWith $\\alpha = \\frac{1}{1+\\theta}$ and $1-\\alpha = \\frac{\\theta}{1+\\theta}$, this matches SES.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> ARIMA(0,1,1) with $\\theta > 0$ gives $\\alpha < 0.5$, which is non-invertible. Standard SES with optimized α usually gives $\\alpha > 0.5$ (invertible range).\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You apply SES to monthly sales data and get optimal α = 0.95. What does this suggest about your data? What should you consider?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> α = 0.95 means almost all weight on the most recent observation. This suggests:\n\n1. **High volatility or frequent level shifts** in the data\n2. **Possible trend** that SES is trying to track by being very responsive\n3. **Potential outliers** pulling the optimization toward high α\n4. **Near-random-walk behavior**\n\n**What to consider:**\n1. Check for trend → use Holt's method instead\n2. Look for outliers → they inflate optimal α\n3. Plot the series and fitted values → see if SES is \"chasing\" the data\n4. Try Holt's or Holt-Winters → may give better forecasts with lower α\n5. Consider ARIMA(0,1,1) → the θ would be near 0, confirming random walk\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Very high α often indicates model misspecification (missing trend or seasonality), not that SES is appropriate. Investigate before accepting.\n</div>\n</div>\n</details>", "line_start": 132, "level": 2}, {"heading": "References", "content": "1. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 8.\n2. Hyndman, R. J., Koehler, A. B., Ord, J. K., & Snyder, R. D. (2008). *Forecasting with Exponential Smoothing*. Springer.\n3. Gardner, E. S. (1985). Exponential smoothing: The state of the art. *Journal of Forecasting*, 4(1), 1-28.\n4. Brown, R. G. (1959). *Statistical Forecasting for Inventory Control*. McGraw-Hill.", "line_start": 245, "level": 1}]}, "docs/en/forecasting/prediction-intervals.md": {"path": "docs/en/forecasting/prediction-intervals.md", "title": "Prediction Intervals", "content": "# Prediction Intervals\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Prediction intervals quantify forecast uncertainty, giving a range where future values will likely fall. For ARIMA: PI width grows with horizon due to accumulated uncertainty. Key formula: $\\hat{y}_{T+h} \\pm z_{\\alpha/2}\\sigma_h$ where $\\sigma_h$ depends on model. Intervals assume normality; bootstrap provides non-parametric alternative.\n</div>\n\n## Core Definitions\n\n**Point Forecast:** Single best estimate of future value\n$$\\hat{y}_{T+h|T} = E[y_{T+h}|y_1,\\ldots,y_T]$$\n\n**Prediction Interval:** Range containing future value with probability $(1-\\alpha)$\n$$[\\hat{y}_{T+h|T} - z_{\\alpha/2}\\sigma_h, \\hat{y}_{T+h|T} + z_{\\alpha/2}\\sigma_h]$$\n\n**Forecast Error:** $e_{T+h|T} = y_{T+h} - \\hat{y}_{T+h|T}$\n\n**Forecast Variance:** $\\sigma_h^2 = \\text{Var}(e_{T+h|T})$\n\n**Coverage Probability:** Proportion of actual values falling within PI (should match nominal level).\n\n## Math and Derivations\n\n### ARMA(p,q) Forecast Variance\n\nThe h-step forecast error can be written as:\n$$e_{T+h|T} = \\sum_{j=0}^{h-1}\\psi_j\\epsilon_{T+h-j}$$\n\nwhere $\\psi_j$ are MA(∞) coefficients.\n\nForecast variance:\n$$\\sigma_h^2 = \\sigma_\\epsilon^2\\sum_{j=0}^{h-1}\\psi_j^2$$\n\n### Specific Models\n\n**AR(1):** $y_t = \\phi y_{t-1} + \\epsilon_t$\n$$\\psi_j = \\phi^j$$\n$$\\sigma_h^2 = \\sigma_\\epsilon^2\\frac{1-\\phi^{2h}}{1-\\phi^2}$$\n\nAs $h \\to \\infty$: $\\sigma_h^2 \\to \\sigma_\\epsilon^2/(1-\\phi^2) = \\text{Var}(y_t)$\n\n**MA(1):** $y_t = \\epsilon_t + \\theta\\epsilon_{t-1}$\n$$\\sigma_1^2 = \\sigma_\\epsilon^2$$\n$$\\sigma_h^2 = \\sigma_\\epsilon^2(1+\\theta^2) \\text{ for } h \\geq 2$$\n\n**Random Walk (ARIMA(0,1,0)):**\n$$\\sigma_h^2 = h\\sigma_\\epsilon^2$$\n\nVariance grows linearly; PI width grows as $\\sqrt{h}$.\n\n### Gaussian Prediction Intervals\n\nUnder normality:\n$$y_{T+h}|y_{1:T} \\sim N(\\hat{y}_{T+h|T}, \\sigma_h^2)$$\n\n95% PI: $\\hat{y}_{T+h|T} \\pm 1.96\\sigma_h$\n80% PI: $\\hat{y}_{T+h|T} \\pm 1.28\\sigma_h$\n\n### Accounting for Parameter Uncertainty\n\nWhen parameters are estimated, additional uncertainty:\n$$\\text{Var}(e_{T+h|T}) \\approx \\sigma_h^2 + \\frac{\\sigma_h^2}{n}\\sum_{j=0}^{h-1}\\left(\\frac{\\partial\\psi_j}{\\partial\\theta}\\right)^2\\text{Var}(\\hat{\\theta})$$\n\nFor large samples, parameter uncertainty is small relative to intrinsic forecast uncertainty.\n\n## Algorithm/Model Sketch\n\n**Computing Prediction Intervals:**\n\n```\n1. Fit model, estimate parameters θ̂ and σ̂²\n2. For each horizon h = 1, ..., H:\n   a. Compute point forecast ŷ_{T+h|T}\n   b. Calculate ψ₀, ψ₁, ..., ψ_{h-1} (MA coefficients)\n   c. Compute σ̂ₕ² = σ̂² × Σψⱼ²\n   d. Form interval: ŷ_{T+h|T} ± z_{α/2} × σ̂ₕ\n\n3. For non-normal data, use bootstrap:\n   a. Generate B bootstrap samples\n   b. Refit model on each\n   c. Generate forecasts\n   d. Take percentiles of forecast distribution\n```\n\n**Bootstrap Prediction Intervals:**\n```\nFor b = 1 to B:\n   1. Sample residuals with replacement: ε*[1:n]\n   2. Generate bootstrap series y* using model\n   3. Refit model to y*\n   4. Generate forecasts ŷ*_{T+1:T+H}\n\nTake 2.5% and 97.5% percentiles → 95% PI\n```\n\n## Common Pitfalls\n\n1. **Ignoring PI widening**: For I(d) processes, PIs grow without bound. Don't expect tight long-range forecasts.\n\n2. **Assuming constant width**: Only stationary AR(∞) processes have bounded PI. Most models have widening intervals.\n\n3. **Undercoverage**: If actual coverage < nominal, model may be misspecified or variance underestimated.\n\n4. **Overcoverage**: If actual coverage >> nominal, model may be over-conservative or wrong distributional assumption.\n\n5. **Non-normality**: For skewed or heavy-tailed data, Gaussian PIs may be too narrow. Use bootstrap.\n\n6. **Ignoring parameter uncertainty**: Small sample → parameter estimates uncertain → PIs wider than formula suggests.\n\n## Mini Example\n\n```python\nimport numpy as np\nfrom statsmodels.tsa.arima.model import ARIMA\n\n# Generate AR(1) data\nnp.random.seed(42)\nphi = 0.7\nsigma = 1.0\nn = 200\ny = np.zeros(n)\nfor t in range(1, n):\n    y[t] = phi * y[t-1] + np.random.randn() * sigma\n\n# Fit model\nmodel = ARIMA(y, order=(1, 0, 0)).fit()\n\n# Get forecasts with prediction intervals\nforecast_obj = model.get_forecast(steps=20)\nforecast = forecast_obj.predicted_mean\nconf_int = forecast_obj.conf_int(alpha=0.05)  # 95% PI\n\nprint(\"Forecast with 95% PI:\")\nfor h in [1, 5, 10, 20]:\n    print(f\"  h={h}: {forecast.iloc[h-1]:.2f} \"\n          f\"[{conf_int.iloc[h-1, 0]:.2f}, {conf_int.iloc[h-1, 1]:.2f}]\")\n\n# Theoretical width for AR(1)\nphi_hat = model.arparams[0]\nsigma_hat = np.sqrt(model.scale)\nfor h in [1, 5, 10, 20]:\n    var_h = sigma_hat**2 * (1 - phi_hat**(2*h)) / (1 - phi_hat**2)\n    width_theory = 2 * 1.96 * np.sqrt(var_h)\n    width_actual = conf_int.iloc[h-1, 1] - conf_int.iloc[h-1, 0]\n    print(f\"h={h}: Theory width={width_theory:.2f}, Actual={width_actual:.2f}\")\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why do prediction intervals widen with forecast horizon for most time series models?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Future shocks are unknown and accumulate over time.\n\nFor h-step forecast, we don't know $\\epsilon_{T+1}, \\ldots, \\epsilon_{T+h}$. The forecast error:\n$$e_{T+h|T} = \\sum_{j=0}^{h-1}\\psi_j\\epsilon_{T+h-j}$$\n\nMore unknown shocks → more variance → wider interval.\n\n**Exceptions:**\n- Mean-reverting processes (stationary AR) converge to unconditional variance\n- But for random walk (unit root), variance grows linearly with h\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Expecting tight long-range forecasts. For I(1) processes, 1-year-ahead PI is much wider than 1-day-ahead. This is fundamental, not a modeling failure.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> What does it mean if your prediction intervals have 85% coverage when they're supposed to have 95%?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> **Undercoverage** — actual values fall outside the PI more often than expected.\n\n**Possible causes:**\n1. **Model misspecification**: True process not captured (missing components)\n2. **Non-normality**: Heavy tails cause more extreme values\n3. **Variance underestimated**: $\\hat{\\sigma}$ too small\n4. **Structural changes**: Model fit to stable period, tested on volatile period\n5. **Parameter uncertainty ignored**: Especially problematic in small samples\n\n**Remedies:**\n- Use bootstrap PIs\n- Check residual diagnostics\n- Consider heavier-tailed distributions\n- Widen intervals manually (e.g., use 99% for conservative 95%)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Trusting nominal coverage without validation. Always check empirical coverage on holdout data.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Derive the h-step forecast variance for ARIMA(0,1,0) (random walk).</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> $\\sigma_h^2 = h\\sigma_\\epsilon^2$\n\n**Derivation:**\nRandom walk: $y_t = y_{t-1} + \\epsilon_t$\n\nThe h-step forecast:\n$$y_{T+h} = y_T + \\sum_{j=1}^{h}\\epsilon_{T+j}$$\n\nBest forecast: $\\hat{y}_{T+h|T} = y_T$ (current value)\n\nForecast error:\n$$e_{T+h|T} = y_{T+h} - y_T = \\sum_{j=1}^{h}\\epsilon_{T+j}$$\n\nSince $\\epsilon_j$ are independent:\n$$\\text{Var}(e_{T+h|T}) = \\sum_{j=1}^{h}\\sigma_\\epsilon^2 = h\\sigma_\\epsilon^2$$\n\n**PI:** $y_T \\pm 1.96\\sigma_\\epsilon\\sqrt{h}$\n\nWidth grows as $\\sqrt{h}$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> For random walk, long-horizon PIs become very wide. A 100-step-ahead 95% PI is 10× wider than 1-step-ahead.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Why does a stationary AR(1) have bounded prediction interval width as $h \\to \\infty$?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> For stationary AR(1) with $|\\phi| < 1$:\n\n$$\\sigma_h^2 = \\sigma_\\epsilon^2\\frac{1-\\phi^{2h}}{1-\\phi^2}$$\n\nAs $h \\to \\infty$: $\\phi^{2h} \\to 0$\n\n$$\\lim_{h\\to\\infty}\\sigma_h^2 = \\frac{\\sigma_\\epsilon^2}{1-\\phi^2} = \\text{Var}(y_t)$$\n\n**Intuition:** For stationary processes, distant future values are independent of current observation. The forecast converges to unconditional mean, and uncertainty converges to unconditional variance.\n\nThe PI width converges to $2 \\times 1.96 \\times \\sqrt{\\text{Var}(y_t)}$ — the interval you'd give without any data.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Bounded PI doesn't mean narrow PI. The unconditional variance can still be large, especially for $\\phi$ near 1.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> Your model produces 95% prediction intervals, but stakeholders want to know the \"worst case.\" How do you translate PIs for business use?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Several approaches:\n\n1. **Use higher confidence level**: 99% PI gives more conservative bounds\n   - Upper 99% PI ≈ upper bound for planning\n\n2. **Report specific percentiles**:\n   - \"95% chance demand is below X\"\n   - \"5% chance it exceeds Y\"\n\n3. **Scenario analysis**:\n   - Best case: lower 80% bound\n   - Expected: point forecast\n   - Worst case: upper 95% or 99% bound\n\n4. **Distribution summary**:\n   - Most likely range: 50% PI\n   - Reasonable range: 80% PI\n   - Extreme scenarios: 95% PI\n\n5. **Risk quantiles**: \"10% chance of loss exceeding $Z\"\n\n**Key message:** PIs are probability statements. \"Worst case\" depends on acceptable risk level.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using upper 95% bound as \"worst case\" — this still has 2.5% chance of being exceeded. For true tail risk, use higher percentiles or extreme value methods.\n</div>\n</div>\n</details>\n\n## References\n\n1. Chatfield, C. (1993). Calculating interval forecasts. *Journal of Business & Economic Statistics*, 11(2), 121-135.\n2. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 5.\n3. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis*. Wiley. Chapter 5.\n4. Thombs, L. A., & Schucany, W. R. (1990). Bootstrap prediction intervals for autoregression. *JASA*, 85(410), 486-492.\n", "sections": [{"heading": "Prediction Intervals", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Prediction intervals quantify forecast uncertainty, giving a range where future values will likely fall. For ARIMA: PI width grows with horizon due to accumulated uncertainty. Key formula: $\\hat{y}_{T+h} \\pm z_{\\alpha/2}\\sigma_h$ where $\\sigma_h$ depends on model. Intervals assume normality; bootstrap provides non-parametric alternative.\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**Point Forecast:** Single best estimate of future value\n$$\\hat{y}_{T+h|T} = E[y_{T+h}|y_1,\\ldots,y_T]$$\n\n**Prediction Interval:** Range containing future value with probability $(1-\\alpha)$\n$$[\\hat{y}_{T+h|T} - z_{\\alpha/2}\\sigma_h, \\hat{y}_{T+h|T} + z_{\\alpha/2}\\sigma_h]$$\n\n**Forecast Error:** $e_{T+h|T} = y_{T+h} - \\hat{y}_{T+h|T}$\n\n**Forecast Variance:** $\\sigma_h^2 = \\text{Var}(e_{T+h|T})$\n\n**Coverage Probability:** Proportion of actual values falling within PI (should match nominal level).", "line_start": 7, "level": 2}, {"heading": "ARMA(p,q) Forecast Variance", "content": "The h-step forecast error can be written as:\n$$e_{T+h|T} = \\sum_{j=0}^{h-1}\\psi_j\\epsilon_{T+h-j}$$\n\nwhere $\\psi_j$ are MA(∞) coefficients.\n\nForecast variance:\n$$\\sigma_h^2 = \\sigma_\\epsilon^2\\sum_{j=0}^{h-1}\\psi_j^2$$", "line_start": 23, "level": 3}, {"heading": "Specific Models", "content": "**AR(1):** $y_t = \\phi y_{t-1} + \\epsilon_t$\n$$\\psi_j = \\phi^j$$\n$$\\sigma_h^2 = \\sigma_\\epsilon^2\\frac{1-\\phi^{2h}}{1-\\phi^2}$$\n\nAs $h \\to \\infty$: $\\sigma_h^2 \\to \\sigma_\\epsilon^2/(1-\\phi^2) = \\text{Var}(y_t)$\n\n**MA(1):** $y_t = \\epsilon_t + \\theta\\epsilon_{t-1}$\n$$\\sigma_1^2 = \\sigma_\\epsilon^2$$\n$$\\sigma_h^2 = \\sigma_\\epsilon^2(1+\\theta^2) \\text{ for } h \\geq 2$$\n\n**Random Walk (ARIMA(0,1,0)):**\n$$\\sigma_h^2 = h\\sigma_\\epsilon^2$$\n\nVariance grows linearly; PI width grows as $\\sqrt{h}$.", "line_start": 33, "level": 3}, {"heading": "Gaussian Prediction Intervals", "content": "Under normality:\n$$y_{T+h}|y_{1:T} \\sim N(\\hat{y}_{T+h|T}, \\sigma_h^2)$$\n\n95% PI: $\\hat{y}_{T+h|T} \\pm 1.96\\sigma_h$\n80% PI: $\\hat{y}_{T+h|T} \\pm 1.28\\sigma_h$", "line_start": 50, "level": 3}, {"heading": "Accounting for Parameter Uncertainty", "content": "When parameters are estimated, additional uncertainty:\n$$\\text{Var}(e_{T+h|T}) \\approx \\sigma_h^2 + \\frac{\\sigma_h^2}{n}\\sum_{j=0}^{h-1}\\left(\\frac{\\partial\\psi_j}{\\partial\\theta}\\right)^2\\text{Var}(\\hat{\\theta})$$\n\nFor large samples, parameter uncertainty is small relative to intrinsic forecast uncertainty.", "line_start": 58, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**Computing Prediction Intervals:**\n\n```\n1. Fit model, estimate parameters θ̂ and σ̂²\n2. For each horizon h = 1, ..., H:\n   a. Compute point forecast ŷ_{T+h|T}\n   b. Calculate ψ₀, ψ₁, ..., ψ_{h-1} (MA coefficients)\n   c. Compute σ̂ₕ² = σ̂² × Σψⱼ²\n   d. Form interval: ŷ_{T+h|T} ± z_{α/2} × σ̂ₕ\n\n3. For non-normal data, use bootstrap:\n   a. Generate B bootstrap samples\n   b. Refit model on each\n   c. Generate forecasts\n   d. Take percentiles of forecast distribution\n```\n\n**Bootstrap Prediction Intervals:**\n```\nFor b = 1 to B:\n   1. Sample residuals with replacement: ε*[1:n]\n   2. Generate bootstrap series y* using model\n   3. Refit model to y*\n   4. Generate forecasts ŷ*_{T+1:T+H}\n\nTake 2.5% and 97.5% percentiles → 95% PI\n```", "line_start": 65, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Ignoring PI widening**: For I(d) processes, PIs grow without bound. Don't expect tight long-range forecasts.\n\n2. **Assuming constant width**: Only stationary AR(∞) processes have bounded PI. Most models have widening intervals.\n\n3. **Undercoverage**: If actual coverage < nominal, model may be misspecified or variance underestimated.\n\n4. **Overcoverage**: If actual coverage >> nominal, model may be over-conservative or wrong distributional assumption.\n\n5. **Non-normality**: For skewed or heavy-tailed data, Gaussian PIs may be too narrow. Use bootstrap.\n\n6. **Ignoring parameter uncertainty**: Small sample → parameter estimates uncertain → PIs wider than formula suggests.", "line_start": 95, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.arima.model import ARIMA", "line_start": 109, "level": 1}, {"heading": "Generate AR(1) data", "content": "np.random.seed(42)\nphi = 0.7\nsigma = 1.0\nn = 200\ny = np.zeros(n)\nfor t in range(1, n):\n    y[t] = phi * y[t-1] + np.random.randn() * sigma", "line_start": 115, "level": 1}, {"heading": "Fit model", "content": "model = ARIMA(y, order=(1, 0, 0)).fit()", "line_start": 124, "level": 1}, {"heading": "Get forecasts with prediction intervals", "content": "forecast_obj = model.get_forecast(steps=20)\nforecast = forecast_obj.predicted_mean\nconf_int = forecast_obj.conf_int(alpha=0.05)  # 95% PI\n\nprint(\"Forecast with 95% PI:\")\nfor h in [1, 5, 10, 20]:\n    print(f\"  h={h}: {forecast.iloc[h-1]:.2f} \"\n          f\"[{conf_int.iloc[h-1, 0]:.2f}, {conf_int.iloc[h-1, 1]:.2f}]\")", "line_start": 127, "level": 1}, {"heading": "Theoretical width for AR(1)", "content": "phi_hat = model.arparams[0]\nsigma_hat = np.sqrt(model.scale)\nfor h in [1, 5, 10, 20]:\n    var_h = sigma_hat**2 * (1 - phi_hat**(2*h)) / (1 - phi_hat**2)\n    width_theory = 2 * 1.96 * np.sqrt(var_h)\n    width_actual = conf_int.iloc[h-1, 1] - conf_int.iloc[h-1, 0]\n    print(f\"h={h}: Theory width={width_theory:.2f}, Actual={width_actual:.2f}\")\n```", "line_start": 137, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why do prediction intervals widen with forecast horizon for most time series models?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Future shocks are unknown and accumulate over time.\n\nFor h-step forecast, we don't know $\\epsilon_{T+1}, \\ldots, \\epsilon_{T+h}$. The forecast error:\n$$e_{T+h|T} = \\sum_{j=0}^{h-1}\\psi_j\\epsilon_{T+h-j}$$\n\nMore unknown shocks → more variance → wider interval.\n\n**Exceptions:**\n- Mean-reverting processes (stationary AR) converge to unconditional variance\n- But for random walk (unit root), variance grows linearly with h\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Expecting tight long-range forecasts. For I(1) processes, 1-year-ahead PI is much wider than 1-day-ahead. This is fundamental, not a modeling failure.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> What does it mean if your prediction intervals have 85% coverage when they're supposed to have 95%?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> **Undercoverage** — actual values fall outside the PI more often than expected.\n\n**Possible causes:**\n1. **Model misspecification**: True process not captured (missing components)\n2. **Non-normality**: Heavy tails cause more extreme values\n3. **Variance underestimated**: $\\hat{\\sigma}$ too small\n4. **Structural changes**: Model fit to stable period, tested on volatile period\n5. **Parameter uncertainty ignored**: Especially problematic in small samples\n\n**Remedies:**\n- Use bootstrap PIs\n- Check residual diagnostics\n- Consider heavier-tailed distributions\n- Widen intervals manually (e.g., use 99% for conservative 95%)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Trusting nominal coverage without validation. Always check empirical coverage on holdout data.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Derive the h-step forecast variance for ARIMA(0,1,0) (random walk).</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> $\\sigma_h^2 = h\\sigma_\\epsilon^2$\n\n**Derivation:**\nRandom walk: $y_t = y_{t-1} + \\epsilon_t$\n\nThe h-step forecast:\n$$y_{T+h} = y_T + \\sum_{j=1}^{h}\\epsilon_{T+j}$$\n\nBest forecast: $\\hat{y}_{T+h|T} = y_T$ (current value)\n\nForecast error:\n$$e_{T+h|T} = y_{T+h} - y_T = \\sum_{j=1}^{h}\\epsilon_{T+j}$$\n\nSince $\\epsilon_j$ are independent:\n$$\\text{Var}(e_{T+h|T}) = \\sum_{j=1}^{h}\\sigma_\\epsilon^2 = h\\sigma_\\epsilon^2$$\n\n**PI:** $y_T \\pm 1.96\\sigma_\\epsilon\\sqrt{h}$\n\nWidth grows as $\\sqrt{h}$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> For random walk, long-horizon PIs become very wide. A 100-step-ahead 95% PI is 10× wider than 1-step-ahead.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Why does a stationary AR(1) have bounded prediction interval width as $h \\to \\infty$?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> For stationary AR(1) with $|\\phi| < 1$:\n\n$$\\sigma_h^2 = \\sigma_\\epsilon^2\\frac{1-\\phi^{2h}}{1-\\phi^2}$$\n\nAs $h \\to \\infty$: $\\phi^{2h} \\to 0$\n\n$$\\lim_{h\\to\\infty}\\sigma_h^2 = \\frac{\\sigma_\\epsilon^2}{1-\\phi^2} = \\text{Var}(y_t)$$\n\n**Intuition:** For stationary processes, distant future values are independent of current observation. The forecast converges to unconditional mean, and uncertainty converges to unconditional variance.\n\nThe PI width converges to $2 \\times 1.96 \\times \\sqrt{\\text{Var}(y_t)}$ — the interval you'd give without any data.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Bounded PI doesn't mean narrow PI. The unconditional variance can still be large, especially for $\\phi$ near 1.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> Your model produces 95% prediction intervals, but stakeholders want to know the \"worst case.\" How do you translate PIs for business use?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Several approaches:\n\n1. **Use higher confidence level**: 99% PI gives more conservative bounds\n   - Upper 99% PI ≈ upper bound for planning\n\n2. **Report specific percentiles**:\n   - \"95% chance demand is below X\"\n   - \"5% chance it exceeds Y\"\n\n3. **Scenario analysis**:\n   - Best case: lower 80% bound\n   - Expected: point forecast\n   - Worst case: upper 95% or 99% bound\n\n4. **Distribution summary**:\n   - Most likely range: 50% PI\n   - Reasonable range: 80% PI\n   - Extreme scenarios: 95% PI\n\n5. **Risk quantiles**: \"10% chance of loss exceeding $Z\"\n\n**Key message:** PIs are probability statements. \"Worst case\" depends on acceptable risk level.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using upper 95% bound as \"worst case\" — this still has 2.5% chance of being exceeded. For true tail risk, use higher percentiles or extreme value methods.\n</div>\n</div>\n</details>", "line_start": 147, "level": 2}, {"heading": "References", "content": "1. Chatfield, C. (1993). Calculating interval forecasts. *Journal of Business & Economic Statistics*, 11(2), 121-135.\n2. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 5.\n3. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis*. Wiley. Chapter 5.\n4. Thombs, L. A., & Schucany, W. R. (1990). Bootstrap prediction intervals for autoregression. *JASA*, 85(410), 486-492.", "line_start": 280, "level": 1}]}, "docs/en/forecasting/multi-step.md": {"path": "docs/en/forecasting/multi-step.md", "title": "Multi-step Forecasting", "content": "# Multi-step Forecasting\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Multi-step forecasting predicts multiple future values. Three strategies: recursive (iterate 1-step), direct (separate model per horizon), MIMO (multiple-input-multiple-output). Recursive accumulates errors but uses single model; direct avoids error accumulation but needs h models. For ARIMA, recursive is standard. For ML, direct often preferred.\n</div>\n\n## Core Definitions\n\n**Multi-step Forecast**: Predict $y_{T+1}, y_{T+2}, \\ldots, y_{T+H}$ given $y_1, \\ldots, y_T$.\n\n**Forecast Horizon (H)**: Number of steps ahead to predict.\n\n**Strategies:**\n\n1. **Recursive (Iterated)**: Use 1-step model repeatedly, feeding predictions as inputs\n2. **Direct**: Train separate model for each horizon h\n3. **MIMO**: Single model outputs all horizons simultaneously\n4. **DirRec**: Hybrid of direct and recursive\n\n## Math and Derivations\n\n### Recursive Strategy\n\nTrain model: $\\hat{y}_{t+1} = f(y_t, y_{t-1}, \\ldots)$\n\nFor h-step forecast:\n$$\\hat{y}_{T+1} = f(y_T, y_{T-1}, \\ldots)$$\n$$\\hat{y}_{T+2} = f(\\hat{y}_{T+1}, y_T, \\ldots)$$\n$$\\hat{y}_{T+h} = f(\\hat{y}_{T+h-1}, \\hat{y}_{T+h-2}, \\ldots)$$\n\n**Properties:**\n- Uses single model\n- Consistent with underlying DGP\n- Error accumulates through iterations\n\n### Direct Strategy\n\nTrain h separate models:\n$$\\hat{y}_{t+h}^{(h)} = f_h(y_t, y_{t-1}, \\ldots)$$\n\nEach model directly predicts h steps ahead.\n\n**Properties:**\n- No error propagation\n- Requires h models\n- Each model trained on different target\n- May violate consistency across horizons\n\n### Error Analysis\n\n**Recursive error:**\n$$e_{T+h}^{rec} = \\sum_{j=1}^{h}\\alpha_j\\epsilon_{T+j} + O(\\text{model error})$$\n\nError compounds through iterations.\n\n**Direct error:**\n$$e_{T+h}^{dir} = \\epsilon_{T+h}^{(h)} + O(\\text{model error}_h)$$\n\nNo compounding, but model $f_h$ may be less efficient.\n\n### Theoretical Comparison\n\n**Theorem (Ben Taieb & Hyndman):**\nUnder correct model specification:\n- Recursive is optimal (MSFE-minimizing)\n- Direct is consistent but less efficient\n\nUnder misspecification:\n- Direct may outperform recursive\n- Recursive compounds misspecification errors\n\n## Algorithm/Model Sketch\n\n**Strategy Selection Guidelines:**\n\n```\nIF model is well-specified (ARIMA, ETS):\n   USE recursive\n   - Theoretical optimality\n   - Proper uncertainty quantification\n\nELIF using ML/nonparametric methods:\n   USE direct\n   - Avoids error accumulation\n   - Each horizon optimized separately\n\nELIF forecast horizons are related:\n   USE MIMO\n   - Single model, multiple outputs\n   - Can capture horizon dependencies\n\nFOR robust approach:\n   COMBINE recursive and direct\n   - Average forecasts\n   - Often improves accuracy\n```\n\n**MIMO Implementation:**\n```python\n# Train model to predict H horizons at once\n# Input: features X\n# Output: [y_{t+1}, y_{t+2}, ..., y_{t+H}]\n\nX_train, Y_train = create_mimo_data(y, lags=p, horizon=H)\nmodel = MultiOutputRegressor(base_model)\nmodel.fit(X_train, Y_train)\n\n# Forecast\nX_new = get_features(y[-p:])\nforecasts = model.predict(X_new)  # Returns [ŷ_{T+1}, ..., ŷ_{T+H}]\n```\n\n## Common Pitfalls\n\n1. **Using recursive with ML**: Tree-based models don't extrapolate well; recursive strategy can produce flat or exploding forecasts.\n\n2. **Ignoring error accumulation**: For long horizons, recursive ARIMA uncertainty grows. Don't trust tight intervals at h=100.\n\n3. **Direct model inconsistency**: Direct models for h=5 and h=6 may give $\\hat{y}_{T+6} < \\hat{y}_{T+5}$ (non-monotonic when trend expected).\n\n4. **Computational cost**: Direct requires H models. For H=365 (daily data, 1 year), this is expensive.\n\n5. **Different targets, same features**: Direct models at different horizons have different optimal features. Using same features for all h is suboptimal.\n\n6. **Ignoring seasonality in direct**: Direct model for h=12 on monthly data should capture annual pattern, but training data may not provide enough signal.\n\n## Mini Example\n\n```python\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.multioutput import MultiOutputRegressor\n\ndef create_lagged_data(y, lags, horizon):\n    \"\"\"Create dataset for direct/MIMO forecasting.\"\"\"\n    X, Y = [], []\n    for t in range(lags, len(y) - horizon):\n        X.append(y[t-lags:t][::-1])  # [y_{t-1}, y_{t-2}, ...]\n        Y.append(y[t:t+horizon])      # [y_t, y_{t+1}, ...]\n    return np.array(X), np.array(Y)\n\n# Generate AR(2) data\nnp.random.seed(42)\nn = 500\ny = np.zeros(n)\nfor t in range(2, n):\n    y[t] = 0.5 * y[t-1] + 0.3 * y[t-2] + np.random.randn()\n\n# Split\ntrain, test = y[:400], y[400:]\nH = 10  # Forecast horizon\n\n# Recursive strategy\nfrom statsmodels.tsa.ar_model import AutoReg\nmodel_rec = AutoReg(train, lags=2).fit()\nforecast_rec = model_rec.forecast(H)\n\n# Direct strategy\nX_train, Y_train = create_lagged_data(train, lags=5, horizon=H)\ndirect_models = [Ridge().fit(X_train, Y_train[:, h]) for h in range(H)]\nX_new = train[-5:][::-1].reshape(1, -1)\nforecast_dir = np.array([m.predict(X_new)[0] for m in direct_models])\n\n# MIMO strategy\nmimo_model = MultiOutputRegressor(Ridge()).fit(X_train, Y_train)\nforecast_mimo = mimo_model.predict(X_new)[0]\n\n# Compare\nprint(\"Forecasts comparison:\")\nprint(f\"Recursive: {np.round(forecast_rec[:5], 2)}\")\nprint(f\"Direct:    {np.round(forecast_dir[:5], 2)}\")\nprint(f\"MIMO:      {np.round(forecast_mimo[:5], 2)}\")\nprint(f\"Actual:    {np.round(test[:5], 2)}\")\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why does the recursive strategy accumulate errors while direct does not?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Recursive:** At each step, uses predicted values as inputs:\n$$\\hat{y}_{T+2} = f(\\hat{y}_{T+1}, y_T, \\ldots)$$\n\nError in $\\hat{y}_{T+1}$ affects $\\hat{y}_{T+2}$, which affects $\\hat{y}_{T+3}$, etc.\n\n**Direct:** Each horizon uses only actual observed values:\n$$\\hat{y}_{T+h} = f_h(y_T, y_{T-1}, \\ldots)$$\n\nErrors at different horizons are independent (given the data).\n\n**Trade-off:**\n- Recursive: consistent but error compounds\n- Direct: no compounding but less efficient (separate models)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Thinking direct is always better. For well-specified models, recursive is theoretically optimal. Direct wins mainly under misspecification.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> When would you prefer direct over recursive forecasting?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Prefer direct when:\n\n1. **Model misspecification**: If 1-step model is wrong, recursive compounds errors\n2. **ML methods**: Trees/NNs often perform poorly in recursive mode\n3. **Horizon-specific patterns**: Different dynamics at different horizons\n4. **Long horizons**: Recursive uncertainty explodes; direct stays bounded\n5. **Non-stationary features**: Recursive may drift; direct anchors to data\n\nPrefer recursive when:\n- Model is well-specified (ARIMA, ETS)\n- Need consistent probability framework\n- Computational efficiency matters\n- Understanding model dynamics is important\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using recursive with gradient boosting — trees don't extrapolate, leading to flat/constant long-horizon forecasts.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> For AR(1) with $y_t = \\phi y_{t-1} + \\epsilon_t$, derive the h-step recursive forecast and show it equals the direct optimal forecast.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Recursive:**\n$$\\hat{y}_{T+1|T} = \\phi y_T$$\n$$\\hat{y}_{T+2|T} = \\phi \\hat{y}_{T+1|T} = \\phi^2 y_T$$\n$$\\hat{y}_{T+h|T} = \\phi^h y_T$$\n\n**Direct (optimal):**\nThe conditional expectation:\n$$E[y_{T+h}|y_T] = E[\\phi^h y_T + \\sum_{j=0}^{h-1}\\phi^j\\epsilon_{T+h-j}|y_T]$$\n$$= \\phi^h y_T + 0 = \\phi^h y_T$$\n\nThey're identical! For correctly specified linear models, recursive = direct optimal.\n\n**Key insight:** When model is correct, feeding $\\hat{y}_{T+j}$ in place of $y_{T+j}$ gives the same result as computing $E[y_{T+h}|y_{1:T}]$ directly.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> This equivalence holds for linear models. For nonlinear models, recursive ≠ direct even when correctly specified.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> How does the forecast error variance differ between recursive and direct strategies?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Recursive (correct model):**\n$$\\text{Var}(e_{T+h}^{rec}) = \\sigma^2\\sum_{j=0}^{h-1}\\psi_j^2$$\n\nThis is the theoretical minimum (Cramér-Rao bound for linear prediction).\n\n**Direct (correct model):**\n$$\\text{Var}(e_{T+h}^{dir}) = \\text{Var}(e_{T+h}^{rec}) + \\text{estimation variance}_h$$\n\nDirect adds variance because model $f_h$ is estimated less efficiently than the 1-step model (less data effectively used).\n\n**Under misspecification:**\n- Recursive: $\\text{Var}(e_{T+h}^{rec}) \\approx h \\times \\text{bias}^2 + \\text{variance}$\n- Direct: $\\text{Var}(e_{T+h}^{dir}) \\approx \\text{bias}_h^2 + \\text{variance}_h$\n\nDirect doesn't compound bias across horizons.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Assuming direct always has larger variance. Under misspecification, direct often wins because it avoids compounding the bias.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You need to forecast daily sales 30 days ahead. You have an XGBoost model. Which strategy do you use?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> **Direct** or **MIMO** strategy.\n\n**Why not recursive:**\n- XGBoost is a tree-based model that doesn't extrapolate\n- Recursive feeding predictions back leads to:\n  - Forecasts that flatten to mean\n  - Or erratic behavior if predictions drift outside training range\n- 30-step recursion compounds errors significantly\n\n**Recommended approach:**\n1. **Direct:** Train 30 separate XGBoost models\n   - Horizon-specific optimization\n   - Can use different features per horizon\n   - Computationally more expensive\n\n2. **MIMO:** Train one multi-output model\n   - Use `MultiOutputRegressor(XGBRegressor())`\n   - Or custom multi-output architecture\n   - More efficient than 30 models\n\n3. **Hybrid:** Use LightGBM/XGBoost for short horizons, average with simpler model for longer horizons\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using recursive XGBoost — forecasts often degrade to constant or oscillate. Always validate multi-step behavior before production.\n</div>\n</div>\n</details>\n\n## References\n\n1. Ben Taieb, S., & Hyndman, R. J. (2014). A gradient boosting approach to the Kaggle load forecasting competition. *IJF*, 30(2), 382-394.\n2. Chevillon, G. (2007). Direct multi-step estimation and forecasting. *Journal of Economic Surveys*, 21(4), 746-785.\n3. Marcellino, M., Stock, J. H., & Watson, M. W. (2006). A comparison of direct and iterated multistep AR methods for forecasting macroeconomic time series. *Journal of Econometrics*, 135(1-2), 499-526.\n4. Ben Taieb, S., Bontempi, G., Atiya, A. F., & Sorjamaa, A. (2012). A review and comparison of strategies for multi-step ahead time series forecasting based on the NN5 forecasting competition. *Expert Systems with Applications*, 39(8), 7067-7083.\n", "sections": [{"heading": "Multi-step Forecasting", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Multi-step forecasting predicts multiple future values. Three strategies: recursive (iterate 1-step), direct (separate model per horizon), MIMO (multiple-input-multiple-output). Recursive accumulates errors but uses single model; direct avoids error accumulation but needs h models. For ARIMA, recursive is standard. For ML, direct often preferred.\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**Multi-step Forecast**: Predict $y_{T+1}, y_{T+2}, \\ldots, y_{T+H}$ given $y_1, \\ldots, y_T$.\n\n**Forecast Horizon (H)**: Number of steps ahead to predict.\n\n**Strategies:**\n\n1. **Recursive (Iterated)**: Use 1-step model repeatedly, feeding predictions as inputs\n2. **Direct**: Train separate model for each horizon h\n3. **MIMO**: Single model outputs all horizons simultaneously\n4. **DirRec**: Hybrid of direct and recursive", "line_start": 7, "level": 2}, {"heading": "Recursive Strategy", "content": "Train model: $\\hat{y}_{t+1} = f(y_t, y_{t-1}, \\ldots)$\n\nFor h-step forecast:\n$$\\hat{y}_{T+1} = f(y_T, y_{T-1}, \\ldots)$$\n$$\\hat{y}_{T+2} = f(\\hat{y}_{T+1}, y_T, \\ldots)$$\n$$\\hat{y}_{T+h} = f(\\hat{y}_{T+h-1}, \\hat{y}_{T+h-2}, \\ldots)$$\n\n**Properties:**\n- Uses single model\n- Consistent with underlying DGP\n- Error accumulates through iterations", "line_start": 22, "level": 3}, {"heading": "Direct Strategy", "content": "Train h separate models:\n$$\\hat{y}_{t+h}^{(h)} = f_h(y_t, y_{t-1}, \\ldots)$$\n\nEach model directly predicts h steps ahead.\n\n**Properties:**\n- No error propagation\n- Requires h models\n- Each model trained on different target\n- May violate consistency across horizons", "line_start": 36, "level": 3}, {"heading": "Error Analysis", "content": "**Recursive error:**\n$$e_{T+h}^{rec} = \\sum_{j=1}^{h}\\alpha_j\\epsilon_{T+j} + O(\\text{model error})$$\n\nError compounds through iterations.\n\n**Direct error:**\n$$e_{T+h}^{dir} = \\epsilon_{T+h}^{(h)} + O(\\text{model error}_h)$$\n\nNo compounding, but model $f_h$ may be less efficient.", "line_start": 49, "level": 3}, {"heading": "Theoretical Comparison", "content": "**Theorem (Ben Taieb & Hyndman):**\nUnder correct model specification:\n- Recursive is optimal (MSFE-minimizing)\n- Direct is consistent but less efficient\n\nUnder misspecification:\n- Direct may outperform recursive\n- Recursive compounds misspecification errors", "line_start": 61, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**Strategy Selection Guidelines:**\n\n```\nIF model is well-specified (ARIMA, ETS):\n   USE recursive\n   - Theoretical optimality\n   - Proper uncertainty quantification\n\nELIF using ML/nonparametric methods:\n   USE direct\n   - Avoids error accumulation\n   - Each horizon optimized separately\n\nELIF forecast horizons are related:\n   USE MIMO\n   - Single model, multiple outputs\n   - Can capture horizon dependencies\n\nFOR robust approach:\n   COMBINE recursive and direct\n   - Average forecasts\n   - Often improves accuracy\n```\n\n**MIMO Implementation:**\n```python", "line_start": 72, "level": 1}, {"heading": "Output: [y_{t+1}, y_{t+2}, ..., y_{t+H}]", "content": "X_train, Y_train = create_mimo_data(y, lags=p, horizon=H)\nmodel = MultiOutputRegressor(base_model)\nmodel.fit(X_train, Y_train)", "line_start": 102, "level": 1}, {"heading": "Forecast", "content": "X_new = get_features(y[-p:])\nforecasts = model.predict(X_new)  # Returns [ŷ_{T+1}, ..., ŷ_{T+H}]\n```", "line_start": 108, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Using recursive with ML**: Tree-based models don't extrapolate well; recursive strategy can produce flat or exploding forecasts.\n\n2. **Ignoring error accumulation**: For long horizons, recursive ARIMA uncertainty grows. Don't trust tight intervals at h=100.\n\n3. **Direct model inconsistency**: Direct models for h=5 and h=6 may give $\\hat{y}_{T+6} < \\hat{y}_{T+5}$ (non-monotonic when trend expected).\n\n4. **Computational cost**: Direct requires H models. For H=365 (daily data, 1 year), this is expensive.\n\n5. **Different targets, same features**: Direct models at different horizons have different optimal features. Using same features for all h is suboptimal.\n\n6. **Ignoring seasonality in direct**: Direct model for h=12 on monthly data should capture annual pattern, but training data may not provide enough signal.", "line_start": 113, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.multioutput import MultiOutputRegressor\n\ndef create_lagged_data(y, lags, horizon):\n    \"\"\"Create dataset for direct/MIMO forecasting.\"\"\"\n    X, Y = [], []\n    for t in range(lags, len(y) - horizon):\n        X.append(y[t-lags:t][::-1])  # [y_{t-1}, y_{t-2}, ...]\n        Y.append(y[t:t+horizon])      # [y_t, y_{t+1}, ...]\n    return np.array(X), np.array(Y)", "line_start": 127, "level": 1}, {"heading": "Generate AR(2) data", "content": "np.random.seed(42)\nn = 500\ny = np.zeros(n)\nfor t in range(2, n):\n    y[t] = 0.5 * y[t-1] + 0.3 * y[t-2] + np.random.randn()", "line_start": 142, "level": 1}, {"heading": "Split", "content": "train, test = y[:400], y[400:]\nH = 10  # Forecast horizon", "line_start": 149, "level": 1}, {"heading": "Recursive strategy", "content": "from statsmodels.tsa.ar_model import AutoReg\nmodel_rec = AutoReg(train, lags=2).fit()\nforecast_rec = model_rec.forecast(H)", "line_start": 153, "level": 1}, {"heading": "Direct strategy", "content": "X_train, Y_train = create_lagged_data(train, lags=5, horizon=H)\ndirect_models = [Ridge().fit(X_train, Y_train[:, h]) for h in range(H)]\nX_new = train[-5:][::-1].reshape(1, -1)\nforecast_dir = np.array([m.predict(X_new)[0] for m in direct_models])", "line_start": 158, "level": 1}, {"heading": "MIMO strategy", "content": "mimo_model = MultiOutputRegressor(Ridge()).fit(X_train, Y_train)\nforecast_mimo = mimo_model.predict(X_new)[0]", "line_start": 164, "level": 1}, {"heading": "Compare", "content": "print(\"Forecasts comparison:\")\nprint(f\"Recursive: {np.round(forecast_rec[:5], 2)}\")\nprint(f\"Direct:    {np.round(forecast_dir[:5], 2)}\")\nprint(f\"MIMO:      {np.round(forecast_mimo[:5], 2)}\")\nprint(f\"Actual:    {np.round(test[:5], 2)}\")\n```", "line_start": 168, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why does the recursive strategy accumulate errors while direct does not?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Recursive:** At each step, uses predicted values as inputs:\n$$\\hat{y}_{T+2} = f(\\hat{y}_{T+1}, y_T, \\ldots)$$\n\nError in $\\hat{y}_{T+1}$ affects $\\hat{y}_{T+2}$, which affects $\\hat{y}_{T+3}$, etc.\n\n**Direct:** Each horizon uses only actual observed values:\n$$\\hat{y}_{T+h} = f_h(y_T, y_{T-1}, \\ldots)$$\n\nErrors at different horizons are independent (given the data).\n\n**Trade-off:**\n- Recursive: consistent but error compounds\n- Direct: no compounding but less efficient (separate models)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Thinking direct is always better. For well-specified models, recursive is theoretically optimal. Direct wins mainly under misspecification.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> When would you prefer direct over recursive forecasting?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Prefer direct when:\n\n1. **Model misspecification**: If 1-step model is wrong, recursive compounds errors\n2. **ML methods**: Trees/NNs often perform poorly in recursive mode\n3. **Horizon-specific patterns**: Different dynamics at different horizons\n4. **Long horizons**: Recursive uncertainty explodes; direct stays bounded\n5. **Non-stationary features**: Recursive may drift; direct anchors to data\n\nPrefer recursive when:\n- Model is well-specified (ARIMA, ETS)\n- Need consistent probability framework\n- Computational efficiency matters\n- Understanding model dynamics is important\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using recursive with gradient boosting — trees don't extrapolate, leading to flat/constant long-horizon forecasts.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> For AR(1) with $y_t = \\phi y_{t-1} + \\epsilon_t$, derive the h-step recursive forecast and show it equals the direct optimal forecast.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Recursive:**\n$$\\hat{y}_{T+1|T} = \\phi y_T$$\n$$\\hat{y}_{T+2|T} = \\phi \\hat{y}_{T+1|T} = \\phi^2 y_T$$\n$$\\hat{y}_{T+h|T} = \\phi^h y_T$$\n\n**Direct (optimal):**\nThe conditional expectation:\n$$E[y_{T+h}|y_T] = E[\\phi^h y_T + \\sum_{j=0}^{h-1}\\phi^j\\epsilon_{T+h-j}|y_T]$$\n$$= \\phi^h y_T + 0 = \\phi^h y_T$$\n\nThey're identical! For correctly specified linear models, recursive = direct optimal.\n\n**Key insight:** When model is correct, feeding $\\hat{y}_{T+j}$ in place of $y_{T+j}$ gives the same result as computing $E[y_{T+h}|y_{1:T}]$ directly.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> This equivalence holds for linear models. For nonlinear models, recursive ≠ direct even when correctly specified.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> How does the forecast error variance differ between recursive and direct strategies?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Recursive (correct model):**\n$$\\text{Var}(e_{T+h}^{rec}) = \\sigma^2\\sum_{j=0}^{h-1}\\psi_j^2$$\n\nThis is the theoretical minimum (Cramér-Rao bound for linear prediction).\n\n**Direct (correct model):**\n$$\\text{Var}(e_{T+h}^{dir}) = \\text{Var}(e_{T+h}^{rec}) + \\text{estimation variance}_h$$\n\nDirect adds variance because model $f_h$ is estimated less efficiently than the 1-step model (less data effectively used).\n\n**Under misspecification:**\n- Recursive: $\\text{Var}(e_{T+h}^{rec}) \\approx h \\times \\text{bias}^2 + \\text{variance}$\n- Direct: $\\text{Var}(e_{T+h}^{dir}) \\approx \\text{bias}_h^2 + \\text{variance}_h$\n\nDirect doesn't compound bias across horizons.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Assuming direct always has larger variance. Under misspecification, direct often wins because it avoids compounding the bias.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You need to forecast daily sales 30 days ahead. You have an XGBoost model. Which strategy do you use?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> **Direct** or **MIMO** strategy.\n\n**Why not recursive:**\n- XGBoost is a tree-based model that doesn't extrapolate\n- Recursive feeding predictions back leads to:\n  - Forecasts that flatten to mean\n  - Or erratic behavior if predictions drift outside training range\n- 30-step recursion compounds errors significantly\n\n**Recommended approach:**\n1. **Direct:** Train 30 separate XGBoost models\n   - Horizon-specific optimization\n   - Can use different features per horizon\n   - Computationally more expensive\n\n2. **MIMO:** Train one multi-output model\n   - Use `MultiOutputRegressor(XGBRegressor())`\n   - Or custom multi-output architecture\n   - More efficient than 30 models\n\n3. **Hybrid:** Use LightGBM/XGBoost for short horizons, average with simpler model for longer horizons\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using recursive XGBoost — forecasts often degrade to constant or oscillate. Always validate multi-step behavior before production.\n</div>\n</div>\n</details>", "line_start": 176, "level": 2}, {"heading": "References", "content": "1. Ben Taieb, S., & Hyndman, R. J. (2014). A gradient boosting approach to the Kaggle load forecasting competition. *IJF*, 30(2), 382-394.\n2. Chevillon, G. (2007). Direct multi-step estimation and forecasting. *Journal of Economic Surveys*, 21(4), 746-785.\n3. Marcellino, M., Stock, J. H., & Watson, M. W. (2006). A comparison of direct and iterated multistep AR methods for forecasting macroeconomic time series. *Journal of Econometrics*, 135(1-2), 499-526.\n4. Ben Taieb, S., Bontempi, G., Atiya, A. F., & Sorjamaa, A. (2012). A review and comparison of strategies for multi-step ahead time series forecasting based on the NN5 forecasting competition. *Expert Systems with Applications*, 39(8), 7067-7083.", "line_start": 314, "level": 1}]}, "docs/en/interview/interview-questions.md": {"path": "docs/en/interview/interview-questions.md", "title": "Common Interview Questions", "content": "# Common Interview Questions\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> This section compiles frequently asked time series interview questions with concise answers. Topics span fundamentals (stationarity, ACF/PACF), model selection (ARIMA vs. ETS vs. ML), practical challenges (seasonality, missing data, forecasting strategy), and advanced topics (state space, deep learning). Use this as a quick review before interviews.\n</div>\n\n## Fundamentals\n\n### Q1: What is stationarity and why does it matter?\n\n**Answer:** A stationary process has constant mean, constant variance, and autocovariance that depends only on lag (not time). It matters because:\n- Most classical models (ARIMA, VAR) assume stationarity\n- Non-stationary series have unpredictable long-term behavior\n- Statistical tests and inference require stationarity\n\n**Test for it:** ADF (null: unit root), KPSS (null: stationary). Use both for robust conclusions.\n\n**Make it stationary:** Differencing (stochastic trend), detrending (deterministic trend), log transform (changing variance).\n\n### Q2: Explain the difference between ACF and PACF.\n\n**Answer:**\n- **ACF:** Total correlation between $y_t$ and $y_{t-k}$, including indirect effects through intermediate lags\n- **PACF:** Direct correlation between $y_t$ and $y_{t-k}$ after removing effects of $y_{t-1}, \\ldots, y_{t-k+1}$\n\n**Use for identification:**\n- ACF cuts off at lag q → MA(q)\n- PACF cuts off at lag p → AR(p)\n- Both tail off → ARMA\n\n### Q3: How do you handle seasonality?\n\n**Answer:** Multiple approaches:\n1. **Seasonal differencing:** $(1-L^s)y_t$ removes seasonal unit root\n2. **SARIMA:** Explicit seasonal AR/MA terms\n3. **Seasonal decomposition:** STL separates trend, seasonal, remainder\n4. **Fourier terms:** Sin/cos features at seasonal frequencies\n5. **Dummy variables:** For short seasonal periods\n\n## Model Selection\n\n### Q4: When would you choose ARIMA over exponential smoothing (ETS)?\n\n**Answer:**\n- **ARIMA:** When series has complex autocorrelation structure, when you need to include external regressors (ARIMAX), when interpretability of AR/MA structure matters\n- **ETS:** When you want automatic model selection among 30 variants, when forecasting is primary goal, when you need proper state space framework for intervals\n\n**In practice:** Both often give similar forecasts. Use whichever is easier to implement and interpret for your context.\n\n### Q5: How do you choose between classical methods and machine learning?\n\n**Answer:**\n\n| Factor | Classical (ARIMA/ETS) | ML (RF, LSTM, etc.) |\n|--------|----------------------|---------------------|\n| Data size | Small-medium | Large |\n| Interpretability | High | Low |\n| Uncertainty | Well-calibrated intervals | Harder to quantify |\n| Multiple series | Separate models | Can share patterns |\n| Complex patterns | Limited | Can learn anything |\n| Computational cost | Low | High |\n\n**Rule of thumb:** Start with classical baselines. Use ML if you have lots of data AND classical methods underperform.\n\n### Q6: What information criteria do you use for model selection? Explain AIC vs BIC.\n\n**Answer:**\n- **AIC = -2ln(L) + 2k:** Optimizes prediction error; can overfit with large samples\n- **BIC = -2ln(L) + k·ln(n):** Consistent (selects true model); more parsimonious\n- **AICc:** Corrected AIC for small samples (use when n/k < 40)\n\n**Guideline:** Use AICc for forecasting, BIC for inference/interpretation.\n\n## Practical Challenges\n\n### Q7: How do you handle missing values in time series?\n\n**Answer:**\n1. **Linear interpolation:** For sporadic missing values\n2. **Forward/backward fill:** When last known value is reasonable\n3. **Seasonal imputation:** Fill with same period from previous cycle\n4. **Model-based:** Kalman filter or EM algorithm\n5. **Missing indicator:** Add binary feature, let model learn\n\n**Never:** Delete rows (breaks temporal continuity) or use mean imputation globally.\n\n### Q8: What is data leakage in time series? How do you prevent it?\n\n**Answer:** Using future information when training or creating features.\n\n**Common sources:**\n- Random train-test split (future in training set)\n- Rolling features without shift (includes current value)\n- Scaling on full data (test statistics in training)\n- External features not available at forecast time\n\n**Prevention:**\n- Always split temporally\n- Use `.shift(1).rolling()` for features\n- Fit scalers on training data only\n- Verify feature availability in production\n\n### Q9: Explain multi-step forecasting strategies.\n\n**Answer:**\n1. **Recursive:** Use 1-step model, iterate (feed predictions back)\n2. **Direct:** Train separate model for each horizon\n3. **MIMO:** Single model outputs all horizons\n\n**Trade-offs:**\n- Recursive: One model, but errors accumulate\n- Direct: No error accumulation, but h models needed\n- MIMO: Balance, but needs careful architecture\n\n**For ARIMA:** Recursive is standard and optimal.\n**For ML:** Direct often better (avoids compounding errors).\n\n### Q10: How do you evaluate forecast accuracy?\n\n**Answer:**\n\n**Metrics:**\n- MAE: Easy to interpret, robust to outliers\n- RMSE: Penalizes large errors more\n- MAPE: Percentage errors, but undefined at zero\n- MASE: Scale-free, compares to naive forecast (MASE < 1 is good)\n\n**Evaluation:**\n- Use rolling origin cross-validation\n- Match evaluation horizon to business need\n- Always compare to baselines (naive, seasonal naive)\n- Check prediction interval coverage\n\n## Advanced Topics\n\n### Q11: What is the Kalman filter and when would you use it?\n\n**Answer:** Recursive algorithm for optimal state estimation in linear Gaussian state space models.\n\n**Use cases:**\n- Tracking unobserved components (level, trend)\n- Online filtering (update as data arrives)\n- Missing data handling (natural framework)\n- Time-varying parameters\n\n**Connection:** ETS, ARIMA, structural time series are all state space models; Kalman filter provides unified estimation.\n\n### Q12: Explain Granger causality. What are its limitations?\n\n**Answer:** X Granger-causes Y if past X improves prediction of Y beyond Y's own past.\n\n**Limitations:**\n- **Not true causation:** Correlation due to common causes gives spurious GC\n- **Requires stationarity:** Standard tests need stationary data\n- **Sensitive to lag selection:** Results change with different lags\n- **Omitted variable bias:** Missing Z that causes both X and Y\n- **Contemporaneous effects missed:** Only tests lagged relationships\n\n### Q13: When should you use deep learning for time series?\n\n**Answer:** Consider DL when:\n- Large dataset (thousands+ observations)\n- Multiple related series (can share representations)\n- Complex patterns (nonlinear, interaction effects)\n- Long-range dependencies (attention mechanisms help)\n\n**Avoid DL when:**\n- Small dataset (ARIMA usually wins)\n- Interpretability required\n- Simple patterns (Occam's razor)\n- Computational constraints\n\n**Popular architectures:** LSTM, TCN (temporal CNN), Transformers\n\n### Q14: How do you detect and handle structural breaks?\n\n**Answer:**\n\n**Detection:**\n- Visual inspection\n- Chow test (tests for break at known point)\n- CUSUM (cumulative sum of residuals)\n- PELT algorithm (multiple change-points)\n\n**Handling:**\n- Regime-switching models (Markov switching)\n- Structural break dummies in regression\n- Train only on post-break data\n- Time-varying parameters\n\n### Q15: What is forecast reconciliation?\n\n**Answer:** Ensuring forecasts at different aggregation levels are consistent.\n\n**Example:** Product forecasts should sum to category forecast, which sums to total.\n\n**Approaches:**\n- **Top-down:** Forecast aggregate, distribute\n- **Bottom-up:** Forecast individuals, sum\n- **Optimal reconciliation:** Combine all levels optimally (MinT approach)\n\n**Why it matters:** Inconsistent forecasts confuse planning (inventory, budgets).\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What are the three components of the Box-Jenkins methodology?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Identification, Estimation, Diagnostic Checking\n\n1. **Identification:** Determine (p, d, q) using ACF/PACF, stationarity tests\n2. **Estimation:** Fit model using MLE or conditional least squares\n3. **Diagnostic Checking:** Verify residuals are white noise (Ljung-Box, ACF plots)\n\nIterate if diagnostics fail: return to identification.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Skipping diagnostics. A model with good AIC can still have autocorrelated residuals, indicating misspecification.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Explain the bias-variance trade-off in time series forecasting.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n- **Bias:** Systematic error from wrong model assumptions (underfitting)\n- **Variance:** Sensitivity to training data (overfitting)\n\n**In time series context:**\n- Simple model (AR(1)): May miss patterns (high bias) but stable (low variance)\n- Complex model (ARIMA(5,1,5)): Captures patterns but unstable (high variance)\n\n**Manifestation:**\n- High bias: Poor training fit, similar performance on test\n- High variance: Great training fit, poor test performance\n\n**Regularization:** AIC/BIC penalize complexity; cross-validation estimates out-of-sample error.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Only looking at training error. Always evaluate on holdout or via cross-validation.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> What is the forecast function for ARIMA(0,1,1)? How does it relate to exponential smoothing?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\nARIMA(0,1,1): $(1-L)y_t = (1+\\theta L)\\epsilon_t$\n\nForecast function:\n$$\\hat{y}_{T+h|T} = y_T + \\theta\\hat{\\epsilon}_T = y_T + \\theta(y_T - \\hat{y}_{T|T-1})$$\n\nThis equals Simple Exponential Smoothing with $\\alpha = 1/(1+\\theta)$:\n$$\\hat{y}_{T+h|T} = \\alpha y_T + (1-\\alpha)\\hat{y}_{T|T-1}$$\n\n**Connection:** SES is optimal for local level model / ARIMA(0,1,1).\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Treating ARIMA and ETS as completely different. They're deeply connected; many ETS models have ARIMA equivalents.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> How would you test for unit root in the presence of seasonality?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\nStandard ADF tests for non-seasonal unit root. For seasonal unit root:\n\n1. **HEGY test:** Tests for unit roots at seasonal and zero frequencies\n2. **Canova-Hansen:** Tests null of stationarity at seasonal frequencies\n3. **OCSB test:** Specifically for seasonal unit roots\n\n**Practical approach:**\n1. First, test for seasonal unit root (OCSB/HEGY)\n2. If present, apply seasonal differencing $(1-L^s)$\n3. Then test seasonally differenced series for regular unit root (ADF)\n4. Apply regular differencing if needed\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using ADF directly on seasonal data. ADF may reject unit root due to seasonality, not because series is stationary.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You're asked to forecast daily sales for a retailer. Walk through your approach.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**1. Understand the data:**\n- Length of history, granularity\n- Missing values, outliers\n- External factors (holidays, promotions)\n\n**2. Explore patterns:**\n- Plot series, decomposition\n- Check for trend, weekly/annual seasonality\n- Examine ACF/PACF\n\n**3. Baseline models:**\n- Naive (yesterday's sales)\n- Seasonal naive (same day last week)\n- Moving average\n\n**4. Candidate models:**\n- SARIMA (captures ARMA + seasonality)\n- ETS (automatic model selection)\n- Prophet (handles holidays, easy to use)\n- XGBoost with lag features (if many series)\n\n**5. Evaluation:**\n- Rolling origin CV (last 8-12 weeks)\n- Metrics: MAE, MAPE, MASE\n- Compare to baselines\n\n**6. Production considerations:**\n- Forecast horizon needed\n- Update frequency\n- Uncertainty communication\n- Monitoring and retraining schedule\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Jumping to complex models. Start simple (seasonal naive is often hard to beat) and add complexity only if justified by evaluation.\n</div>\n</div>\n</details>\n\n## References\n\n1. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts.\n2. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis*. Wiley.\n3. Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press.\n4. Shumway, R. H., & Stoffer, D. S. (2017). *Time Series Analysis and Its Applications*. Springer.\n", "sections": [{"heading": "Common Interview Questions", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> This section compiles frequently asked time series interview questions with concise answers. Topics span fundamentals (stationarity, ACF/PACF), model selection (ARIMA vs. ETS vs. ML), practical challenges (seasonality, missing data, forecasting strategy), and advanced topics (state space, deep learning). Use this as a quick review before interviews.\n</div>", "line_start": 1, "level": 2}, {"heading": "Q1: What is stationarity and why does it matter?", "content": "**Answer:** A stationary process has constant mean, constant variance, and autocovariance that depends only on lag (not time). It matters because:\n- Most classical models (ARIMA, VAR) assume stationarity\n- Non-stationary series have unpredictable long-term behavior\n- Statistical tests and inference require stationarity\n\n**Test for it:** ADF (null: unit root), KPSS (null: stationary). Use both for robust conclusions.\n\n**Make it stationary:** Differencing (stochastic trend), detrending (deterministic trend), log transform (changing variance).", "line_start": 9, "level": 3}, {"heading": "Q2: Explain the difference between ACF and PACF.", "content": "**Answer:**\n- **ACF:** Total correlation between $y_t$ and $y_{t-k}$, including indirect effects through intermediate lags\n- **PACF:** Direct correlation between $y_t$ and $y_{t-k}$ after removing effects of $y_{t-1}, \\ldots, y_{t-k+1}$\n\n**Use for identification:**\n- ACF cuts off at lag q → MA(q)\n- PACF cuts off at lag p → AR(p)\n- Both tail off → ARMA", "line_start": 20, "level": 3}, {"heading": "Q3: How do you handle seasonality?", "content": "**Answer:** Multiple approaches:\n1. **Seasonal differencing:** $(1-L^s)y_t$ removes seasonal unit root\n2. **SARIMA:** Explicit seasonal AR/MA terms\n3. **Seasonal decomposition:** STL separates trend, seasonal, remainder\n4. **Fourier terms:** Sin/cos features at seasonal frequencies\n5. **Dummy variables:** For short seasonal periods", "line_start": 31, "level": 2}, {"heading": "Q4: When would you choose ARIMA over exponential smoothing (ETS)?", "content": "**Answer:**\n- **ARIMA:** When series has complex autocorrelation structure, when you need to include external regressors (ARIMAX), when interpretability of AR/MA structure matters\n- **ETS:** When you want automatic model selection among 30 variants, when forecasting is primary goal, when you need proper state space framework for intervals\n\n**In practice:** Both often give similar forecasts. Use whichever is easier to implement and interpret for your context.", "line_start": 42, "level": 3}, {"heading": "Q5: How do you choose between classical methods and machine learning?", "content": "**Answer:**\n\n| Factor | Classical (ARIMA/ETS) | ML (RF, LSTM, etc.) |\n|--------|----------------------|---------------------|\n| Data size | Small-medium | Large |\n| Interpretability | High | Low |\n| Uncertainty | Well-calibrated intervals | Harder to quantify |\n| Multiple series | Separate models | Can share patterns |\n| Complex patterns | Limited | Can learn anything |\n| Computational cost | Low | High |\n\n**Rule of thumb:** Start with classical baselines. Use ML if you have lots of data AND classical methods underperform.", "line_start": 50, "level": 3}, {"heading": "Q6: What information criteria do you use for model selection? Explain AIC vs BIC.", "content": "**Answer:**\n- **AIC = -2ln(L) + 2k:** Optimizes prediction error; can overfit with large samples\n- **BIC = -2ln(L) + k·ln(n):** Consistent (selects true model); more parsimonious\n- **AICc:** Corrected AIC for small samples (use when n/k < 40)\n\n**Guideline:** Use AICc for forecasting, BIC for inference/interpretation.", "line_start": 65, "level": 2}, {"heading": "Q7: How do you handle missing values in time series?", "content": "**Answer:**\n1. **Linear interpolation:** For sporadic missing values\n2. **Forward/backward fill:** When last known value is reasonable\n3. **Seasonal imputation:** Fill with same period from previous cycle\n4. **Model-based:** Kalman filter or EM algorithm\n5. **Missing indicator:** Add binary feature, let model learn\n\n**Never:** Delete rows (breaks temporal continuity) or use mean imputation globally.", "line_start": 76, "level": 3}, {"heading": "Q8: What is data leakage in time series? How do you prevent it?", "content": "**Answer:** Using future information when training or creating features.\n\n**Common sources:**\n- Random train-test split (future in training set)\n- Rolling features without shift (includes current value)\n- Scaling on full data (test statistics in training)\n- External features not available at forecast time\n\n**Prevention:**\n- Always split temporally\n- Use `.shift(1).rolling()` for features\n- Fit scalers on training data only\n- Verify feature availability in production", "line_start": 87, "level": 3}, {"heading": "Q9: Explain multi-step forecasting strategies.", "content": "**Answer:**\n1. **Recursive:** Use 1-step model, iterate (feed predictions back)\n2. **Direct:** Train separate model for each horizon\n3. **MIMO:** Single model outputs all horizons\n\n**Trade-offs:**\n- Recursive: One model, but errors accumulate\n- Direct: No error accumulation, but h models needed\n- MIMO: Balance, but needs careful architecture\n\n**For ARIMA:** Recursive is standard and optimal.\n**For ML:** Direct often better (avoids compounding errors).", "line_start": 103, "level": 3}, {"heading": "Q10: How do you evaluate forecast accuracy?", "content": "**Answer:**\n\n**Metrics:**\n- MAE: Easy to interpret, robust to outliers\n- RMSE: Penalizes large errors more\n- MAPE: Percentage errors, but undefined at zero\n- MASE: Scale-free, compares to naive forecast (MASE < 1 is good)\n\n**Evaluation:**\n- Use rolling origin cross-validation\n- Match evaluation horizon to business need\n- Always compare to baselines (naive, seasonal naive)\n- Check prediction interval coverage", "line_start": 118, "level": 2}, {"heading": "Q11: What is the Kalman filter and when would you use it?", "content": "**Answer:** Recursive algorithm for optimal state estimation in linear Gaussian state space models.\n\n**Use cases:**\n- Tracking unobserved components (level, trend)\n- Online filtering (update as data arrives)\n- Missing data handling (natural framework)\n- Time-varying parameters\n\n**Connection:** ETS, ARIMA, structural time series are all state space models; Kalman filter provides unified estimation.", "line_start": 136, "level": 3}, {"heading": "Q12: Explain Granger causality. What are its limitations?", "content": "**Answer:** X Granger-causes Y if past X improves prediction of Y beyond Y's own past.\n\n**Limitations:**\n- **Not true causation:** Correlation due to common causes gives spurious GC\n- **Requires stationarity:** Standard tests need stationary data\n- **Sensitive to lag selection:** Results change with different lags\n- **Omitted variable bias:** Missing Z that causes both X and Y\n- **Contemporaneous effects missed:** Only tests lagged relationships", "line_start": 148, "level": 3}, {"heading": "Q13: When should you use deep learning for time series?", "content": "**Answer:** Consider DL when:\n- Large dataset (thousands+ observations)\n- Multiple related series (can share representations)\n- Complex patterns (nonlinear, interaction effects)\n- Long-range dependencies (attention mechanisms help)\n\n**Avoid DL when:**\n- Small dataset (ARIMA usually wins)\n- Interpretability required\n- Simple patterns (Occam's razor)\n- Computational constraints\n\n**Popular architectures:** LSTM, TCN (temporal CNN), Transformers", "line_start": 159, "level": 3}, {"heading": "Q14: How do you detect and handle structural breaks?", "content": "**Answer:**\n\n**Detection:**\n- Visual inspection\n- Chow test (tests for break at known point)\n- CUSUM (cumulative sum of residuals)\n- PELT algorithm (multiple change-points)\n\n**Handling:**\n- Regime-switching models (Markov switching)\n- Structural break dummies in regression\n- Train only on post-break data\n- Time-varying parameters", "line_start": 175, "level": 3}, {"heading": "Q15: What is forecast reconciliation?", "content": "**Answer:** Ensuring forecasts at different aggregation levels are consistent.\n\n**Example:** Product forecasts should sum to category forecast, which sums to total.\n\n**Approaches:**\n- **Top-down:** Forecast aggregate, distribute\n- **Bottom-up:** Forecast individuals, sum\n- **Optimal reconciliation:** Combine all levels optimally (MinT approach)\n\n**Why it matters:** Inconsistent forecasts confuse planning (inventory, budgets).", "line_start": 191, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What are the three components of the Box-Jenkins methodology?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Identification, Estimation, Diagnostic Checking\n\n1. **Identification:** Determine (p, d, q) using ACF/PACF, stationarity tests\n2. **Estimation:** Fit model using MLE or conditional least squares\n3. **Diagnostic Checking:** Verify residuals are white noise (Ljung-Box, ACF plots)\n\nIterate if diagnostics fail: return to identification.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Skipping diagnostics. A model with good AIC can still have autocorrelated residuals, indicating misspecification.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Explain the bias-variance trade-off in time series forecasting.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n- **Bias:** Systematic error from wrong model assumptions (underfitting)\n- **Variance:** Sensitivity to training data (overfitting)\n\n**In time series context:**\n- Simple model (AR(1)): May miss patterns (high bias) but stable (low variance)\n- Complex model (ARIMA(5,1,5)): Captures patterns but unstable (high variance)\n\n**Manifestation:**\n- High bias: Poor training fit, similar performance on test\n- High variance: Great training fit, poor test performance\n\n**Regularization:** AIC/BIC penalize complexity; cross-validation estimates out-of-sample error.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Only looking at training error. Always evaluate on holdout or via cross-validation.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> What is the forecast function for ARIMA(0,1,1)? How does it relate to exponential smoothing?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\nARIMA(0,1,1): $(1-L)y_t = (1+\\theta L)\\epsilon_t$\n\nForecast function:\n$$\\hat{y}_{T+h|T} = y_T + \\theta\\hat{\\epsilon}_T = y_T + \\theta(y_T - \\hat{y}_{T|T-1})$$\n\nThis equals Simple Exponential Smoothing with $\\alpha = 1/(1+\\theta)$:\n$$\\hat{y}_{T+h|T} = \\alpha y_T + (1-\\alpha)\\hat{y}_{T|T-1}$$\n\n**Connection:** SES is optimal for local level model / ARIMA(0,1,1).\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Treating ARIMA and ETS as completely different. They're deeply connected; many ETS models have ARIMA equivalents.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> How would you test for unit root in the presence of seasonality?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\nStandard ADF tests for non-seasonal unit root. For seasonal unit root:\n\n1. **HEGY test:** Tests for unit roots at seasonal and zero frequencies\n2. **Canova-Hansen:** Tests null of stationarity at seasonal frequencies\n3. **OCSB test:** Specifically for seasonal unit roots\n\n**Practical approach:**\n1. First, test for seasonal unit root (OCSB/HEGY)\n2. If present, apply seasonal differencing $(1-L^s)$\n3. Then test seasonally differenced series for regular unit root (ADF)\n4. Apply regular differencing if needed\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using ADF directly on seasonal data. ADF may reject unit root due to seasonality, not because series is stationary.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You're asked to forecast daily sales for a retailer. Walk through your approach.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**1. Understand the data:**\n- Length of history, granularity\n- Missing values, outliers\n- External factors (holidays, promotions)\n\n**2. Explore patterns:**\n- Plot series, decomposition\n- Check for trend, weekly/annual seasonality\n- Examine ACF/PACF\n\n**3. Baseline models:**\n- Naive (yesterday's sales)\n- Seasonal naive (same day last week)\n- Moving average\n\n**4. Candidate models:**\n- SARIMA (captures ARMA + seasonality)\n- ETS (automatic model selection)\n- Prophet (handles holidays, easy to use)\n- XGBoost with lag features (if many series)\n\n**5. Evaluation:**\n- Rolling origin CV (last 8-12 weeks)\n- Metrics: MAE, MAPE, MASE\n- Compare to baselines\n\n**6. Production considerations:**\n- Forecast horizon needed\n- Update frequency\n- Uncertainty communication\n- Monitoring and retraining schedule\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Jumping to complex models. Start simple (seasonal naive is often hard to beat) and add complexity only if justified by evaluation.\n</div>\n</div>\n</details>", "line_start": 204, "level": 2}, {"heading": "References", "content": "1. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts.\n2. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis*. Wiley.\n3. Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press.\n4. Shumway, R. H., & Stoffer, D. S. (2017). *Time Series Analysis and Its Applications*. Springer.", "line_start": 336, "level": 1}]}, "docs/en/model-selection/cross-validation.md": {"path": "docs/en/model-selection/cross-validation.md", "title": "Cross-Validation for Time Series", "content": "# Cross-Validation for Time Series\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Standard k-fold CV breaks temporal order and causes data leakage. Time series CV uses rolling/expanding windows: train on past, test on future. Key methods: rolling origin, blocked CV, h-step-ahead CV. Always respect temporal order. CV estimates out-of-sample error; use for model selection and hyperparameter tuning.\n</div>\n\n## Core Definitions\n\n**Time Series Cross-Validation:**\nEvaluate model performance by repeatedly:\n1. Training on past data\n2. Testing on future data (never seen during training)\n3. Rolling the window forward\n\n**Rolling Origin Evaluation:**\n```\nTrain: [1, ..., t]     → Test: [t+1, ..., t+h]\nTrain: [1, ..., t+1]   → Test: [t+2, ..., t+h+1]\n...\nTrain: [1, ..., T-h]   → Test: [T-h+1, ..., T]\n```\n\n**Expanding Window:** Training set grows; uses all past data.\n\n**Sliding Window:** Training set is fixed size; drops oldest data.\n\n## Math and Derivations\n\n### Rolling Origin Forecast Error\n\nFor origin $t$ and horizon $h$:\n$$e_{t+h|t} = y_{t+h} - \\hat{y}_{t+h|t}$$\n\nAverage over all origins:\n$$\\text{RMSE}(h) = \\sqrt{\\frac{1}{T-t_0-h+1}\\sum_{t=t_0}^{T-h}e_{t+h|t}^2}$$\n\n### Forecast Accuracy Metrics\n\n**MAE (Mean Absolute Error):**\n$$\\text{MAE} = \\frac{1}{n}\\sum|e_t|$$\n\n**RMSE (Root Mean Squared Error):**\n$$\\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum e_t^2}$$\n\n**MAPE (Mean Absolute Percentage Error):**\n$$\\text{MAPE} = \\frac{100}{n}\\sum\\left|\\frac{e_t}{y_t}\\right|$$\n\n**SMAPE (Symmetric MAPE):**\n$$\\text{sMAPE} = \\frac{200}{n}\\sum\\frac{|e_t|}{|y_t| + |\\hat{y}_t|}$$\n\n**MASE (Mean Absolute Scaled Error):**\n$$\\text{MASE} = \\frac{\\text{MAE}}{\\frac{1}{n-1}\\sum_{t=2}^{n}|y_t - y_{t-1}|}$$\n\nMASE < 1 means better than naive forecast.\n\n### Why Standard CV Fails\n\nStandard k-fold CV:\n- Randomly splits data\n- Training fold may contain future observations\n- Test fold may contain past observations\n\nThis causes **data leakage**: model sees future information during training, giving optimistic error estimates.\n\n## Algorithm/Model Sketch\n\n**Rolling Origin CV:**\n\n```python\ndef rolling_origin_cv(y, model_fn, min_train, horizon, step=1):\n    \"\"\"\n    y: time series\n    model_fn: function that fits model and returns forecasts\n    min_train: minimum training size\n    horizon: forecast horizon\n    step: how much to move origin each iteration\n    \"\"\"\n    errors = []\n\n    for t in range(min_train, len(y) - horizon, step):\n        # Train on [0:t], test on [t:t+horizon]\n        train = y[:t]\n        test = y[t:t+horizon]\n\n        # Fit and forecast\n        forecast = model_fn(train, horizon)\n\n        # Store errors\n        errors.append(test - forecast)\n\n    return np.array(errors)\n```\n\n**Blocked CV (for related series):**\n```\nFold 1: Train [blocks 2,3,4,5] → Test [block 1]\nFold 2: Train [blocks 1,3,4,5] → Test [block 2]\n...\n```\n\nBlocks are contiguous time periods. Less ideal but useful when multiple series share parameters.\n\n## Common Pitfalls\n\n1. **Using standard k-fold CV**: Breaks temporal order, causes leakage. Never use for time series.\n\n2. **Testing on training period**: Even with rolling origin, some implementations accidentally include overlapping data.\n\n3. **Ignoring horizon**: CV for h=1 doesn't guarantee good h=10 performance. Match CV horizon to application.\n\n4. **Fixed origin only**: Testing from single origin underestimates variance. Use multiple origins.\n\n5. **Computation cost**: Full rolling CV with refitting is expensive. Consider step > 1 or fixed models.\n\n6. **Non-representative windows**: If dynamics change, old data may mislead. Consider sliding window.\n\n## Mini Example\n\n```python\nimport numpy as np\nfrom statsmodels.tsa.arima.model import ARIMA\n\ndef mase(actual, forecast, train):\n    \"\"\"Mean Absolute Scaled Error.\"\"\"\n    mae = np.mean(np.abs(actual - forecast))\n    naive_mae = np.mean(np.abs(np.diff(train)))\n    return mae / naive_mae\n\n# Generate data\nnp.random.seed(42)\nn = 200\ny = np.cumsum(np.random.randn(n)) + 0.1 * np.arange(n)\n\n# Rolling origin CV\nmin_train = 100\nhorizon = 5\nstep = 5\n\nresults = {'ARIMA(1,1,0)': [], 'ARIMA(1,1,1)': [], 'ARIMA(2,1,1)': []}\n\nfor t in range(min_train, len(y) - horizon, step):\n    train = y[:t]\n    test = y[t:t+horizon]\n\n    for name, order in [('ARIMA(1,1,0)', (1,1,0)),\n                        ('ARIMA(1,1,1)', (1,1,1)),\n                        ('ARIMA(2,1,1)', (2,1,1))]:\n        try:\n            model = ARIMA(train, order=order).fit()\n            forecast = model.forecast(horizon)\n            error = mase(test, forecast, train)\n            results[name].append(error)\n        except:\n            results[name].append(np.nan)\n\n# Compare models\nprint(\"Cross-Validation Results (MASE):\")\nfor name, errors in results.items():\n    valid_errors = [e for e in errors if not np.isnan(e)]\n    print(f\"  {name}: Mean={np.mean(valid_errors):.3f}, \"\n          f\"Std={np.std(valid_errors):.3f}\")\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why does standard k-fold cross-validation fail for time series?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Standard k-fold CV randomly assigns observations to folds, breaking temporal order. This causes:\n\n1. **Data leakage**: Training data may include future observations relative to test data\n2. **Unrealistic evaluation**: In practice, you never have future data to train on\n3. **Optimistic error estimates**: Model implicitly learns from future, inflating apparent accuracy\n4. **Autocorrelation ignored**: Nearby points in train and test are correlated, reducing effective test independence\n\n**Example:** If test fold contains y[50:60] and train contains y[55:100], the model uses y[55:60] (future!) during training.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using sklearn's `cross_val_score` directly on time series. Always use `TimeSeriesSplit` or custom rolling evaluation.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> What's the difference between expanding window and sliding window CV?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Expanding window:**\n- Training set grows: [1:t], [1:t+1], [1:t+2], ...\n- Uses all historical data\n- Better for stable processes\n- More data → lower variance estimates\n\n**Sliding window:**\n- Training set is fixed size: [t-w:t], [t-w+1:t+1], ...\n- Drops oldest data\n- Better for non-stationary/evolving processes\n- Adapts to recent patterns\n\n**Choice depends on:**\n- Stationarity: non-stationary → sliding\n- Data availability: limited → expanding\n- Concept drift: present → sliding\n- Computational cost: sliding is more expensive (always refits)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using expanding window when dynamics change. Old data misleads the model. Check for structural breaks.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Why is MASE preferred over MAPE for forecast evaluation?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> MASE advantages:\n\n1. **Scale-independent**: Like MAPE, but handles zeros\n2. **No division by zero**: MAPE fails when $y_t = 0$\n3. **Symmetric**: Doesn't favor under/over-prediction asymmetrically (unlike MAPE)\n4. **Benchmark comparison**: MASE < 1 means better than naive\n5. **Well-defined for intermittent series**: Common in demand forecasting\n\n**Formula:**\n$$\\text{MASE} = \\frac{\\text{MAE}}{\\text{MAE}_{\\text{naive}}}$$\n\nwhere MAE_naive uses seasonal naive or 1-step naive as benchmark.\n\n**MAPE problems:**\n- Infinite when $y_t = 0$\n- Asymmetric: 50% error on y=100 (predict 50 or 150) treated differently\n- Scale-dependent interpretation\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using MAPE for intermittent demand or data with zeros — gives undefined or misleading results.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> How do you choose the minimum training size for rolling origin CV?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Balance between:\n\n1. **Statistical requirements:**\n   - Need enough data for reliable estimation\n   - Rule: at least 3-5 observations per parameter\n   - For ARIMA(p,d,q): min ~50 + 10(p+q) observations\n\n2. **Practical considerations:**\n   - More training → better model estimates\n   - But also → fewer CV folds → higher variance of CV estimate\n   - Typical: 60-80% of data for first training set\n\n3. **Domain knowledge:**\n   - If dynamics change, recent data matters more\n   - Full business cycles should be included (e.g., full year for seasonal)\n\n**Formula guidance:**\n$$\\text{min\\_train} = \\max(50, 2 \\times m, 5k + 10)$$\n\nwhere m = seasonal period, k = number of parameters.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using too small min_train gives unreliable early models; using too large leaves few CV folds for variance estimation.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You have 3 years of daily data (1095 observations) and need to select a model for 7-day forecasting. Design a CV scheme.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Recommended scheme:**\n\n```\nmin_train = 365 (1 full year to capture seasonality)\nhorizon = 7\nstep = 7 (weekly, reduces computation)\n```\n\nThis gives: (1095 - 365 - 7) / 7 ≈ 103 CV folds\n\n**Implementation:**\n```python\nfor t in range(365, 1095-7, 7):\n    train = data[:t]\n    test = data[t:t+7]\n    # Fit and evaluate\n```\n\n**Considerations:**\n1. **Include full seasonality**: 365 days captures annual pattern\n2. **Match horizon**: CV horizon = production horizon (7 days)\n3. **Step = horizon**: Non-overlapping test sets for independence\n4. **Metrics**: Use MASE, MAE, RMSE at each horizon h=1,...,7\n\n**Variants:**\n- Sliding window: train on last 365 days only (if non-stationary)\n- Gap: skip 1-2 days between train/test to simulate production delay\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using step=1 with 1095 observations → 723 fits, very slow. Use larger step for efficiency.\n</div>\n</div>\n</details>\n\n## References\n\n1. Bergmeir, C., & Benítez, J. M. (2012). On the use of cross-validation for time series predictor evaluation. *Information Sciences*, 191, 192-213.\n2. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 5.\n3. Tashman, L. J. (2000). Out-of-sample tests of forecasting accuracy: an analysis and review. *IJF*, 16(4), 437-450.\n4. Cerqueira, V., Torgo, L., & Mozetič, I. (2020). Evaluating time series forecasting models: An empirical study on performance estimation methods. *Machine Learning*, 109(11), 1997-2028.\n", "sections": [{"heading": "Cross-Validation for Time Series", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Standard k-fold CV breaks temporal order and causes data leakage. Time series CV uses rolling/expanding windows: train on past, test on future. Key methods: rolling origin, blocked CV, h-step-ahead CV. Always respect temporal order. CV estimates out-of-sample error; use for model selection and hyperparameter tuning.\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**Time Series Cross-Validation:**\nEvaluate model performance by repeatedly:\n1. Training on past data\n2. Testing on future data (never seen during training)\n3. Rolling the window forward\n\n**Rolling Origin Evaluation:**\n```\nTrain: [1, ..., t]     → Test: [t+1, ..., t+h]\nTrain: [1, ..., t+1]   → Test: [t+2, ..., t+h+1]\n...\nTrain: [1, ..., T-h]   → Test: [T-h+1, ..., T]\n```\n\n**Expanding Window:** Training set grows; uses all past data.\n\n**Sliding Window:** Training set is fixed size; drops oldest data.", "line_start": 7, "level": 2}, {"heading": "Rolling Origin Forecast Error", "content": "For origin $t$ and horizon $h$:\n$$e_{t+h|t} = y_{t+h} - \\hat{y}_{t+h|t}$$\n\nAverage over all origins:\n$$\\text{RMSE}(h) = \\sqrt{\\frac{1}{T-t_0-h+1}\\sum_{t=t_0}^{T-h}e_{t+h|t}^2}$$", "line_start": 29, "level": 3}, {"heading": "Forecast Accuracy Metrics", "content": "**MAE (Mean Absolute Error):**\n$$\\text{MAE} = \\frac{1}{n}\\sum|e_t|$$\n\n**RMSE (Root Mean Squared Error):**\n$$\\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum e_t^2}$$\n\n**MAPE (Mean Absolute Percentage Error):**\n$$\\text{MAPE} = \\frac{100}{n}\\sum\\left|\\frac{e_t}{y_t}\\right|$$\n\n**SMAPE (Symmetric MAPE):**\n$$\\text{sMAPE} = \\frac{200}{n}\\sum\\frac{|e_t|}{|y_t| + |\\hat{y}_t|}$$\n\n**MASE (Mean Absolute Scaled Error):**\n$$\\text{MASE} = \\frac{\\text{MAE}}{\\frac{1}{n-1}\\sum_{t=2}^{n}|y_t - y_{t-1}|}$$\n\nMASE < 1 means better than naive forecast.", "line_start": 37, "level": 3}, {"heading": "Why Standard CV Fails", "content": "Standard k-fold CV:\n- Randomly splits data\n- Training fold may contain future observations\n- Test fold may contain past observations\n\nThis causes **data leakage**: model sees future information during training, giving optimistic error estimates.", "line_start": 56, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**Rolling Origin CV:**\n\n```python\ndef rolling_origin_cv(y, model_fn, min_train, horizon, step=1):\n    \"\"\"\n    y: time series\n    model_fn: function that fits model and returns forecasts\n    min_train: minimum training size\n    horizon: forecast horizon\n    step: how much to move origin each iteration\n    \"\"\"\n    errors = []\n\n    for t in range(min_train, len(y) - horizon, step):\n        # Train on [0:t], test on [t:t+horizon]\n        train = y[:t]\n        test = y[t:t+horizon]\n\n        # Fit and forecast\n        forecast = model_fn(train, horizon)\n\n        # Store errors\n        errors.append(test - forecast)\n\n    return np.array(errors)\n```\n\n**Blocked CV (for related series):**\n```\nFold 1: Train [blocks 2,3,4,5] → Test [block 1]\nFold 2: Train [blocks 1,3,4,5] → Test [block 2]\n...\n```\n\nBlocks are contiguous time periods. Less ideal but useful when multiple series share parameters.", "line_start": 65, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Using standard k-fold CV**: Breaks temporal order, causes leakage. Never use for time series.\n\n2. **Testing on training period**: Even with rolling origin, some implementations accidentally include overlapping data.\n\n3. **Ignoring horizon**: CV for h=1 doesn't guarantee good h=10 performance. Match CV horizon to application.\n\n4. **Fixed origin only**: Testing from single origin underestimates variance. Use multiple origins.\n\n5. **Computation cost**: Full rolling CV with refitting is expensive. Consider step > 1 or fixed models.\n\n6. **Non-representative windows**: If dynamics change, old data may mislead. Consider sliding window.", "line_start": 103, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.arima.model import ARIMA\n\ndef mase(actual, forecast, train):\n    \"\"\"Mean Absolute Scaled Error.\"\"\"\n    mae = np.mean(np.abs(actual - forecast))\n    naive_mae = np.mean(np.abs(np.diff(train)))\n    return mae / naive_mae", "line_start": 117, "level": 1}, {"heading": "Generate data", "content": "np.random.seed(42)\nn = 200\ny = np.cumsum(np.random.randn(n)) + 0.1 * np.arange(n)", "line_start": 129, "level": 1}, {"heading": "Rolling origin CV", "content": "min_train = 100\nhorizon = 5\nstep = 5\n\nresults = {'ARIMA(1,1,0)': [], 'ARIMA(1,1,1)': [], 'ARIMA(2,1,1)': []}\n\nfor t in range(min_train, len(y) - horizon, step):\n    train = y[:t]\n    test = y[t:t+horizon]\n\n    for name, order in [('ARIMA(1,1,0)', (1,1,0)),\n                        ('ARIMA(1,1,1)', (1,1,1)),\n                        ('ARIMA(2,1,1)', (2,1,1))]:\n        try:\n            model = ARIMA(train, order=order).fit()\n            forecast = model.forecast(horizon)\n            error = mase(test, forecast, train)\n            results[name].append(error)\n        except:\n            results[name].append(np.nan)", "line_start": 134, "level": 1}, {"heading": "Compare models", "content": "print(\"Cross-Validation Results (MASE):\")\nfor name, errors in results.items():\n    valid_errors = [e for e in errors if not np.isnan(e)]\n    print(f\"  {name}: Mean={np.mean(valid_errors):.3f}, \"\n          f\"Std={np.std(valid_errors):.3f}\")\n```", "line_start": 156, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why does standard k-fold cross-validation fail for time series?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Standard k-fold CV randomly assigns observations to folds, breaking temporal order. This causes:\n\n1. **Data leakage**: Training data may include future observations relative to test data\n2. **Unrealistic evaluation**: In practice, you never have future data to train on\n3. **Optimistic error estimates**: Model implicitly learns from future, inflating apparent accuracy\n4. **Autocorrelation ignored**: Nearby points in train and test are correlated, reducing effective test independence\n\n**Example:** If test fold contains y[50:60] and train contains y[55:100], the model uses y[55:60] (future!) during training.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using sklearn's `cross_val_score` directly on time series. Always use `TimeSeriesSplit` or custom rolling evaluation.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> What's the difference between expanding window and sliding window CV?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Expanding window:**\n- Training set grows: [1:t], [1:t+1], [1:t+2], ...\n- Uses all historical data\n- Better for stable processes\n- More data → lower variance estimates\n\n**Sliding window:**\n- Training set is fixed size: [t-w:t], [t-w+1:t+1], ...\n- Drops oldest data\n- Better for non-stationary/evolving processes\n- Adapts to recent patterns\n\n**Choice depends on:**\n- Stationarity: non-stationary → sliding\n- Data availability: limited → expanding\n- Concept drift: present → sliding\n- Computational cost: sliding is more expensive (always refits)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using expanding window when dynamics change. Old data misleads the model. Check for structural breaks.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Why is MASE preferred over MAPE for forecast evaluation?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> MASE advantages:\n\n1. **Scale-independent**: Like MAPE, but handles zeros\n2. **No division by zero**: MAPE fails when $y_t = 0$\n3. **Symmetric**: Doesn't favor under/over-prediction asymmetrically (unlike MAPE)\n4. **Benchmark comparison**: MASE < 1 means better than naive\n5. **Well-defined for intermittent series**: Common in demand forecasting\n\n**Formula:**\n$$\\text{MASE} = \\frac{\\text{MAE}}{\\text{MAE}_{\\text{naive}}}$$\n\nwhere MAE_naive uses seasonal naive or 1-step naive as benchmark.\n\n**MAPE problems:**\n- Infinite when $y_t = 0$\n- Asymmetric: 50% error on y=100 (predict 50 or 150) treated differently\n- Scale-dependent interpretation\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using MAPE for intermittent demand or data with zeros — gives undefined or misleading results.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> How do you choose the minimum training size for rolling origin CV?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Balance between:\n\n1. **Statistical requirements:**\n   - Need enough data for reliable estimation\n   - Rule: at least 3-5 observations per parameter\n   - For ARIMA(p,d,q): min ~50 + 10(p+q) observations\n\n2. **Practical considerations:**\n   - More training → better model estimates\n   - But also → fewer CV folds → higher variance of CV estimate\n   - Typical: 60-80% of data for first training set\n\n3. **Domain knowledge:**\n   - If dynamics change, recent data matters more\n   - Full business cycles should be included (e.g., full year for seasonal)\n\n**Formula guidance:**\n$$\\text{min\\_train} = \\max(50, 2 \\times m, 5k + 10)$$\n\nwhere m = seasonal period, k = number of parameters.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using too small min_train gives unreliable early models; using too large leaves few CV folds for variance estimation.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You have 3 years of daily data (1095 observations) and need to select a model for 7-day forecasting. Design a CV scheme.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Recommended scheme:**\n\n```\nmin_train = 365 (1 full year to capture seasonality)\nhorizon = 7\nstep = 7 (weekly, reduces computation)\n```\n\nThis gives: (1095 - 365 - 7) / 7 ≈ 103 CV folds\n\n**Implementation:**\n```python\nfor t in range(365, 1095-7, 7):\n    train = data[:t]\n    test = data[t:t+7]\n    # Fit and evaluate\n```\n\n**Considerations:**\n1. **Include full seasonality**: 365 days captures annual pattern\n2. **Match horizon**: CV horizon = production horizon (7 days)\n3. **Step = horizon**: Non-overlapping test sets for independence\n4. **Metrics**: Use MASE, MAE, RMSE at each horizon h=1,...,7\n\n**Variants:**\n- Sliding window: train on last 365 days only (if non-stationary)\n- Gap: skip 1-2 days between train/test to simulate production delay\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using step=1 with 1095 observations → 723 fits, very slow. Use larger step for efficiency.\n</div>\n</div>\n</details>", "line_start": 164, "level": 2}, {"heading": "References", "content": "1. Bergmeir, C., & Benítez, J. M. (2012). On the use of cross-validation for time series predictor evaluation. *Information Sciences*, 191, 192-213.\n2. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 5.\n3. Tashman, L. J. (2000). Out-of-sample tests of forecasting accuracy: an analysis and review. *IJF*, 16(4), 437-450.\n4. Cerqueira, V., Torgo, L., & Mozetič, I. (2020). Evaluating time series forecasting models: An empirical study on performance estimation methods. *Machine Learning*, 109(11), 1997-2028.", "line_start": 314, "level": 1}]}, "docs/en/model-selection/information-criteria.md": {"path": "docs/en/model-selection/information-criteria.md", "title": "Information Criteria", "content": "# Information Criteria\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Information criteria balance model fit against complexity. AIC = -2log(L) + 2k favors prediction; BIC = -2log(L) + k·log(n) favors true model recovery. Lower is better. AIC tends to select larger models; BIC is more parsimonious. For small samples, use AICc. When criteria disagree, consider your goal: prediction (AIC) vs. inference (BIC).\n</div>\n\n## Core Definitions\n\n**AIC (Akaike Information Criterion):**\n$$\\text{AIC} = -2\\ln(\\hat{L}) + 2k$$\n\n**BIC (Bayesian/Schwarz Information Criterion):**\n$$\\text{BIC} = -2\\ln(\\hat{L}) + k\\ln(n)$$\n\n**AICc (Corrected AIC):**\n$$\\text{AICc} = \\text{AIC} + \\frac{2k(k+1)}{n-k-1}$$\n\n**Components:**\n- $\\hat{L}$: Maximum likelihood\n- $k$: Number of estimated parameters\n- $n$: Sample size\n\n## Math and Derivations\n\n### AIC Derivation (Intuition)\n\nAIC minimizes expected Kullback-Leibler divergence between true and fitted model:\n$$\\text{KL}(f||g_{\\hat{\\theta}}) = E_f[\\ln f(y)] - E_f[\\ln g_{\\hat{\\theta}}(y)]$$\n\nAkaike showed:\n$$E[-2\\ln g_{\\hat{\\theta}}(y_{new})] \\approx -2\\ln g_{\\hat{\\theta}}(y) + 2k$$\n\nMinimizing AIC approximately minimizes out-of-sample prediction error.\n\n### BIC Derivation (Intuition)\n\nBIC approximates log marginal likelihood:\n$$\\ln p(y|M) \\approx \\ln p(y|\\hat{\\theta},M) - \\frac{k}{2}\\ln(n)$$\n\nBIC is consistent: if true model is among candidates, BIC selects it with probability → 1 as n → ∞.\n\n### For Gaussian Time Series\n\nWith residual variance $\\hat{\\sigma}^2$:\n$$\\text{AIC} = n\\ln(\\hat{\\sigma}^2) + 2k$$\n$$\\text{BIC} = n\\ln(\\hat{\\sigma}^2) + k\\ln(n)$$\n\n### Penalty Comparison\n\n| n | AIC penalty | BIC penalty |\n|---|-------------|-------------|\n| 8 | 2k | 2.08k |\n| 20 | 2k | 3.00k |\n| 100 | 2k | 4.61k |\n| 1000 | 2k | 6.91k |\n\nBIC penalty grows with n; AIC stays constant.\n\n## Algorithm/Model Sketch\n\n**Model Selection Procedure:**\n\n```\n1. Define candidate models: M₁, M₂, ..., Mₘ\n2. For each model Mᵢ:\n   - Fit model by MLE\n   - Compute AIC and BIC\n\n3. Rank by criterion:\n   - For prediction: prefer AIC (or AICc for small n)\n   - For inference: prefer BIC\n\n4. Compare top candidates:\n   - ΔAIC < 2: essentially equivalent\n   - ΔAIC 2-7: some support for better model\n   - ΔAIC > 10: strong support for better model\n\n5. Validate:\n   - Check residual diagnostics for selected model\n   - Consider out-of-sample testing\n```\n\n**Akaike Weights:**\n$$w_i = \\frac{\\exp(-\\frac{1}{2}\\Delta\\text{AIC}_i)}{\\sum_j\\exp(-\\frac{1}{2}\\Delta\\text{AIC}_j)}$$\n\nGives probability-like weights for model averaging.\n\n## Common Pitfalls\n\n1. **Treating criteria as absolute**: Only relative values matter. AIC = 1000 vs AIC = 1002 is a meaningful comparison.\n\n2. **Ignoring sample size for AIC/BIC choice**: For n < 40, AICc is essential. For large n, BIC may be too parsimonious.\n\n3. **Using wrong likelihood**: Comparing models with different transformations (log vs. level) requires adjusting likelihood.\n\n4. **Overfitting with AIC in large samples**: As n grows, AIC allows increasingly complex models. Consider BIC for parsimony.\n\n5. **Ignoring ties**: If ΔAIC < 2, models are equivalent. Don't over-interpret small differences.\n\n6. **Forgetting model checking**: Lowest IC doesn't guarantee good model. Always check residuals.\n\n## Mini Example\n\n```python\nimport numpy as np\nfrom statsmodels.tsa.arima.model import ARIMA\n\n# Generate ARMA(1,1) data\nnp.random.seed(42)\nn = 200\nphi, theta = 0.7, 0.3\neps = np.random.randn(n + 1)\ny = np.zeros(n)\nfor t in range(1, n):\n    y[t] = phi * y[t-1] + eps[t] + theta * eps[t-1]\n\n# Fit candidate models\ncandidates = [\n    ('AR(1)', (1, 0, 0)),\n    ('AR(2)', (2, 0, 0)),\n    ('MA(1)', (0, 0, 1)),\n    ('MA(2)', (0, 0, 2)),\n    ('ARMA(1,1)', (1, 0, 1)),\n    ('ARMA(2,1)', (2, 0, 1)),\n]\n\nresults = []\nfor name, order in candidates:\n    model = ARIMA(y, order=order).fit()\n    results.append({\n        'Model': name,\n        'AIC': model.aic,\n        'BIC': model.bic,\n        'k': sum(order) + 1  # +1 for variance\n    })\n\n# Display sorted by AIC\nimport pandas as pd\ndf = pd.DataFrame(results).sort_values('AIC')\ndf['ΔAIC'] = df['AIC'] - df['AIC'].min()\ndf['ΔBIC'] = df['BIC'] - df['BIC'].min()\nprint(df.to_string(index=False))\n\n# True model ARMA(1,1) should rank well\nprint(f\"\\nBest by AIC: {df.iloc[0]['Model']}\")\nprint(f\"Best by BIC: {df.sort_values('BIC').iloc[0]['Model']}\")\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why does BIC tend to select simpler models than AIC?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> BIC has a stronger penalty for parameters: $k\\ln(n)$ vs $2k$.\n\nFor $n > 8$: $\\ln(n) > 2$, so BIC penalizes each parameter more.\n\n**Mathematical comparison:**\n- AIC adds $2k$ regardless of sample size\n- BIC adds $k\\ln(n)$, which grows with n\n\nFor n = 100: BIC adds 4.6k vs AIC's 2k per parameter.\n\n**Consequence:** BIC requires stronger likelihood improvement to justify additional parameters, leading to simpler models.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Thinking simpler is always better. BIC can underfit when true model is complex. For forecasting, AIC often wins because it allows capturing more signal.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> When should you use AICc instead of AIC?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Use AICc when sample size is small relative to number of parameters.\n\n**Rule of thumb:** Use AICc when $n/k < 40$.\n\n**Why AICc?**\nAIC is derived asymptotically. For small samples, it underpenalizes complexity, leading to overfitting.\n\nAICc correction: $\\frac{2k(k+1)}{n-k-1}$\n\nThis additional term is large when n ≈ k but vanishes as n → ∞.\n\n**Example:**\n- n = 50, k = 5\n- AIC penalty: 10\n- AICc penalty: 10 + 2(5)(6)/(50-6) ≈ 10 + 1.4 = 11.4\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using AIC by default without checking n/k ratio. For small samples, AIC systematically selects overly complex models.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Derive the formula for AICc from AIC.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> AICc adds a bias correction term:\n\n$$\\text{AICc} = \\text{AIC} + \\frac{2k(k+1)}{n-k-1}$$\n\n**Derivation sketch:**\nFor regression with Gaussian errors, Hurvich and Tsai (1989) showed:\n\n$$E[\\text{AIC}] = E[-2\\ln L] + 2k$$\n\nhas bias when n is small. The exact expected value:\n\n$$E[-2\\ln L(\\hat{\\theta})] + \\frac{2kn}{n-k-1}$$\n\nleads to:\n$$\\text{AICc} = -2\\ln L + \\frac{2kn}{n-k-1} = \\text{AIC} + \\frac{2k^2 + 2k}{n-k-1}$$\n\nAs $n \\to \\infty$: $\\frac{2k(k+1)}{n-k-1} \\to 0$, so AICc → AIC.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> AICc formula assumes residual variance is estimated. For restricted cases, different corrections apply.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Show that BIC is consistent (selects true model) while AIC is not.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**BIC Consistency:**\nFor nested models, consider true model $M_0$ (k₀ params) vs larger $M_1$ (k₁ > k₀).\n\n$$\\text{BIC}_1 - \\text{BIC}_0 = -2(\\ln L_1 - \\ln L_0) + (k_1 - k_0)\\ln n$$\n\nBy likelihood ratio theory: $-2(\\ln L_1 - \\ln L_0) = O_p(1)$ (bounded)\nBut penalty: $(k_1 - k_0)\\ln n \\to \\infty$\n\nSo $P(\\text{BIC}_1 > \\text{BIC}_0) \\to 1$.\n\n**AIC Inconsistency:**\n$$\\text{AIC}_1 - \\text{AIC}_0 = -2(\\ln L_1 - \\ln L_0) + 2(k_1 - k_0)$$\n\nExtra parameters add fixed penalty 2(k₁-k₀), while likelihood improvement is $O_p(1)$. There's always positive probability that extra parameters improve fit enough to offset the fixed penalty.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Consistency ≠ better forecasts. AIC minimizes prediction error; BIC identifies true model. Different goals.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> AIC selects ARIMA(2,1,2) while BIC selects ARIMA(1,1,1). AIC difference is 4. Which do you choose?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> It depends on your goal and context:\n\n**For forecasting:** Lean toward ARIMA(2,1,2) (AIC's choice)\n- ΔAIC = 4 suggests meaningful predictive improvement\n- Extra complexity may capture real dynamics\n\n**For interpretation:** Lean toward ARIMA(1,1,1) (BIC's choice)\n- Simpler, more interpretable\n- Less risk of overfitting\n\n**Recommended approach:**\n1. Compare out-of-sample forecast accuracy\n2. Check residual diagnostics for both\n3. If similar performance, prefer simpler\n4. Consider ensemble/averaging\n\n**Decision matrix:**\n\n| Factor | Favors (2,1,2) | Favors (1,1,1) |\n|--------|----------------|----------------|\n| Large sample | ✓ | |\n| Short forecast horizon | ✓ | |\n| Complex dynamics expected | ✓ | |\n| Interpretability needed | | ✓ |\n| Small sample | | ✓ |\n| Long forecast horizon | | ✓ |\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Picking one criterion dogmatically. Use domain knowledge, validation, and judgment alongside IC.\n</div>\n</div>\n</details>\n\n## References\n\n1. Akaike, H. (1974). A new look at the statistical model identification. *IEEE Transactions on Automatic Control*, 19(6), 716-723.\n2. Schwarz, G. (1978). Estimating the dimension of a model. *Annals of Statistics*, 6(2), 461-464.\n3. Burnham, K. P., & Anderson, D. R. (2002). *Model Selection and Multimodel Inference*. Springer.\n4. Hurvich, C. M., & Tsai, C. L. (1989). Regression and time series model selection in small samples. *Biometrika*, 76(2), 297-307.\n", "sections": [{"heading": "Information Criteria", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Information criteria balance model fit against complexity. AIC = -2log(L) + 2k favors prediction; BIC = -2log(L) + k·log(n) favors true model recovery. Lower is better. AIC tends to select larger models; BIC is more parsimonious. For small samples, use AICc. When criteria disagree, consider your goal: prediction (AIC) vs. inference (BIC).\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**AIC (Akaike Information Criterion):**\n$$\\text{AIC} = -2\\ln(\\hat{L}) + 2k$$\n\n**BIC (Bayesian/Schwarz Information Criterion):**\n$$\\text{BIC} = -2\\ln(\\hat{L}) + k\\ln(n)$$\n\n**AICc (Corrected AIC):**\n$$\\text{AICc} = \\text{AIC} + \\frac{2k(k+1)}{n-k-1}$$\n\n**Components:**\n- $\\hat{L}$: Maximum likelihood\n- $k$: Number of estimated parameters\n- $n$: Sample size", "line_start": 7, "level": 2}, {"heading": "AIC Derivation (Intuition)", "content": "AIC minimizes expected Kullback-Leibler divergence between true and fitted model:\n$$\\text{KL}(f||g_{\\hat{\\theta}}) = E_f[\\ln f(y)] - E_f[\\ln g_{\\hat{\\theta}}(y)]$$\n\nAkaike showed:\n$$E[-2\\ln g_{\\hat{\\theta}}(y_{new})] \\approx -2\\ln g_{\\hat{\\theta}}(y) + 2k$$\n\nMinimizing AIC approximately minimizes out-of-sample prediction error.", "line_start": 25, "level": 3}, {"heading": "BIC Derivation (Intuition)", "content": "BIC approximates log marginal likelihood:\n$$\\ln p(y|M) \\approx \\ln p(y|\\hat{\\theta},M) - \\frac{k}{2}\\ln(n)$$\n\nBIC is consistent: if true model is among candidates, BIC selects it with probability → 1 as n → ∞.", "line_start": 35, "level": 3}, {"heading": "For Gaussian Time Series", "content": "With residual variance $\\hat{\\sigma}^2$:\n$$\\text{AIC} = n\\ln(\\hat{\\sigma}^2) + 2k$$\n$$\\text{BIC} = n\\ln(\\hat{\\sigma}^2) + k\\ln(n)$$", "line_start": 42, "level": 3}, {"heading": "Penalty Comparison", "content": "| n | AIC penalty | BIC penalty |\n|---|-------------|-------------|\n| 8 | 2k | 2.08k |\n| 20 | 2k | 3.00k |\n| 100 | 2k | 4.61k |\n| 1000 | 2k | 6.91k |\n\nBIC penalty grows with n; AIC stays constant.", "line_start": 48, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**Model Selection Procedure:**\n\n```\n1. Define candidate models: M₁, M₂, ..., Mₘ\n2. For each model Mᵢ:\n   - Fit model by MLE\n   - Compute AIC and BIC\n\n3. Rank by criterion:\n   - For prediction: prefer AIC (or AICc for small n)\n   - For inference: prefer BIC\n\n4. Compare top candidates:\n   - ΔAIC < 2: essentially equivalent\n   - ΔAIC 2-7: some support for better model\n   - ΔAIC > 10: strong support for better model\n\n5. Validate:\n   - Check residual diagnostics for selected model\n   - Consider out-of-sample testing\n```\n\n**Akaike Weights:**\n$$w_i = \\frac{\\exp(-\\frac{1}{2}\\Delta\\text{AIC}_i)}{\\sum_j\\exp(-\\frac{1}{2}\\Delta\\text{AIC}_j)}$$\n\nGives probability-like weights for model averaging.", "line_start": 59, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Treating criteria as absolute**: Only relative values matter. AIC = 1000 vs AIC = 1002 is a meaningful comparison.\n\n2. **Ignoring sample size for AIC/BIC choice**: For n < 40, AICc is essential. For large n, BIC may be too parsimonious.\n\n3. **Using wrong likelihood**: Comparing models with different transformations (log vs. level) requires adjusting likelihood.\n\n4. **Overfitting with AIC in large samples**: As n grows, AIC allows increasingly complex models. Consider BIC for parsimony.\n\n5. **Ignoring ties**: If ΔAIC < 2, models are equivalent. Don't over-interpret small differences.\n\n6. **Forgetting model checking**: Lowest IC doesn't guarantee good model. Always check residuals.", "line_start": 88, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.arima.model import ARIMA", "line_start": 102, "level": 1}, {"heading": "Generate ARMA(1,1) data", "content": "np.random.seed(42)\nn = 200\nphi, theta = 0.7, 0.3\neps = np.random.randn(n + 1)\ny = np.zeros(n)\nfor t in range(1, n):\n    y[t] = phi * y[t-1] + eps[t] + theta * eps[t-1]", "line_start": 108, "level": 1}, {"heading": "Fit candidate models", "content": "candidates = [\n    ('AR(1)', (1, 0, 0)),\n    ('AR(2)', (2, 0, 0)),\n    ('MA(1)', (0, 0, 1)),\n    ('MA(2)', (0, 0, 2)),\n    ('ARMA(1,1)', (1, 0, 1)),\n    ('ARMA(2,1)', (2, 0, 1)),\n]\n\nresults = []\nfor name, order in candidates:\n    model = ARIMA(y, order=order).fit()\n    results.append({\n        'Model': name,\n        'AIC': model.aic,\n        'BIC': model.bic,\n        'k': sum(order) + 1  # +1 for variance\n    })", "line_start": 117, "level": 1}, {"heading": "Display sorted by AIC", "content": "import pandas as pd\ndf = pd.DataFrame(results).sort_values('AIC')\ndf['ΔAIC'] = df['AIC'] - df['AIC'].min()\ndf['ΔBIC'] = df['BIC'] - df['BIC'].min()\nprint(df.to_string(index=False))", "line_start": 137, "level": 1}, {"heading": "True model ARMA(1,1) should rank well", "content": "print(f\"\\nBest by AIC: {df.iloc[0]['Model']}\")\nprint(f\"Best by BIC: {df.sort_values('BIC').iloc[0]['Model']}\")\n```", "line_start": 144, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why does BIC tend to select simpler models than AIC?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> BIC has a stronger penalty for parameters: $k\\ln(n)$ vs $2k$.\n\nFor $n > 8$: $\\ln(n) > 2$, so BIC penalizes each parameter more.\n\n**Mathematical comparison:**\n- AIC adds $2k$ regardless of sample size\n- BIC adds $k\\ln(n)$, which grows with n\n\nFor n = 100: BIC adds 4.6k vs AIC's 2k per parameter.\n\n**Consequence:** BIC requires stronger likelihood improvement to justify additional parameters, leading to simpler models.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Thinking simpler is always better. BIC can underfit when true model is complex. For forecasting, AIC often wins because it allows capturing more signal.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> When should you use AICc instead of AIC?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Use AICc when sample size is small relative to number of parameters.\n\n**Rule of thumb:** Use AICc when $n/k < 40$.\n\n**Why AICc?**\nAIC is derived asymptotically. For small samples, it underpenalizes complexity, leading to overfitting.\n\nAICc correction: $\\frac{2k(k+1)}{n-k-1}$\n\nThis additional term is large when n ≈ k but vanishes as n → ∞.\n\n**Example:**\n- n = 50, k = 5\n- AIC penalty: 10\n- AICc penalty: 10 + 2(5)(6)/(50-6) ≈ 10 + 1.4 = 11.4\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using AIC by default without checking n/k ratio. For small samples, AIC systematically selects overly complex models.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Derive the formula for AICc from AIC.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> AICc adds a bias correction term:\n\n$$\\text{AICc} = \\text{AIC} + \\frac{2k(k+1)}{n-k-1}$$\n\n**Derivation sketch:**\nFor regression with Gaussian errors, Hurvich and Tsai (1989) showed:\n\n$$E[\\text{AIC}] = E[-2\\ln L] + 2k$$\n\nhas bias when n is small. The exact expected value:\n\n$$E[-2\\ln L(\\hat{\\theta})] + \\frac{2kn}{n-k-1}$$\n\nleads to:\n$$\\text{AICc} = -2\\ln L + \\frac{2kn}{n-k-1} = \\text{AIC} + \\frac{2k^2 + 2k}{n-k-1}$$\n\nAs $n \\to \\infty$: $\\frac{2k(k+1)}{n-k-1} \\to 0$, so AICc → AIC.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> AICc formula assumes residual variance is estimated. For restricted cases, different corrections apply.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Show that BIC is consistent (selects true model) while AIC is not.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**BIC Consistency:**\nFor nested models, consider true model $M_0$ (k₀ params) vs larger $M_1$ (k₁ > k₀).\n\n$$\\text{BIC}_1 - \\text{BIC}_0 = -2(\\ln L_1 - \\ln L_0) + (k_1 - k_0)\\ln n$$\n\nBy likelihood ratio theory: $-2(\\ln L_1 - \\ln L_0) = O_p(1)$ (bounded)\nBut penalty: $(k_1 - k_0)\\ln n \\to \\infty$\n\nSo $P(\\text{BIC}_1 > \\text{BIC}_0) \\to 1$.\n\n**AIC Inconsistency:**\n$$\\text{AIC}_1 - \\text{AIC}_0 = -2(\\ln L_1 - \\ln L_0) + 2(k_1 - k_0)$$\n\nExtra parameters add fixed penalty 2(k₁-k₀), while likelihood improvement is $O_p(1)$. There's always positive probability that extra parameters improve fit enough to offset the fixed penalty.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Consistency ≠ better forecasts. AIC minimizes prediction error; BIC identifies true model. Different goals.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> AIC selects ARIMA(2,1,2) while BIC selects ARIMA(1,1,1). AIC difference is 4. Which do you choose?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> It depends on your goal and context:\n\n**For forecasting:** Lean toward ARIMA(2,1,2) (AIC's choice)\n- ΔAIC = 4 suggests meaningful predictive improvement\n- Extra complexity may capture real dynamics\n\n**For interpretation:** Lean toward ARIMA(1,1,1) (BIC's choice)\n- Simpler, more interpretable\n- Less risk of overfitting\n\n**Recommended approach:**\n1. Compare out-of-sample forecast accuracy\n2. Check residual diagnostics for both\n3. If similar performance, prefer simpler\n4. Consider ensemble/averaging\n\n**Decision matrix:**\n\n| Factor | Favors (2,1,2) | Favors (1,1,1) |\n|--------|----------------|----------------|\n| Large sample | ✓ | |\n| Short forecast horizon | ✓ | |\n| Complex dynamics expected | ✓ | |\n| Interpretability needed | | ✓ |\n| Small sample | | ✓ |\n| Long forecast horizon | | ✓ |\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Picking one criterion dogmatically. Use domain knowledge, validation, and judgment alongside IC.\n</div>\n</div>\n</details>", "line_start": 149, "level": 2}, {"heading": "References", "content": "1. Akaike, H. (1974). A new look at the statistical model identification. *IEEE Transactions on Automatic Control*, 19(6), 716-723.\n2. Schwarz, G. (1978). Estimating the dimension of a model. *Annals of Statistics*, 6(2), 461-464.\n3. Burnham, K. P., & Anderson, D. R. (2002). *Model Selection and Multimodel Inference*. Springer.\n4. Hurvich, C. M., & Tsai, C. L. (1989). Regression and time series model selection in small samples. *Biometrika*, 76(2), 297-307.", "line_start": 291, "level": 1}]}, "docs/en/model-selection/residual-diagnostics.md": {"path": "docs/en/model-selection/residual-diagnostics.md", "title": "Residual Diagnostics", "content": "# Residual Diagnostics\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Residuals should be white noise if model is adequate. Check via: ACF plot (no significant spikes), Ljung-Box test (p > 0.05), normality (Q-Q plot), and homoskedasticity (constant variance). Patterns in residuals indicate model inadequacy: autocorrelation suggests missing AR/MA terms; changing variance suggests GARCH; trends suggest wrong differencing.\n</div>\n\n## Core Definitions\n\n**Residuals:** $e_t = y_t - \\hat{y}_{t|t-1}$ (one-step-ahead prediction errors)\n\n**Standardized Residuals:** $z_t = e_t / \\hat{\\sigma}$ (should be approximately N(0,1))\n\n**White Noise Properties:**\n- $E[e_t] = 0$\n- $\\text{Var}(e_t) = \\sigma^2$ (constant)\n- $\\text{Cov}(e_t, e_{t-k}) = 0$ for $k \\neq 0$\n\n## Math and Derivations\n\n### Ljung-Box Test\n\nTests whether autocorrelations are jointly zero.\n\n$$Q(m) = n(n+2)\\sum_{k=1}^{m}\\frac{\\hat{\\rho}_k^2}{n-k}$$\n\nUnder H₀ (white noise): $Q(m) \\sim \\chi^2_{m-p-q}$ (adjusted for estimated parameters)\n\n**Decision:** Reject H₀ if Q > critical value (or p < α)\n\n### Jarque-Bera Normality Test\n\n$$JB = \\frac{n}{6}\\left(S^2 + \\frac{(K-3)^2}{4}\\right)$$\n\nwhere S = skewness, K = kurtosis.\n\nUnder H₀ (normality): $JB \\sim \\chi^2_2$\n\n### ARCH-LM Test for Heteroskedasticity\n\nTest if $e_t^2$ depends on past squared residuals:\n$$e_t^2 = \\alpha_0 + \\alpha_1 e_{t-1}^2 + \\cdots + \\alpha_p e_{t-p}^2 + v_t$$\n\nTest statistic: $nR^2 \\sim \\chi^2_p$ under H₀ (homoskedasticity)\n\n### Runs Test for Randomness\n\nCounts runs (consecutive same-sign residuals). Too few runs suggests autocorrelation; too many suggests over-differencing.\n\n## Algorithm/Model Sketch\n\n**Diagnostic Checklist:**\n\n```\n1. MEAN ZERO\n   □ Mean of residuals ≈ 0\n   □ Plot residuals over time: no trend\n\n2. NO AUTOCORRELATION\n   □ ACF plot: all spikes within ±1.96/√n bands\n   □ PACF plot: no patterns\n   □ Ljung-Box test: p > 0.05 at multiple lags\n\n3. CONSTANT VARIANCE\n   □ Plot residuals vs time: no fanning/clustering\n   □ Plot residuals vs fitted: no pattern\n   □ ARCH test: p > 0.05\n\n4. NORMALITY (less critical)\n   □ Histogram: roughly bell-shaped\n   □ Q-Q plot: points on diagonal\n   □ Jarque-Bera: p > 0.05\n\n5. NO OUTLIERS\n   □ |standardized residuals| < 3 mostly\n   □ Check any points > 3 for data issues\n```\n\n**Interpretation of Violations:**\n\n| Violation | Interpretation | Fix |\n|-----------|---------------|-----|\n| Lag 1 ACF spike | Missing MA(1) | Add MA term |\n| Lag 1 PACF spike | Missing AR(1) | Add AR term |\n| Seasonal spikes | Missing seasonal | Add seasonal terms |\n| Slow ACF decay | Under-differencing | Increase d |\n| Negative ACF at lag 1 | Over-differencing | Decrease d |\n| Changing variance | Heteroskedasticity | GARCH, log-transform |\n| Non-normality | Heavy tails | Robust methods, outlier treatment |\n\n## Common Pitfalls\n\n1. **Over-testing**: With many lags, some will be significant by chance. Focus on early lags and patterns.\n\n2. **Ignoring degrees of freedom**: Ljung-Box df = m - p - q, not m. Wrong df gives wrong p-values.\n\n3. **Choosing m poorly**: Too small m misses long-range dependence; too large has low power. Rule: m ≈ min(10, n/5).\n\n4. **Normality obsession**: Non-normality is often acceptable. Autocorrelation is the critical check.\n\n5. **Missing patterns at seasonal lags**: Always check ACF at lags 12, 24 (monthly), 7, 14 (daily), etc.\n\n6. **Confusing residuals and innovations**: For MA models, residuals ≠ true innovations. Some autocorrelation is expected in finite samples.\n\n## Mini Example\n\n```python\nimport numpy as np\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nfrom statsmodels.graphics.tsaplots import plot_acf\nimport scipy.stats as stats\n\n# Generate data and fit intentionally wrong model\nnp.random.seed(42)\nn = 200\n# True: ARMA(1,1)\nphi, theta = 0.7, 0.4\neps = np.random.randn(n + 1)\ny = np.zeros(n)\nfor t in range(1, n):\n    y[t] = phi * y[t-1] + eps[t] + theta * eps[t-1]\n\n# Fit AR(1) only (missing MA term)\nmodel_wrong = ARIMA(y, order=(1, 0, 0)).fit()\nresid = model_wrong.resid\n\nprint(\"=== Residual Diagnostics ===\\n\")\n\n# 1. Mean\nprint(f\"1. Mean: {np.mean(resid):.4f} (should be ≈ 0)\")\n\n# 2. Autocorrelation\nprint(\"\\n2. Autocorrelation:\")\nlb_test = acorr_ljungbox(resid, lags=[5, 10, 15], return_df=True)\nprint(lb_test)\n\n# 3. Normality\njb_stat, jb_p = stats.jarque_bera(resid)\nprint(f\"\\n3. Normality (Jarque-Bera): stat={jb_stat:.2f}, p={jb_p:.4f}\")\n\n# 4. Check ACF\nacf_vals = np.correlate(resid, resid, mode='full')\nacf_vals = acf_vals[len(acf_vals)//2:] / acf_vals[len(acf_vals)//2]\nprint(f\"\\n4. ACF at lag 1: {acf_vals[1]:.3f} (significant if |.| > {1.96/np.sqrt(n):.3f})\")\n\n# Compare with correct model\nmodel_correct = ARIMA(y, order=(1, 0, 1)).fit()\nresid_correct = model_correct.resid\nlb_correct = acorr_ljungbox(resid_correct, lags=[5, 10, 15], return_df=True)\nprint(\"\\n=== Correct Model (ARMA(1,1)) ===\")\nprint(lb_correct)\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why is checking residual autocorrelation more important than checking normality?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Autocorrelation matters more because:**\n1. **Biased forecasts**: Residual autocorrelation means systematic patterns remain unexploited\n2. **Invalid inference**: Standard errors and confidence intervals assume independence\n3. **Model inadequacy**: Autocorrelation directly indicates missing structure\n4. **Fixable**: Can add AR/MA terms to remove autocorrelation\n\n**Normality is less critical because:**\n1. **Robust methods exist**: Point forecasts don't require normality\n2. **CLT helps**: Averages become normal even if residuals aren't\n3. **Only affects intervals**: Normality matters mainly for prediction intervals\n4. **Often ignorable**: Heavy tails don't bias forecasts, just widen intervals\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Spending effort on normality transformations while ignoring autocorrelation. Fix autocorrelation first; normality can often be ignored.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> What does a significant negative spike at lag 1 in the residual ACF suggest?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Likely **over-differencing**.\n\n**Explanation:**\nDifferencing a stationary series introduces MA(1) with θ ≈ -1:\n$$(1-L)y_t = \\epsilon_t - \\epsilon_{t-1} \\text{ approximately}$$\n\nThis has ACF: $\\rho(1) = -1/(1+1) = -0.5$\n\nSo large negative lag-1 ACF (around -0.3 to -0.5) suggests you differenced a series that was already stationary.\n\n**Action:**\n1. Re-test original series for stationarity\n2. Try model without differencing\n3. Compare AIC between d=0 and d=1\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Reflexively differencing because it's \"standard procedure.\" Check stationarity tests and residuals before and after differencing.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> The Ljung-Box test uses degrees of freedom m-p-q. Why subtract p+q?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> We subtract estimated parameters to account for their effect on residuals.\n\n**Explanation:**\nFor a fitted ARMA(p,q), the residuals $e_t = y_t - \\hat{y}_{t|t-1}$ are computed using estimated $\\hat{\\phi}$, $\\hat{\\theta}$.\n\nThe estimation process uses up information from the data, reducing effective degrees of freedom. Specifically:\n- p AR parameters constrain p lagged autocorrelations\n- q MA parameters constrain q lagged autocorrelations\n\nUnder H₀, the test statistic:\n$$Q(m) \\sim \\chi^2_{m-p-q}$$\n\nnot $\\chi^2_m$. Using m degrees of freedom would reject too often (test is oversized).\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Software may not adjust df automatically. Verify that p and q are subtracted; otherwise p-values are wrong.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> How do you interpret the ARCH-LM test for residuals?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> ARCH-LM tests for conditional heteroskedasticity — whether variance depends on past volatility.\n\n**Procedure:**\n1. Compute squared residuals $e_t^2$\n2. Regress: $e_t^2 = \\alpha_0 + \\alpha_1 e_{t-1}^2 + \\cdots + \\alpha_p e_{t-p}^2$\n3. Test: $H_0$: all $\\alpha_i = 0$ (homoskedasticity)\n\n**Test statistic:** $nR^2 \\sim \\chi^2_p$\n\n**Interpretation:**\n- p < 0.05: Evidence of ARCH effects; variance clusters\n- p > 0.05: No evidence; constant variance OK\n\n**If significant:**\n- Consider GARCH model\n- Or variance-stabilizing transform (log)\n- Prediction intervals need adjustment\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Ignoring ARCH effects leads to prediction intervals that are too narrow during volatile periods and too wide during calm periods.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You fit ARIMA(1,1,1) and residual diagnostics show: Ljung-Box p=0.02 at lag 10, but p=0.15 at lags 5 and 15. Q-Q plot shows slight heavy tails. What do you conclude?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Model is likely adequate; don't over-interpret the lag-10 result.\n\n**Analysis:**\n1. **Ljung-Box at lag 10:** p=0.02 is borderline. But lags 5 and 15 are fine.\n   - Could be spurious (multiple testing)\n   - Or minor model inadequacy that doesn't matter for forecasting\n\n2. **Heavy tails:** Common in economic/financial data\n   - Doesn't invalidate forecasts\n   - Affects prediction intervals (may need wider)\n\n**Recommended actions:**\n1. Check ACF visually — isolated spike at lag 10 likely noise\n2. Compare to simpler models (ARIMA(1,1,0)) — if similar forecasts, prefer simpler\n3. For intervals, consider bootstrap or t-distribution\n4. Validate on holdout data — ultimate test\n\n**Conclusion:** Accept model unless holdout validation shows problems. Perfect residuals are unrealistic; \"good enough\" is the standard.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Chasing perfect diagnostics. Adding parameters to fix one borderline test often causes overfitting. Focus on forecast performance.\n</div>\n</div>\n</details>\n\n## References\n\n1. Ljung, G. M., & Box, G. E. P. (1978). On a measure of lack of fit in time series models. *Biometrika*, 65(2), 297-303.\n2. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis*. Wiley. Chapter 8.\n3. Tsay, R. S. (2010). *Analysis of Financial Time Series*. Wiley. Chapter 2.\n4. Engle, R. F. (1982). Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation. *Econometrica*, 50(4), 987-1007.\n", "sections": [{"heading": "Residual Diagnostics", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Residuals should be white noise if model is adequate. Check via: ACF plot (no significant spikes), Ljung-Box test (p > 0.05), normality (Q-Q plot), and homoskedasticity (constant variance). Patterns in residuals indicate model inadequacy: autocorrelation suggests missing AR/MA terms; changing variance suggests GARCH; trends suggest wrong differencing.\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**Residuals:** $e_t = y_t - \\hat{y}_{t|t-1}$ (one-step-ahead prediction errors)\n\n**Standardized Residuals:** $z_t = e_t / \\hat{\\sigma}$ (should be approximately N(0,1))\n\n**White Noise Properties:**\n- $E[e_t] = 0$\n- $\\text{Var}(e_t) = \\sigma^2$ (constant)\n- $\\text{Cov}(e_t, e_{t-k}) = 0$ for $k \\neq 0$", "line_start": 7, "level": 2}, {"heading": "Ljung-Box Test", "content": "Tests whether autocorrelations are jointly zero.\n\n$$Q(m) = n(n+2)\\sum_{k=1}^{m}\\frac{\\hat{\\rho}_k^2}{n-k}$$\n\nUnder H₀ (white noise): $Q(m) \\sim \\chi^2_{m-p-q}$ (adjusted for estimated parameters)\n\n**Decision:** Reject H₀ if Q > critical value (or p < α)", "line_start": 20, "level": 3}, {"heading": "Jarque-Bera Normality Test", "content": "$$JB = \\frac{n}{6}\\left(S^2 + \\frac{(K-3)^2}{4}\\right)$$\n\nwhere S = skewness, K = kurtosis.\n\nUnder H₀ (normality): $JB \\sim \\chi^2_2$", "line_start": 30, "level": 3}, {"heading": "ARCH-LM Test for Heteroskedasticity", "content": "Test if $e_t^2$ depends on past squared residuals:\n$$e_t^2 = \\alpha_0 + \\alpha_1 e_{t-1}^2 + \\cdots + \\alpha_p e_{t-p}^2 + v_t$$\n\nTest statistic: $nR^2 \\sim \\chi^2_p$ under H₀ (homoskedasticity)", "line_start": 38, "level": 3}, {"heading": "Runs Test for Randomness", "content": "Counts runs (consecutive same-sign residuals). Too few runs suggests autocorrelation; too many suggests over-differencing.", "line_start": 45, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**Diagnostic Checklist:**\n\n```\n1. MEAN ZERO\n   □ Mean of residuals ≈ 0\n   □ Plot residuals over time: no trend\n\n2. NO AUTOCORRELATION\n   □ ACF plot: all spikes within ±1.96/√n bands\n   □ PACF plot: no patterns\n   □ Ljung-Box test: p > 0.05 at multiple lags\n\n3. CONSTANT VARIANCE\n   □ Plot residuals vs time: no fanning/clustering\n   □ Plot residuals vs fitted: no pattern\n   □ ARCH test: p > 0.05\n\n4. NORMALITY (less critical)\n   □ Histogram: roughly bell-shaped\n   □ Q-Q plot: points on diagonal\n   □ Jarque-Bera: p > 0.05\n\n5. NO OUTLIERS\n   □ |standardized residuals| < 3 mostly\n   □ Check any points > 3 for data issues\n```\n\n**Interpretation of Violations:**\n\n| Violation | Interpretation | Fix |\n|-----------|---------------|-----|\n| Lag 1 ACF spike | Missing MA(1) | Add MA term |\n| Lag 1 PACF spike | Missing AR(1) | Add AR term |\n| Seasonal spikes | Missing seasonal | Add seasonal terms |\n| Slow ACF decay | Under-differencing | Increase d |\n| Negative ACF at lag 1 | Over-differencing | Decrease d |\n| Changing variance | Heteroskedasticity | GARCH, log-transform |\n| Non-normality | Heavy tails | Robust methods, outlier treatment |", "line_start": 49, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Over-testing**: With many lags, some will be significant by chance. Focus on early lags and patterns.\n\n2. **Ignoring degrees of freedom**: Ljung-Box df = m - p - q, not m. Wrong df gives wrong p-values.\n\n3. **Choosing m poorly**: Too small m misses long-range dependence; too large has low power. Rule: m ≈ min(10, n/5).\n\n4. **Normality obsession**: Non-normality is often acceptable. Autocorrelation is the critical check.\n\n5. **Missing patterns at seasonal lags**: Always check ACF at lags 12, 24 (monthly), 7, 14 (daily), etc.\n\n6. **Confusing residuals and innovations**: For MA models, residuals ≠ true innovations. Some autocorrelation is expected in finite samples.", "line_start": 90, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nfrom statsmodels.graphics.tsaplots import plot_acf\nimport scipy.stats as stats", "line_start": 104, "level": 1}, {"heading": "Generate data and fit intentionally wrong model", "content": "np.random.seed(42)\nn = 200", "line_start": 113, "level": 1}, {"heading": "True: ARMA(1,1)", "content": "phi, theta = 0.7, 0.4\neps = np.random.randn(n + 1)\ny = np.zeros(n)\nfor t in range(1, n):\n    y[t] = phi * y[t-1] + eps[t] + theta * eps[t-1]", "line_start": 116, "level": 1}, {"heading": "Fit AR(1) only (missing MA term)", "content": "model_wrong = ARIMA(y, order=(1, 0, 0)).fit()\nresid = model_wrong.resid\n\nprint(\"=== Residual Diagnostics ===\\n\")", "line_start": 123, "level": 1}, {"heading": "1. Mean", "content": "print(f\"1. Mean: {np.mean(resid):.4f} (should be ≈ 0)\")", "line_start": 129, "level": 1}, {"heading": "2. Autocorrelation", "content": "print(\"\\n2. Autocorrelation:\")\nlb_test = acorr_ljungbox(resid, lags=[5, 10, 15], return_df=True)\nprint(lb_test)", "line_start": 132, "level": 1}, {"heading": "3. Normality", "content": "jb_stat, jb_p = stats.jarque_bera(resid)\nprint(f\"\\n3. Normality (Jarque-Bera): stat={jb_stat:.2f}, p={jb_p:.4f}\")", "line_start": 137, "level": 1}, {"heading": "4. Check ACF", "content": "acf_vals = np.correlate(resid, resid, mode='full')\nacf_vals = acf_vals[len(acf_vals)//2:] / acf_vals[len(acf_vals)//2]\nprint(f\"\\n4. ACF at lag 1: {acf_vals[1]:.3f} (significant if |.| > {1.96/np.sqrt(n):.3f})\")", "line_start": 141, "level": 1}, {"heading": "Compare with correct model", "content": "model_correct = ARIMA(y, order=(1, 0, 1)).fit()\nresid_correct = model_correct.resid\nlb_correct = acorr_ljungbox(resid_correct, lags=[5, 10, 15], return_df=True)\nprint(\"\\n=== Correct Model (ARMA(1,1)) ===\")\nprint(lb_correct)\n```", "line_start": 146, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why is checking residual autocorrelation more important than checking normality?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Autocorrelation matters more because:**\n1. **Biased forecasts**: Residual autocorrelation means systematic patterns remain unexploited\n2. **Invalid inference**: Standard errors and confidence intervals assume independence\n3. **Model inadequacy**: Autocorrelation directly indicates missing structure\n4. **Fixable**: Can add AR/MA terms to remove autocorrelation\n\n**Normality is less critical because:**\n1. **Robust methods exist**: Point forecasts don't require normality\n2. **CLT helps**: Averages become normal even if residuals aren't\n3. **Only affects intervals**: Normality matters mainly for prediction intervals\n4. **Often ignorable**: Heavy tails don't bias forecasts, just widen intervals\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Spending effort on normality transformations while ignoring autocorrelation. Fix autocorrelation first; normality can often be ignored.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> What does a significant negative spike at lag 1 in the residual ACF suggest?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Likely **over-differencing**.\n\n**Explanation:**\nDifferencing a stationary series introduces MA(1) with θ ≈ -1:\n$$(1-L)y_t = \\epsilon_t - \\epsilon_{t-1} \\text{ approximately}$$\n\nThis has ACF: $\\rho(1) = -1/(1+1) = -0.5$\n\nSo large negative lag-1 ACF (around -0.3 to -0.5) suggests you differenced a series that was already stationary.\n\n**Action:**\n1. Re-test original series for stationarity\n2. Try model without differencing\n3. Compare AIC between d=0 and d=1\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Reflexively differencing because it's \"standard procedure.\" Check stationarity tests and residuals before and after differencing.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> The Ljung-Box test uses degrees of freedom m-p-q. Why subtract p+q?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> We subtract estimated parameters to account for their effect on residuals.\n\n**Explanation:**\nFor a fitted ARMA(p,q), the residuals $e_t = y_t - \\hat{y}_{t|t-1}$ are computed using estimated $\\hat{\\phi}$, $\\hat{\\theta}$.\n\nThe estimation process uses up information from the data, reducing effective degrees of freedom. Specifically:\n- p AR parameters constrain p lagged autocorrelations\n- q MA parameters constrain q lagged autocorrelations\n\nUnder H₀, the test statistic:\n$$Q(m) \\sim \\chi^2_{m-p-q}$$\n\nnot $\\chi^2_m$. Using m degrees of freedom would reject too often (test is oversized).\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Software may not adjust df automatically. Verify that p and q are subtracted; otherwise p-values are wrong.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> How do you interpret the ARCH-LM test for residuals?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> ARCH-LM tests for conditional heteroskedasticity — whether variance depends on past volatility.\n\n**Procedure:**\n1. Compute squared residuals $e_t^2$\n2. Regress: $e_t^2 = \\alpha_0 + \\alpha_1 e_{t-1}^2 + \\cdots + \\alpha_p e_{t-p}^2$\n3. Test: $H_0$: all $\\alpha_i = 0$ (homoskedasticity)\n\n**Test statistic:** $nR^2 \\sim \\chi^2_p$\n\n**Interpretation:**\n- p < 0.05: Evidence of ARCH effects; variance clusters\n- p > 0.05: No evidence; constant variance OK\n\n**If significant:**\n- Consider GARCH model\n- Or variance-stabilizing transform (log)\n- Prediction intervals need adjustment\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Ignoring ARCH effects leads to prediction intervals that are too narrow during volatile periods and too wide during calm periods.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You fit ARIMA(1,1,1) and residual diagnostics show: Ljung-Box p=0.02 at lag 10, but p=0.15 at lags 5 and 15. Q-Q plot shows slight heavy tails. What do you conclude?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Model is likely adequate; don't over-interpret the lag-10 result.\n\n**Analysis:**\n1. **Ljung-Box at lag 10:** p=0.02 is borderline. But lags 5 and 15 are fine.\n   - Could be spurious (multiple testing)\n   - Or minor model inadequacy that doesn't matter for forecasting\n\n2. **Heavy tails:** Common in economic/financial data\n   - Doesn't invalidate forecasts\n   - Affects prediction intervals (may need wider)\n\n**Recommended actions:**\n1. Check ACF visually — isolated spike at lag 10 likely noise\n2. Compare to simpler models (ARIMA(1,1,0)) — if similar forecasts, prefer simpler\n3. For intervals, consider bootstrap or t-distribution\n4. Validate on holdout data — ultimate test\n\n**Conclusion:** Accept model unless holdout validation shows problems. Perfect residuals are unrealistic; \"good enough\" is the standard.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Chasing perfect diagnostics. Adding parameters to fix one borderline test often causes overfitting. Focus on forecast performance.\n</div>\n</div>\n</details>", "line_start": 154, "level": 2}, {"heading": "References", "content": "1. Ljung, G. M., & Box, G. E. P. (1978). On a measure of lack of fit in time series models. *Biometrika*, 65(2), 297-303.\n2. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis*. Wiley. Chapter 8.\n3. Tsay, R. S. (2010). *Analysis of Financial Time Series*. Wiley. Chapter 2.\n4. Engle, R. F. (1982). Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation. *Econometrica*, 50(4), 987-1007.", "line_start": 286, "level": 1}]}, "docs/en/practical/practical-modeling.md": {"path": "docs/en/practical/practical-modeling.md", "title": "Practical Time Series Modeling", "content": "# Practical Time Series Modeling\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Practical modeling involves: proper train/test splits (temporal, never random), backtesting with rolling windows, handling deployment (model updates, monitoring). Key concerns: data leakage, concept drift, uncertainty quantification. Production tips: start simple (naive baselines), document assumptions, monitor forecast accuracy over time, have fallback strategies.\n</div>\n\n## Core Definitions\n\n**Backtesting:** Historical simulation of how model would have performed.\n\n**Walk-Forward Validation:** Expanding window validation mimicking production.\n\n**Concept Drift:** When relationship between features and target changes over time.\n\n**Model Retraining:** Updating model with new data periodically.\n\n**Forecast Reconciliation:** Ensuring forecasts at different aggregation levels are consistent.\n\n## Math and Derivations\n\n### Rolling Origin Backtest\n\nFor origins $T_1, T_2, \\ldots, T_m$ and horizon h:\n$$\\text{RMSE}(h) = \\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(y_{T_i+h} - \\hat{y}_{T_i+h|T_i})^2}$$\n\n### Forecast Bias\n\n$$\\text{Bias} = \\frac{1}{n}\\sum_{t=1}^{n}(y_t - \\hat{y}_t)$$\n\n- Positive bias: systematic under-prediction\n- Negative bias: systematic over-prediction\n\n### Tracking Signal (for monitoring)\n\n$$TS_t = \\frac{\\sum_{i=1}^{t}e_i}{\\text{MAD}}$$\n\nIf |TS| > 4, model may be biased and needs retraining.\n\n### Prediction Interval Coverage\n\n$$\\text{Coverage} = \\frac{1}{n}\\sum_{t=1}^{n}\\mathbf{1}(y_t \\in PI_t)$$\n\n95% PI should have ~95% coverage; significantly less indicates miscalibration.\n\n## Algorithm/Model Sketch\n\n**Production Forecasting Pipeline:**\n\n```python\ndef production_pipeline(data, config):\n    \"\"\"\n    Complete forecasting pipeline for production.\n    \"\"\"\n    # 1. Data validation\n    validate_data(data)\n\n    # 2. Feature engineering\n    features = create_features(data)\n\n    # 3. Train-test split (temporal)\n    train, holdout = temporal_split(features, config['holdout_size'])\n\n    # 4. Model selection via cross-validation\n    best_model = None\n    best_score = float('inf')\n\n    for model_class in config['candidate_models']:\n        score = time_series_cv(train, model_class, config['cv_folds'])\n        if score < best_score:\n            best_model = model_class\n            best_score = score\n\n    # 5. Final training on full training set\n    model = best_model.fit(train)\n\n    # 6. Holdout evaluation\n    holdout_metrics = evaluate(model, holdout)\n\n    # 7. Retrain on all data for deployment\n    final_model = best_model.fit(features)\n\n    # 8. Generate forecasts with intervals\n    forecasts = final_model.forecast(config['horizon'])\n    intervals = final_model.prediction_intervals(config['horizon'])\n\n    return {\n        'model': final_model,\n        'forecasts': forecasts,\n        'intervals': intervals,\n        'metrics': holdout_metrics\n    }\n```\n\n**Monitoring Dashboard Metrics:**\n\n| Metric | Good | Warning | Action |\n|--------|------|---------|--------|\n| MAPE | < 10% | 10-20% | > 20%: investigate |\n| Bias | ≈ 0 | |bias| > 1σ | |bias| > 2σ: retrain |\n| PI Coverage | 90-100% | 80-90% | < 80%: recalibrate |\n| Tracking Signal | |TS| < 4 | 4-6 | > 6: retrain |\n\n## Common Pitfalls\n\n1. **Random train-test split:** Causes data leakage. Always use temporal splits.\n\n2. **Optimizing wrong metric:** Minimize business-relevant loss (e.g., asymmetric cost), not just RMSE.\n\n3. **No baseline comparison:** Claim \"model works\" without comparing to naive/seasonal naive.\n\n4. **Static model:** Not retraining as new data arrives. Monitor performance and retrain regularly.\n\n5. **Ignoring prediction intervals:** Point forecasts without uncertainty mislead decision-makers.\n\n6. **Overfitting to holdout:** If you tune on holdout multiple times, it becomes training data. Use nested CV.\n\n## Mini Example\n\n```python\nimport numpy as np\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\ndef mape(y_true, y_pred):\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ndef tracking_signal(errors):\n    cumsum = np.cumsum(errors)\n    mad = np.mean(np.abs(errors))\n    return cumsum[-1] / mad if mad > 0 else 0\n\n# Simulate production monitoring\nnp.random.seed(42)\nn_periods = 12  # 12 months of monitoring\n\nactuals = 100 + np.random.randn(n_periods) * 10\nforecasts = actuals + np.random.randn(n_periods) * 5 + 2  # slight bias\n\nerrors = actuals - forecasts\n\n# Calculate monitoring metrics\nprint(\"=== Forecast Monitoring Report ===\\n\")\nprint(f\"MAE: {mean_absolute_error(actuals, forecasts):.2f}\")\nprint(f\"RMSE: {np.sqrt(mean_squared_error(actuals, forecasts)):.2f}\")\nprint(f\"MAPE: {mape(actuals, forecasts):.2f}%\")\nprint(f\"Bias: {np.mean(errors):.2f}\")\nprint(f\"Tracking Signal: {tracking_signal(errors):.2f}\")\n\n# PI coverage check (simulated 95% intervals)\npi_width = 1.96 * np.std(errors)\nlower = forecasts - pi_width\nupper = forecasts + pi_width\ncoverage = np.mean((actuals >= lower) & (actuals <= upper))\nprint(f\"PI Coverage: {coverage*100:.1f}%\")\n\n# Alert check\nprint(\"\\n=== Alerts ===\")\nif abs(np.mean(errors)) > 2 * np.std(errors):\n    print(\"WARNING: Significant forecast bias detected!\")\nif abs(tracking_signal(errors)) > 4:\n    print(\"WARNING: Tracking signal exceeds threshold - consider retraining\")\nif coverage < 0.80:\n    print(\"WARNING: Prediction interval coverage too low\")\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why is random train-test split wrong for time series?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Random splits cause data leakage:\n- Future observations appear in training set\n- Past observations appear in test set\n- Model \"sees\" future information during training\n\n**Consequences:**\n- Overoptimistic evaluation metrics\n- Model fails in production where future isn't available\n- Temporal patterns learned incorrectly\n\n**Correct approach:**\n```\nTrain: [1, ..., T]    Test: [T+1, ..., T+h]\n```\nAlways train on past, test on future.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using sklearn's train_test_split() or KFold directly on time series. Use TimeSeriesSplit or manual temporal split.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> What is concept drift and how do you detect it?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Concept drift = the relationship between features and target changes over time.\n\n**Types:**\n- **Sudden:** Abrupt change (e.g., COVID impact)\n- **Gradual:** Slow shift (e.g., customer behavior evolution)\n- **Seasonal:** Recurring pattern changes\n- **Recurring:** Oscillates between states\n\n**Detection methods:**\n1. **Performance monitoring:** Increasing error over time\n2. **Statistical tests:** Compare recent vs historical distributions\n3. **Control charts:** Track forecast errors, flag out-of-control\n4. **Tracking signal:** Cumulative bias indicates drift\n\n**Response:**\n- Retrain on recent data\n- Use adaptive models (exponential smoothing)\n- Reduce lookback window\n- Add regime indicators\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Assuming model stays accurate forever. Schedule regular monitoring and retraining.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> How do you calculate and interpret the tracking signal?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n$$TS_t = \\frac{\\text{RSFE}_t}{\\text{MAD}_t} = \\frac{\\sum_{i=1}^{t}e_i}{\\frac{1}{t}\\sum_{i=1}^{t}|e_i|}$$\n\n**Interpretation:**\n- TS ≈ 0: No systematic bias\n- TS > 0: Systematic under-forecasting\n- TS < 0: Systematic over-forecasting\n- |TS| > 4: Likely significant bias (action needed)\n\n**Why use it:**\n- Normalizes by MAD for comparability\n- Accumulates evidence over time\n- Distinguishes random errors from systematic bias\n\n**Update frequency:**\nCheck monthly or quarterly; daily TS is noisy.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Reacting to every TS fluctuation. Wait for sustained signal (multiple periods with |TS| > 4) before retraining.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> What does it mean if your 95% prediction intervals have 75% coverage?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The model underestimates uncertainty—intervals are too narrow.\n\n**Possible causes:**\n1. **Model misspecification:** True residuals are larger than estimated\n2. **Heavy tails:** Data has outliers not captured by normal assumption\n3. **Non-constant variance:** Heteroskedasticity not modeled\n4. **Missing patterns:** Unmodeled seasonality or trend adds variance\n\n**Solutions:**\n1. Use bootstrap prediction intervals (more robust)\n2. Apply variance adjustment: multiply width by coverage correction factor\n3. Model heteroskedasticity (GARCH) or use quantile regression\n4. Improve base model to capture more patterns\n\n**Adjustment formula:**\nIf coverage = 75% but target = 95%, scale factor ≈ $z_{0.975}/z_{0.875}$ = 1.96/1.15 ≈ 1.7\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Reporting narrow intervals to seem accurate. Stakeholders need truthful uncertainty; too-narrow intervals cause poor decisions.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> Your demand forecasting model works well in testing but production accuracy is much worse. What might be causing this?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Common causes of train-test vs production gap:\n\n1. **Data leakage in testing:**\n   - Features used information not available at forecast time\n   - Train-test split not strictly temporal\n\n2. **Feature availability:**\n   - Features available historically but delayed in production\n   - External data sources not updated in real-time\n\n3. **Distribution shift:**\n   - Test period was unusual/lucky\n   - Production faces different conditions (seasonality, promotions)\n\n4. **Data quality:**\n   - Production data has errors/delays not in historical\n   - Missing values handled differently\n\n5. **Target leakage:**\n   - Test evaluated at easy horizons; production needs longer\n\n**Diagnosis:**\n1. Verify no leakage in feature engineering\n2. Compare feature distributions: test vs production\n3. Backtest over multiple periods (not just one)\n4. Monitor input data quality in production\n5. Check if production horizon matches testing\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Testing on convenient period and assuming it generalizes. Always backtest across multiple train-test splits spanning different conditions.\n</div>\n</div>\n</details>\n\n## References\n\n1. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapters 5, 12.\n2. Makridakis, S., Spiliotis, E., & Assimakopoulos, V. (2020). The M4 Competition: 100,000 time series and 61 forecasting methods. *IJF*, 36(1), 54-74.\n3. Gama, J., Žliobaitė, I., Bifet, A., Pechenizkiy, M., & Bouchachia, A. (2014). A survey on concept drift adaptation. *ACM Computing Surveys*, 46(4), 1-37.\n4. Kolassa, S. (2016). Evaluating predictive count data distributions in retail sales forecasting. *IJF*, 32(3), 788-803.\n", "sections": [{"heading": "Practical Time Series Modeling", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Practical modeling involves: proper train/test splits (temporal, never random), backtesting with rolling windows, handling deployment (model updates, monitoring). Key concerns: data leakage, concept drift, uncertainty quantification. Production tips: start simple (naive baselines), document assumptions, monitor forecast accuracy over time, have fallback strategies.\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**Backtesting:** Historical simulation of how model would have performed.\n\n**Walk-Forward Validation:** Expanding window validation mimicking production.\n\n**Concept Drift:** When relationship between features and target changes over time.\n\n**Model Retraining:** Updating model with new data periodically.\n\n**Forecast Reconciliation:** Ensuring forecasts at different aggregation levels are consistent.", "line_start": 7, "level": 2}, {"heading": "Rolling Origin Backtest", "content": "For origins $T_1, T_2, \\ldots, T_m$ and horizon h:\n$$\\text{RMSE}(h) = \\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(y_{T_i+h} - \\hat{y}_{T_i+h|T_i})^2}$$", "line_start": 21, "level": 3}, {"heading": "Forecast Bias", "content": "$$\\text{Bias} = \\frac{1}{n}\\sum_{t=1}^{n}(y_t - \\hat{y}_t)$$\n\n- Positive bias: systematic under-prediction\n- Negative bias: systematic over-prediction", "line_start": 26, "level": 3}, {"heading": "Tracking Signal (for monitoring)", "content": "$$TS_t = \\frac{\\sum_{i=1}^{t}e_i}{\\text{MAD}}$$\n\nIf |TS| > 4, model may be biased and needs retraining.", "line_start": 33, "level": 3}, {"heading": "Prediction Interval Coverage", "content": "$$\\text{Coverage} = \\frac{1}{n}\\sum_{t=1}^{n}\\mathbf{1}(y_t \\in PI_t)$$\n\n95% PI should have ~95% coverage; significantly less indicates miscalibration.", "line_start": 39, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**Production Forecasting Pipeline:**\n\n```python\ndef production_pipeline(data, config):\n    \"\"\"\n    Complete forecasting pipeline for production.\n    \"\"\"\n    # 1. Data validation\n    validate_data(data)\n\n    # 2. Feature engineering\n    features = create_features(data)\n\n    # 3. Train-test split (temporal)\n    train, holdout = temporal_split(features, config['holdout_size'])\n\n    # 4. Model selection via cross-validation\n    best_model = None\n    best_score = float('inf')\n\n    for model_class in config['candidate_models']:\n        score = time_series_cv(train, model_class, config['cv_folds'])\n        if score < best_score:\n            best_model = model_class\n            best_score = score\n\n    # 5. Final training on full training set\n    model = best_model.fit(train)\n\n    # 6. Holdout evaluation\n    holdout_metrics = evaluate(model, holdout)\n\n    # 7. Retrain on all data for deployment\n    final_model = best_model.fit(features)\n\n    # 8. Generate forecasts with intervals\n    forecasts = final_model.forecast(config['horizon'])\n    intervals = final_model.prediction_intervals(config['horizon'])\n\n    return {\n        'model': final_model,\n        'forecasts': forecasts,\n        'intervals': intervals,\n        'metrics': holdout_metrics\n    }\n```\n\n**Monitoring Dashboard Metrics:**\n\n| Metric | Good | Warning | Action |\n|--------|------|---------|--------|\n| MAPE | < 10% | 10-20% | > 20%: investigate |\n| Bias | ≈ 0 | |bias| > 1σ | |bias| > 2σ: retrain |\n| PI Coverage | 90-100% | 80-90% | < 80%: recalibrate |\n| Tracking Signal | |TS| < 4 | 4-6 | > 6: retrain |", "line_start": 45, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Random train-test split:** Causes data leakage. Always use temporal splits.\n\n2. **Optimizing wrong metric:** Minimize business-relevant loss (e.g., asymmetric cost), not just RMSE.\n\n3. **No baseline comparison:** Claim \"model works\" without comparing to naive/seasonal naive.\n\n4. **Static model:** Not retraining as new data arrives. Monitor performance and retrain regularly.\n\n5. **Ignoring prediction intervals:** Point forecasts without uncertainty mislead decision-makers.\n\n6. **Overfitting to holdout:** If you tune on holdout multiple times, it becomes training data. Use nested CV.", "line_start": 103, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\ndef mape(y_true, y_pred):\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ndef tracking_signal(errors):\n    cumsum = np.cumsum(errors)\n    mad = np.mean(np.abs(errors))\n    return cumsum[-1] / mad if mad > 0 else 0", "line_start": 117, "level": 1}, {"heading": "Simulate production monitoring", "content": "np.random.seed(42)\nn_periods = 12  # 12 months of monitoring\n\nactuals = 100 + np.random.randn(n_periods) * 10\nforecasts = actuals + np.random.randn(n_periods) * 5 + 2  # slight bias\n\nerrors = actuals - forecasts", "line_start": 131, "level": 1}, {"heading": "Calculate monitoring metrics", "content": "print(\"=== Forecast Monitoring Report ===\\n\")\nprint(f\"MAE: {mean_absolute_error(actuals, forecasts):.2f}\")\nprint(f\"RMSE: {np.sqrt(mean_squared_error(actuals, forecasts)):.2f}\")\nprint(f\"MAPE: {mape(actuals, forecasts):.2f}%\")\nprint(f\"Bias: {np.mean(errors):.2f}\")\nprint(f\"Tracking Signal: {tracking_signal(errors):.2f}\")", "line_start": 140, "level": 1}, {"heading": "PI coverage check (simulated 95% intervals)", "content": "pi_width = 1.96 * np.std(errors)\nlower = forecasts - pi_width\nupper = forecasts + pi_width\ncoverage = np.mean((actuals >= lower) & (actuals <= upper))\nprint(f\"PI Coverage: {coverage*100:.1f}%\")", "line_start": 148, "level": 1}, {"heading": "Alert check", "content": "print(\"\\n=== Alerts ===\")\nif abs(np.mean(errors)) > 2 * np.std(errors):\n    print(\"WARNING: Significant forecast bias detected!\")\nif abs(tracking_signal(errors)) > 4:\n    print(\"WARNING: Tracking signal exceeds threshold - consider retraining\")\nif coverage < 0.80:\n    print(\"WARNING: Prediction interval coverage too low\")\n```", "line_start": 155, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why is random train-test split wrong for time series?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Random splits cause data leakage:\n- Future observations appear in training set\n- Past observations appear in test set\n- Model \"sees\" future information during training\n\n**Consequences:**\n- Overoptimistic evaluation metrics\n- Model fails in production where future isn't available\n- Temporal patterns learned incorrectly\n\n**Correct approach:**\n```\nTrain: [1, ..., T]    Test: [T+1, ..., T+h]\n```\nAlways train on past, test on future.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using sklearn's train_test_split() or KFold directly on time series. Use TimeSeriesSplit or manual temporal split.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> What is concept drift and how do you detect it?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Concept drift = the relationship between features and target changes over time.\n\n**Types:**\n- **Sudden:** Abrupt change (e.g., COVID impact)\n- **Gradual:** Slow shift (e.g., customer behavior evolution)\n- **Seasonal:** Recurring pattern changes\n- **Recurring:** Oscillates between states\n\n**Detection methods:**\n1. **Performance monitoring:** Increasing error over time\n2. **Statistical tests:** Compare recent vs historical distributions\n3. **Control charts:** Track forecast errors, flag out-of-control\n4. **Tracking signal:** Cumulative bias indicates drift\n\n**Response:**\n- Retrain on recent data\n- Use adaptive models (exponential smoothing)\n- Reduce lookback window\n- Add regime indicators\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Assuming model stays accurate forever. Schedule regular monitoring and retraining.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> How do you calculate and interpret the tracking signal?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n$$TS_t = \\frac{\\text{RSFE}_t}{\\text{MAD}_t} = \\frac{\\sum_{i=1}^{t}e_i}{\\frac{1}{t}\\sum_{i=1}^{t}|e_i|}$$\n\n**Interpretation:**\n- TS ≈ 0: No systematic bias\n- TS > 0: Systematic under-forecasting\n- TS < 0: Systematic over-forecasting\n- |TS| > 4: Likely significant bias (action needed)\n\n**Why use it:**\n- Normalizes by MAD for comparability\n- Accumulates evidence over time\n- Distinguishes random errors from systematic bias\n\n**Update frequency:**\nCheck monthly or quarterly; daily TS is noisy.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Reacting to every TS fluctuation. Wait for sustained signal (multiple periods with |TS| > 4) before retraining.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> What does it mean if your 95% prediction intervals have 75% coverage?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The model underestimates uncertainty—intervals are too narrow.\n\n**Possible causes:**\n1. **Model misspecification:** True residuals are larger than estimated\n2. **Heavy tails:** Data has outliers not captured by normal assumption\n3. **Non-constant variance:** Heteroskedasticity not modeled\n4. **Missing patterns:** Unmodeled seasonality or trend adds variance\n\n**Solutions:**\n1. Use bootstrap prediction intervals (more robust)\n2. Apply variance adjustment: multiply width by coverage correction factor\n3. Model heteroskedasticity (GARCH) or use quantile regression\n4. Improve base model to capture more patterns\n\n**Adjustment formula:**\nIf coverage = 75% but target = 95%, scale factor ≈ $z_{0.975}/z_{0.875}$ = 1.96/1.15 ≈ 1.7\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Reporting narrow intervals to seem accurate. Stakeholders need truthful uncertainty; too-narrow intervals cause poor decisions.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> Your demand forecasting model works well in testing but production accuracy is much worse. What might be causing this?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Common causes of train-test vs production gap:\n\n1. **Data leakage in testing:**\n   - Features used information not available at forecast time\n   - Train-test split not strictly temporal\n\n2. **Feature availability:**\n   - Features available historically but delayed in production\n   - External data sources not updated in real-time\n\n3. **Distribution shift:**\n   - Test period was unusual/lucky\n   - Production faces different conditions (seasonality, promotions)\n\n4. **Data quality:**\n   - Production data has errors/delays not in historical\n   - Missing values handled differently\n\n5. **Target leakage:**\n   - Test evaluated at easy horizons; production needs longer\n\n**Diagnosis:**\n1. Verify no leakage in feature engineering\n2. Compare feature distributions: test vs production\n3. Backtest over multiple periods (not just one)\n4. Monitor input data quality in production\n5. Check if production horizon matches testing\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Testing on convenient period and assuming it generalizes. Always backtest across multiple train-test splits spanning different conditions.\n</div>\n</div>\n</details>", "line_start": 165, "level": 2}, {"heading": "References", "content": "1. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapters 5, 12.\n2. Makridakis, S., Spiliotis, E., & Assimakopoulos, V. (2020). The M4 Competition: 100,000 time series and 61 forecasting methods. *IJF*, 36(1), 54-74.\n3. Gama, J., Žliobaitė, I., Bifet, A., Pechenizkiy, M., & Bouchachia, A. (2014). A survey on concept drift adaptation. *ACM Computing Surveys*, 46(4), 1-37.\n4. Kolassa, S. (2016). Evaluating predictive count data distributions in retail sales forecasting. *IJF*, 32(3), 788-803.", "line_start": 315, "level": 1}]}, "docs/en/features/feature-engineering.md": {"path": "docs/en/features/feature-engineering.md", "title": "Time Series Feature Engineering", "content": "# Time Series Feature Engineering\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Feature engineering transforms raw time series into ML-ready inputs. Key features: lags, rolling statistics, date/time features, Fourier terms for seasonality. Scaling: standardize or min-max, but fit only on training data. Handle missing data via interpolation or indicators. Avoid leakage: never use future information in features.\n</div>\n\n## Core Definitions\n\n**Lag Features:** Past values as predictors\n$$X_{lag_k} = y_{t-k}$$\n\n**Rolling Features:** Statistics over windows\n$$X_{roll\\_mean\\_k} = \\frac{1}{k}\\sum_{i=1}^{k}y_{t-i}$$\n\n**Date Features:** Extracted from timestamp\n- Hour, day of week, month, quarter\n- Is_weekend, is_holiday\n- Days since event\n\n**Fourier Features:** Seasonal patterns as sine/cosine\n$$X_{sin_k} = \\sin\\left(\\frac{2\\pi k t}{m}\\right), \\quad X_{cos_k} = \\cos\\left(\\frac{2\\pi k t}{m}\\right)$$\n\n## Math and Derivations\n\n### Fourier Terms for Seasonality\n\nFor seasonal period m, capture pattern with K harmonics:\n$$s(t) = \\sum_{k=1}^{K}\\left[\\alpha_k\\sin\\left(\\frac{2\\pi kt}{m}\\right) + \\beta_k\\cos\\left(\\frac{2\\pi kt}{m}\\right)\\right]$$\n\n**K selection:**\n- K = m/2: captures all seasonal frequencies\n- K = 2-4: often sufficient for smooth patterns\n- Use AIC to select optimal K\n\n**Why Fourier:**\n- Handles non-integer and long seasonal periods\n- Works with any ML model\n- Parsimonious: 2K features vs m dummy variables\n\n### Rolling Statistics\n\n**Rolling mean (simple moving average):**\n$$\\bar{y}_t^{(w)} = \\frac{1}{w}\\sum_{i=0}^{w-1}y_{t-i}$$\n\n**Rolling standard deviation:**\n$$s_t^{(w)} = \\sqrt{\\frac{1}{w-1}\\sum_{i=0}^{w-1}(y_{t-i} - \\bar{y}_t^{(w)})^2}$$\n\n**Exponential moving average:**\n$$\\text{EMA}_t = \\alpha y_t + (1-\\alpha)\\text{EMA}_{t-1}$$\n\n### Scaling Methods\n\n**Standardization (z-score):**\n$$y_{scaled} = \\frac{y - \\mu_{train}}{\\sigma_{train}}$$\n\n**Min-max scaling:**\n$$y_{scaled} = \\frac{y - \\min_{train}}{\\max_{train} - \\min_{train}}$$\n\n**Robust scaling:**\n$$y_{scaled} = \\frac{y - \\text{median}_{train}}{\\text{IQR}_{train}}$$\n\n**Critical:** Always fit scaler on training data only!\n\n## Algorithm/Model Sketch\n\n**Feature Engineering Pipeline:**\n\n```python\ndef create_features(df, target_col='y', lags=[1,2,3,7],\n                   rolling_windows=[7,14,30]):\n    features = df.copy()\n\n    # Lag features\n    for lag in lags:\n        features[f'lag_{lag}'] = features[target_col].shift(lag)\n\n    # Rolling features\n    for w in rolling_windows:\n        features[f'roll_mean_{w}'] = features[target_col].shift(1).rolling(w).mean()\n        features[f'roll_std_{w}'] = features[target_col].shift(1).rolling(w).std()\n        features[f'roll_min_{w}'] = features[target_col].shift(1).rolling(w).min()\n        features[f'roll_max_{w}'] = features[target_col].shift(1).rolling(w).max()\n\n    # Date features (if datetime index)\n    features['hour'] = features.index.hour\n    features['dayofweek'] = features.index.dayofweek\n    features['month'] = features.index.month\n    features['is_weekend'] = features.index.dayofweek >= 5\n\n    # Fourier features for annual seasonality\n    day_of_year = features.index.dayofyear\n    for k in range(1, 4):\n        features[f'sin_{k}'] = np.sin(2 * np.pi * k * day_of_year / 365.25)\n        features[f'cos_{k}'] = np.cos(2 * np.pi * k * day_of_year / 365.25)\n\n    return features.dropna()\n```\n\n**Train-Test Split for Time Series:**\n```python\n# WRONG: random split\nX_train, X_test = train_test_split(X)  # Data leakage!\n\n# RIGHT: temporal split\ntrain_end = int(len(X) * 0.8)\nX_train, X_test = X[:train_end], X[train_end:]\n```\n\n## Common Pitfalls\n\n1. **Using future information:** Lag features must use shift(k) where k ≥ 1. shift(0) = leakage.\n\n2. **Scaling on full data:** Fit scaler on training data only. Otherwise test data statistics leak into training.\n\n3. **Rolling windows including current value:** Rolling mean should be `.shift(1).rolling(w)`, not `.rolling(w)`.\n\n4. **Missing values from lags:** First k observations have NaN after creating lag_k. Drop or impute.\n\n5. **Too many features:** With many lags and rolling windows, dimensionality explodes. Use feature selection.\n\n6. **Non-stationarity in features:** If target is non-stationary, lag features inherit it. Consider differencing.\n\n## Mini Example\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# Generate sample data\nnp.random.seed(42)\nn = 365\ndates = pd.date_range('2023-01-01', periods=n, freq='D')\ntrend = np.arange(n) * 0.1\nseasonal = 10 * np.sin(2 * np.pi * np.arange(n) / 365)\nnoise = np.random.randn(n) * 2\ny = trend + seasonal + noise\n\ndf = pd.DataFrame({'y': y}, index=dates)\n\n# Create features\ndef make_features(df):\n    features = df.copy()\n    # Lags\n    for lag in [1, 7, 14, 28]:\n        features[f'lag_{lag}'] = features['y'].shift(lag)\n    # Rolling\n    features['roll_mean_7'] = features['y'].shift(1).rolling(7).mean()\n    features['roll_std_7'] = features['y'].shift(1).rolling(7).std()\n    # Calendar\n    features['dayofweek'] = features.index.dayofweek\n    features['month'] = features.index.month\n    # Fourier (annual)\n    doy = features.index.dayofyear\n    features['sin_annual'] = np.sin(2 * np.pi * doy / 365.25)\n    features['cos_annual'] = np.cos(2 * np.pi * doy / 365.25)\n    return features.dropna()\n\nfeatures = make_features(df)\n\n# Train-test split (temporal)\ntrain_size = 300\ntrain = features[:train_size]\ntest = features[train_size:]\n\nX_train = train.drop('y', axis=1)\ny_train = train['y']\nX_test = test.drop('y', axis=1)\ny_test = test['y']\n\n# Fit model\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)\n\n# Evaluate\ny_pred = model.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(f\"Test RMSE: {rmse:.2f}\")\n\n# Feature importance\nimportance = pd.Series(model.feature_importances_, index=X_train.columns)\nprint(\"\\nTop features:\")\nprint(importance.sort_values(ascending=False).head())\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why must you fit the scaler only on training data?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Fitting on full data causes data leakage—test data statistics influence training.\n\n**Problem:**\n```python\nscaler.fit(X)  # Uses test data statistics\nX_train_scaled = scaler.transform(X_train)  # Training influenced by test\n```\n\nThe model \"knows\" about test data range/distribution during training, giving optimistic evaluation.\n\n**Correct approach:**\n```python\nscaler.fit(X_train)  # Only training statistics\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)  # Apply same transformation\n```\n\n**Real-world analogy:** In production, you don't have future data to compute statistics.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using sklearn pipelines incorrectly. Always split BEFORE creating pipeline, or use TimeSeriesSplit in cross-validation.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> When would you use Fourier features instead of dummy variables for seasonality?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Use Fourier when:**\n1. **Long periods:** Weekly seasonality = 7 dummies; annual = 365 dummies vs 2-6 Fourier\n2. **Non-integer periods:** 365.25 days/year can't be captured with dummies\n3. **Smooth patterns:** Seasonality follows sinusoidal shape\n4. **Linear models:** Fourier terms capture cycles naturally\n\n**Use dummies when:**\n1. **Short periods:** Day of week (7 levels) is manageable\n2. **Sharp patterns:** \"Monday effect\" is discrete, not smooth\n3. **Interpretability:** Coefficients directly show day effects\n4. **Non-sinusoidal:** Pattern doesn't fit sine/cosine shape\n\n**Hybrid:** Can use both—Fourier for smooth annual, dummies for weekly.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using 52 dummies for weekly seasonality in daily data. Fourier with K=2-4 is more efficient and generalizes better.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Explain why shift(1).rolling(w).mean() is correct but rolling(w).mean() causes leakage.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Without shift:**\n```python\nrolling_mean[t] = mean(y[t-w+1], ..., y[t])  # Includes y[t]!\n```\nWhen predicting y[t], using rolling_mean[t] includes y[t] itself → leakage.\n\n**With shift:**\n```python\nrolling_mean[t] = mean(y[t-w], ..., y[t-1])  # Excludes y[t]\n```\nOnly uses past values → no leakage.\n\n**Mathematical notation:**\n- Wrong: $\\bar{y}_t = \\frac{1}{w}\\sum_{i=0}^{w-1}y_{t-i}$ includes $y_t$\n- Correct: $\\bar{y}_{t-1} = \\frac{1}{w}\\sum_{i=1}^{w}y_{t-i}$ excludes $y_t$\n\nThe shift moves the window back by one time step.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Pandas rolling default includes current value. Always add .shift(1) before .rolling() for features.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> How many Fourier terms (K) do you need to fully represent seasonality of period m?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> K = m/2 terms fully represent any periodic pattern of period m.\n\n**Explanation:**\nBy Fourier's theorem, any periodic function can be represented as:\n$$f(t) = \\sum_{k=1}^{\\infty}\\left[a_k\\sin\\left(\\frac{2\\pi kt}{m}\\right) + b_k\\cos\\left(\\frac{2\\pi kt}{m}\\right)\\right]$$\n\nFor discrete data with period m, frequencies above k = m/2 alias to lower frequencies (Nyquist).\n\n**Practical:**\n- K = m/2: Full representation (2K = m parameters, same as dummies)\n- K = 2-4: Often sufficient; smooth patterns don't need high harmonics\n- Use AIC/BIC: Add terms until no improvement\n\n**Example:** Annual seasonality in daily data\n- Full: K = 365/2 ≈ 182 (overkill)\n- Typical: K = 3-5 (6-10 parameters)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using K = m/2 when K = 3 suffices. Extra terms add noise and reduce interpretability.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> Your lag features have 30% missing values at the start due to the lag window. How do you handle this?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Several options:\n\n1. **Drop rows (simplest):**\n   ```python\n   features = features.dropna()\n   ```\n   - Lose initial observations\n   - OK if plenty of data\n\n2. **Fill with first available:**\n   ```python\n   features = features.fillna(method='bfill')\n   ```\n   - Uses earliest available value\n   - Slight bias but preserves data\n\n3. **Use target mean/median:**\n   ```python\n   features['lag_7'] = features['lag_7'].fillna(features['y'].mean())\n   ```\n   - Neutral imputation\n   - Works for tree models\n\n4. **Missing indicator:**\n   ```python\n   features['lag_7_missing'] = features['lag_7'].isna().astype(int)\n   features['lag_7'] = features['lag_7'].fillna(0)\n   ```\n   - Model learns to handle missing\n   - Most flexible\n\n5. **Shorter warmup lags:**\n   - Use lag_1 at start, add longer lags as available\n   - Complex but maximizes data\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Dropping 30% of data when you have limited observations. Try imputation first; validate on holdout to check impact.\n</div>\n</div>\n</details>\n\n## References\n\n1. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 7.\n2. Christ, M., Braun, N., Neuffer, J., & Kempa-Liehr, A. W. (2018). Time series feature extraction on basis of scalable hypothesis tests. *Neurocomputing*, 307, 72-77.\n3. Fulcher, B. D., & Jones, N. S. (2017). hctsa: A computational framework for automated time-series phenotyping. *Journal of Open Research Software*, 5(1).\n4. Brownlee, J. (2018). *Deep Learning for Time Series Forecasting*. Machine Learning Mastery.\n", "sections": [{"heading": "Time Series Feature Engineering", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Feature engineering transforms raw time series into ML-ready inputs. Key features: lags, rolling statistics, date/time features, Fourier terms for seasonality. Scaling: standardize or min-max, but fit only on training data. Handle missing data via interpolation or indicators. Avoid leakage: never use future information in features.\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**Lag Features:** Past values as predictors\n$$X_{lag_k} = y_{t-k}$$\n\n**Rolling Features:** Statistics over windows\n$$X_{roll\\_mean\\_k} = \\frac{1}{k}\\sum_{i=1}^{k}y_{t-i}$$\n\n**Date Features:** Extracted from timestamp\n- Hour, day of week, month, quarter\n- Is_weekend, is_holiday\n- Days since event\n\n**Fourier Features:** Seasonal patterns as sine/cosine\n$$X_{sin_k} = \\sin\\left(\\frac{2\\pi k t}{m}\\right), \\quad X_{cos_k} = \\cos\\left(\\frac{2\\pi k t}{m}\\right)$$", "line_start": 7, "level": 2}, {"heading": "Fourier Terms for Seasonality", "content": "For seasonal period m, capture pattern with K harmonics:\n$$s(t) = \\sum_{k=1}^{K}\\left[\\alpha_k\\sin\\left(\\frac{2\\pi kt}{m}\\right) + \\beta_k\\cos\\left(\\frac{2\\pi kt}{m}\\right)\\right]$$\n\n**K selection:**\n- K = m/2: captures all seasonal frequencies\n- K = 2-4: often sufficient for smooth patterns\n- Use AIC to select optimal K\n\n**Why Fourier:**\n- Handles non-integer and long seasonal periods\n- Works with any ML model\n- Parsimonious: 2K features vs m dummy variables", "line_start": 25, "level": 3}, {"heading": "Rolling Statistics", "content": "**Rolling mean (simple moving average):**\n$$\\bar{y}_t^{(w)} = \\frac{1}{w}\\sum_{i=0}^{w-1}y_{t-i}$$\n\n**Rolling standard deviation:**\n$$s_t^{(w)} = \\sqrt{\\frac{1}{w-1}\\sum_{i=0}^{w-1}(y_{t-i} - \\bar{y}_t^{(w)})^2}$$\n\n**Exponential moving average:**\n$$\\text{EMA}_t = \\alpha y_t + (1-\\alpha)\\text{EMA}_{t-1}$$", "line_start": 40, "level": 3}, {"heading": "Scaling Methods", "content": "**Standardization (z-score):**\n$$y_{scaled} = \\frac{y - \\mu_{train}}{\\sigma_{train}}$$\n\n**Min-max scaling:**\n$$y_{scaled} = \\frac{y - \\min_{train}}{\\max_{train} - \\min_{train}}$$\n\n**Robust scaling:**\n$$y_{scaled} = \\frac{y - \\text{median}_{train}}{\\text{IQR}_{train}}$$\n\n**Critical:** Always fit scaler on training data only!", "line_start": 51, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**Feature Engineering Pipeline:**\n\n```python\ndef create_features(df, target_col='y', lags=[1,2,3,7],\n                   rolling_windows=[7,14,30]):\n    features = df.copy()\n\n    # Lag features\n    for lag in lags:\n        features[f'lag_{lag}'] = features[target_col].shift(lag)\n\n    # Rolling features\n    for w in rolling_windows:\n        features[f'roll_mean_{w}'] = features[target_col].shift(1).rolling(w).mean()\n        features[f'roll_std_{w}'] = features[target_col].shift(1).rolling(w).std()\n        features[f'roll_min_{w}'] = features[target_col].shift(1).rolling(w).min()\n        features[f'roll_max_{w}'] = features[target_col].shift(1).rolling(w).max()\n\n    # Date features (if datetime index)\n    features['hour'] = features.index.hour\n    features['dayofweek'] = features.index.dayofweek\n    features['month'] = features.index.month\n    features['is_weekend'] = features.index.dayofweek >= 5\n\n    # Fourier features for annual seasonality\n    day_of_year = features.index.dayofyear\n    for k in range(1, 4):\n        features[f'sin_{k}'] = np.sin(2 * np.pi * k * day_of_year / 365.25)\n        features[f'cos_{k}'] = np.cos(2 * np.pi * k * day_of_year / 365.25)\n\n    return features.dropna()\n```\n\n**Train-Test Split for Time Series:**\n```python", "line_start": 64, "level": 1}, {"heading": "WRONG: random split", "content": "X_train, X_test = train_test_split(X)  # Data leakage!", "line_start": 101, "level": 1}, {"heading": "RIGHT: temporal split", "content": "train_end = int(len(X) * 0.8)\nX_train, X_test = X[:train_end], X[train_end:]\n```", "line_start": 104, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Using future information:** Lag features must use shift(k) where k ≥ 1. shift(0) = leakage.\n\n2. **Scaling on full data:** Fit scaler on training data only. Otherwise test data statistics leak into training.\n\n3. **Rolling windows including current value:** Rolling mean should be `.shift(1).rolling(w)`, not `.rolling(w)`.\n\n4. **Missing values from lags:** First k observations have NaN after creating lag_k. Drop or impute.\n\n5. **Too many features:** With many lags and rolling windows, dimensionality explodes. Use feature selection.\n\n6. **Non-stationarity in features:** If target is non-stationary, lag features inherit it. Consider differencing.", "line_start": 109, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error", "line_start": 123, "level": 1}, {"heading": "Generate sample data", "content": "np.random.seed(42)\nn = 365\ndates = pd.date_range('2023-01-01', periods=n, freq='D')\ntrend = np.arange(n) * 0.1\nseasonal = 10 * np.sin(2 * np.pi * np.arange(n) / 365)\nnoise = np.random.randn(n) * 2\ny = trend + seasonal + noise\n\ndf = pd.DataFrame({'y': y}, index=dates)", "line_start": 131, "level": 1}, {"heading": "Create features", "content": "def make_features(df):\n    features = df.copy()\n    # Lags\n    for lag in [1, 7, 14, 28]:\n        features[f'lag_{lag}'] = features['y'].shift(lag)\n    # Rolling\n    features['roll_mean_7'] = features['y'].shift(1).rolling(7).mean()\n    features['roll_std_7'] = features['y'].shift(1).rolling(7).std()\n    # Calendar\n    features['dayofweek'] = features.index.dayofweek\n    features['month'] = features.index.month\n    # Fourier (annual)\n    doy = features.index.dayofyear\n    features['sin_annual'] = np.sin(2 * np.pi * doy / 365.25)\n    features['cos_annual'] = np.cos(2 * np.pi * doy / 365.25)\n    return features.dropna()\n\nfeatures = make_features(df)", "line_start": 142, "level": 1}, {"heading": "Train-test split (temporal)", "content": "train_size = 300\ntrain = features[:train_size]\ntest = features[train_size:]\n\nX_train = train.drop('y', axis=1)\ny_train = train['y']\nX_test = test.drop('y', axis=1)\ny_test = test['y']", "line_start": 162, "level": 1}, {"heading": "Fit model", "content": "model = RandomForestRegressor(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)", "line_start": 172, "level": 1}, {"heading": "Evaluate", "content": "y_pred = model.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(f\"Test RMSE: {rmse:.2f}\")", "line_start": 176, "level": 1}, {"heading": "Feature importance", "content": "importance = pd.Series(model.feature_importances_, index=X_train.columns)\nprint(\"\\nTop features:\")\nprint(importance.sort_values(ascending=False).head())\n```", "line_start": 181, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why must you fit the scaler only on training data?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Fitting on full data causes data leakage—test data statistics influence training.\n\n**Problem:**\n```python\nscaler.fit(X)  # Uses test data statistics\nX_train_scaled = scaler.transform(X_train)  # Training influenced by test\n```\n\nThe model \"knows\" about test data range/distribution during training, giving optimistic evaluation.\n\n**Correct approach:**\n```python\nscaler.fit(X_train)  # Only training statistics\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)  # Apply same transformation\n```\n\n**Real-world analogy:** In production, you don't have future data to compute statistics.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using sklearn pipelines incorrectly. Always split BEFORE creating pipeline, or use TimeSeriesSplit in cross-validation.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> When would you use Fourier features instead of dummy variables for seasonality?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Use Fourier when:**\n1. **Long periods:** Weekly seasonality = 7 dummies; annual = 365 dummies vs 2-6 Fourier\n2. **Non-integer periods:** 365.25 days/year can't be captured with dummies\n3. **Smooth patterns:** Seasonality follows sinusoidal shape\n4. **Linear models:** Fourier terms capture cycles naturally\n\n**Use dummies when:**\n1. **Short periods:** Day of week (7 levels) is manageable\n2. **Sharp patterns:** \"Monday effect\" is discrete, not smooth\n3. **Interpretability:** Coefficients directly show day effects\n4. **Non-sinusoidal:** Pattern doesn't fit sine/cosine shape\n\n**Hybrid:** Can use both—Fourier for smooth annual, dummies for weekly.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using 52 dummies for weekly seasonality in daily data. Fourier with K=2-4 is more efficient and generalizes better.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Explain why shift(1).rolling(w).mean() is correct but rolling(w).mean() causes leakage.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Without shift:**\n```python\nrolling_mean[t] = mean(y[t-w+1], ..., y[t])  # Includes y[t]!\n```\nWhen predicting y[t], using rolling_mean[t] includes y[t] itself → leakage.\n\n**With shift:**\n```python\nrolling_mean[t] = mean(y[t-w], ..., y[t-1])  # Excludes y[t]\n```\nOnly uses past values → no leakage.\n\n**Mathematical notation:**\n- Wrong: $\\bar{y}_t = \\frac{1}{w}\\sum_{i=0}^{w-1}y_{t-i}$ includes $y_t$\n- Correct: $\\bar{y}_{t-1} = \\frac{1}{w}\\sum_{i=1}^{w}y_{t-i}$ excludes $y_t$\n\nThe shift moves the window back by one time step.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Pandas rolling default includes current value. Always add .shift(1) before .rolling() for features.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> How many Fourier terms (K) do you need to fully represent seasonality of period m?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> K = m/2 terms fully represent any periodic pattern of period m.\n\n**Explanation:**\nBy Fourier's theorem, any periodic function can be represented as:\n$$f(t) = \\sum_{k=1}^{\\infty}\\left[a_k\\sin\\left(\\frac{2\\pi kt}{m}\\right) + b_k\\cos\\left(\\frac{2\\pi kt}{m}\\right)\\right]$$\n\nFor discrete data with period m, frequencies above k = m/2 alias to lower frequencies (Nyquist).\n\n**Practical:**\n- K = m/2: Full representation (2K = m parameters, same as dummies)\n- K = 2-4: Often sufficient; smooth patterns don't need high harmonics\n- Use AIC/BIC: Add terms until no improvement\n\n**Example:** Annual seasonality in daily data\n- Full: K = 365/2 ≈ 182 (overkill)\n- Typical: K = 3-5 (6-10 parameters)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using K = m/2 when K = 3 suffices. Extra terms add noise and reduce interpretability.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> Your lag features have 30% missing values at the start due to the lag window. How do you handle this?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Several options:\n\n1. **Drop rows (simplest):**\n   ```python\n   features = features.dropna()\n   ```\n   - Lose initial observations\n   - OK if plenty of data\n\n2. **Fill with first available:**\n   ```python\n   features = features.fillna(method='bfill')\n   ```\n   - Uses earliest available value\n   - Slight bias but preserves data\n\n3. **Use target mean/median:**\n   ```python\n   features['lag_7'] = features['lag_7'].fillna(features['y'].mean())\n   ```\n   - Neutral imputation\n   - Works for tree models\n\n4. **Missing indicator:**\n   ```python\n   features['lag_7_missing'] = features['lag_7'].isna().astype(int)\n   features['lag_7'] = features['lag_7'].fillna(0)\n   ```\n   - Model learns to handle missing\n   - Most flexible\n\n5. **Shorter warmup lags:**\n   - Use lag_1 at start, add longer lags as available\n   - Complex but maximizes data\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Dropping 30% of data when you have limited observations. Try imputation first; validate on holdout to check impact.\n</div>\n</div>\n</details>", "line_start": 187, "level": 2}, {"heading": "References", "content": "1. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 7.\n2. Christ, M., Braun, N., Neuffer, J., & Kempa-Liehr, A. W. (2018). Time series feature extraction on basis of scalable hypothesis tests. *Neurocomputing*, 307, 72-77.\n3. Fulcher, B. D., & Jones, N. S. (2017). hctsa: A computational framework for automated time-series phenotyping. *Journal of Open Research Software*, 5(1).\n4. Brownlee, J. (2018). *Deep Learning for Time Series Forecasting*. Machine Learning Mastery.", "line_start": 346, "level": 1}]}, "docs/en/state-space/kalman-filter.md": {"path": "docs/en/state-space/kalman-filter.md", "title": "State Space Models and Kalman Filter", "content": "# State Space Models and Kalman Filter\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> State space models represent time series via hidden states evolving over time. Kalman filter recursively estimates states: prediction step (propagate state forward), update step (incorporate new observation). Optimal for linear Gaussian systems. Core equations: $x_t = Fx_{t-1} + w_t$, $y_t = Hx_t + v_t$. Unifies ARIMA, exponential smoothing, and structural models.\n</div>\n\n## Core Definitions\n\n**State Space Representation:**\n\nState equation: $\\mathbf{x}_t = \\mathbf{F}\\mathbf{x}_{t-1} + \\mathbf{w}_t$, $\\mathbf{w}_t \\sim N(0, \\mathbf{Q})$\n\nObservation equation: $y_t = \\mathbf{H}\\mathbf{x}_t + v_t$, $v_t \\sim N(0, R)$\n\n**Components:**\n- $\\mathbf{x}_t$: State vector (unobserved)\n- $y_t$: Observation (data)\n- $\\mathbf{F}$: State transition matrix\n- $\\mathbf{H}$: Observation matrix\n- $\\mathbf{Q}$: State noise covariance\n- $R$: Observation noise variance\n\n**Local Level Model (simplest):**\n$$\\mu_t = \\mu_{t-1} + \\eta_t, \\quad \\eta_t \\sim N(0, \\sigma^2_\\eta)$$\n$$y_t = \\mu_t + \\epsilon_t, \\quad \\epsilon_t \\sim N(0, \\sigma^2_\\epsilon)$$\n\n## Math and Derivations\n\n### Kalman Filter Recursions\n\n**Prediction Step:**\n$$\\hat{\\mathbf{x}}_{t|t-1} = \\mathbf{F}\\hat{\\mathbf{x}}_{t-1|t-1}$$\n$$\\mathbf{P}_{t|t-1} = \\mathbf{F}\\mathbf{P}_{t-1|t-1}\\mathbf{F}' + \\mathbf{Q}$$\n\n**Update Step:**\n$$\\mathbf{K}_t = \\mathbf{P}_{t|t-1}\\mathbf{H}'(\\mathbf{H}\\mathbf{P}_{t|t-1}\\mathbf{H}' + R)^{-1}$$\n$$\\hat{\\mathbf{x}}_{t|t} = \\hat{\\mathbf{x}}_{t|t-1} + \\mathbf{K}_t(y_t - \\mathbf{H}\\hat{\\mathbf{x}}_{t|t-1})$$\n$$\\mathbf{P}_{t|t} = (\\mathbf{I} - \\mathbf{K}_t\\mathbf{H})\\mathbf{P}_{t|t-1}$$\n\n**Key quantities:**\n- $\\hat{\\mathbf{x}}_{t|t}$: Filtered state estimate (using data up to t)\n- $\\mathbf{P}_{t|t}$: State covariance (uncertainty)\n- $\\mathbf{K}_t$: Kalman gain (weight on new observation)\n- $v_t = y_t - \\mathbf{H}\\hat{\\mathbf{x}}_{t|t-1}$: Innovation (prediction error)\n\n### Local Level Model Kalman Filter\n\nState: $\\mathbf{x}_t = \\mu_t$ (scalar)\nMatrices: $F = 1$, $H = 1$, $Q = \\sigma^2_\\eta$, $R = \\sigma^2_\\epsilon$\n\nRecursions:\n$$\\hat{\\mu}_{t|t-1} = \\hat{\\mu}_{t-1|t-1}$$\n$$P_{t|t-1} = P_{t-1|t-1} + \\sigma^2_\\eta$$\n$$K_t = \\frac{P_{t|t-1}}{P_{t|t-1} + \\sigma^2_\\epsilon}$$\n$$\\hat{\\mu}_{t|t} = \\hat{\\mu}_{t|t-1} + K_t(y_t - \\hat{\\mu}_{t|t-1})$$\n$$P_{t|t} = (1 - K_t)P_{t|t-1}$$\n\nAs $t \\to \\infty$: $K_t \\to K^* = $ steady-state gain (related to SES α).\n\n### Local Linear Trend Model\n\nState: $\\mathbf{x}_t = (\\mu_t, \\beta_t)'$ (level and trend)\n\n$$\\begin{pmatrix} \\mu_t \\\\ \\beta_t \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} \\mu_{t-1} \\\\ \\beta_{t-1} \\end{pmatrix} + \\begin{pmatrix} \\eta_t \\\\ \\zeta_t \\end{pmatrix}$$\n\n$$y_t = (1, 0)\\begin{pmatrix} \\mu_t \\\\ \\beta_t \\end{pmatrix} + \\epsilon_t$$\n\n## Algorithm/Model Sketch\n\n**Kalman Filter Algorithm:**\n\n```\nInput: y[1:n], F, H, Q, R, x0, P0\nOutput: filtered states, innovations, likelihood\n\nInitialize:\n  x_hat = x0\n  P = P0\n  log_lik = 0\n\nFor t = 1 to n:\n  # Prediction\n  x_pred = F @ x_hat\n  P_pred = F @ P @ F' + Q\n\n  # Innovation\n  v = y[t] - H @ x_pred\n  S = H @ P_pred @ H' + R\n\n  # Update\n  K = P_pred @ H' @ inv(S)\n  x_hat = x_pred + K @ v\n  P = (I - K @ H) @ P_pred\n\n  # Likelihood contribution\n  log_lik += -0.5 * (log(det(S)) + v' @ inv(S) @ v)\n\nReturn x_hat_history, P_history, log_lik\n```\n\n**Kalman Smoother** (uses all data):\n$$\\hat{\\mathbf{x}}_{t|n} = \\hat{\\mathbf{x}}_{t|t} + \\mathbf{J}_t(\\hat{\\mathbf{x}}_{t+1|n} - \\hat{\\mathbf{x}}_{t+1|t})$$\n\nwhere $\\mathbf{J}_t = \\mathbf{P}_{t|t}\\mathbf{F}'\\mathbf{P}_{t+1|t}^{-1}$\n\n## Common Pitfalls\n\n1. **Numerical instability**: Covariance matrices can become non-positive-definite. Use square-root or UD factorization.\n\n2. **Wrong initialization**: Poor $\\mathbf{x}_0, \\mathbf{P}_0$ affects early estimates. Use diffuse initialization for unknown initial states.\n\n3. **Model misspecification**: Kalman filter is optimal only for true model. Non-Gaussian or nonlinear systems need extensions (EKF, UKF, particle filter).\n\n4. **Forgetting the smoother**: For historical analysis (not real-time), use Kalman smoother to incorporate future observations.\n\n5. **Over-complicated state space**: Can represent many models but simple alternatives (ARIMA) may be easier.\n\n6. **Confusing filter vs. forecast**: Filter = estimate given data up to t. Forecast = prediction beyond observed data.\n\n## Mini Example\n\n```python\nimport numpy as np\n\ndef kalman_filter_local_level(y, sigma_eta, sigma_eps, mu0=None, P0=1000):\n    \"\"\"Kalman filter for local level model.\"\"\"\n    n = len(y)\n\n    # Initialize\n    mu_filt = np.zeros(n)\n    P_filt = np.zeros(n)\n    mu_pred = mu0 if mu0 is not None else y[0]\n    P_pred = P0\n\n    for t in range(n):\n        # Prediction (for t > 0)\n        if t > 0:\n            mu_pred = mu_filt[t-1]\n            P_pred = P_filt[t-1] + sigma_eta**2\n\n        # Update\n        K = P_pred / (P_pred + sigma_eps**2)\n        mu_filt[t] = mu_pred + K * (y[t] - mu_pred)\n        P_filt[t] = (1 - K) * P_pred\n\n    return mu_filt, P_filt\n\n# Generate local level data\nnp.random.seed(42)\nn = 100\nsigma_eta, sigma_eps = 0.5, 1.0\n\n# True states\nmu_true = np.cumsum(np.random.randn(n) * sigma_eta)\n# Observations\ny = mu_true + np.random.randn(n) * sigma_eps\n\n# Run Kalman filter\nmu_hat, P_hat = kalman_filter_local_level(y, sigma_eta, sigma_eps)\n\nprint(\"Kalman Filter Results:\")\nprint(f\"Final state estimate: {mu_hat[-1]:.2f}\")\nprint(f\"True final state: {mu_true[-1]:.2f}\")\nprint(f\"Final state std: {np.sqrt(P_hat[-1]):.3f}\")\n\n# Steady-state Kalman gain\nK_steady = (-sigma_eps**2 + np.sqrt(sigma_eps**4 + 4*sigma_eta**2*sigma_eps**2)) / (2*sigma_eps**2)\nprint(f\"\\nSteady-state Kalman gain: {K_steady:.3f}\")\nprint(f\"Equivalent SES alpha: {K_steady:.3f}\")\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What is the intuition behind the Kalman gain?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The Kalman gain $K_t$ balances trust in prediction vs. trust in observation.\n\n$$K_t = \\frac{P_{t|t-1}}{P_{t|t-1} + R} = \\frac{\\text{prediction uncertainty}}{\\text{prediction uncertainty} + \\text{observation noise}}$$\n\n**Interpretation:**\n- High $K_t$ (close to 1): Observation is trusted more; state updates significantly\n- Low $K_t$ (close to 0): Prediction is trusted more; observation has little effect\n\n**When is K high?**\n- High state uncertainty ($P$ large)\n- Low observation noise ($R$ small)\n\n**When is K low?**\n- Low state uncertainty ($P$ small)\n- High observation noise ($R$ large)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Thinking gain is fixed. It evolves as uncertainty changes. Early on, gain may be high (uncertain prior); later, it stabilizes as filter \"learns.\"\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> How does the local level model relate to simple exponential smoothing?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> They're equivalent! Local level model with specific noise ratio produces SES.\n\n**Local level:**\n$$\\mu_t = \\mu_{t-1} + \\eta_t, \\quad y_t = \\mu_t + \\epsilon_t$$\n\n**Kalman update:**\n$$\\hat{\\mu}_{t|t} = \\hat{\\mu}_{t|t-1} + K(y_t - \\hat{\\mu}_{t|t-1})$$\n\n**At steady state:** $K \\to K^* = \\alpha$ (SES smoothing parameter)\n\nThe relationship:\n$$\\alpha = \\frac{-\\sigma_\\epsilon^2 + \\sqrt{\\sigma_\\epsilon^4 + 4\\sigma_\\eta^2\\sigma_\\epsilon^2}}{2\\sigma_\\eta^2}$$\n\n**Key insight:** SES is the steady-state Kalman filter for local level model. Kalman provides:\n- Optimal initialization\n- Proper uncertainty quantification\n- Connection to likelihood\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using SES without recognizing it assumes specific state space structure. If dynamics differ, SES may be suboptimal.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Derive the steady-state Kalman gain for the local level model.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> At steady state, $P_{t|t} = P^*$ (constant).\n\n**Recursion:**\n$$P_{t|t-1} = P_{t-1|t-1} + Q = P^* + Q$$\n$$P_{t|t} = (1 - K_t)P_{t|t-1} = P^*$$\n\n**Steady state condition:**\n$$P^* = (1 - K^*)(P^* + Q)$$\n$$P^* = P^* + Q - K^*(P^* + Q)$$\n$$K^* = \\frac{Q}{P^* + Q}$$\n\nAlso: $K^* = \\frac{P^* + Q}{P^* + Q + R}$\n\nSolving for $P^*$:\n$$P^* = \\frac{-R + \\sqrt{R^2 + 4QR}}{2}$$\n\nAnd:\n$$K^* = \\frac{-R + \\sqrt{R^2 + 4QR}}{2Q}$$\n\nFor Q = $\\sigma_\\eta^2$, R = $\\sigma_\\epsilon^2$, this gives the SES α relationship.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Steady state may take many iterations to reach. For short series, time-varying gain matters.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> What is the difference between filtered and smoothed state estimates?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Filtered:** $\\hat{x}_{t|t} = E[x_t | y_1, \\ldots, y_t]$\n- Uses observations up to time t\n- Real-time estimate\n- From forward Kalman pass\n\n**Smoothed:** $\\hat{x}_{t|n} = E[x_t | y_1, \\ldots, y_n]$\n- Uses all observations\n- Retrospective estimate\n- Requires backward pass after forward\n\n**Key difference:** Smoother uses future observations to refine past state estimates.\n\n**When to use which:**\n- Real-time forecasting: Filter\n- Historical analysis: Smoother\n- Parameter estimation: Smoother (better likelihood)\n\n**Variance relationship:**\n$$\\text{Var}(\\hat{x}_{t|n}) \\leq \\text{Var}(\\hat{x}_{t|t})$$\n\nSmoother is never worse than filter.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using filter estimates for historical analysis when smoother is available and more accurate.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You have GPS measurements with known accuracy (R) but unknown vehicle dynamics (Q). How would you tune the Kalman filter?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Approach 1: Maximum Likelihood**\n- Treat Q as parameter\n- Compute likelihood from innovations: $\\log L = -\\frac{1}{2}\\sum(\\log S_t + v_t^2/S_t)$\n- Optimize Q to maximize likelihood\n\n**Approach 2: Innovation-based tuning**\n- Innovations $v_t$ should be white noise with variance $S_t$\n- If innovations autocorrelated: Q too small\n- If innovation variance >> $S_t$: Q too small\n- If innovation variance << $S_t$: Q too large\n\n**Approach 3: Adaptive filtering**\n- Estimate Q online from recent innovation statistics\n- $\\hat{Q}_t = $ sample variance of recent innovations minus R\n\n**Practical steps:**\n1. Start with Q = R (equal trust)\n2. Check innovation statistics\n3. Grid search or gradient optimization\n4. Validate on holdout data\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Setting Q too small makes filter overconfident and slow to track changes. Too large makes it noisy. Cross-validate!\n</div>\n</div>\n</details>\n\n## References\n\n1. Harvey, A. C. (1990). *Forecasting, Structural Time Series Models and the Kalman Filter*. Cambridge University Press.\n2. Durbin, J., & Koopman, S. J. (2012). *Time Series Analysis by State Space Methods*. Oxford University Press.\n3. Kalman, R. E. (1960). A new approach to linear filtering and prediction problems. *Journal of Basic Engineering*, 82(1), 35-45.\n4. Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press. Chapter 13.\n", "sections": [{"heading": "State Space Models and Kalman Filter", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> State space models represent time series via hidden states evolving over time. Kalman filter recursively estimates states: prediction step (propagate state forward), update step (incorporate new observation). Optimal for linear Gaussian systems. Core equations: $x_t = Fx_{t-1} + w_t$, $y_t = Hx_t + v_t$. Unifies ARIMA, exponential smoothing, and structural models.\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**State Space Representation:**\n\nState equation: $\\mathbf{x}_t = \\mathbf{F}\\mathbf{x}_{t-1} + \\mathbf{w}_t$, $\\mathbf{w}_t \\sim N(0, \\mathbf{Q})$\n\nObservation equation: $y_t = \\mathbf{H}\\mathbf{x}_t + v_t$, $v_t \\sim N(0, R)$\n\n**Components:**\n- $\\mathbf{x}_t$: State vector (unobserved)\n- $y_t$: Observation (data)\n- $\\mathbf{F}$: State transition matrix\n- $\\mathbf{H}$: Observation matrix\n- $\\mathbf{Q}$: State noise covariance\n- $R$: Observation noise variance\n\n**Local Level Model (simplest):**\n$$\\mu_t = \\mu_{t-1} + \\eta_t, \\quad \\eta_t \\sim N(0, \\sigma^2_\\eta)$$\n$$y_t = \\mu_t + \\epsilon_t, \\quad \\epsilon_t \\sim N(0, \\sigma^2_\\epsilon)$$", "line_start": 7, "level": 2}, {"heading": "Kalman Filter Recursions", "content": "**Prediction Step:**\n$$\\hat{\\mathbf{x}}_{t|t-1} = \\mathbf{F}\\hat{\\mathbf{x}}_{t-1|t-1}$$\n$$\\mathbf{P}_{t|t-1} = \\mathbf{F}\\mathbf{P}_{t-1|t-1}\\mathbf{F}' + \\mathbf{Q}$$\n\n**Update Step:**\n$$\\mathbf{K}_t = \\mathbf{P}_{t|t-1}\\mathbf{H}'(\\mathbf{H}\\mathbf{P}_{t|t-1}\\mathbf{H}' + R)^{-1}$$\n$$\\hat{\\mathbf{x}}_{t|t} = \\hat{\\mathbf{x}}_{t|t-1} + \\mathbf{K}_t(y_t - \\mathbf{H}\\hat{\\mathbf{x}}_{t|t-1})$$\n$$\\mathbf{P}_{t|t} = (\\mathbf{I} - \\mathbf{K}_t\\mathbf{H})\\mathbf{P}_{t|t-1}$$\n\n**Key quantities:**\n- $\\hat{\\mathbf{x}}_{t|t}$: Filtered state estimate (using data up to t)\n- $\\mathbf{P}_{t|t}$: State covariance (uncertainty)\n- $\\mathbf{K}_t$: Kalman gain (weight on new observation)\n- $v_t = y_t - \\mathbf{H}\\hat{\\mathbf{x}}_{t|t-1}$: Innovation (prediction error)", "line_start": 29, "level": 3}, {"heading": "Local Level Model Kalman Filter", "content": "State: $\\mathbf{x}_t = \\mu_t$ (scalar)\nMatrices: $F = 1$, $H = 1$, $Q = \\sigma^2_\\eta$, $R = \\sigma^2_\\epsilon$\n\nRecursions:\n$$\\hat{\\mu}_{t|t-1} = \\hat{\\mu}_{t-1|t-1}$$\n$$P_{t|t-1} = P_{t-1|t-1} + \\sigma^2_\\eta$$\n$$K_t = \\frac{P_{t|t-1}}{P_{t|t-1} + \\sigma^2_\\epsilon}$$\n$$\\hat{\\mu}_{t|t} = \\hat{\\mu}_{t|t-1} + K_t(y_t - \\hat{\\mu}_{t|t-1})$$\n$$P_{t|t} = (1 - K_t)P_{t|t-1}$$\n\nAs $t \\to \\infty$: $K_t \\to K^* = $ steady-state gain (related to SES α).", "line_start": 46, "level": 3}, {"heading": "Local Linear Trend Model", "content": "State: $\\mathbf{x}_t = (\\mu_t, \\beta_t)'$ (level and trend)\n\n$$\\begin{pmatrix} \\mu_t \\\\ \\beta_t \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} \\mu_{t-1} \\\\ \\beta_{t-1} \\end{pmatrix} + \\begin{pmatrix} \\eta_t \\\\ \\zeta_t \\end{pmatrix}$$\n\n$$y_t = (1, 0)\\begin{pmatrix} \\mu_t \\\\ \\beta_t \\end{pmatrix} + \\epsilon_t$$", "line_start": 60, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**Kalman Filter Algorithm:**\n\n```\nInput: y[1:n], F, H, Q, R, x0, P0\nOutput: filtered states, innovations, likelihood\n\nInitialize:\n  x_hat = x0\n  P = P0\n  log_lik = 0\n\nFor t = 1 to n:\n  # Prediction\n  x_pred = F @ x_hat\n  P_pred = F @ P @ F' + Q\n\n  # Innovation\n  v = y[t] - H @ x_pred\n  S = H @ P_pred @ H' + R\n\n  # Update\n  K = P_pred @ H' @ inv(S)\n  x_hat = x_pred + K @ v\n  P = (I - K @ H) @ P_pred\n\n  # Likelihood contribution\n  log_lik += -0.5 * (log(det(S)) + v' @ inv(S) @ v)\n\nReturn x_hat_history, P_history, log_lik\n```\n\n**Kalman Smoother** (uses all data):\n$$\\hat{\\mathbf{x}}_{t|n} = \\hat{\\mathbf{x}}_{t|t} + \\mathbf{J}_t(\\hat{\\mathbf{x}}_{t+1|n} - \\hat{\\mathbf{x}}_{t+1|t})$$\n\nwhere $\\mathbf{J}_t = \\mathbf{P}_{t|t}\\mathbf{F}'\\mathbf{P}_{t+1|t}^{-1}$", "line_start": 68, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Numerical instability**: Covariance matrices can become non-positive-definite. Use square-root or UD factorization.\n\n2. **Wrong initialization**: Poor $\\mathbf{x}_0, \\mathbf{P}_0$ affects early estimates. Use diffuse initialization for unknown initial states.\n\n3. **Model misspecification**: Kalman filter is optimal only for true model. Non-Gaussian or nonlinear systems need extensions (EKF, UKF, particle filter).\n\n4. **Forgetting the smoother**: For historical analysis (not real-time), use Kalman smoother to incorporate future observations.\n\n5. **Over-complicated state space**: Can represent many models but simple alternatives (ARIMA) may be easier.\n\n6. **Confusing filter vs. forecast**: Filter = estimate given data up to t. Forecast = prediction beyond observed data.", "line_start": 106, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\n\ndef kalman_filter_local_level(y, sigma_eta, sigma_eps, mu0=None, P0=1000):\n    \"\"\"Kalman filter for local level model.\"\"\"\n    n = len(y)\n\n    # Initialize\n    mu_filt = np.zeros(n)\n    P_filt = np.zeros(n)\n    mu_pred = mu0 if mu0 is not None else y[0]\n    P_pred = P0\n\n    for t in range(n):\n        # Prediction (for t > 0)\n        if t > 0:\n            mu_pred = mu_filt[t-1]\n            P_pred = P_filt[t-1] + sigma_eta**2\n\n        # Update\n        K = P_pred / (P_pred + sigma_eps**2)\n        mu_filt[t] = mu_pred + K * (y[t] - mu_pred)\n        P_filt[t] = (1 - K) * P_pred\n\n    return mu_filt, P_filt", "line_start": 120, "level": 1}, {"heading": "Generate local level data", "content": "np.random.seed(42)\nn = 100\nsigma_eta, sigma_eps = 0.5, 1.0", "line_start": 148, "level": 1}, {"heading": "True states", "content": "mu_true = np.cumsum(np.random.randn(n) * sigma_eta)", "line_start": 153, "level": 1}, {"heading": "Observations", "content": "y = mu_true + np.random.randn(n) * sigma_eps", "line_start": 155, "level": 1}, {"heading": "Run Kalman filter", "content": "mu_hat, P_hat = kalman_filter_local_level(y, sigma_eta, sigma_eps)\n\nprint(\"Kalman Filter Results:\")\nprint(f\"Final state estimate: {mu_hat[-1]:.2f}\")\nprint(f\"True final state: {mu_true[-1]:.2f}\")\nprint(f\"Final state std: {np.sqrt(P_hat[-1]):.3f}\")", "line_start": 158, "level": 1}, {"heading": "Steady-state Kalman gain", "content": "K_steady = (-sigma_eps**2 + np.sqrt(sigma_eps**4 + 4*sigma_eta**2*sigma_eps**2)) / (2*sigma_eps**2)\nprint(f\"\\nSteady-state Kalman gain: {K_steady:.3f}\")\nprint(f\"Equivalent SES alpha: {K_steady:.3f}\")\n```", "line_start": 166, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What is the intuition behind the Kalman gain?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The Kalman gain $K_t$ balances trust in prediction vs. trust in observation.\n\n$$K_t = \\frac{P_{t|t-1}}{P_{t|t-1} + R} = \\frac{\\text{prediction uncertainty}}{\\text{prediction uncertainty} + \\text{observation noise}}$$\n\n**Interpretation:**\n- High $K_t$ (close to 1): Observation is trusted more; state updates significantly\n- Low $K_t$ (close to 0): Prediction is trusted more; observation has little effect\n\n**When is K high?**\n- High state uncertainty ($P$ large)\n- Low observation noise ($R$ small)\n\n**When is K low?**\n- Low state uncertainty ($P$ small)\n- High observation noise ($R$ large)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Thinking gain is fixed. It evolves as uncertainty changes. Early on, gain may be high (uncertain prior); later, it stabilizes as filter \"learns.\"\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> How does the local level model relate to simple exponential smoothing?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> They're equivalent! Local level model with specific noise ratio produces SES.\n\n**Local level:**\n$$\\mu_t = \\mu_{t-1} + \\eta_t, \\quad y_t = \\mu_t + \\epsilon_t$$\n\n**Kalman update:**\n$$\\hat{\\mu}_{t|t} = \\hat{\\mu}_{t|t-1} + K(y_t - \\hat{\\mu}_{t|t-1})$$\n\n**At steady state:** $K \\to K^* = \\alpha$ (SES smoothing parameter)\n\nThe relationship:\n$$\\alpha = \\frac{-\\sigma_\\epsilon^2 + \\sqrt{\\sigma_\\epsilon^4 + 4\\sigma_\\eta^2\\sigma_\\epsilon^2}}{2\\sigma_\\eta^2}$$\n\n**Key insight:** SES is the steady-state Kalman filter for local level model. Kalman provides:\n- Optimal initialization\n- Proper uncertainty quantification\n- Connection to likelihood\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using SES without recognizing it assumes specific state space structure. If dynamics differ, SES may be suboptimal.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Derive the steady-state Kalman gain for the local level model.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> At steady state, $P_{t|t} = P^*$ (constant).\n\n**Recursion:**\n$$P_{t|t-1} = P_{t-1|t-1} + Q = P^* + Q$$\n$$P_{t|t} = (1 - K_t)P_{t|t-1} = P^*$$\n\n**Steady state condition:**\n$$P^* = (1 - K^*)(P^* + Q)$$\n$$P^* = P^* + Q - K^*(P^* + Q)$$\n$$K^* = \\frac{Q}{P^* + Q}$$\n\nAlso: $K^* = \\frac{P^* + Q}{P^* + Q + R}$\n\nSolving for $P^*$:\n$$P^* = \\frac{-R + \\sqrt{R^2 + 4QR}}{2}$$\n\nAnd:\n$$K^* = \\frac{-R + \\sqrt{R^2 + 4QR}}{2Q}$$\n\nFor Q = $\\sigma_\\eta^2$, R = $\\sigma_\\epsilon^2$, this gives the SES α relationship.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Steady state may take many iterations to reach. For short series, time-varying gain matters.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> What is the difference between filtered and smoothed state estimates?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Filtered:** $\\hat{x}_{t|t} = E[x_t | y_1, \\ldots, y_t]$\n- Uses observations up to time t\n- Real-time estimate\n- From forward Kalman pass\n\n**Smoothed:** $\\hat{x}_{t|n} = E[x_t | y_1, \\ldots, y_n]$\n- Uses all observations\n- Retrospective estimate\n- Requires backward pass after forward\n\n**Key difference:** Smoother uses future observations to refine past state estimates.\n\n**When to use which:**\n- Real-time forecasting: Filter\n- Historical analysis: Smoother\n- Parameter estimation: Smoother (better likelihood)\n\n**Variance relationship:**\n$$\\text{Var}(\\hat{x}_{t|n}) \\leq \\text{Var}(\\hat{x}_{t|t})$$\n\nSmoother is never worse than filter.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using filter estimates for historical analysis when smoother is available and more accurate.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You have GPS measurements with known accuracy (R) but unknown vehicle dynamics (Q). How would you tune the Kalman filter?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Approach 1: Maximum Likelihood**\n- Treat Q as parameter\n- Compute likelihood from innovations: $\\log L = -\\frac{1}{2}\\sum(\\log S_t + v_t^2/S_t)$\n- Optimize Q to maximize likelihood\n\n**Approach 2: Innovation-based tuning**\n- Innovations $v_t$ should be white noise with variance $S_t$\n- If innovations autocorrelated: Q too small\n- If innovation variance >> $S_t$: Q too small\n- If innovation variance << $S_t$: Q too large\n\n**Approach 3: Adaptive filtering**\n- Estimate Q online from recent innovation statistics\n- $\\hat{Q}_t = $ sample variance of recent innovations minus R\n\n**Practical steps:**\n1. Start with Q = R (equal trust)\n2. Check innovation statistics\n3. Grid search or gradient optimization\n4. Validate on holdout data\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Setting Q too small makes filter overconfident and slow to track changes. Too large makes it noisy. Cross-validate!\n</div>\n</div>\n</details>", "line_start": 172, "level": 2}, {"heading": "References", "content": "1. Harvey, A. C. (1990). *Forecasting, Structural Time Series Models and the Kalman Filter*. Cambridge University Press.\n2. Durbin, J., & Koopman, S. J. (2012). *Time Series Analysis by State Space Methods*. Oxford University Press.\n3. Kalman, R. E. (1960). A new approach to linear filtering and prediction problems. *Journal of Basic Engineering*, 82(1), 35-45.\n4. Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press. Chapter 13.", "line_start": 326, "level": 1}]}, "docs/en/anomaly-detection/anomaly-detection.md": {"path": "docs/en/anomaly-detection/anomaly-detection.md", "title": "Anomaly Detection", "content": "# Anomaly Detection\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Anomaly detection identifies unusual observations—point anomalies (single outliers), collective anomalies (unusual sequences), or contextual anomalies (unusual given context). Methods: statistical (z-score, IQR), model-based (forecast residuals), distance-based (LOF), and ML (isolation forest, autoencoders). Key challenge: defining \"normal\" and choosing threshold.\n</div>\n\n## Core Definitions\n\n**Anomaly Types:**\n- **Point anomaly:** Single unusual value (e.g., spike)\n- **Collective anomaly:** Sequence that's unusual as a group\n- **Contextual anomaly:** Normal value in wrong context (e.g., high AC usage in winter)\n\n**Detection Settings:**\n- **Supervised:** Labeled normal/anomaly data\n- **Semi-supervised:** Only normal data for training\n- **Unsupervised:** No labels, detect statistical outliers\n\n## Math and Derivations\n\n### Statistical Methods\n\n**Z-score:**\n$$z_t = \\frac{y_t - \\bar{y}}{s}$$\n\nAnomaly if $|z_t| > 3$ (or chosen threshold)\n\n**Modified Z-score (robust):**\n$$M_t = \\frac{0.6745(y_t - \\text{median})}{\\text{MAD}}$$\n\nwhere MAD = median(|y - median(y)|)\n\n**IQR Method:**\n$$\\text{Anomaly if } y_t < Q_1 - 1.5\\times IQR \\text{ or } y_t > Q_3 + 1.5\\times IQR$$\n\n### Model-Based Detection\n\nFit time series model, flag large residuals:\n$$e_t = y_t - \\hat{y}_{t|t-1}$$\n\nAnomaly if $|e_t| > k \\times \\hat{\\sigma}$ (typically k = 3)\n\n**Advantages:**\n- Accounts for trend and seasonality\n- Adapts to changing patterns\n- More sensitive to contextual anomalies\n\n### Isolation Forest\n\nAnomalies are easier to isolate (require fewer splits).\n\n**Algorithm:**\n1. Build random trees by random splits\n2. Anomaly score = average path length to isolate point\n3. Short path → anomaly\n\n**Score:**\n$$s(x, n) = 2^{-E[h(x)]/c(n)}$$\n\nwhere h(x) = path length, c(n) = average path length in random tree.\n\n### Local Outlier Factor (LOF)\n\nCompares local density to neighbors' density:\n$$LOF(x) = \\frac{\\sum_{o \\in N_k(x)} \\frac{lrd(o)}{lrd(x)}}{|N_k(x)|}$$\n\nLOF >> 1 → anomaly (lower density than neighbors)\n\n## Algorithm/Model Sketch\n\n**Time Series Anomaly Detection Pipeline:**\n\n```python\ndef detect_anomalies(y, method='model', threshold=3):\n    if method == 'zscore':\n        z = (y - np.mean(y)) / np.std(y)\n        return np.abs(z) > threshold\n\n    elif method == 'model':\n        # Fit model and get residuals\n        model = fit_model(y)\n        residuals = y - model.fittedvalues\n        sigma = np.std(residuals)\n        return np.abs(residuals) > threshold * sigma\n\n    elif method == 'rolling':\n        # Rolling window approach\n        window = 30\n        rolling_mean = y.rolling(window).mean()\n        rolling_std = y.rolling(window).std()\n        z = (y - rolling_mean) / rolling_std\n        return np.abs(z) > threshold\n```\n\n**Threshold Selection:**\n- Fixed (z > 3): Simple but may not fit data\n- Percentile (top 1%): Adapts to distribution\n- Domain-specific: Based on cost of false positives/negatives\n- Extreme Value Theory: For tail events\n\n## Common Pitfalls\n\n1. **Masking:** One anomaly affects mean/std, hiding others. Use robust statistics or median-based methods.\n\n2. **Swamping:** Normal points flagged due to anomaly influence. Sequential cleaning or robust fitting helps.\n\n3. **Non-stationarity:** Using global statistics when local context matters. Use rolling windows or model residuals.\n\n4. **Wrong threshold:** Fixed threshold may be too sensitive or too conservative. Tune based on validation data.\n\n5. **Ignoring seasonality:** Saturday sales ≠ weekday anomaly. Model seasonal patterns first.\n\n6. **Collective anomalies:** Point methods miss unusual sequences. Use sequence-aware methods.\n\n## Mini Example\n\n```python\nimport numpy as np\nfrom scipy import stats\n\n# Generate data with anomalies\nnp.random.seed(42)\nn = 200\ny = np.sin(2 * np.pi * np.arange(n) / 50) + np.random.randn(n) * 0.3\n\n# Insert anomalies\nanomaly_idx = [50, 100, 150]\ny[anomaly_idx[0]] += 5   # Positive spike\ny[anomaly_idx[1]] -= 4   # Negative spike\ny[anomaly_idx[2]] += 3   # Smaller spike\n\n# Method 1: Simple z-score\nz_scores = np.abs(stats.zscore(y))\ndetected_zscore = np.where(z_scores > 3)[0]\nprint(f\"Z-score detected: {detected_zscore}\")\n\n# Method 2: Rolling z-score\nwindow = 20\nrolling_mean = np.convolve(y, np.ones(window)/window, mode='same')\nrolling_std = np.array([np.std(y[max(0,i-window):i+1]) for i in range(n)])\nrolling_z = np.abs((y - rolling_mean) / rolling_std)\ndetected_rolling = np.where(rolling_z > 3)[0]\nprint(f\"Rolling z-score detected: {detected_rolling[:10]}...\")  # May have more\n\n# Method 3: IQR\nQ1, Q3 = np.percentile(y, [25, 75])\nIQR = Q3 - Q1\nlower, upper = Q1 - 1.5*IQR, Q3 + 1.5*IQR\ndetected_iqr = np.where((y < lower) | (y > upper))[0]\nprint(f\"IQR detected: {detected_iqr}\")\n\n# Method 4: Model-based (simple AR)\nfrom statsmodels.tsa.ar_model import AutoReg\nmodel = AutoReg(y, lags=5).fit()\nresiduals = model.resid\nres_z = np.abs(stats.zscore(residuals))\ndetected_model = np.where(res_z > 3)[0] + 5  # Adjust for lag offset\nprint(f\"Model-based detected: {detected_model}\")\n\nprint(f\"\\nTrue anomalies: {anomaly_idx}\")\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What is the difference between a point anomaly and a contextual anomaly?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Point anomaly:** Value is unusual regardless of context.\n- Example: Temperature reading of 500°F\n- Detected by: Global statistics, simple thresholds\n\n**Contextual anomaly:** Value is normal in some contexts, unusual in current context.\n- Example: 80°F temperature is normal in summer, anomalous in winter\n- Detected by: Model residuals, conditional distributions\n\n**Why distinction matters:**\n- Point methods (z-score) miss contextual anomalies\n- Need context-aware methods for seasonal/temporal patterns\n- False positives if using wrong method\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using simple z-score on seasonal data. A December value might be normal for December but flagged as anomaly compared to annual mean.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Why is the median absolute deviation (MAD) preferred over standard deviation for anomaly detection?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> MAD is robust to outliers; standard deviation is not.\n\n**Problem with std:**\nSingle large outlier → inflates std → makes all z-scores smaller → outlier \"masks\" itself and others\n\n**MAD robustness:**\n$$\\text{MAD} = \\text{median}(|y_i - \\text{median}(y)|)$$\n\n- Median is not affected by extreme values\n- 50% of data must be outliers to significantly affect MAD\n- Breaking point: 50% vs ~0% for std\n\n**Scale factor:**\nFor normal data: $\\sigma \\approx 1.4826 \\times \\text{MAD}$\n\nUse modified z-score: $M = \\frac{0.6745(y - \\text{median})}{\\text{MAD}}$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using std-based z-scores when data may contain multiple outliers. Masking effect hides true anomalies.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Derive the modified z-score formula using MAD.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The modified z-score normalizes by MAD instead of std:\n\n$$M_i = \\frac{y_i - \\tilde{y}}{\\text{MAD}}$$\n\nwhere $\\tilde{y}$ = median(y).\n\n**For comparison with standard z-score:**\nFor normal distribution: $E[\\text{MAD}] = \\Phi^{-1}(0.75) \\times \\sigma \\approx 0.6745\\sigma$\n\nSo: $\\sigma \\approx \\frac{\\text{MAD}}{0.6745}$\n\n**Modified z-score (scaled):**\n$$M_i = \\frac{0.6745(y_i - \\tilde{y})}{\\text{MAD}}$$\n\nNow M has same scale as standard z-score under normality.\nThreshold M > 3.5 often used (equivalent to |z| > 3).\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using threshold 3 for modified z-score without scaling factor. The raw MAD-based score has different scale than standard z.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> How does Isolation Forest detect anomalies without computing distances?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Isolation Forest uses tree-based isolation:\n\n**Key insight:** Anomalies are few and different → easier to isolate (separate from rest).\n\n**Algorithm:**\n1. Randomly select feature and split value\n2. Recursively partition data\n3. Anomaly score = average path length to isolate point\n\n**Why anomalies have short paths:**\n- Normal points: surrounded by similar points, need many splits\n- Anomalies: isolated, few splits separate them\n\n**Score formula:**\n$$s(x,n) = 2^{-\\frac{E[h(x)]}{c(n)}}$$\n\n- $E[h(x)]$ = expected path length for x\n- $c(n)$ = average path length in binary search tree of n points\n- $s \\to 1$: anomaly; $s \\to 0.5$: normal; $s \\to 0$: very normal\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Isolation Forest assumes anomalies are isolated. Clustered anomalies (collective) may be missed.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You deploy an anomaly detection system and it triggers 50 alerts per day. After investigation, 45 are false positives. How do you improve?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> 10% precision is problematic. Approaches:\n\n1. **Raise threshold:**\n   - Increase z-score cutoff from 3 to 4\n   - Reduces false positives but may miss true anomalies\n\n2. **Add context:**\n   - Use time-of-day, day-of-week features\n   - Model seasonal patterns\n   - Contextual anomalies only within context\n\n3. **Ensemble methods:**\n   - Combine multiple detectors\n   - Flag only if majority agree\n\n4. **Learn from feedback:**\n   - Label false positives as normal\n   - Retrain semi-supervised model\n\n5. **Two-stage detection:**\n   - First stage: sensitive (catch all anomalies)\n   - Second stage: verify (filter false positives)\n\n6. **Domain rules:**\n   - Add business logic filters\n   - Known patterns that aren't anomalies\n\n**Metrics to track:** Precision, recall, F1-score at different thresholds.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Optimizing only for catching anomalies (recall). High false positive rate leads to alert fatigue and ignored warnings.\n</div>\n</div>\n</details>\n\n## References\n\n1. Chandola, V., Banerjee, A., & Kumar, V. (2009). Anomaly detection: A survey. *ACM Computing Surveys*, 41(3), 1-58.\n2. Liu, F. T., Ting, K. M., & Zhou, Z. H. (2008). Isolation forest. *ICDM*, 413-422.\n3. Breunig, M. M., Kriegel, H. P., Ng, R. T., & Sander, J. (2000). LOF: Identifying density-based local outliers. *SIGMOD*, 93-104.\n4. Hochenbaum, J., Vallis, O. S., & Kejariwal, A. (2017). Automatic anomaly detection in the cloud via statistical learning. *arXiv:1704.07706*.\n", "sections": [{"heading": "Anomaly Detection", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Anomaly detection identifies unusual observations—point anomalies (single outliers), collective anomalies (unusual sequences), or contextual anomalies (unusual given context). Methods: statistical (z-score, IQR), model-based (forecast residuals), distance-based (LOF), and ML (isolation forest, autoencoders). Key challenge: defining \"normal\" and choosing threshold.\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**Anomaly Types:**\n- **Point anomaly:** Single unusual value (e.g., spike)\n- **Collective anomaly:** Sequence that's unusual as a group\n- **Contextual anomaly:** Normal value in wrong context (e.g., high AC usage in winter)\n\n**Detection Settings:**\n- **Supervised:** Labeled normal/anomaly data\n- **Semi-supervised:** Only normal data for training\n- **Unsupervised:** No labels, detect statistical outliers", "line_start": 7, "level": 2}, {"heading": "Statistical Methods", "content": "**Z-score:**\n$$z_t = \\frac{y_t - \\bar{y}}{s}$$\n\nAnomaly if $|z_t| > 3$ (or chosen threshold)\n\n**Modified Z-score (robust):**\n$$M_t = \\frac{0.6745(y_t - \\text{median})}{\\text{MAD}}$$\n\nwhere MAD = median(|y - median(y)|)\n\n**IQR Method:**\n$$\\text{Anomaly if } y_t < Q_1 - 1.5\\times IQR \\text{ or } y_t > Q_3 + 1.5\\times IQR$$", "line_start": 21, "level": 3}, {"heading": "Model-Based Detection", "content": "Fit time series model, flag large residuals:\n$$e_t = y_t - \\hat{y}_{t|t-1}$$\n\nAnomaly if $|e_t| > k \\times \\hat{\\sigma}$ (typically k = 3)\n\n**Advantages:**\n- Accounts for trend and seasonality\n- Adapts to changing patterns\n- More sensitive to contextual anomalies", "line_start": 36, "level": 3}, {"heading": "Isolation Forest", "content": "Anomalies are easier to isolate (require fewer splits).\n\n**Algorithm:**\n1. Build random trees by random splits\n2. Anomaly score = average path length to isolate point\n3. Short path → anomaly\n\n**Score:**\n$$s(x, n) = 2^{-E[h(x)]/c(n)}$$\n\nwhere h(x) = path length, c(n) = average path length in random tree.", "line_start": 48, "level": 3}, {"heading": "Local Outlier Factor (LOF)", "content": "Compares local density to neighbors' density:\n$$LOF(x) = \\frac{\\sum_{o \\in N_k(x)} \\frac{lrd(o)}{lrd(x)}}{|N_k(x)|}$$\n\nLOF >> 1 → anomaly (lower density than neighbors)", "line_start": 62, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**Time Series Anomaly Detection Pipeline:**\n\n```python\ndef detect_anomalies(y, method='model', threshold=3):\n    if method == 'zscore':\n        z = (y - np.mean(y)) / np.std(y)\n        return np.abs(z) > threshold\n\n    elif method == 'model':\n        # Fit model and get residuals\n        model = fit_model(y)\n        residuals = y - model.fittedvalues\n        sigma = np.std(residuals)\n        return np.abs(residuals) > threshold * sigma\n\n    elif method == 'rolling':\n        # Rolling window approach\n        window = 30\n        rolling_mean = y.rolling(window).mean()\n        rolling_std = y.rolling(window).std()\n        z = (y - rolling_mean) / rolling_std\n        return np.abs(z) > threshold\n```\n\n**Threshold Selection:**\n- Fixed (z > 3): Simple but may not fit data\n- Percentile (top 1%): Adapts to distribution\n- Domain-specific: Based on cost of false positives/negatives\n- Extreme Value Theory: For tail events", "line_start": 69, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Masking:** One anomaly affects mean/std, hiding others. Use robust statistics or median-based methods.\n\n2. **Swamping:** Normal points flagged due to anomaly influence. Sequential cleaning or robust fitting helps.\n\n3. **Non-stationarity:** Using global statistics when local context matters. Use rolling windows or model residuals.\n\n4. **Wrong threshold:** Fixed threshold may be too sensitive or too conservative. Tune based on validation data.\n\n5. **Ignoring seasonality:** Saturday sales ≠ weekday anomaly. Model seasonal patterns first.\n\n6. **Collective anomalies:** Point methods miss unusual sequences. Use sequence-aware methods.", "line_start": 101, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom scipy import stats", "line_start": 115, "level": 1}, {"heading": "Generate data with anomalies", "content": "np.random.seed(42)\nn = 200\ny = np.sin(2 * np.pi * np.arange(n) / 50) + np.random.randn(n) * 0.3", "line_start": 121, "level": 1}, {"heading": "Insert anomalies", "content": "anomaly_idx = [50, 100, 150]\ny[anomaly_idx[0]] += 5   # Positive spike\ny[anomaly_idx[1]] -= 4   # Negative spike\ny[anomaly_idx[2]] += 3   # Smaller spike", "line_start": 126, "level": 1}, {"heading": "Method 1: Simple z-score", "content": "z_scores = np.abs(stats.zscore(y))\ndetected_zscore = np.where(z_scores > 3)[0]\nprint(f\"Z-score detected: {detected_zscore}\")", "line_start": 132, "level": 1}, {"heading": "Method 2: Rolling z-score", "content": "window = 20\nrolling_mean = np.convolve(y, np.ones(window)/window, mode='same')\nrolling_std = np.array([np.std(y[max(0,i-window):i+1]) for i in range(n)])\nrolling_z = np.abs((y - rolling_mean) / rolling_std)\ndetected_rolling = np.where(rolling_z > 3)[0]\nprint(f\"Rolling z-score detected: {detected_rolling[:10]}...\")  # May have more", "line_start": 137, "level": 1}, {"heading": "Method 3: IQR", "content": "Q1, Q3 = np.percentile(y, [25, 75])\nIQR = Q3 - Q1\nlower, upper = Q1 - 1.5*IQR, Q3 + 1.5*IQR\ndetected_iqr = np.where((y < lower) | (y > upper))[0]\nprint(f\"IQR detected: {detected_iqr}\")", "line_start": 145, "level": 1}, {"heading": "Method 4: Model-based (simple AR)", "content": "from statsmodels.tsa.ar_model import AutoReg\nmodel = AutoReg(y, lags=5).fit()\nresiduals = model.resid\nres_z = np.abs(stats.zscore(residuals))\ndetected_model = np.where(res_z > 3)[0] + 5  # Adjust for lag offset\nprint(f\"Model-based detected: {detected_model}\")\n\nprint(f\"\\nTrue anomalies: {anomaly_idx}\")\n```", "line_start": 152, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What is the difference between a point anomaly and a contextual anomaly?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Point anomaly:** Value is unusual regardless of context.\n- Example: Temperature reading of 500°F\n- Detected by: Global statistics, simple thresholds\n\n**Contextual anomaly:** Value is normal in some contexts, unusual in current context.\n- Example: 80°F temperature is normal in summer, anomalous in winter\n- Detected by: Model residuals, conditional distributions\n\n**Why distinction matters:**\n- Point methods (z-score) miss contextual anomalies\n- Need context-aware methods for seasonal/temporal patterns\n- False positives if using wrong method\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using simple z-score on seasonal data. A December value might be normal for December but flagged as anomaly compared to annual mean.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Why is the median absolute deviation (MAD) preferred over standard deviation for anomaly detection?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> MAD is robust to outliers; standard deviation is not.\n\n**Problem with std:**\nSingle large outlier → inflates std → makes all z-scores smaller → outlier \"masks\" itself and others\n\n**MAD robustness:**\n$$\\text{MAD} = \\text{median}(|y_i - \\text{median}(y)|)$$\n\n- Median is not affected by extreme values\n- 50% of data must be outliers to significantly affect MAD\n- Breaking point: 50% vs ~0% for std\n\n**Scale factor:**\nFor normal data: $\\sigma \\approx 1.4826 \\times \\text{MAD}$\n\nUse modified z-score: $M = \\frac{0.6745(y - \\text{median})}{\\text{MAD}}$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using std-based z-scores when data may contain multiple outliers. Masking effect hides true anomalies.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Derive the modified z-score formula using MAD.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The modified z-score normalizes by MAD instead of std:\n\n$$M_i = \\frac{y_i - \\tilde{y}}{\\text{MAD}}$$\n\nwhere $\\tilde{y}$ = median(y).\n\n**For comparison with standard z-score:**\nFor normal distribution: $E[\\text{MAD}] = \\Phi^{-1}(0.75) \\times \\sigma \\approx 0.6745\\sigma$\n\nSo: $\\sigma \\approx \\frac{\\text{MAD}}{0.6745}$\n\n**Modified z-score (scaled):**\n$$M_i = \\frac{0.6745(y_i - \\tilde{y})}{\\text{MAD}}$$\n\nNow M has same scale as standard z-score under normality.\nThreshold M > 3.5 often used (equivalent to |z| > 3).\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using threshold 3 for modified z-score without scaling factor. The raw MAD-based score has different scale than standard z.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> How does Isolation Forest detect anomalies without computing distances?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Isolation Forest uses tree-based isolation:\n\n**Key insight:** Anomalies are few and different → easier to isolate (separate from rest).\n\n**Algorithm:**\n1. Randomly select feature and split value\n2. Recursively partition data\n3. Anomaly score = average path length to isolate point\n\n**Why anomalies have short paths:**\n- Normal points: surrounded by similar points, need many splits\n- Anomalies: isolated, few splits separate them\n\n**Score formula:**\n$$s(x,n) = 2^{-\\frac{E[h(x)]}{c(n)}}$$\n\n- $E[h(x)]$ = expected path length for x\n- $c(n)$ = average path length in binary search tree of n points\n- $s \\to 1$: anomaly; $s \\to 0.5$: normal; $s \\to 0$: very normal\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Isolation Forest assumes anomalies are isolated. Clustered anomalies (collective) may be missed.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You deploy an anomaly detection system and it triggers 50 alerts per day. After investigation, 45 are false positives. How do you improve?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> 10% precision is problematic. Approaches:\n\n1. **Raise threshold:**\n   - Increase z-score cutoff from 3 to 4\n   - Reduces false positives but may miss true anomalies\n\n2. **Add context:**\n   - Use time-of-day, day-of-week features\n   - Model seasonal patterns\n   - Contextual anomalies only within context\n\n3. **Ensemble methods:**\n   - Combine multiple detectors\n   - Flag only if majority agree\n\n4. **Learn from feedback:**\n   - Label false positives as normal\n   - Retrain semi-supervised model\n\n5. **Two-stage detection:**\n   - First stage: sensitive (catch all anomalies)\n   - Second stage: verify (filter false positives)\n\n6. **Domain rules:**\n   - Add business logic filters\n   - Known patterns that aren't anomalies\n\n**Metrics to track:** Precision, recall, F1-score at different thresholds.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Optimizing only for catching anomalies (recall). High false positive rate leads to alert fatigue and ignored warnings.\n</div>\n</div>\n</details>", "line_start": 163, "level": 2}, {"heading": "References", "content": "1. Chandola, V., Banerjee, A., & Kumar, V. (2009). Anomaly detection: A survey. *ACM Computing Surveys*, 41(3), 1-58.\n2. Liu, F. T., Ting, K. M., & Zhou, Z. H. (2008). Isolation forest. *ICDM*, 413-422.\n3. Breunig, M. M., Kriegel, H. P., Ng, R. T., & Sander, J. (2000). LOF: Identifying density-based local outliers. *SIGMOD*, 93-104.\n4. Hochenbaum, J., Vallis, O. S., & Kejariwal, A. (2017). Automatic anomaly detection in the cloud via statistical learning. *arXiv:1704.07706*.", "line_start": 313, "level": 1}]}, "docs/en/decomposition/classical.md": {"path": "docs/en/decomposition/classical.md", "title": "Classical Decomposition", "content": "# Classical Decomposition\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Classical decomposition separates a series into trend, seasonal, and irregular components using moving averages. Additive: $y = T + S + I$. Multiplicative: $y = T \\times S \\times I$. Simple but assumes constant seasonal pattern and is sensitive to outliers. Trend extracted via centered moving average; seasonal via period averages.\n</div>\n\n## Core Definitions\n\n**Additive Model:**\n$$y_t = T_t + S_t + I_t$$\n\nUsed when seasonal variation is constant regardless of level.\n\n**Multiplicative Model:**\n$$y_t = T_t \\times S_t \\times I_t$$\n\nUsed when seasonal variation scales with level.\n\n**Components:**\n- $T_t$: Trend-cycle (smooth underlying level)\n- $S_t$: Seasonal (periodic pattern repeating every $m$ periods)\n- $I_t$: Irregular/residual (random noise)\n\n**Seasonal Indices:**\n- Additive: $S_t$ values sum to 0 over one period\n- Multiplicative: $S_t$ values average to 1 over one period\n\n## Math and Derivations\n\n### Trend Extraction via Moving Average\n\n**Centered Moving Average (CMA):**\n\nFor odd $m$ (e.g., $m=7$):\n$$T_t = \\frac{1}{m}\\sum_{j=-(m-1)/2}^{(m-1)/2} y_{t+j}$$\n\nFor even $m$ (e.g., $m=12$):\n$$T_t = \\frac{1}{2m}\\left(y_{t-m/2} + 2\\sum_{j=-(m/2-1)}^{m/2-1} y_{t+j} + y_{t+m/2}\\right)$$\n\nThis is a $2 \\times m$-MA: first $m$-MA, then 2-MA to center.\n\n### Seasonal Index Calculation\n\n**Additive:**\n1. Detrend: $y_t - T_t$\n2. Average detrended values for each season: $\\bar{S}_s = \\frac{1}{k}\\sum_{j} (y_{s+jm} - T_{s+jm})$\n3. Normalize: $S_s = \\bar{S}_s - \\frac{1}{m}\\sum_{s=1}^{m}\\bar{S}_s$\n\n**Multiplicative:**\n1. Detrend: $y_t / T_t$\n2. Average ratios for each season: $\\bar{S}_s = \\frac{1}{k}\\sum_{j} \\frac{y_{s+jm}}{T_{s+jm}}$\n3. Normalize: $S_s = \\bar{S}_s \\times \\frac{m}{\\sum_{s=1}^{m}\\bar{S}_s}$\n\n### Properties of Moving Average\n\nThe $m$-point moving average:\n- Removes seasonality of period $m$ (averages over full cycle)\n- Smooths high-frequency noise\n- Introduces lag of $(m-1)/2$ periods\n- Loses $(m-1)/2$ observations at each end\n\n**Frequency response:** MA is a low-pass filter that attenuates frequencies $\\geq 1/m$.\n\n## Algorithm/Model Sketch\n\n**Classical Decomposition Algorithm:**\n\n```\nInput: y[1:n], seasonal period m, type (additive/multiplicative)\nOutput: Trend T, Seasonal S, Irregular I\n\n1. TREND EXTRACTION\n   - Compute centered moving average of y\n   - T[t] = CMA(y, m) for t = m/2+1 to n-m/2\n   - End points: use extrapolation or leave missing\n\n2. DETREND\n   - Additive: D[t] = y[t] - T[t]\n   - Multiplicative: D[t] = y[t] / T[t]\n\n3. SEASONAL INDICES\n   - Group D[t] by season (1 to m)\n   - Average each group: S_raw[s] = mean(D[s], D[s+m], D[s+2m],...)\n   - Normalize:\n     - Additive: S[s] = S_raw[s] - mean(S_raw)\n     - Multiplicative: S[s] = S_raw[s] × m / sum(S_raw)\n\n4. SEASONAL COMPONENT\n   - S[t] = S[t mod m] (replicate indices across series)\n\n5. IRREGULAR\n   - Additive: I[t] = y[t] - T[t] - S[t]\n   - Multiplicative: I[t] = y[t] / (T[t] × S[t])\n\nReturn T, S, I\n```\n\n## Common Pitfalls\n\n1. **Fixed seasonal pattern**: Classical decomposition assumes the same seasonal pattern throughout. Doesn't adapt to evolving seasonality.\n\n2. **Outlier sensitivity**: One outlier affects trend (via MA) and seasonal indices. No robust fitting.\n\n3. **End-point loss**: Lose $m/2$ observations at each end. Problematic for short series.\n\n4. **Wrong model type**: Using additive when data is multiplicative (or vice versa) gives poor decomposition.\n\n5. **Calendar effects**: Doesn't handle trading days, Easter, etc. These appear in irregular component.\n\n6. **Non-integer period**: Requires integer $m$. For 365.25 days/year, need alternative methods.\n\n## Mini Example\n\n```python\nimport numpy as np\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# Generate multiplicative seasonal data\nnp.random.seed(42)\nn = 48  # 4 years monthly\nt = np.arange(n)\ntrend = 100 + 2 * t\nseasonal_mult = 1 + 0.3 * np.sin(2 * np.pi * t / 12)\ny = trend * seasonal_mult * (1 + 0.05 * np.random.randn(n))\n\n# Classical decomposition (multiplicative)\nresult = seasonal_decompose(y, model='multiplicative', period=12)\n\nprint(\"Seasonal indices (should repeat):\")\nprint(np.round(result.seasonal[:12], 3))\n\nprint(\"\\nTrend (first and last available):\")\nprint(f\"  First: {result.trend[~np.isnan(result.trend)][0]:.1f}\")\nprint(f\"  Last: {result.trend[~np.isnan(result.trend)][-1]:.1f}\")\n\n# Compare additive (wrong model for this data)\nresult_add = seasonal_decompose(y, model='additive', period=12)\nprint(\"\\nCompare residual std:\")\nprint(f\"  Multiplicative: {np.nanstd(result.resid):.3f}\")\nprint(f\"  Additive: {np.nanstd(result_add.resid):.3f}\")\n# Multiplicative should have smaller residual std\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> When should you use additive vs. multiplicative decomposition?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Additive** when:\n- Seasonal fluctuations are roughly constant in absolute terms\n- Low and high values have similar seasonal swings\n- Example: Temperature (±10°F regardless of base temp)\n\n**Multiplicative** when:\n- Seasonal fluctuations are proportional to level\n- Percentage variation is constant\n- Example: Retail sales (December is 20% above average regardless of total sales)\n\n**Decision test:**\n1. Plot series — do seasonal swings grow with level?\n2. Compute: std(seasonal) / mean(level) for different periods\n   - If ratio is constant → multiplicative\n   - If std(seasonal) is constant → additive\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Defaulting to additive. Many economic/business series are multiplicative because growth compounds.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Why does the moving average remove seasonality of period m?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> An $m$-point moving average includes exactly one complete seasonal cycle. Since seasonal components sum to zero (additive) or average to one (multiplicative) over a cycle, they cancel out.\n\n**Mathematical explanation (additive):**\n$$\\text{MA}_t = \\frac{1}{m}\\sum_{j=0}^{m-1}(T_{t+j} + S_{t+j} + I_{t+j})$$\n\nIf trend is locally constant and $\\sum_{j=0}^{m-1}S_{t+j} = 0$:\n$$\\text{MA}_t \\approx T_t + \\frac{1}{m}\\sum_{j=0}^{m-1}I_{t+j}$$\n\nThe seasonal cancels; only trend and smoothed noise remain.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using wrong MA order. If true period is 12 but you use MA(6), seasonality won't be fully removed.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> For a 12-point centered moving average of monthly data, how many observations are lost at each end?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> 6 observations at each end (12 total).\n\n**Derivation:**\nFor even $m=12$, the 2×12-MA formula at time $t$ uses:\n$$\\frac{1}{24}(y_{t-6} + 2y_{t-5} + \\cdots + 2y_{t+5} + y_{t+6})$$\n\nThis requires observations from $t-6$ to $t+6$.\n\nAt the start: Can only compute for $t \\geq 7$ (need $y_1, \\ldots, y_{13}$)\nAt the end: Can only compute for $t \\leq n-6$ (need data through $y_{n}$)\n\nSo positions 1-6 and (n-5)-n are missing → 12 total missing values.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> For short series (say, 2 years = 24 points), losing 12 points means half the data has no trend estimate. Consider STL or parametric trend instead.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Why must seasonal indices be normalized, and what constraint do they satisfy?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Why normalize:**\nWithout normalization, average of raw seasonal indices might not be zero (additive) or one (multiplicative), causing systematic bias in trend or irregular.\n\n**Constraints:**\n\nAdditive: $\\sum_{s=1}^{m} S_s = 0$\n- Ensures seasonality doesn't shift the overall level\n- Positive seasons offset by negative seasons\n\nMultiplicative: $\\sum_{s=1}^{m} S_s = m$ (equivalently, average = 1)\n- Ensures seasonal factors don't inflate/deflate overall level\n- Factors > 1 offset by factors < 1\n\n**Normalization formulas:**\n- Additive: $S_s^{new} = S_s^{raw} - \\bar{S}^{raw}$\n- Multiplicative: $S_s^{new} = S_s^{raw} \\times m / \\sum S^{raw}$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Forgetting normalization leads to decomposition where $T + S + I \\neq y$ due to bias in seasonal.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You decompose monthly sales data and find the irregular component has strong autocorrelation at lag 1. What does this indicate and how would you address it?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Lag-1 autocorrelation in irregular indicates:\n1. The model hasn't captured all systematic patterns\n2. Short-term dynamics exist beyond trend and seasonality\n3. Possibly: month-to-month momentum not in seasonal pattern\n\n**How to address:**\n\n1. **Model the irregular:** Fit ARIMA to irregular component\n   - If AR(1), incorporate into forecasting\n   - STL+ARIMA or similar pipeline\n\n2. **Use better decomposition:** STL can adapt to changing patterns that classical misses\n\n3. **Consider SARIMA directly:** Handles trend, seasonality, AND autocorrelation in one model\n\n4. **Check for calendar effects:** Trading days, holidays may create autocorrelation\n\n5. **Use ETS:** State space models can capture autocorrelated errors\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Ignoring irregular autocorrelation in forecasting. This underestimates forecast uncertainty and may bias predictions.\n</div>\n</div>\n</details>\n\n## References\n\n1. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 3.\n2. Makridakis, S., Wheelwright, S. C., & Hyndman, R. J. (1998). *Forecasting: Methods and Applications*. Wiley. Chapter 4.\n3. Brockwell, P. J., & Davis, R. A. (2016). *Introduction to Time Series and Forecasting*. Springer. Chapter 1.\n4. Census Bureau. (2017). X-13ARIMA-SEATS Reference Manual. U.S. Census Bureau.\n", "sections": [{"heading": "Classical Decomposition", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Classical decomposition separates a series into trend, seasonal, and irregular components using moving averages. Additive: $y = T + S + I$. Multiplicative: $y = T \\times S \\times I$. Simple but assumes constant seasonal pattern and is sensitive to outliers. Trend extracted via centered moving average; seasonal via period averages.\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**Additive Model:**\n$$y_t = T_t + S_t + I_t$$\n\nUsed when seasonal variation is constant regardless of level.\n\n**Multiplicative Model:**\n$$y_t = T_t \\times S_t \\times I_t$$\n\nUsed when seasonal variation scales with level.\n\n**Components:**\n- $T_t$: Trend-cycle (smooth underlying level)\n- $S_t$: Seasonal (periodic pattern repeating every $m$ periods)\n- $I_t$: Irregular/residual (random noise)\n\n**Seasonal Indices:**\n- Additive: $S_t$ values sum to 0 over one period\n- Multiplicative: $S_t$ values average to 1 over one period", "line_start": 7, "level": 2}, {"heading": "Trend Extraction via Moving Average", "content": "**Centered Moving Average (CMA):**\n\nFor odd $m$ (e.g., $m=7$):\n$$T_t = \\frac{1}{m}\\sum_{j=-(m-1)/2}^{(m-1)/2} y_{t+j}$$\n\nFor even $m$ (e.g., $m=12$):\n$$T_t = \\frac{1}{2m}\\left(y_{t-m/2} + 2\\sum_{j=-(m/2-1)}^{m/2-1} y_{t+j} + y_{t+m/2}\\right)$$\n\nThis is a $2 \\times m$-MA: first $m$-MA, then 2-MA to center.", "line_start": 30, "level": 3}, {"heading": "Seasonal Index Calculation", "content": "**Additive:**\n1. Detrend: $y_t - T_t$\n2. Average detrended values for each season: $\\bar{S}_s = \\frac{1}{k}\\sum_{j} (y_{s+jm} - T_{s+jm})$\n3. Normalize: $S_s = \\bar{S}_s - \\frac{1}{m}\\sum_{s=1}^{m}\\bar{S}_s$\n\n**Multiplicative:**\n1. Detrend: $y_t / T_t$\n2. Average ratios for each season: $\\bar{S}_s = \\frac{1}{k}\\sum_{j} \\frac{y_{s+jm}}{T_{s+jm}}$\n3. Normalize: $S_s = \\bar{S}_s \\times \\frac{m}{\\sum_{s=1}^{m}\\bar{S}_s}$", "line_start": 42, "level": 3}, {"heading": "Properties of Moving Average", "content": "The $m$-point moving average:\n- Removes seasonality of period $m$ (averages over full cycle)\n- Smooths high-frequency noise\n- Introduces lag of $(m-1)/2$ periods\n- Loses $(m-1)/2$ observations at each end\n\n**Frequency response:** MA is a low-pass filter that attenuates frequencies $\\geq 1/m$.", "line_start": 54, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**Classical Decomposition Algorithm:**\n\n```\nInput: y[1:n], seasonal period m, type (additive/multiplicative)\nOutput: Trend T, Seasonal S, Irregular I\n\n1. TREND EXTRACTION\n   - Compute centered moving average of y\n   - T[t] = CMA(y, m) for t = m/2+1 to n-m/2\n   - End points: use extrapolation or leave missing\n\n2. DETREND\n   - Additive: D[t] = y[t] - T[t]\n   - Multiplicative: D[t] = y[t] / T[t]\n\n3. SEASONAL INDICES\n   - Group D[t] by season (1 to m)\n   - Average each group: S_raw[s] = mean(D[s], D[s+m], D[s+2m],...)\n   - Normalize:\n     - Additive: S[s] = S_raw[s] - mean(S_raw)\n     - Multiplicative: S[s] = S_raw[s] × m / sum(S_raw)\n\n4. SEASONAL COMPONENT\n   - S[t] = S[t mod m] (replicate indices across series)\n\n5. IRREGULAR\n   - Additive: I[t] = y[t] - T[t] - S[t]\n   - Multiplicative: I[t] = y[t] / (T[t] × S[t])\n\nReturn T, S, I\n```", "line_start": 64, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Fixed seasonal pattern**: Classical decomposition assumes the same seasonal pattern throughout. Doesn't adapt to evolving seasonality.\n\n2. **Outlier sensitivity**: One outlier affects trend (via MA) and seasonal indices. No robust fitting.\n\n3. **End-point loss**: Lose $m/2$ observations at each end. Problematic for short series.\n\n4. **Wrong model type**: Using additive when data is multiplicative (or vice versa) gives poor decomposition.\n\n5. **Calendar effects**: Doesn't handle trading days, Easter, etc. These appear in irregular component.\n\n6. **Non-integer period**: Requires integer $m$. For 365.25 days/year, need alternative methods.", "line_start": 98, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.seasonal import seasonal_decompose", "line_start": 112, "level": 1}, {"heading": "Generate multiplicative seasonal data", "content": "np.random.seed(42)\nn = 48  # 4 years monthly\nt = np.arange(n)\ntrend = 100 + 2 * t\nseasonal_mult = 1 + 0.3 * np.sin(2 * np.pi * t / 12)\ny = trend * seasonal_mult * (1 + 0.05 * np.random.randn(n))", "line_start": 118, "level": 1}, {"heading": "Classical decomposition (multiplicative)", "content": "result = seasonal_decompose(y, model='multiplicative', period=12)\n\nprint(\"Seasonal indices (should repeat):\")\nprint(np.round(result.seasonal[:12], 3))\n\nprint(\"\\nTrend (first and last available):\")\nprint(f\"  First: {result.trend[~np.isnan(result.trend)][0]:.1f}\")\nprint(f\"  Last: {result.trend[~np.isnan(result.trend)][-1]:.1f}\")", "line_start": 126, "level": 1}, {"heading": "Compare additive (wrong model for this data)", "content": "result_add = seasonal_decompose(y, model='additive', period=12)\nprint(\"\\nCompare residual std:\")\nprint(f\"  Multiplicative: {np.nanstd(result.resid):.3f}\")\nprint(f\"  Additive: {np.nanstd(result_add.resid):.3f}\")", "line_start": 136, "level": 1}, {"heading": "Multiplicative should have smaller residual std", "content": "```", "line_start": 141, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> When should you use additive vs. multiplicative decomposition?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Additive** when:\n- Seasonal fluctuations are roughly constant in absolute terms\n- Low and high values have similar seasonal swings\n- Example: Temperature (±10°F regardless of base temp)\n\n**Multiplicative** when:\n- Seasonal fluctuations are proportional to level\n- Percentage variation is constant\n- Example: Retail sales (December is 20% above average regardless of total sales)\n\n**Decision test:**\n1. Plot series — do seasonal swings grow with level?\n2. Compute: std(seasonal) / mean(level) for different periods\n   - If ratio is constant → multiplicative\n   - If std(seasonal) is constant → additive\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Defaulting to additive. Many economic/business series are multiplicative because growth compounds.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Why does the moving average remove seasonality of period m?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> An $m$-point moving average includes exactly one complete seasonal cycle. Since seasonal components sum to zero (additive) or average to one (multiplicative) over a cycle, they cancel out.\n\n**Mathematical explanation (additive):**\n$$\\text{MA}_t = \\frac{1}{m}\\sum_{j=0}^{m-1}(T_{t+j} + S_{t+j} + I_{t+j})$$\n\nIf trend is locally constant and $\\sum_{j=0}^{m-1}S_{t+j} = 0$:\n$$\\text{MA}_t \\approx T_t + \\frac{1}{m}\\sum_{j=0}^{m-1}I_{t+j}$$\n\nThe seasonal cancels; only trend and smoothed noise remain.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using wrong MA order. If true period is 12 but you use MA(6), seasonality won't be fully removed.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> For a 12-point centered moving average of monthly data, how many observations are lost at each end?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> 6 observations at each end (12 total).\n\n**Derivation:**\nFor even $m=12$, the 2×12-MA formula at time $t$ uses:\n$$\\frac{1}{24}(y_{t-6} + 2y_{t-5} + \\cdots + 2y_{t+5} + y_{t+6})$$\n\nThis requires observations from $t-6$ to $t+6$.\n\nAt the start: Can only compute for $t \\geq 7$ (need $y_1, \\ldots, y_{13}$)\nAt the end: Can only compute for $t \\leq n-6$ (need data through $y_{n}$)\n\nSo positions 1-6 and (n-5)-n are missing → 12 total missing values.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> For short series (say, 2 years = 24 points), losing 12 points means half the data has no trend estimate. Consider STL or parametric trend instead.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Why must seasonal indices be normalized, and what constraint do they satisfy?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Why normalize:**\nWithout normalization, average of raw seasonal indices might not be zero (additive) or one (multiplicative), causing systematic bias in trend or irregular.\n\n**Constraints:**\n\nAdditive: $\\sum_{s=1}^{m} S_s = 0$\n- Ensures seasonality doesn't shift the overall level\n- Positive seasons offset by negative seasons\n\nMultiplicative: $\\sum_{s=1}^{m} S_s = m$ (equivalently, average = 1)\n- Ensures seasonal factors don't inflate/deflate overall level\n- Factors > 1 offset by factors < 1\n\n**Normalization formulas:**\n- Additive: $S_s^{new} = S_s^{raw} - \\bar{S}^{raw}$\n- Multiplicative: $S_s^{new} = S_s^{raw} \\times m / \\sum S^{raw}$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Forgetting normalization leads to decomposition where $T + S + I \\neq y$ due to bias in seasonal.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You decompose monthly sales data and find the irregular component has strong autocorrelation at lag 1. What does this indicate and how would you address it?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Lag-1 autocorrelation in irregular indicates:\n1. The model hasn't captured all systematic patterns\n2. Short-term dynamics exist beyond trend and seasonality\n3. Possibly: month-to-month momentum not in seasonal pattern\n\n**How to address:**\n\n1. **Model the irregular:** Fit ARIMA to irregular component\n   - If AR(1), incorporate into forecasting\n   - STL+ARIMA or similar pipeline\n\n2. **Use better decomposition:** STL can adapt to changing patterns that classical misses\n\n3. **Consider SARIMA directly:** Handles trend, seasonality, AND autocorrelation in one model\n\n4. **Check for calendar effects:** Trading days, holidays may create autocorrelation\n\n5. **Use ETS:** State space models can capture autocorrelated errors\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Ignoring irregular autocorrelation in forecasting. This underestimates forecast uncertainty and may bias predictions.\n</div>\n</div>\n</details>", "line_start": 144, "level": 2}, {"heading": "References", "content": "1. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 3.\n2. Makridakis, S., Wheelwright, S. C., & Hyndman, R. J. (1998). *Forecasting: Methods and Applications*. Wiley. Chapter 4.\n3. Brockwell, P. J., & Davis, R. A. (2016). *Introduction to Time Series and Forecasting*. Springer. Chapter 1.\n4. Census Bureau. (2017). X-13ARIMA-SEATS Reference Manual. U.S. Census Bureau.", "line_start": 275, "level": 1}]}, "docs/en/decomposition/stl.md": {"path": "docs/en/decomposition/stl.md", "title": "STL Decomposition", "content": "# STL Decomposition\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> STL (Seasonal and Trend decomposition using Loess) robustly separates a series into trend, seasonal, and remainder components using local regression. Unlike classical decomposition, STL handles any seasonal period and is robust to outliers. Key parameters: seasonal window (odd integer ≥ 7) and trend window. Remainder should be stationary for forecasting.\n</div>\n\n## Core Definitions\n\n**Additive Decomposition:**\n$$y_t = T_t + S_t + R_t$$\n\n- $T_t$: Trend component (smooth, long-term movement)\n- $S_t$: Seasonal component (periodic pattern, sums to ~0 over each cycle)\n- $R_t$: Remainder/residual (everything else)\n\n**Loess (Locally Estimated Scatterplot Smoothing):**\nLocal polynomial regression using weighted least squares. Weights decrease with distance from target point.\n\n**Key STL Parameters:**\n- `seasonal`: Seasonal period (e.g., 12 for monthly)\n- `seasonal_deg`: Degree of seasonal polynomial (0 or 1)\n- `trend`: Trend smoothing window (odd integer, default depends on seasonal)\n- `robust`: Whether to use robust fitting (downweight outliers)\n\n## Math and Derivations\n\n### Loess Smoother\n\nFor point $x_0$, fit weighted polynomial:\n$$\\min_{\\beta} \\sum_{i=1}^{n} w_i(x_0)(y_i - \\beta_0 - \\beta_1(x_i - x_0))^2$$\n\nWeights use tricube function:\n$$w(u) = \\begin{cases} (1-|u|^3)^3 & |u| < 1 \\\\ 0 & |u| \\geq 1 \\end{cases}$$\n\nDistance scaled by bandwidth $h$: $u_i = |x_i - x_0|/h$\n\n### STL Algorithm (Simplified)\n\n**Outer loop** (for robustness):\n1. Initialize: $R_t^{(0)} = 0$, $T_t^{(0)} = $ loess smooth of $y_t$\n\n**Inner loop**:\n2. **Detrend**: $y_t - T_t^{(k-1)}$\n3. **Cycle-subseries smoothing**: For each season $s=1,\\ldots,m$, smooth values at positions $s, s+m, s+2m, \\ldots$ using loess\n4. **Low-pass filter**: Remove low-frequency from seasonal\n5. **Deseasonalize**: $y_t - S_t^{(k)}$\n6. **Trend extraction**: Loess smooth of deseasonalized series\n\nRepeat inner loop until convergence; outer loop updates robustness weights.\n\n### Robustness Weights\n\nAfter each outer iteration, compute residuals and assign weights:\n$$\\rho_t = |R_t|$$\n$$h = 6 \\cdot \\text{median}(|\\rho_t|)$$\n$$w_t = B(\\rho_t/h)$$\n\nwhere $B$ is the bisquare function: $B(u) = (1-u^2)^2$ for $|u| < 1$, else 0.\n\nOutliers get downweighted in subsequent iterations.\n\n## Algorithm/Model Sketch\n\n**STL Decomposition Steps:**\n\n```\nInput: y[1:n], seasonal period m, parameters\nOutput: Trend T, Seasonal S, Remainder R\n\n1. Initialize trend T = loess(y) or moving average\n2. For k = 1 to n_outer:\n\n   For j = 1 to n_inner:\n      a. Detrend: D = y - T\n      b. For each position i in 1...m:\n         - Extract subseries: values at i, i+m, i+2m,...\n         - Smooth subseries with loess\n         - Store smoothed seasonal values\n      c. Low-pass filter seasonal (remove trend leakage)\n      d. Subtract filtered seasonal from raw seasonal → S\n      e. Deseasonalize: y - S\n      f. Smooth deseasonalized → T\n\n   Update robustness weights based on R = y - T - S\n\n3. Return T, S, R = y - T - S\n```\n\n## Common Pitfalls\n\n1. **Wrong seasonal period**: STL requires correct m. If m is wrong, seasonal won't be captured properly.\n\n2. **Over-smoothing trend**: Too large trend window removes real variation. Under-smoothing captures noise.\n\n3. **Seasonal leakage into trend**: If seasonal window too small, trend absorbs some seasonality.\n\n4. **Not using robust mode**: Outliers distort both trend and seasonal. Always try `robust=True` first.\n\n5. **Assuming multiplicative works directly**: STL is additive. For multiplicative, log-transform first, then decompose, then exponentiate.\n\n6. **Ignoring remainder**: Remainder should look like noise. Strong patterns indicate model inadequacy.\n\n## Mini Example\n\n```python\nimport numpy as np\nfrom statsmodels.tsa.seasonal import STL\nimport matplotlib.pyplot as plt\n\n# Generate data with trend, seasonality, and outliers\nnp.random.seed(42)\nn = 120\nt = np.arange(n)\ntrend = 50 + 0.3 * t\nseasonal = 10 * np.sin(2 * np.pi * t / 12)\nnoise = np.random.randn(n) * 3\ny = trend + seasonal + noise\n\n# Add outliers\ny[50] += 40\ny[80] -= 35\n\n# STL decomposition (robust)\nstl = STL(y, period=12, robust=True)\nresult = stl.fit()\n\nprint(\"Component statistics:\")\nprint(f\"Trend range: [{result.trend.min():.1f}, {result.trend.max():.1f}]\")\nprint(f\"Seasonal range: [{result.seasonal.min():.1f}, {result.seasonal.max():.1f}]\")\nprint(f\"Remainder std: {result.resid.std():.2f}\")\n\n# Check if outliers are in remainder (they should be)\nprint(f\"\\nRemainder at outlier positions:\")\nprint(f\"  t=50: {result.resid[50]:.1f}\")\nprint(f\"  t=80: {result.resid[80]:.1f}\")\n\n# Plot\nfig = result.plot()\nplt.tight_layout()\nplt.show()\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why is STL preferred over classical decomposition in many applications?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> STL advantages:\n\n1. **Flexibility**: Works with any seasonal period, not just 4 or 12\n2. **Robustness**: Outliers don't distort estimates (with robust=True)\n3. **Control**: Adjustable smoothness via window parameters\n4. **Evolving seasonal**: Can capture slowly changing seasonal patterns\n5. **No end-point issues**: Loess handles boundaries better than moving averages\n\nClassical decomposition:\n- Assumes fixed seasonal pattern\n- Sensitive to outliers\n- Moving average loses observations at ends\n- Limited to standard frequencies\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using classical decomposition by default. STL is almost always better, especially with outliers or evolving patterns.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> How does the robustness mechanism in STL work?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Iterative reweighting:\n\n1. First pass: fit STL normally, compute residuals\n2. Identify outliers: large |residuals| relative to median\n3. Assign weights: outliers get weight → 0, normal points → 1\n4. Refit STL with weighted observations\n5. Repeat until convergence\n\n**Weight function (bisquare):**\n$$w = (1 - (r/h)^2)^2$$\n\nwhere $r$ = |residual|, $h$ = 6 × median(|residuals|)\n\nOutliers (large $r$) get near-zero weights and don't influence the fit.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Not using robust mode then wondering why one outlier distorts the entire seasonal pattern. Always start with robust=True.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> In loess smoothing, why is the tricube weight function used?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Tricube function $w(u) = (1-|u|^3)^3$:\n\n1. **Smooth**: Continuous and differentiable, giving smooth fitted curves\n2. **Compact support**: Zero beyond bandwidth, so distant points don't influence fit\n3. **Downweighting**: Smoothly decreases influence with distance\n4. **Computationally nice**: Simple polynomial form\n\n**Properties:**\n- $w(0) = 1$ (full weight at target point)\n- $w(u) \\to 0$ smoothly as $|u| \\to 1$\n- $w(u) = 0$ for $|u| \\geq 1$\n\nAlternative: Gaussian weights have infinite support (all points contribute), which is less local.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Choosing bandwidth too small → jagged fit; too large → over-smoothed. STL uses data-driven defaults but tuning may help.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Why does STL use a low-pass filter on the seasonal component?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The low-pass filter removes trend that leaked into the seasonal during cycle-subseries smoothing.\n\n**Problem:** When smoothing each season's subseries (e.g., all Januaries), if there's trend, the subseries average drifts. This drift appears as low-frequency content in the seasonal.\n\n**Solution:** Apply moving average to the seasonal across full cycles:\n$$L_t = \\frac{1}{m}\\sum_{j=-(m-1)/2}^{(m-1)/2} S^*_{t+j}$$\n\nThen subtract: $S_t = S^*_t - L_t$\n\nThis ensures seasonal averages to zero over each cycle.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Without the low-pass filter, seasonal component captures some trend, leaving residual with trend pattern.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> After STL decomposition, your remainder shows a clear AR(1) pattern. What does this mean and what should you do?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> An AR(1) pattern in remainder means:\n- STL captured trend and seasonality\n- But short-term autocorrelation remains\n- This is common and expected\n\n**What to do:**\n1. **For forecasting**: Model remainder with AR(1) or ARIMA\n   - Forecast trend (extrapolation or drift)\n   - Forecast seasonal (repeat pattern)\n   - Forecast remainder with AR(1)\n   - Combine: $\\hat{y} = \\hat{T} + \\hat{S} + \\hat{R}$\n\n2. **STL + ARIMA pipeline:**\n   ```python\n   stl_result = STL(y, period=12).fit()\n   remainder = stl_result.resid\n   arima_model = ARIMA(remainder, order=(1,0,0)).fit()\n   ```\n\n3. **Consider ETS/SARIMA directly**: They handle all components in one model.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Ignoring remainder autocorrelation in forecasting. This underestimates short-term uncertainty and biases predictions.\n</div>\n</div>\n</details>\n\n## References\n\n1. Cleveland, R. B., Cleveland, W. S., McRae, J. E., & Terpenning, I. (1990). STL: A seasonal-trend decomposition procedure based on loess. *Journal of Official Statistics*, 6(1), 3-73.\n2. Cleveland, W. S., & Devlin, S. J. (1988). Locally weighted regression: An approach to regression analysis by local fitting. *JASA*, 83(403), 596-610.\n3. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 3.\n4. Dokumentov, A., & Hyndman, R. J. (2015). STR: A seasonal-trend decomposition procedure based on regression. *Monash Econometrics Working Papers*.\n", "sections": [{"heading": "STL Decomposition", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> STL (Seasonal and Trend decomposition using Loess) robustly separates a series into trend, seasonal, and remainder components using local regression. Unlike classical decomposition, STL handles any seasonal period and is robust to outliers. Key parameters: seasonal window (odd integer ≥ 7) and trend window. Remainder should be stationary for forecasting.\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**Additive Decomposition:**\n$$y_t = T_t + S_t + R_t$$\n\n- $T_t$: Trend component (smooth, long-term movement)\n- $S_t$: Seasonal component (periodic pattern, sums to ~0 over each cycle)\n- $R_t$: Remainder/residual (everything else)\n\n**Loess (Locally Estimated Scatterplot Smoothing):**\nLocal polynomial regression using weighted least squares. Weights decrease with distance from target point.\n\n**Key STL Parameters:**\n- `seasonal`: Seasonal period (e.g., 12 for monthly)\n- `seasonal_deg`: Degree of seasonal polynomial (0 or 1)\n- `trend`: Trend smoothing window (odd integer, default depends on seasonal)\n- `robust`: Whether to use robust fitting (downweight outliers)", "line_start": 7, "level": 2}, {"heading": "Loess Smoother", "content": "For point $x_0$, fit weighted polynomial:\n$$\\min_{\\beta} \\sum_{i=1}^{n} w_i(x_0)(y_i - \\beta_0 - \\beta_1(x_i - x_0))^2$$\n\nWeights use tricube function:\n$$w(u) = \\begin{cases} (1-|u|^3)^3 & |u| < 1 \\\\ 0 & |u| \\geq 1 \\end{cases}$$\n\nDistance scaled by bandwidth $h$: $u_i = |x_i - x_0|/h$", "line_start": 27, "level": 3}, {"heading": "STL Algorithm (Simplified)", "content": "**Outer loop** (for robustness):\n1. Initialize: $R_t^{(0)} = 0$, $T_t^{(0)} = $ loess smooth of $y_t$\n\n**Inner loop**:\n2. **Detrend**: $y_t - T_t^{(k-1)}$\n3. **Cycle-subseries smoothing**: For each season $s=1,\\ldots,m$, smooth values at positions $s, s+m, s+2m, \\ldots$ using loess\n4. **Low-pass filter**: Remove low-frequency from seasonal\n5. **Deseasonalize**: $y_t - S_t^{(k)}$\n6. **Trend extraction**: Loess smooth of deseasonalized series\n\nRepeat inner loop until convergence; outer loop updates robustness weights.", "line_start": 37, "level": 3}, {"heading": "Robustness Weights", "content": "After each outer iteration, compute residuals and assign weights:\n$$\\rho_t = |R_t|$$\n$$h = 6 \\cdot \\text{median}(|\\rho_t|)$$\n$$w_t = B(\\rho_t/h)$$\n\nwhere $B$ is the bisquare function: $B(u) = (1-u^2)^2$ for $|u| < 1$, else 0.\n\nOutliers get downweighted in subsequent iterations.", "line_start": 51, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**STL Decomposition Steps:**\n\n```\nInput: y[1:n], seasonal period m, parameters\nOutput: Trend T, Seasonal S, Remainder R\n\n1. Initialize trend T = loess(y) or moving average\n2. For k = 1 to n_outer:\n\n   For j = 1 to n_inner:\n      a. Detrend: D = y - T\n      b. For each position i in 1...m:\n         - Extract subseries: values at i, i+m, i+2m,...\n         - Smooth subseries with loess\n         - Store smoothed seasonal values\n      c. Low-pass filter seasonal (remove trend leakage)\n      d. Subtract filtered seasonal from raw seasonal → S\n      e. Deseasonalize: y - S\n      f. Smooth deseasonalized → T\n\n   Update robustness weights based on R = y - T - S\n\n3. Return T, S, R = y - T - S\n```", "line_start": 62, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Wrong seasonal period**: STL requires correct m. If m is wrong, seasonal won't be captured properly.\n\n2. **Over-smoothing trend**: Too large trend window removes real variation. Under-smoothing captures noise.\n\n3. **Seasonal leakage into trend**: If seasonal window too small, trend absorbs some seasonality.\n\n4. **Not using robust mode**: Outliers distort both trend and seasonal. Always try `robust=True` first.\n\n5. **Assuming multiplicative works directly**: STL is additive. For multiplicative, log-transform first, then decompose, then exponentiate.\n\n6. **Ignoring remainder**: Remainder should look like noise. Strong patterns indicate model inadequacy.", "line_start": 89, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.seasonal import STL\nimport matplotlib.pyplot as plt", "line_start": 103, "level": 1}, {"heading": "Generate data with trend, seasonality, and outliers", "content": "np.random.seed(42)\nn = 120\nt = np.arange(n)\ntrend = 50 + 0.3 * t\nseasonal = 10 * np.sin(2 * np.pi * t / 12)\nnoise = np.random.randn(n) * 3\ny = trend + seasonal + noise", "line_start": 110, "level": 1}, {"heading": "Add outliers", "content": "y[50] += 40\ny[80] -= 35", "line_start": 119, "level": 1}, {"heading": "STL decomposition (robust)", "content": "stl = STL(y, period=12, robust=True)\nresult = stl.fit()\n\nprint(\"Component statistics:\")\nprint(f\"Trend range: [{result.trend.min():.1f}, {result.trend.max():.1f}]\")\nprint(f\"Seasonal range: [{result.seasonal.min():.1f}, {result.seasonal.max():.1f}]\")\nprint(f\"Remainder std: {result.resid.std():.2f}\")", "line_start": 123, "level": 1}, {"heading": "Check if outliers are in remainder (they should be)", "content": "print(f\"\\nRemainder at outlier positions:\")\nprint(f\"  t=50: {result.resid[50]:.1f}\")\nprint(f\"  t=80: {result.resid[80]:.1f}\")", "line_start": 132, "level": 1}, {"heading": "Plot", "content": "fig = result.plot()\nplt.tight_layout()\nplt.show()\n```", "line_start": 137, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why is STL preferred over classical decomposition in many applications?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> STL advantages:\n\n1. **Flexibility**: Works with any seasonal period, not just 4 or 12\n2. **Robustness**: Outliers don't distort estimates (with robust=True)\n3. **Control**: Adjustable smoothness via window parameters\n4. **Evolving seasonal**: Can capture slowly changing seasonal patterns\n5. **No end-point issues**: Loess handles boundaries better than moving averages\n\nClassical decomposition:\n- Assumes fixed seasonal pattern\n- Sensitive to outliers\n- Moving average loses observations at ends\n- Limited to standard frequencies\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using classical decomposition by default. STL is almost always better, especially with outliers or evolving patterns.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> How does the robustness mechanism in STL work?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Iterative reweighting:\n\n1. First pass: fit STL normally, compute residuals\n2. Identify outliers: large |residuals| relative to median\n3. Assign weights: outliers get weight → 0, normal points → 1\n4. Refit STL with weighted observations\n5. Repeat until convergence\n\n**Weight function (bisquare):**\n$$w = (1 - (r/h)^2)^2$$\n\nwhere $r$ = |residual|, $h$ = 6 × median(|residuals|)\n\nOutliers (large $r$) get near-zero weights and don't influence the fit.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Not using robust mode then wondering why one outlier distorts the entire seasonal pattern. Always start with robust=True.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> In loess smoothing, why is the tricube weight function used?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Tricube function $w(u) = (1-|u|^3)^3$:\n\n1. **Smooth**: Continuous and differentiable, giving smooth fitted curves\n2. **Compact support**: Zero beyond bandwidth, so distant points don't influence fit\n3. **Downweighting**: Smoothly decreases influence with distance\n4. **Computationally nice**: Simple polynomial form\n\n**Properties:**\n- $w(0) = 1$ (full weight at target point)\n- $w(u) \\to 0$ smoothly as $|u| \\to 1$\n- $w(u) = 0$ for $|u| \\geq 1$\n\nAlternative: Gaussian weights have infinite support (all points contribute), which is less local.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Choosing bandwidth too small → jagged fit; too large → over-smoothed. STL uses data-driven defaults but tuning may help.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Why does STL use a low-pass filter on the seasonal component?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The low-pass filter removes trend that leaked into the seasonal during cycle-subseries smoothing.\n\n**Problem:** When smoothing each season's subseries (e.g., all Januaries), if there's trend, the subseries average drifts. This drift appears as low-frequency content in the seasonal.\n\n**Solution:** Apply moving average to the seasonal across full cycles:\n$$L_t = \\frac{1}{m}\\sum_{j=-(m-1)/2}^{(m-1)/2} S^*_{t+j}$$\n\nThen subtract: $S_t = S^*_t - L_t$\n\nThis ensures seasonal averages to zero over each cycle.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Without the low-pass filter, seasonal component captures some trend, leaving residual with trend pattern.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> After STL decomposition, your remainder shows a clear AR(1) pattern. What does this mean and what should you do?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> An AR(1) pattern in remainder means:\n- STL captured trend and seasonality\n- But short-term autocorrelation remains\n- This is common and expected\n\n**What to do:**\n1. **For forecasting**: Model remainder with AR(1) or ARIMA\n   - Forecast trend (extrapolation or drift)\n   - Forecast seasonal (repeat pattern)\n   - Forecast remainder with AR(1)\n   - Combine: $\\hat{y} = \\hat{T} + \\hat{S} + \\hat{R}$\n\n2. **STL + ARIMA pipeline:**\n   ```python\n   stl_result = STL(y, period=12).fit()\n   remainder = stl_result.resid\n   arima_model = ARIMA(remainder, order=(1,0,0)).fit()\n   ```\n\n3. **Consider ETS/SARIMA directly**: They handle all components in one model.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Ignoring remainder autocorrelation in forecasting. This underestimates short-term uncertainty and biases predictions.\n</div>\n</div>\n</details>", "line_start": 143, "level": 2}, {"heading": "References", "content": "1. Cleveland, R. B., Cleveland, W. S., McRae, J. E., & Terpenning, I. (1990). STL: A seasonal-trend decomposition procedure based on loess. *Journal of Official Statistics*, 6(1), 3-73.\n2. Cleveland, W. S., & Devlin, S. J. (1988). Locally weighted regression: An approach to regression analysis by local fitting. *JASA*, 83(403), 596-610.\n3. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 3.\n4. Dokumentov, A., & Hyndman, R. J. (2015). STR: A seasonal-trend decomposition procedure based on regression. *Monash Econometrics Working Papers*.", "line_start": 270, "level": 1}]}, "docs/en/change-detection/change-point.md": {"path": "docs/en/change-detection/change-point.md", "title": "Change-Point Detection", "content": "# Change-Point Detection\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Change-point detection identifies times when statistical properties (mean, variance, trend) shift. Methods: CUSUM (cumulative sum), PELT (penalized exact linear time), Bayesian online detection. Key trade-off: sensitivity vs. false positives. Applications: process monitoring, regime detection, structural breaks in economics. For offline detection, use dynamic programming; for online, use sequential methods.\n</div>\n\n## Core Definitions\n\n**Change-Point:** Time $\\tau$ where distribution parameters change:\n$$y_t \\sim \\begin{cases} F_1(\\theta_1) & t < \\tau \\\\ F_2(\\theta_2) & t \\geq \\tau \\end{cases}$$\n\n**Types:**\n- Mean shift: $\\mu_1 \\neq \\mu_2$\n- Variance change: $\\sigma_1^2 \\neq \\sigma_2^2$\n- Trend break: Slope changes\n- Multiple change-points: $\\tau_1 < \\tau_2 < \\cdots < \\tau_k$\n\n**Settings:**\n- Offline: All data available, find all change-points\n- Online: Sequential, detect changes as they occur\n\n## Math and Derivations\n\n### CUSUM (Cumulative Sum)\n\nFor mean detection with known parameters:\n$$S_t = \\sum_{i=1}^{t}(y_i - \\mu_0)$$\n\nUnder H₀ (no change): $S_t$ fluctuates around 0\nUnder H₁ (mean shift at τ): $S_t$ trends away from 0 after τ\n\n**Page's CUSUM:**\n$$C_t^+ = \\max(0, C_{t-1}^+ + y_t - \\mu_0 - k)$$\n$$C_t^- = \\max(0, C_{t-1}^- - y_t + \\mu_0 - k)$$\n\nSignal change when $C_t^+ > h$ or $C_t^- > h$\n\nParameters: $k$ (allowance), $h$ (threshold)\n\n### Binary Segmentation\n\nGreedy algorithm for multiple change-points:\n\n1. Test for one change-point in [1, T]\n2. If found at τ₁, recursively search [1, τ₁) and [τ₁, T]\n3. Continue until no more significant changes\n\nCost function (e.g., RSS):\n$$C(y_{s:t}) = \\sum_{i=s}^{t}(y_i - \\bar{y}_{s:t})^2$$\n\n### PELT (Pruned Exact Linear Time)\n\nOptimal partitioning via dynamic programming:\n$$F(t) = \\min_{s < t}\\{F(s) + C(y_{s+1:t}) + \\beta\\}$$\n\nwhere β is penalty per change-point.\n\n**Pruning:** Eliminate suboptimal segmentations to achieve O(n) complexity.\n\n### Bayesian Online Change-Point Detection\n\nMaintain probability distribution over run length $r_t$ (time since last change):\n$$P(r_t | y_{1:t}) \\propto P(y_t | r_t, y_{1:t-1}) P(r_t | r_{t-1})$$\n\nGrowth probability: $P(r_t = r_{t-1} + 1)$\nChange probability: $P(r_t = 0)$\n\n## Algorithm/Model Sketch\n\n**Offline Detection (PELT):**\n\n```python\ndef pelt(y, penalty, min_size=2):\n    n = len(y)\n    F = [0]  # F[t] = min cost for y[0:t]\n    cp = [[]]  # change-points for optimal segmentation\n\n    for t in range(1, n + 1):\n        candidates = []\n        for s in range(max(0, t - max_segments), t):\n            if t - s >= min_size:\n                cost = F[s] + segment_cost(y[s:t]) + penalty\n                candidates.append((cost, s))\n\n        best_cost, best_s = min(candidates)\n        F.append(best_cost)\n        cp.append(cp[best_s] + [best_s] if best_s > 0 else [])\n\n    return cp[-1]\n```\n\n**Online Detection (CUSUM):**\n\n```python\ndef cusum_online(y, mu0, k, h):\n    n = len(y)\n    C_plus, C_minus = 0, 0\n    alarms = []\n\n    for t in range(n):\n        C_plus = max(0, C_plus + y[t] - mu0 - k)\n        C_minus = max(0, C_minus - y[t] + mu0 - k)\n\n        if C_plus > h or C_minus > h:\n            alarms.append(t)\n            C_plus, C_minus = 0, 0  # Reset\n\n    return alarms\n```\n\n## Common Pitfalls\n\n1. **Penalty selection**: Too small → over-segmentation; too large → miss changes. Use BIC-based penalties or cross-validation.\n\n2. **Minimum segment length**: Very short segments are often noise. Enforce minimum size constraint.\n\n3. **Multiple testing**: Testing many potential change-points inflates false positives. Adjust thresholds.\n\n4. **Model misspecification**: Assuming wrong distribution (e.g., normal for heavy-tailed data) affects detection.\n\n5. **Gradual vs. abrupt changes**: Most methods assume sudden shifts. Gradual changes may appear as multiple small changes.\n\n6. **Online delay**: Online methods detect changes with delay. Trade-off between speed and accuracy.\n\n## Mini Example\n\n```python\nimport numpy as np\nimport ruptures as rpt\n\n# Generate data with 2 change-points\nnp.random.seed(42)\nn = 300\n\n# Three segments with different means\ny = np.concatenate([\n    np.random.randn(100) + 0,      # mean 0\n    np.random.randn(100) + 3,      # mean 3\n    np.random.randn(100) + 1       # mean 1\n])\n\n# PELT detection\nalgo = rpt.Pelt(model=\"l2\", min_size=10).fit(y)\nchange_points = algo.predict(pen=10)\nprint(f\"Detected change-points: {change_points[:-1]}\")\nprint(f\"True change-points: [100, 200]\")\n\n# Binary segmentation\nalgo_binseg = rpt.Binseg(model=\"l2\", min_size=10).fit(y)\nchange_points_bs = algo_binseg.predict(n_bkps=2)\nprint(f\"BinSeg change-points: {change_points_bs[:-1]}\")\n\n# Bayesian approach (conceptual)\n# Using online Bayesian detection\nfrom scipy.stats import norm\n\ndef bocpd_simple(y, hazard=0.01, mu0=0, sigma=1):\n    \"\"\"Simplified Bayesian Online CPD.\"\"\"\n    n = len(y)\n    R = np.zeros((n + 1, n + 1))  # Run length probabilities\n    R[0, 0] = 1\n\n    for t in range(1, n + 1):\n        # Predictive probability under each run length\n        predprob = norm.pdf(y[t-1], mu0, sigma)  # Simplified\n\n        # Growth probability\n        R[t, 1:t+1] = R[t-1, :t] * predprob * (1 - hazard)\n        # Change probability\n        R[t, 0] = np.sum(R[t-1, :t] * predprob * hazard)\n        # Normalize\n        R[t, :] /= R[t, :].sum()\n\n    return R\n\nR = bocpd_simple(y, hazard=0.01, mu0=1, sigma=1.5)\nprint(f\"Max probability run length at end: {np.argmax(R[-1])}\")\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What is the trade-off between online and offline change-point detection?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Online detection:**\n- ✓ Real-time alerts\n- ✓ No need for full data\n- ✗ Detection delay (need evidence to accumulate)\n- ✗ Can't revise past decisions\n- Use case: Process monitoring, fraud detection\n\n**Offline detection:**\n- ✓ Uses all data for optimal segmentation\n- ✓ Can find exact change locations\n- ✓ More accurate (global optimization)\n- ✗ Not real-time\n- Use case: Historical analysis, model building\n\n**Hybrid:** Some methods (e.g., Bayesian online) can be run offline for retrospective analysis while also providing online capability.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using online methods for offline analysis. If all data is available, use PELT or exact methods for better accuracy.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> How do you choose the penalty parameter in PELT or similar methods?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Several approaches:\n\n1. **BIC-style penalty:** $\\beta = \\log(n) \\times k$ where k = parameters per segment\n   - Theoretically justified\n   - Can be conservative\n\n2. **Cross-validation:**\n   - Hold out data, select penalty minimizing prediction error\n   - Computationally intensive\n\n3. **Elbow method:**\n   - Plot cost vs. number of change-points\n   - Select \"elbow\" where diminishing returns begin\n\n4. **Domain knowledge:**\n   - Expected number of changes\n   - Cost of false positives vs. missed detections\n\n**SIC/MBIC:** Modified BIC for change-point specific context.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using fixed penalty across different datasets. Optimal penalty depends on signal-to-noise ratio, segment lengths, and number of observations.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Derive the expected value of CUSUM under H₀ (no change) and H₁ (mean shift).</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Under H₀:** $y_t \\sim N(\\mu_0, \\sigma^2)$\n$$E[S_t] = E\\left[\\sum_{i=1}^{t}(y_i - \\mu_0)\\right] = \\sum_{i=1}^{t}E[y_i - \\mu_0] = 0$$\n\nCUSUM fluctuates around 0 (random walk behavior).\n\n**Under H₁:** Mean shifts to $\\mu_1$ at time $\\tau$\n$$E[S_t] = \\sum_{i=1}^{\\tau-1}(\\mu_0 - \\mu_0) + \\sum_{i=\\tau}^{t}(\\mu_1 - \\mu_0) = (t - \\tau + 1)(\\mu_1 - \\mu_0)$$\n\nAfter change, CUSUM drifts linearly away from 0 at rate $(\\mu_1 - \\mu_0)$.\n\n**Key insight:** The drift rate equals the mean shift magnitude, making CUSUM sensitive to sustained changes.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> CUSUM assumes known pre-change mean $\\mu_0$. If estimated, use standardized CUSUM or adjust thresholds.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Why does binary segmentation not guarantee finding the optimal solution?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Binary segmentation is greedy—it makes locally optimal choices without considering global structure.\n\n**Problem scenario:**\nConsider data with two close change-points:\n- Segment 1: mean 0, length 80\n- Segment 2: mean 2, length 20\n- Segment 3: mean 0, length 100\n\nBinary segmentation first finds the \"best\" single split, which might be around position 100 (between segments 2 and 3). Then it searches [0,100] and [100,200] separately.\n\nBut the small segment 2 might be better detected by finding BOTH change-points (80 and 100) together.\n\n**Solution:** Use exact methods (PELT, optimal partitioning) that consider all possible segmentations.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Assuming binary segmentation is \"good enough.\" For complex signals with varying segment sizes, exact methods can be significantly better.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You're monitoring server response times. You detect a \"change-point\" but it turns out to be a single outlier. How do you prevent false alarms from outliers?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Several strategies:\n\n1. **Robust cost functions:**\n   - Use L1 (absolute) instead of L2 (squared)\n   - Huber loss\n   - Median-based detection\n\n2. **Minimum segment length:**\n   - Require at least k observations in each segment\n   - Single outliers can't form segments\n\n3. **Pre-filtering:**\n   - Apply median filter or outlier removal first\n   - Then detect changes\n\n4. **Multi-scale detection:**\n   - Detect at multiple resolutions\n   - True changes appear at all scales; outliers don't\n\n5. **Confirmation period:**\n   - Don't alarm on first deviation\n   - Require sustained change (e.g., CUSUM with appropriate k)\n\n6. **Model-based:**\n   - Use models that explicitly include outlier component\n   - Separate outliers from level shifts\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using squared error cost with heavy-tailed data. Single large values dominate and trigger false change-points.\n</div>\n</div>\n</details>\n\n## References\n\n1. Truong, C., Oudre, L., & Vayatis, N. (2020). Selective review of offline change point detection methods. *Signal Processing*, 167, 107299.\n2. Killick, R., Fearnhead, P., & Eckley, I. A. (2012). Optimal detection of changepoints with a linear computational cost. *JASA*, 107(500), 1590-1598.\n3. Adams, R. P., & MacKay, D. J. (2007). Bayesian online changepoint detection. *arXiv:0710.3742*.\n4. Page, E. S. (1954). Continuous inspection schemes. *Biometrika*, 41(1/2), 100-115.\n", "sections": [{"heading": "Change-Point Detection", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Change-point detection identifies times when statistical properties (mean, variance, trend) shift. Methods: CUSUM (cumulative sum), PELT (penalized exact linear time), Bayesian online detection. Key trade-off: sensitivity vs. false positives. Applications: process monitoring, regime detection, structural breaks in economics. For offline detection, use dynamic programming; for online, use sequential methods.\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**Change-Point:** Time $\\tau$ where distribution parameters change:\n$$y_t \\sim \\begin{cases} F_1(\\theta_1) & t < \\tau \\\\ F_2(\\theta_2) & t \\geq \\tau \\end{cases}$$\n\n**Types:**\n- Mean shift: $\\mu_1 \\neq \\mu_2$\n- Variance change: $\\sigma_1^2 \\neq \\sigma_2^2$\n- Trend break: Slope changes\n- Multiple change-points: $\\tau_1 < \\tau_2 < \\cdots < \\tau_k$\n\n**Settings:**\n- Offline: All data available, find all change-points\n- Online: Sequential, detect changes as they occur", "line_start": 7, "level": 2}, {"heading": "CUSUM (Cumulative Sum)", "content": "For mean detection with known parameters:\n$$S_t = \\sum_{i=1}^{t}(y_i - \\mu_0)$$\n\nUnder H₀ (no change): $S_t$ fluctuates around 0\nUnder H₁ (mean shift at τ): $S_t$ trends away from 0 after τ\n\n**Page's CUSUM:**\n$$C_t^+ = \\max(0, C_{t-1}^+ + y_t - \\mu_0 - k)$$\n$$C_t^- = \\max(0, C_{t-1}^- - y_t + \\mu_0 - k)$$\n\nSignal change when $C_t^+ > h$ or $C_t^- > h$\n\nParameters: $k$ (allowance), $h$ (threshold)", "line_start": 24, "level": 3}, {"heading": "Binary Segmentation", "content": "Greedy algorithm for multiple change-points:\n\n1. Test for one change-point in [1, T]\n2. If found at τ₁, recursively search [1, τ₁) and [τ₁, T]\n3. Continue until no more significant changes\n\nCost function (e.g., RSS):\n$$C(y_{s:t}) = \\sum_{i=s}^{t}(y_i - \\bar{y}_{s:t})^2$$", "line_start": 40, "level": 3}, {"heading": "PELT (Pruned Exact Linear Time)", "content": "Optimal partitioning via dynamic programming:\n$$F(t) = \\min_{s < t}\\{F(s) + C(y_{s+1:t}) + \\beta\\}$$\n\nwhere β is penalty per change-point.\n\n**Pruning:** Eliminate suboptimal segmentations to achieve O(n) complexity.", "line_start": 51, "level": 3}, {"heading": "Bayesian Online Change-Point Detection", "content": "Maintain probability distribution over run length $r_t$ (time since last change):\n$$P(r_t | y_{1:t}) \\propto P(y_t | r_t, y_{1:t-1}) P(r_t | r_{t-1})$$\n\nGrowth probability: $P(r_t = r_{t-1} + 1)$\nChange probability: $P(r_t = 0)$", "line_start": 60, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**Offline Detection (PELT):**\n\n```python\ndef pelt(y, penalty, min_size=2):\n    n = len(y)\n    F = [0]  # F[t] = min cost for y[0:t]\n    cp = [[]]  # change-points for optimal segmentation\n\n    for t in range(1, n + 1):\n        candidates = []\n        for s in range(max(0, t - max_segments), t):\n            if t - s >= min_size:\n                cost = F[s] + segment_cost(y[s:t]) + penalty\n                candidates.append((cost, s))\n\n        best_cost, best_s = min(candidates)\n        F.append(best_cost)\n        cp.append(cp[best_s] + [best_s] if best_s > 0 else [])\n\n    return cp[-1]\n```\n\n**Online Detection (CUSUM):**\n\n```python\ndef cusum_online(y, mu0, k, h):\n    n = len(y)\n    C_plus, C_minus = 0, 0\n    alarms = []\n\n    for t in range(n):\n        C_plus = max(0, C_plus + y[t] - mu0 - k)\n        C_minus = max(0, C_minus - y[t] + mu0 - k)\n\n        if C_plus > h or C_minus > h:\n            alarms.append(t)\n            C_plus, C_minus = 0, 0  # Reset\n\n    return alarms\n```", "line_start": 68, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Penalty selection**: Too small → over-segmentation; too large → miss changes. Use BIC-based penalties or cross-validation.\n\n2. **Minimum segment length**: Very short segments are often noise. Enforce minimum size constraint.\n\n3. **Multiple testing**: Testing many potential change-points inflates false positives. Adjust thresholds.\n\n4. **Model misspecification**: Assuming wrong distribution (e.g., normal for heavy-tailed data) affects detection.\n\n5. **Gradual vs. abrupt changes**: Most methods assume sudden shifts. Gradual changes may appear as multiple small changes.\n\n6. **Online delay**: Online methods detect changes with delay. Trade-off between speed and accuracy.", "line_start": 111, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\nimport ruptures as rpt", "line_start": 125, "level": 1}, {"heading": "Generate data with 2 change-points", "content": "np.random.seed(42)\nn = 300", "line_start": 131, "level": 1}, {"heading": "Three segments with different means", "content": "y = np.concatenate([\n    np.random.randn(100) + 0,      # mean 0\n    np.random.randn(100) + 3,      # mean 3\n    np.random.randn(100) + 1       # mean 1\n])", "line_start": 135, "level": 1}, {"heading": "PELT detection", "content": "algo = rpt.Pelt(model=\"l2\", min_size=10).fit(y)\nchange_points = algo.predict(pen=10)\nprint(f\"Detected change-points: {change_points[:-1]}\")\nprint(f\"True change-points: [100, 200]\")", "line_start": 142, "level": 1}, {"heading": "Binary segmentation", "content": "algo_binseg = rpt.Binseg(model=\"l2\", min_size=10).fit(y)\nchange_points_bs = algo_binseg.predict(n_bkps=2)\nprint(f\"BinSeg change-points: {change_points_bs[:-1]}\")", "line_start": 148, "level": 1}, {"heading": "Using online Bayesian detection", "content": "from scipy.stats import norm\n\ndef bocpd_simple(y, hazard=0.01, mu0=0, sigma=1):\n    \"\"\"Simplified Bayesian Online CPD.\"\"\"\n    n = len(y)\n    R = np.zeros((n + 1, n + 1))  # Run length probabilities\n    R[0, 0] = 1\n\n    for t in range(1, n + 1):\n        # Predictive probability under each run length\n        predprob = norm.pdf(y[t-1], mu0, sigma)  # Simplified\n\n        # Growth probability\n        R[t, 1:t+1] = R[t-1, :t] * predprob * (1 - hazard)\n        # Change probability\n        R[t, 0] = np.sum(R[t-1, :t] * predprob * hazard)\n        # Normalize\n        R[t, :] /= R[t, :].sum()\n\n    return R\n\nR = bocpd_simple(y, hazard=0.01, mu0=1, sigma=1.5)\nprint(f\"Max probability run length at end: {np.argmax(R[-1])}\")\n```", "line_start": 154, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What is the trade-off between online and offline change-point detection?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Online detection:**\n- ✓ Real-time alerts\n- ✓ No need for full data\n- ✗ Detection delay (need evidence to accumulate)\n- ✗ Can't revise past decisions\n- Use case: Process monitoring, fraud detection\n\n**Offline detection:**\n- ✓ Uses all data for optimal segmentation\n- ✓ Can find exact change locations\n- ✓ More accurate (global optimization)\n- ✗ Not real-time\n- Use case: Historical analysis, model building\n\n**Hybrid:** Some methods (e.g., Bayesian online) can be run offline for retrospective analysis while also providing online capability.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using online methods for offline analysis. If all data is available, use PELT or exact methods for better accuracy.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> How do you choose the penalty parameter in PELT or similar methods?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Several approaches:\n\n1. **BIC-style penalty:** $\\beta = \\log(n) \\times k$ where k = parameters per segment\n   - Theoretically justified\n   - Can be conservative\n\n2. **Cross-validation:**\n   - Hold out data, select penalty minimizing prediction error\n   - Computationally intensive\n\n3. **Elbow method:**\n   - Plot cost vs. number of change-points\n   - Select \"elbow\" where diminishing returns begin\n\n4. **Domain knowledge:**\n   - Expected number of changes\n   - Cost of false positives vs. missed detections\n\n**SIC/MBIC:** Modified BIC for change-point specific context.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using fixed penalty across different datasets. Optimal penalty depends on signal-to-noise ratio, segment lengths, and number of observations.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Derive the expected value of CUSUM under H₀ (no change) and H₁ (mean shift).</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Under H₀:** $y_t \\sim N(\\mu_0, \\sigma^2)$\n$$E[S_t] = E\\left[\\sum_{i=1}^{t}(y_i - \\mu_0)\\right] = \\sum_{i=1}^{t}E[y_i - \\mu_0] = 0$$\n\nCUSUM fluctuates around 0 (random walk behavior).\n\n**Under H₁:** Mean shifts to $\\mu_1$ at time $\\tau$\n$$E[S_t] = \\sum_{i=1}^{\\tau-1}(\\mu_0 - \\mu_0) + \\sum_{i=\\tau}^{t}(\\mu_1 - \\mu_0) = (t - \\tau + 1)(\\mu_1 - \\mu_0)$$\n\nAfter change, CUSUM drifts linearly away from 0 at rate $(\\mu_1 - \\mu_0)$.\n\n**Key insight:** The drift rate equals the mean shift magnitude, making CUSUM sensitive to sustained changes.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> CUSUM assumes known pre-change mean $\\mu_0$. If estimated, use standardized CUSUM or adjust thresholds.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Why does binary segmentation not guarantee finding the optimal solution?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Binary segmentation is greedy—it makes locally optimal choices without considering global structure.\n\n**Problem scenario:**\nConsider data with two close change-points:\n- Segment 1: mean 0, length 80\n- Segment 2: mean 2, length 20\n- Segment 3: mean 0, length 100\n\nBinary segmentation first finds the \"best\" single split, which might be around position 100 (between segments 2 and 3). Then it searches [0,100] and [100,200] separately.\n\nBut the small segment 2 might be better detected by finding BOTH change-points (80 and 100) together.\n\n**Solution:** Use exact methods (PELT, optimal partitioning) that consider all possible segmentations.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Assuming binary segmentation is \"good enough.\" For complex signals with varying segment sizes, exact methods can be significantly better.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You're monitoring server response times. You detect a \"change-point\" but it turns out to be a single outlier. How do you prevent false alarms from outliers?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Several strategies:\n\n1. **Robust cost functions:**\n   - Use L1 (absolute) instead of L2 (squared)\n   - Huber loss\n   - Median-based detection\n\n2. **Minimum segment length:**\n   - Require at least k observations in each segment\n   - Single outliers can't form segments\n\n3. **Pre-filtering:**\n   - Apply median filter or outlier removal first\n   - Then detect changes\n\n4. **Multi-scale detection:**\n   - Detect at multiple resolutions\n   - True changes appear at all scales; outliers don't\n\n5. **Confirmation period:**\n   - Don't alarm on first deviation\n   - Require sustained change (e.g., CUSUM with appropriate k)\n\n6. **Model-based:**\n   - Use models that explicitly include outlier component\n   - Separate outliers from level shifts\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using squared error cost with heavy-tailed data. Single large values dominate and trigger false change-points.\n</div>\n</div>\n</details>", "line_start": 180, "level": 2}, {"heading": "References", "content": "1. Truong, C., Oudre, L., & Vayatis, N. (2020). Selective review of offline change point detection methods. *Signal Processing*, 167, 107299.\n2. Killick, R., Fearnhead, P., & Eckley, I. A. (2012). Optimal detection of changepoints with a linear computational cost. *JASA*, 107(500), 1590-1598.\n3. Adams, R. P., & MacKay, D. J. (2007). Bayesian online changepoint detection. *arXiv:0710.3742*.\n4. Page, E. S. (1954). Continuous inspection schemes. *Biometrika*, 41(1/2), 100-115.", "line_start": 325, "level": 1}]}, "docs/en/foundations/autocorrelation.md": {"path": "docs/en/foundations/autocorrelation.md", "title": "Autocorrelation and Partial Autocorrelation", "content": "# Autocorrelation and Partial Autocorrelation\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> ACF measures correlation between a series and its lagged values. PACF measures correlation at lag $k$ after removing effects of intermediate lags. ACF/PACF patterns identify AR/MA orders: AR(p) has PACF cutoff at lag p; MA(q) has ACF cutoff at lag q. Sample ACF/PACF have approximate standard error $1/\\sqrt{n}$ under white noise.\n</div>\n\n## Core Definitions\n\n**Autocovariance Function (ACVF)**: For a stationary process with mean $\\mu$:\n$$\\gamma(h) = \\text{Cov}(X_t, X_{t+h}) = E[(X_t - \\mu)(X_{t+h} - \\mu)]$$\n\n**Autocorrelation Function (ACF)**: Normalized autocovariance:\n$$\\rho(h) = \\frac{\\gamma(h)}{\\gamma(0)} = \\text{Corr}(X_t, X_{t+h})$$\n\n**Partial Autocorrelation Function (PACF)**: Correlation between $X_t$ and $X_{t+h}$ after removing the linear dependence on $X_{t+1}, \\ldots, X_{t+h-1}$:\n$$\\phi_{hh} = \\text{Corr}(X_t - \\hat{X}_t, X_{t+h} - \\hat{X}_{t+h})$$\n\nwhere $\\hat{X}_t$ and $\\hat{X}_{t+h}$ are best linear predictors from intermediate values.\n\n**Sample ACF**: Estimated from data:\n$$\\hat{\\rho}(h) = \\frac{\\hat{\\gamma}(h)}{\\hat{\\gamma}(0)} = \\frac{\\sum_{t=1}^{n-h}(X_t - \\bar{X})(X_{t+h} - \\bar{X})}{\\sum_{t=1}^{n}(X_t - \\bar{X})^2}$$\n\n## Math and Derivations\n\n### Properties of ACF\n\nFor any stationary process:\n\n1. $\\rho(0) = 1$\n2. $\\rho(h) = \\rho(-h)$ (symmetry)\n3. $|\\rho(h)| \\leq 1$ for all $h$\n4. $\\rho(h)$ is positive semi-definite: for any $a_1, \\ldots, a_n$:\n   $$\\sum_{i=1}^{n}\\sum_{j=1}^{n} a_i a_j \\rho(i-j) \\geq 0$$\n\n### PACF via Yule-Walker Equations\n\nThe PACF at lag $k$ is the last coefficient $\\phi_{kk}$ in the AR(k) regression:\n$$X_t = \\phi_{k1}X_{t-1} + \\phi_{k2}X_{t-2} + \\cdots + \\phi_{kk}X_{t-k} + \\epsilon_t$$\n\nYule-Walker equations in matrix form:\n$$\\begin{pmatrix} 1 & \\rho(1) & \\cdots & \\rho(k-1) \\\\ \\rho(1) & 1 & \\cdots & \\rho(k-2) \\\\ \\vdots & & \\ddots & \\vdots \\\\ \\rho(k-1) & \\cdots & \\rho(1) & 1 \\end{pmatrix} \\begin{pmatrix} \\phi_{k1} \\\\ \\phi_{k2} \\\\ \\vdots \\\\ \\phi_{kk} \\end{pmatrix} = \\begin{pmatrix} \\rho(1) \\\\ \\rho(2) \\\\ \\vdots \\\\ \\rho(k) \\end{pmatrix}$$\n\n### ACF of AR(1): $X_t = \\phi X_{t-1} + \\epsilon_t$\n\n$$\\rho(h) = \\phi^{|h|}$$\n\nACF decays exponentially (geometrically) for $|\\phi| < 1$.\n\n### ACF of MA(1): $X_t = \\epsilon_t + \\theta\\epsilon_{t-1}$\n\n$$\\rho(1) = \\frac{\\theta}{1+\\theta^2}, \\quad \\rho(h) = 0 \\text{ for } h > 1$$\n\nACF cuts off after lag 1.\n\n### ACF of MA(q)\n\n$$\\rho(h) = 0 \\text{ for } h > q$$\n\n### PACF of AR(p)\n\n$$\\phi_{hh} = 0 \\text{ for } h > p$$\n\n### Variance of Sample ACF\n\nUnder the null hypothesis that the true process is white noise:\n$$\\text{Var}(\\hat{\\rho}(h)) \\approx \\frac{1}{n}$$\n\nSo approximate 95% confidence bands are $\\pm 1.96/\\sqrt{n}$.\n\n**Bartlett's formula** (for MA(q) process):\n$$\\text{Var}(\\hat{\\rho}(h)) \\approx \\frac{1}{n}\\left(1 + 2\\sum_{k=1}^{q}\\rho(k)^2\\right) \\text{ for } h > q$$\n\n## Algorithm/Model Sketch\n\n**Using ACF/PACF for Model Identification:**\n\n| Pattern | ACF | PACF | Model |\n|---------|-----|------|-------|\n| AR(p) | Exponential/sinusoidal decay | Cuts off after lag p | AR(p) |\n| MA(q) | Cuts off after lag q | Exponential/sinusoidal decay | MA(q) |\n| ARMA(p,q) | Tails off | Tails off | ARMA(p,q) |\n| White noise | All near zero | All near zero | No model needed |\n| Non-stationary | Very slow decay | Large spike at lag 1 | Difference first |\n\n**Interpretation procedure:**\n\n```\n1. Plot series - check for stationarity\n2. If non-stationary, difference until stationary\n3. Compute sample ACF and PACF\n4. Check for significant spikes (outside ±1.96/√n bands)\n5. Identify patterns:\n   - ACF cuts off, PACF decays → MA(q) where q = cutoff lag\n   - PACF cuts off, ACF decays → AR(p) where p = cutoff lag\n   - Both decay → ARMA (use information criteria)\n6. Fit candidate models\n7. Check residual ACF/PACF (should be white noise)\n```\n\n## Common Pitfalls\n\n1. **Ignoring confidence bands**: Not all spikes are significant. Use $\\pm 1.96/\\sqrt{n}$ bands and expect ~5% of spikes to exceed by chance.\n\n2. **Confusing \"cuts off\" vs \"tails off\"**: Cutoff means abrupt drop to zero after lag q. Tails off means gradual decay. This distinction determines AR vs MA.\n\n3. **Applying ACF/PACF to non-stationary data**: Results are meaningless for non-stationary series. Always check stationarity first.\n\n4. **Over-interpreting high-lag correlations**: For small samples, high-lag estimates have high variance. Focus on early lags.\n\n5. **Forgetting seasonal lags**: In seasonal data, check lags at seasonal period (e.g., lag 12 for monthly data with annual seasonality).\n\n6. **Neglecting theoretical ACF/PACF**: When validating models, compare sample functions to theoretical ones, not just residuals.\n\n## Mini Example\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.arima_process import ArmaProcess\n\n# Simulate AR(2) process\nnp.random.seed(42)\nar_params = np.array([1, -0.75, 0.25])  # 1 - 0.75L + 0.25L^2\nma_params = np.array([1])\nar2_process = ArmaProcess(ar_params, ma_params)\nar2_data = ar2_process.generate_sample(nsample=300)\n\n# Plot ACF and PACF\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\nplot_acf(ar2_data, ax=axes[0], lags=20, title='ACF of AR(2)')\nplot_pacf(ar2_data, ax=axes[1], lags=20, title='PACF of AR(2)')\nplt.tight_layout()\nplt.show()\n\n# AR(2): PACF should cut off after lag 2, ACF should decay\n# Check significant PACF values\nfrom statsmodels.tsa.stattools import pacf\npacf_values = pacf(ar2_data, nlags=5)\nprint(\"PACF values:\", pacf_values)\n# Expect: significant at lags 1,2; near zero after\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What is the fundamental difference between ACF and PACF? Why do we need both?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> ACF measures total correlation between $X_t$ and $X_{t+h}$, including indirect effects through intermediate lags. PACF measures direct correlation after removing intermediate effects.\n\n<strong>Explanation:</strong> Consider AR(1) with $\\phi = 0.8$. The ACF shows $\\rho(2) = 0.64$ because $X_t$ and $X_{t+2}$ are correlated through $X_{t+1}$. But the PACF at lag 2 is near zero because once we account for $X_{t+1}$, there's no additional direct relationship.\n\nWe need both because:\n- ACF identifies MA order (cuts off at lag q)\n- PACF identifies AR order (cuts off at lag p)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using only ACF for identification. Without PACF, you cannot distinguish AR from MA patterns—both can show decaying ACF, but only AR shows PACF cutoff.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> How would you interpret an ACF that shows a very slow decay with values staying significant past lag 20?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> This strongly suggests non-stationarity. A stationary process should have ACF that decays relatively quickly to zero. Very slow decay indicates a unit root or near-unit root.\n\n<strong>Recommended action:</strong>\n1. Formally test with ADF/KPSS\n2. Difference the series\n3. Re-compute ACF after differencing\n4. The differenced series should show faster decay\n\n**Key insight:** For a random walk, $\\rho(h) \\approx 1$ for all $h$ in finite samples because successive values are highly dependent.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Trying to fit ARMA models to non-stationary data. The resulting parameters will be misleading, and forecasts will be poor. Always ensure stationarity first.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Derive the ACF for MA(1): $X_t = \\epsilon_t + \\theta\\epsilon_{t-1}$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> $\\rho(0) = 1$, $\\rho(1) = \\frac{\\theta}{1+\\theta^2}$, $\\rho(h) = 0$ for $h \\geq 2$.\n\n<strong>Derivation:</strong>\n\n**Variance (lag 0):**\n$$\\gamma(0) = \\text{Var}(X_t) = \\text{Var}(\\epsilon_t + \\theta\\epsilon_{t-1}) = \\sigma^2 + \\theta^2\\sigma^2 = (1+\\theta^2)\\sigma^2$$\n\n**Autocovariance at lag 1:**\n$$\\gamma(1) = \\text{Cov}(X_t, X_{t+1}) = \\text{Cov}(\\epsilon_t + \\theta\\epsilon_{t-1}, \\epsilon_{t+1} + \\theta\\epsilon_t)$$\n$$= \\text{Cov}(\\epsilon_t, \\theta\\epsilon_t) = \\theta\\sigma^2$$\n\n**Autocovariance at lag $h \\geq 2$:**\n$$\\gamma(h) = \\text{Cov}(\\epsilon_t + \\theta\\epsilon_{t-1}, \\epsilon_{t+h} + \\theta\\epsilon_{t+h-1}) = 0$$\n\n(No overlapping $\\epsilon$ terms when $h \\geq 2$)\n\n**ACF:**\n$$\\rho(1) = \\frac{\\gamma(1)}{\\gamma(0)} = \\frac{\\theta\\sigma^2}{(1+\\theta^2)\\sigma^2} = \\frac{\\theta}{1+\\theta^2}$$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Note that $|\\rho(1)| \\leq 0.5$ for MA(1). The maximum occurs at $\\theta = \\pm 1$. If you observe $|\\hat{\\rho}(1)| > 0.5$, it might be AR, not MA.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> For an AR(2) process $X_t = \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + \\epsilon_t$, show that the PACF is zero for all lags $h > 2$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> For AR(p), the PACF $\\phi_{hh} = 0$ for $h > p$ because once $X_{t-1}, \\ldots, X_{t-p}$ are included, adding more lags provides no additional predictive information.\n\n<strong>Explanation:</strong>\n\nThe AR(2) model states that $X_t$ depends only on $X_{t-1}$ and $X_{t-2}$ (plus noise). Therefore:\n\nFor $h = 3$: We regress $X_t$ on $X_{t-1}, X_{t-2}, X_{t-3}$.\n- $X_t = \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + \\epsilon_t$\n- $X_{t-3}$ only affects $X_t$ through $X_{t-2}$ and $X_{t-1}$\n- After conditioning on $X_{t-1}$ and $X_{t-2}$, $X_{t-3}$ adds no information\n- Thus $\\phi_{33} = 0$\n\nBy induction, this holds for all $h > 2$.\n\n**Key equation:** PACF at lag $k$ is the coefficient $\\phi_{kk}$ in the best linear predictor using exactly $k$ lags. For AR(p), the $k$-th coefficient becomes zero when $k > p$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Expecting exact zeros in sample PACF. Due to sampling variability, $\\hat{\\phi}_{hh}$ will be nonzero but should fall within confidence bands for $h > p$.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You compute the sample ACF for a time series with $n=100$ observations. You see that lags 1, 2, 7, and 15 exceed the 95% confidence bands. What is your interpretation?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> With 95% bands and ~15-20 lags tested, expect about 1 spurious significant lag by chance. Lags 1 and 2 are likely real signal. Lag 7 might be real (check for weekly patterns if relevant). Lag 15 is likely spurious.\n\n<strong>Interpretation process:</strong>\n1. Focus on early lags (1, 2, 3) — most likely real\n2. Consider domain knowledge (lag 7 = weekly? lag 12 = monthly?)\n3. Isolated high-lag spikes are often noise\n4. Pattern of significant lags matters more than individual spikes\n5. Lag 15 with $n=100$ has high variance: $\\text{SE} \\approx 1/\\sqrt{100} = 0.1$, and only ~85 pairs contribute\n\n**Confidence band:** $\\pm 1.96/\\sqrt{100} = \\pm 0.196$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Treating every significant lag as meaningful. With many lags, false positives occur. Use sequential testing corrections or focus on meaningful patterns, not isolated spikes.\n</div>\n</div>\n</details>\n\n## References\n\n1. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis: Forecasting and Control*. Wiley. Chapter 2.\n2. Shumway, R. H., & Stoffer, D. S. (2017). *Time Series Analysis and Its Applications*. Springer. Chapter 3.\n3. Brockwell, P. J., & Davis, R. A. (2016). *Introduction to Time Series and Forecasting*. Springer. Chapters 2-3.\n4. Bartlett, M. S. (1946). On the theoretical specification and sampling properties of autocorrelated time-series. *JRSS B*, 8(1), 27-41.\n", "sections": [{"heading": "Autocorrelation and Partial Autocorrelation", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> ACF measures correlation between a series and its lagged values. PACF measures correlation at lag $k$ after removing effects of intermediate lags. ACF/PACF patterns identify AR/MA orders: AR(p) has PACF cutoff at lag p; MA(q) has ACF cutoff at lag q. Sample ACF/PACF have approximate standard error $1/\\sqrt{n}$ under white noise.\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**Autocovariance Function (ACVF)**: For a stationary process with mean $\\mu$:\n$$\\gamma(h) = \\text{Cov}(X_t, X_{t+h}) = E[(X_t - \\mu)(X_{t+h} - \\mu)]$$\n\n**Autocorrelation Function (ACF)**: Normalized autocovariance:\n$$\\rho(h) = \\frac{\\gamma(h)}{\\gamma(0)} = \\text{Corr}(X_t, X_{t+h})$$\n\n**Partial Autocorrelation Function (PACF)**: Correlation between $X_t$ and $X_{t+h}$ after removing the linear dependence on $X_{t+1}, \\ldots, X_{t+h-1}$:\n$$\\phi_{hh} = \\text{Corr}(X_t - \\hat{X}_t, X_{t+h} - \\hat{X}_{t+h})$$\n\nwhere $\\hat{X}_t$ and $\\hat{X}_{t+h}$ are best linear predictors from intermediate values.\n\n**Sample ACF**: Estimated from data:\n$$\\hat{\\rho}(h) = \\frac{\\hat{\\gamma}(h)}{\\hat{\\gamma}(0)} = \\frac{\\sum_{t=1}^{n-h}(X_t - \\bar{X})(X_{t+h} - \\bar{X})}{\\sum_{t=1}^{n}(X_t - \\bar{X})^2}$$", "line_start": 7, "level": 2}, {"heading": "Properties of ACF", "content": "For any stationary process:\n\n1. $\\rho(0) = 1$\n2. $\\rho(h) = \\rho(-h)$ (symmetry)\n3. $|\\rho(h)| \\leq 1$ for all $h$\n4. $\\rho(h)$ is positive semi-definite: for any $a_1, \\ldots, a_n$:\n   $$\\sum_{i=1}^{n}\\sum_{j=1}^{n} a_i a_j \\rho(i-j) \\geq 0$$", "line_start": 25, "level": 3}, {"heading": "PACF via Yule-Walker Equations", "content": "The PACF at lag $k$ is the last coefficient $\\phi_{kk}$ in the AR(k) regression:\n$$X_t = \\phi_{k1}X_{t-1} + \\phi_{k2}X_{t-2} + \\cdots + \\phi_{kk}X_{t-k} + \\epsilon_t$$\n\nYule-Walker equations in matrix form:\n$$\\begin{pmatrix} 1 & \\rho(1) & \\cdots & \\rho(k-1) \\\\ \\rho(1) & 1 & \\cdots & \\rho(k-2) \\\\ \\vdots & & \\ddots & \\vdots \\\\ \\rho(k-1) & \\cdots & \\rho(1) & 1 \\end{pmatrix} \\begin{pmatrix} \\phi_{k1} \\\\ \\phi_{k2} \\\\ \\vdots \\\\ \\phi_{kk} \\end{pmatrix} = \\begin{pmatrix} \\rho(1) \\\\ \\rho(2) \\\\ \\vdots \\\\ \\rho(k) \\end{pmatrix}$$", "line_start": 35, "level": 3}, {"heading": "ACF of AR(1): $X_t = \\phi X_{t-1} + \\epsilon_t$", "content": "$$\\rho(h) = \\phi^{|h|}$$\n\nACF decays exponentially (geometrically) for $|\\phi| < 1$.", "line_start": 43, "level": 3}, {"heading": "ACF of MA(1): $X_t = \\epsilon_t + \\theta\\epsilon_{t-1}$", "content": "$$\\rho(1) = \\frac{\\theta}{1+\\theta^2}, \\quad \\rho(h) = 0 \\text{ for } h > 1$$\n\nACF cuts off after lag 1.", "line_start": 49, "level": 3}, {"heading": "ACF of MA(q)", "content": "$$\\rho(h) = 0 \\text{ for } h > q$$", "line_start": 55, "level": 3}, {"heading": "PACF of AR(p)", "content": "$$\\phi_{hh} = 0 \\text{ for } h > p$$", "line_start": 59, "level": 3}, {"heading": "Variance of Sample ACF", "content": "Under the null hypothesis that the true process is white noise:\n$$\\text{Var}(\\hat{\\rho}(h)) \\approx \\frac{1}{n}$$\n\nSo approximate 95% confidence bands are $\\pm 1.96/\\sqrt{n}$.\n\n**Bartlett's formula** (for MA(q) process):\n$$\\text{Var}(\\hat{\\rho}(h)) \\approx \\frac{1}{n}\\left(1 + 2\\sum_{k=1}^{q}\\rho(k)^2\\right) \\text{ for } h > q$$", "line_start": 63, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**Using ACF/PACF for Model Identification:**\n\n| Pattern | ACF | PACF | Model |\n|---------|-----|------|-------|\n| AR(p) | Exponential/sinusoidal decay | Cuts off after lag p | AR(p) |\n| MA(q) | Cuts off after lag q | Exponential/sinusoidal decay | MA(q) |\n| ARMA(p,q) | Tails off | Tails off | ARMA(p,q) |\n| White noise | All near zero | All near zero | No model needed |\n| Non-stationary | Very slow decay | Large spike at lag 1 | Difference first |\n\n**Interpretation procedure:**\n\n```\n1. Plot series - check for stationarity\n2. If non-stationary, difference until stationary\n3. Compute sample ACF and PACF\n4. Check for significant spikes (outside ±1.96/√n bands)\n5. Identify patterns:\n   - ACF cuts off, PACF decays → MA(q) where q = cutoff lag\n   - PACF cuts off, ACF decays → AR(p) where p = cutoff lag\n   - Both decay → ARMA (use information criteria)\n6. Fit candidate models\n7. Check residual ACF/PACF (should be white noise)\n```", "line_start": 73, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Ignoring confidence bands**: Not all spikes are significant. Use $\\pm 1.96/\\sqrt{n}$ bands and expect ~5% of spikes to exceed by chance.\n\n2. **Confusing \"cuts off\" vs \"tails off\"**: Cutoff means abrupt drop to zero after lag q. Tails off means gradual decay. This distinction determines AR vs MA.\n\n3. **Applying ACF/PACF to non-stationary data**: Results are meaningless for non-stationary series. Always check stationarity first.\n\n4. **Over-interpreting high-lag correlations**: For small samples, high-lag estimates have high variance. Focus on early lags.\n\n5. **Forgetting seasonal lags**: In seasonal data, check lags at seasonal period (e.g., lag 12 for monthly data with annual seasonality).\n\n6. **Neglecting theoretical ACF/PACF**: When validating models, compare sample functions to theoretical ones, not just residuals.", "line_start": 100, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.arima_process import ArmaProcess", "line_start": 114, "level": 1}, {"heading": "Simulate AR(2) process", "content": "np.random.seed(42)\nar_params = np.array([1, -0.75, 0.25])  # 1 - 0.75L + 0.25L^2\nma_params = np.array([1])\nar2_process = ArmaProcess(ar_params, ma_params)\nar2_data = ar2_process.generate_sample(nsample=300)", "line_start": 122, "level": 1}, {"heading": "Plot ACF and PACF", "content": "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\nplot_acf(ar2_data, ax=axes[0], lags=20, title='ACF of AR(2)')\nplot_pacf(ar2_data, ax=axes[1], lags=20, title='PACF of AR(2)')\nplt.tight_layout()\nplt.show()", "line_start": 129, "level": 1}, {"heading": "Check significant PACF values", "content": "from statsmodels.tsa.stattools import pacf\npacf_values = pacf(ar2_data, nlags=5)\nprint(\"PACF values:\", pacf_values)", "line_start": 137, "level": 1}, {"heading": "Expect: significant at lags 1,2; near zero after", "content": "```", "line_start": 141, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What is the fundamental difference between ACF and PACF? Why do we need both?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> ACF measures total correlation between $X_t$ and $X_{t+h}$, including indirect effects through intermediate lags. PACF measures direct correlation after removing intermediate effects.\n\n<strong>Explanation:</strong> Consider AR(1) with $\\phi = 0.8$. The ACF shows $\\rho(2) = 0.64$ because $X_t$ and $X_{t+2}$ are correlated through $X_{t+1}$. But the PACF at lag 2 is near zero because once we account for $X_{t+1}$, there's no additional direct relationship.\n\nWe need both because:\n- ACF identifies MA order (cuts off at lag q)\n- PACF identifies AR order (cuts off at lag p)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using only ACF for identification. Without PACF, you cannot distinguish AR from MA patterns—both can show decaying ACF, but only AR shows PACF cutoff.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> How would you interpret an ACF that shows a very slow decay with values staying significant past lag 20?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> This strongly suggests non-stationarity. A stationary process should have ACF that decays relatively quickly to zero. Very slow decay indicates a unit root or near-unit root.\n\n<strong>Recommended action:</strong>\n1. Formally test with ADF/KPSS\n2. Difference the series\n3. Re-compute ACF after differencing\n4. The differenced series should show faster decay\n\n**Key insight:** For a random walk, $\\rho(h) \\approx 1$ for all $h$ in finite samples because successive values are highly dependent.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Trying to fit ARMA models to non-stationary data. The resulting parameters will be misleading, and forecasts will be poor. Always ensure stationarity first.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Derive the ACF for MA(1): $X_t = \\epsilon_t + \\theta\\epsilon_{t-1}$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> $\\rho(0) = 1$, $\\rho(1) = \\frac{\\theta}{1+\\theta^2}$, $\\rho(h) = 0$ for $h \\geq 2$.\n\n<strong>Derivation:</strong>\n\n**Variance (lag 0):**\n$$\\gamma(0) = \\text{Var}(X_t) = \\text{Var}(\\epsilon_t + \\theta\\epsilon_{t-1}) = \\sigma^2 + \\theta^2\\sigma^2 = (1+\\theta^2)\\sigma^2$$\n\n**Autocovariance at lag 1:**\n$$\\gamma(1) = \\text{Cov}(X_t, X_{t+1}) = \\text{Cov}(\\epsilon_t + \\theta\\epsilon_{t-1}, \\epsilon_{t+1} + \\theta\\epsilon_t)$$\n$$= \\text{Cov}(\\epsilon_t, \\theta\\epsilon_t) = \\theta\\sigma^2$$\n\n**Autocovariance at lag $h \\geq 2$:**\n$$\\gamma(h) = \\text{Cov}(\\epsilon_t + \\theta\\epsilon_{t-1}, \\epsilon_{t+h} + \\theta\\epsilon_{t+h-1}) = 0$$\n\n(No overlapping $\\epsilon$ terms when $h \\geq 2$)\n\n**ACF:**\n$$\\rho(1) = \\frac{\\gamma(1)}{\\gamma(0)} = \\frac{\\theta\\sigma^2}{(1+\\theta^2)\\sigma^2} = \\frac{\\theta}{1+\\theta^2}$$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Note that $|\\rho(1)| \\leq 0.5$ for MA(1). The maximum occurs at $\\theta = \\pm 1$. If you observe $|\\hat{\\rho}(1)| > 0.5$, it might be AR, not MA.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> For an AR(2) process $X_t = \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + \\epsilon_t$, show that the PACF is zero for all lags $h > 2$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> For AR(p), the PACF $\\phi_{hh} = 0$ for $h > p$ because once $X_{t-1}, \\ldots, X_{t-p}$ are included, adding more lags provides no additional predictive information.\n\n<strong>Explanation:</strong>\n\nThe AR(2) model states that $X_t$ depends only on $X_{t-1}$ and $X_{t-2}$ (plus noise). Therefore:\n\nFor $h = 3$: We regress $X_t$ on $X_{t-1}, X_{t-2}, X_{t-3}$.\n- $X_t = \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + \\epsilon_t$\n- $X_{t-3}$ only affects $X_t$ through $X_{t-2}$ and $X_{t-1}$\n- After conditioning on $X_{t-1}$ and $X_{t-2}$, $X_{t-3}$ adds no information\n- Thus $\\phi_{33} = 0$\n\nBy induction, this holds for all $h > 2$.\n\n**Key equation:** PACF at lag $k$ is the coefficient $\\phi_{kk}$ in the best linear predictor using exactly $k$ lags. For AR(p), the $k$-th coefficient becomes zero when $k > p$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Expecting exact zeros in sample PACF. Due to sampling variability, $\\hat{\\phi}_{hh}$ will be nonzero but should fall within confidence bands for $h > p$.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You compute the sample ACF for a time series with $n=100$ observations. You see that lags 1, 2, 7, and 15 exceed the 95% confidence bands. What is your interpretation?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> With 95% bands and ~15-20 lags tested, expect about 1 spurious significant lag by chance. Lags 1 and 2 are likely real signal. Lag 7 might be real (check for weekly patterns if relevant). Lag 15 is likely spurious.\n\n<strong>Interpretation process:</strong>\n1. Focus on early lags (1, 2, 3) — most likely real\n2. Consider domain knowledge (lag 7 = weekly? lag 12 = monthly?)\n3. Isolated high-lag spikes are often noise\n4. Pattern of significant lags matters more than individual spikes\n5. Lag 15 with $n=100$ has high variance: $\\text{SE} \\approx 1/\\sqrt{100} = 0.1$, and only ~85 pairs contribute\n\n**Confidence band:** $\\pm 1.96/\\sqrt{100} = \\pm 0.196$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Treating every significant lag as meaningful. With many lags, false positives occur. Use sequential testing corrections or focus on meaningful patterns, not isolated spikes.\n</div>\n</div>\n</details>", "line_start": 144, "level": 2}, {"heading": "References", "content": "1. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis: Forecasting and Control*. Wiley. Chapter 2.\n2. Shumway, R. H., & Stoffer, D. S. (2017). *Time Series Analysis and Its Applications*. Springer. Chapter 3.\n3. Brockwell, P. J., & Davis, R. A. (2016). *Introduction to Time Series and Forecasting*. Springer. Chapters 2-3.\n4. Bartlett, M. S. (1946). On the theoretical specification and sampling properties of autocorrelated time-series. *JRSS B*, 8(1), 27-41.", "line_start": 260, "level": 1}]}, "docs/en/foundations/stationarity.md": {"path": "docs/en/foundations/stationarity.md", "title": "Stationarity", "content": "# Stationarity\n\n<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Stationarity is the foundational assumption for most classical time series models. A stationary process has constant mean, constant variance, and autocovariance that depends only on lag, not time. Weak (covariance) stationarity is usually sufficient. Test with ADF, KPSS, or PP tests. Non-stationary series can often be made stationary through differencing.\n</div>\n\n## Core Definitions\n\n**Strict (Strong) Stationarity**: A process $\\{X_t\\}$ is strictly stationary if the joint distribution of $(X_{t_1}, X_{t_2}, \\ldots, X_{t_k})$ is identical to $(X_{t_1+h}, X_{t_2+h}, \\ldots, X_{t_k+h})$ for all $k$, all time points $t_1, \\ldots, t_k$, and all shifts $h$.\n\n**Weak (Covariance/Second-Order) Stationarity**: A process is weakly stationary if:\n\n1. $E[X_t] = \\mu$ (constant mean, finite)\n2. $\\text{Var}(X_t) = \\sigma^2 < \\infty$ (constant variance, finite)\n3. $\\text{Cov}(X_t, X_{t+h}) = \\gamma(h)$ (autocovariance depends only on lag $h$)\n\n**Ergodicity** (high-level): An ergodic process allows time averages to converge to ensemble averages. This justifies estimating population parameters from a single realization. Most stationary processes encountered in practice are ergodic.\n\n**Trend Stationarity vs. Difference Stationarity**:\n\n- **Trend stationary**: $X_t = \\mu_t + Y_t$ where $Y_t$ is stationary; remove trend by regression\n- **Difference stationary**: $\\Delta X_t = X_t - X_{t-1}$ is stationary; remove unit root by differencing\n\n## Math and Derivations\n\n### Autocovariance Function\n\nFor a weakly stationary process:\n\n$$\\gamma(h) = \\text{Cov}(X_t, X_{t+h}) = E[(X_t - \\mu)(X_{t+h} - \\mu)]$$\n\nProperties:\n- $\\gamma(0) = \\text{Var}(X_t) = \\sigma^2$\n- $\\gamma(h) = \\gamma(-h)$ (symmetry)\n- $|\\gamma(h)| \\leq \\gamma(0)$ (Cauchy-Schwarz)\n\n### Autocorrelation Function (ACF)\n\n$$\\rho(h) = \\frac{\\gamma(h)}{\\gamma(0)} = \\frac{\\text{Cov}(X_t, X_{t+h})}{\\text{Var}(X_t)}$$\n\nProperties:\n- $\\rho(0) = 1$\n- $|\\rho(h)| \\leq 1$\n- $\\rho(h) = \\rho(-h)$\n\n### Unit Root and Integration\n\nA process has a unit root if $(1-L)X_t$ is stationary where $L$ is the lag operator ($LX_t = X_{t-1}$).\n\n**Random Walk** (unit root example):\n$$X_t = X_{t-1} + \\epsilon_t$$\n\nThis is non-stationary: $\\text{Var}(X_t) = t\\sigma^2_\\epsilon \\to \\infty$.\n\nAfter differencing: $\\Delta X_t = \\epsilon_t$ which is stationary.\n\n### Augmented Dickey-Fuller (ADF) Test\n\nTests for unit root. Model:\n$$\\Delta X_t = \\alpha + \\beta t + \\gamma X_{t-1} + \\sum_{i=1}^{p} \\delta_i \\Delta X_{t-i} + \\epsilon_t$$\n\n- $H_0$: $\\gamma = 0$ (unit root exists, non-stationary)\n- $H_1$: $\\gamma < 0$ (no unit root, stationary)\n\n**KPSS Test** (complementary):\n- $H_0$: Series is stationary\n- $H_1$: Series has a unit root\n\nUse both ADF and KPSS for robust conclusions.\n\n## Algorithm/Model Sketch\n\n**Testing for Stationarity:**\n\n```\n1. Visual inspection: plot series, look for trends/changing variance\n2. ACF plot: stationary series have ACF that decays to zero\n3. ADF test: reject H0 → stationary\n4. KPSS test: fail to reject H0 → stationary\n5. If non-stationary:\n   - Try differencing (for unit root)\n   - Try detrending (for trend stationarity)\n   - Check if seasonal differencing needed\n```\n\n**Making a Series Stationary:**\n\n| Symptom | Solution |\n|---------|----------|\n| Trend (linear) | First difference or detrend |\n| Trend (quadratic) | Second difference |\n| Seasonality | Seasonal difference |\n| Changing variance | Log transform, then difference |\n| Both trend and seasonality | Combine transformations |\n\n## Common Pitfalls\n\n1. **Confusing strict and weak stationarity**: Weak is usually sufficient for ARIMA modeling. Strict is rarely tested directly.\n\n2. **Over-differencing**: Differencing a stationary series introduces unnecessary MA structure. Check ACF—if it's already decaying, don't difference.\n\n3. **Ignoring structural breaks**: A series with a structural break may appear non-stationary but differencing won't help. Consider regime-switching models.\n\n4. **Misinterpreting ADF p-values**: ADF tests for unit root, not stationarity. Low p-value rejects unit root (suggests stationarity). Also use KPSS for confirmation.\n\n5. **Neglecting variance stationarity**: A series can have constant mean but changing variance (heteroskedasticity). Consider GARCH models or transformations.\n\n6. **Seasonal unit roots**: Standard ADF doesn't detect seasonal unit roots. Use HEGY test or seasonal differencing.\n\n## Mini Example\n\n```python\nimport numpy as np\nfrom statsmodels.tsa.stattools import adfuller, kpss\n\n# Generate random walk (non-stationary)\nnp.random.seed(42)\nrandom_walk = np.cumsum(np.random.randn(200))\n\n# Generate stationary AR(1)\nar1 = np.zeros(200)\nfor t in range(1, 200):\n    ar1[t] = 0.7 * ar1[t-1] + np.random.randn()\n\n# ADF test\nadf_rw = adfuller(random_walk)\nprint(f\"Random Walk ADF p-value: {adf_rw[1]:.4f}\")  # High → non-stationary\n\nadf_ar = adfuller(ar1)\nprint(f\"AR(1) ADF p-value: {adf_ar[1]:.4f}\")  # Low → stationary\n\n# First difference of random walk\ndiff_rw = np.diff(random_walk)\nadf_diff = adfuller(diff_rw)\nprint(f\"Differenced RW ADF p-value: {adf_diff[1]:.4f}\")  # Low → stationary\n```\n\n## Quiz\n\n<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What is the difference between strict and weak stationarity? When is weak stationarity sufficient?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Strict stationarity requires the entire joint distribution to be invariant to time shifts. Weak stationarity only requires constant mean, constant variance, and autocovariance depending only on lag.\n\n<strong>Explanation:</strong> Weak stationarity is sufficient for ARIMA-type models because these models only use first and second moments (mean and covariances). The full distributional properties aren't needed for parameter estimation or forecasting.\n\n**Key point:** If a process is strictly stationary with finite second moments, it is also weakly stationary. The reverse is not always true.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Assuming weak stationarity implies Gaussianity. A weakly stationary process can have any marginal distribution—it only constrains the first two moments.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Explain the difference between trend stationarity and difference stationarity. How do you handle each?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Trend stationary: the series has a deterministic trend; subtract the trend to get a stationary residual. Difference stationary: the series has a stochastic trend (unit root); differencing removes the non-stationarity.\n\n<strong>Explanation:</strong>\n- Trend stationary: $X_t = \\alpha + \\beta t + Y_t$ where $Y_t$ is stationary. Fit trend and subtract.\n- Difference stationary: $X_t = X_{t-1} + \\epsilon_t$. First difference: $\\Delta X_t = \\epsilon_t$.\n\nApplying the wrong transformation is inefficient—differencing a trend-stationary series adds MA(1) structure; detrending a difference-stationary series leaves autocorrelation.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using differencing for everything. Always plot the series and consider whether the trend looks deterministic or stochastic.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Show that for a random walk $X_t = X_{t-1} + \\epsilon_t$, the variance grows linearly with time.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> $\\text{Var}(X_t) = t \\cdot \\sigma^2_\\epsilon$\n\n<strong>Derivation:</strong>\nStarting from $X_0 = 0$:\n$$X_t = \\sum_{i=1}^{t} \\epsilon_i$$\n\nSince $\\epsilon_i$ are independent with variance $\\sigma^2_\\epsilon$:\n$$\\text{Var}(X_t) = \\text{Var}\\left(\\sum_{i=1}^{t} \\epsilon_i\\right) = \\sum_{i=1}^{t} \\text{Var}(\\epsilon_i) = t \\cdot \\sigma^2_\\epsilon$$\n\nThis shows variance grows without bound, violating weak stationarity.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Forgetting that cumulative sums of stationary processes are generally non-stationary. Integration (summation) and differentiation have opposite effects on stationarity.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Derive the autocorrelation function for an AR(1) process $X_t = \\phi X_{t-1} + \\epsilon_t$ where $|\\phi| < 1$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> $\\rho(h) = \\phi^{|h|}$\n\n<strong>Derivation:</strong>\nFor stationarity, multiply both sides by $X_{t-h}$ and take expectations:\n$$E[X_t X_{t-h}] = \\phi E[X_{t-1} X_{t-h}] + E[\\epsilon_t X_{t-h}]$$\n\nSince $\\epsilon_t$ is uncorrelated with past values:\n$$\\gamma(h) = \\phi \\gamma(h-1)$$ for $h \\geq 1$\n\nThis is a first-order recurrence with solution:\n$$\\gamma(h) = \\phi^h \\gamma(0)$$\n\nTherefore:\n$$\\rho(h) = \\frac{\\gamma(h)}{\\gamma(0)} = \\phi^h$$\n\nFor $h < 0$, use symmetry: $\\rho(h) = \\phi^{|h|}$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Forgetting the stationarity condition $|\\phi| < 1$. If $|\\phi| \\geq 1$, the process explodes and has no finite variance.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You run an ADF test and get p-value = 0.08, and a KPSS test with p-value = 0.03. What do you conclude? What should you do?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The results are contradictory. ADF suggests possible stationarity (p = 0.08 is borderline), but KPSS rejects stationarity (p = 0.03). This often indicates the series is near-unit-root or has a structural break.\n\n<strong>Recommended actions:</strong>\n1. Plot the series and ACF to visually inspect\n2. Try differencing and re-test\n3. Check for structural breaks (Chow test, CUSUM)\n4. Consider that the series may be fractionally integrated\n5. Use domain knowledge—is non-stationarity expected?\n\n**Key equation:** For KPSS, low p-value rejects stationarity. For ADF, low p-value rejects unit root (supports stationarity).\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Relying on a single test. ADF has low power against near-unit-root alternatives. KPSS can reject stationarity due to serial correlation. Always use multiple approaches and visual inspection.\n</div>\n</div>\n</details>\n\n## References\n\n1. Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press. Chapters 3, 17.\n2. Brockwell, P. J., & Davis, R. A. (2016). *Introduction to Time Series and Forecasting*. Springer. Chapter 1.\n3. Dickey, D. A., & Fuller, W. A. (1979). Distribution of the estimators for autoregressive time series with a unit root. *JASA*, 74(366), 427-431.\n4. Kwiatkowski, D., Phillips, P. C., Schmidt, P., & Shin, Y. (1992). Testing the null hypothesis of stationarity. *Journal of Econometrics*, 54(1-3), 159-178.\n", "sections": [{"heading": "Stationarity", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Stationarity is the foundational assumption for most classical time series models. A stationary process has constant mean, constant variance, and autocovariance that depends only on lag, not time. Weak (covariance) stationarity is usually sufficient. Test with ADF, KPSS, or PP tests. Non-stationary series can often be made stationary through differencing.\n</div>", "line_start": 1, "level": 2}, {"heading": "Core Definitions", "content": "**Strict (Strong) Stationarity**: A process $\\{X_t\\}$ is strictly stationary if the joint distribution of $(X_{t_1}, X_{t_2}, \\ldots, X_{t_k})$ is identical to $(X_{t_1+h}, X_{t_2+h}, \\ldots, X_{t_k+h})$ for all $k$, all time points $t_1, \\ldots, t_k$, and all shifts $h$.\n\n**Weak (Covariance/Second-Order) Stationarity**: A process is weakly stationary if:\n\n1. $E[X_t] = \\mu$ (constant mean, finite)\n2. $\\text{Var}(X_t) = \\sigma^2 < \\infty$ (constant variance, finite)\n3. $\\text{Cov}(X_t, X_{t+h}) = \\gamma(h)$ (autocovariance depends only on lag $h$)\n\n**Ergodicity** (high-level): An ergodic process allows time averages to converge to ensemble averages. This justifies estimating population parameters from a single realization. Most stationary processes encountered in practice are ergodic.\n\n**Trend Stationarity vs. Difference Stationarity**:\n\n- **Trend stationary**: $X_t = \\mu_t + Y_t$ where $Y_t$ is stationary; remove trend by regression\n- **Difference stationary**: $\\Delta X_t = X_t - X_{t-1}$ is stationary; remove unit root by differencing", "line_start": 7, "level": 2}, {"heading": "Autocovariance Function", "content": "For a weakly stationary process:\n\n$$\\gamma(h) = \\text{Cov}(X_t, X_{t+h}) = E[(X_t - \\mu)(X_{t+h} - \\mu)]$$\n\nProperties:\n- $\\gamma(0) = \\text{Var}(X_t) = \\sigma^2$\n- $\\gamma(h) = \\gamma(-h)$ (symmetry)\n- $|\\gamma(h)| \\leq \\gamma(0)$ (Cauchy-Schwarz)", "line_start": 26, "level": 3}, {"heading": "Autocorrelation Function (ACF)", "content": "$$\\rho(h) = \\frac{\\gamma(h)}{\\gamma(0)} = \\frac{\\text{Cov}(X_t, X_{t+h})}{\\text{Var}(X_t)}$$\n\nProperties:\n- $\\rho(0) = 1$\n- $|\\rho(h)| \\leq 1$\n- $\\rho(h) = \\rho(-h)$", "line_start": 37, "level": 3}, {"heading": "Unit Root and Integration", "content": "A process has a unit root if $(1-L)X_t$ is stationary where $L$ is the lag operator ($LX_t = X_{t-1}$).\n\n**Random Walk** (unit root example):\n$$X_t = X_{t-1} + \\epsilon_t$$\n\nThis is non-stationary: $\\text{Var}(X_t) = t\\sigma^2_\\epsilon \\to \\infty$.\n\nAfter differencing: $\\Delta X_t = \\epsilon_t$ which is stationary.", "line_start": 46, "level": 3}, {"heading": "Augmented Dickey-Fuller (ADF) Test", "content": "Tests for unit root. Model:\n$$\\Delta X_t = \\alpha + \\beta t + \\gamma X_{t-1} + \\sum_{i=1}^{p} \\delta_i \\Delta X_{t-i} + \\epsilon_t$$\n\n- $H_0$: $\\gamma = 0$ (unit root exists, non-stationary)\n- $H_1$: $\\gamma < 0$ (no unit root, stationary)\n\n**KPSS Test** (complementary):\n- $H_0$: Series is stationary\n- $H_1$: Series has a unit root\n\nUse both ADF and KPSS for robust conclusions.", "line_start": 57, "level": 2}, {"heading": "Algorithm/Model Sketch", "content": "**Testing for Stationarity:**\n\n```\n1. Visual inspection: plot series, look for trends/changing variance\n2. ACF plot: stationary series have ACF that decays to zero\n3. ADF test: reject H0 → stationary\n4. KPSS test: fail to reject H0 → stationary\n5. If non-stationary:\n   - Try differencing (for unit root)\n   - Try detrending (for trend stationarity)\n   - Check if seasonal differencing needed\n```\n\n**Making a Series Stationary:**\n\n| Symptom | Solution |\n|---------|----------|\n| Trend (linear) | First difference or detrend |\n| Trend (quadratic) | Second difference |\n| Seasonality | Seasonal difference |\n| Changing variance | Log transform, then difference |\n| Both trend and seasonality | Combine transformations |", "line_start": 71, "level": 2}, {"heading": "Common Pitfalls", "content": "1. **Confusing strict and weak stationarity**: Weak is usually sufficient for ARIMA modeling. Strict is rarely tested directly.\n\n2. **Over-differencing**: Differencing a stationary series introduces unnecessary MA structure. Check ACF—if it's already decaying, don't difference.\n\n3. **Ignoring structural breaks**: A series with a structural break may appear non-stationary but differencing won't help. Consider regime-switching models.\n\n4. **Misinterpreting ADF p-values**: ADF tests for unit root, not stationarity. Low p-value rejects unit root (suggests stationarity). Also use KPSS for confirmation.\n\n5. **Neglecting variance stationarity**: A series can have constant mean but changing variance (heteroskedasticity). Consider GARCH models or transformations.\n\n6. **Seasonal unit roots**: Standard ADF doesn't detect seasonal unit roots. Use HEGY test or seasonal differencing.", "line_start": 96, "level": 2}, {"heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.stattools import adfuller, kpss", "line_start": 110, "level": 1}, {"heading": "Generate random walk (non-stationary)", "content": "np.random.seed(42)\nrandom_walk = np.cumsum(np.random.randn(200))", "line_start": 116, "level": 1}, {"heading": "Generate stationary AR(1)", "content": "ar1 = np.zeros(200)\nfor t in range(1, 200):\n    ar1[t] = 0.7 * ar1[t-1] + np.random.randn()", "line_start": 120, "level": 1}, {"heading": "ADF test", "content": "adf_rw = adfuller(random_walk)\nprint(f\"Random Walk ADF p-value: {adf_rw[1]:.4f}\")  # High → non-stationary\n\nadf_ar = adfuller(ar1)\nprint(f\"AR(1) ADF p-value: {adf_ar[1]:.4f}\")  # Low → stationary", "line_start": 125, "level": 1}, {"heading": "First difference of random walk", "content": "diff_rw = np.diff(random_walk)\nadf_diff = adfuller(diff_rw)\nprint(f\"Differenced RW ADF p-value: {adf_diff[1]:.4f}\")  # Low → stationary\n```", "line_start": 132, "level": 2}, {"heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What is the difference between strict and weak stationarity? When is weak stationarity sufficient?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Strict stationarity requires the entire joint distribution to be invariant to time shifts. Weak stationarity only requires constant mean, constant variance, and autocovariance depending only on lag.\n\n<strong>Explanation:</strong> Weak stationarity is sufficient for ARIMA-type models because these models only use first and second moments (mean and covariances). The full distributional properties aren't needed for parameter estimation or forecasting.\n\n**Key point:** If a process is strictly stationary with finite second moments, it is also weakly stationary. The reverse is not always true.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Assuming weak stationarity implies Gaussianity. A weakly stationary process can have any marginal distribution—it only constrains the first two moments.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Explain the difference between trend stationarity and difference stationarity. How do you handle each?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Trend stationary: the series has a deterministic trend; subtract the trend to get a stationary residual. Difference stationary: the series has a stochastic trend (unit root); differencing removes the non-stationarity.\n\n<strong>Explanation:</strong>\n- Trend stationary: $X_t = \\alpha + \\beta t + Y_t$ where $Y_t$ is stationary. Fit trend and subtract.\n- Difference stationary: $X_t = X_{t-1} + \\epsilon_t$. First difference: $\\Delta X_t = \\epsilon_t$.\n\nApplying the wrong transformation is inefficient—differencing a trend-stationary series adds MA(1) structure; detrending a difference-stationary series leaves autocorrelation.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using differencing for everything. Always plot the series and consider whether the trend looks deterministic or stochastic.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Show that for a random walk $X_t = X_{t-1} + \\epsilon_t$, the variance grows linearly with time.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> $\\text{Var}(X_t) = t \\cdot \\sigma^2_\\epsilon$\n\n<strong>Derivation:</strong>\nStarting from $X_0 = 0$:\n$$X_t = \\sum_{i=1}^{t} \\epsilon_i$$\n\nSince $\\epsilon_i$ are independent with variance $\\sigma^2_\\epsilon$:\n$$\\text{Var}(X_t) = \\text{Var}\\left(\\sum_{i=1}^{t} \\epsilon_i\\right) = \\sum_{i=1}^{t} \\text{Var}(\\epsilon_i) = t \\cdot \\sigma^2_\\epsilon$$\n\nThis shows variance grows without bound, violating weak stationarity.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Forgetting that cumulative sums of stationary processes are generally non-stationary. Integration (summation) and differentiation have opposite effects on stationarity.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Derive the autocorrelation function for an AR(1) process $X_t = \\phi X_{t-1} + \\epsilon_t$ where $|\\phi| < 1$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> $\\rho(h) = \\phi^{|h|}$\n\n<strong>Derivation:</strong>\nFor stationarity, multiply both sides by $X_{t-h}$ and take expectations:\n$$E[X_t X_{t-h}] = \\phi E[X_{t-1} X_{t-h}] + E[\\epsilon_t X_{t-h}]$$\n\nSince $\\epsilon_t$ is uncorrelated with past values:\n$$\\gamma(h) = \\phi \\gamma(h-1)$$ for $h \\geq 1$\n\nThis is a first-order recurrence with solution:\n$$\\gamma(h) = \\phi^h \\gamma(0)$$\n\nTherefore:\n$$\\rho(h) = \\frac{\\gamma(h)}{\\gamma(0)} = \\phi^h$$\n\nFor $h < 0$, use symmetry: $\\rho(h) = \\phi^{|h|}$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Forgetting the stationarity condition $|\\phi| < 1$. If $|\\phi| \\geq 1$, the process explodes and has no finite variance.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You run an ADF test and get p-value = 0.08, and a KPSS test with p-value = 0.03. What do you conclude? What should you do?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The results are contradictory. ADF suggests possible stationarity (p = 0.08 is borderline), but KPSS rejects stationarity (p = 0.03). This often indicates the series is near-unit-root or has a structural break.\n\n<strong>Recommended actions:</strong>\n1. Plot the series and ACF to visually inspect\n2. Try differencing and re-test\n3. Check for structural breaks (Chow test, CUSUM)\n4. Consider that the series may be fractionally integrated\n5. Use domain knowledge—is non-stationarity expected?\n\n**Key equation:** For KPSS, low p-value rejects stationarity. For ADF, low p-value rejects unit root (supports stationarity).\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Relying on a single test. ADF has low power against near-unit-root alternatives. KPSS can reject stationarity due to serial correlation. Always use multiple approaches and visual inspection.\n</div>\n</div>\n</details>", "line_start": 138, "level": 2}, {"heading": "References", "content": "1. Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press. Chapters 3, 17.\n2. Brockwell, P. J., & Davis, R. A. (2016). *Introduction to Time Series and Forecasting*. Springer. Chapter 1.\n3. Dickey, D. A., & Fuller, W. A. (1979). Distribution of the estimators for autoregressive time series with a unit root. *JASA*, 74(366), 427-431.\n4. Kwiatkowski, D., Phillips, P. C., Schmidt, P., & Shin, Y. (1992). Testing the null hypothesis of stationarity. *Journal of Econometrics*, 54(1-3), 159-178.", "line_start": 243, "level": 1}]}}, "sections": {"docs/en/index.md#section_0": {"doc_id": "docs/en/index.md", "heading": "Time Series Study Notes", "content": "<div class=\"interview-summary\">\n<strong>Welcome to the Time Series Study Notes</strong> — a comprehensive, bilingual (English/中文) resource for learning time series analysis and forecasting, designed for interview preparation and practical application.\n</div>", "line_start": 1}, "docs/en/index.md#section_1": {"doc_id": "docs/en/index.md", "heading": "What's Included", "content": "This knowledge base covers:\n\n- **Foundations**: Stationarity, autocorrelation, partial autocorrelation\n- **Time Domain Models**: AR, MA, ARMA, ARIMA, SARIMA with identification and estimation\n- **Exponential Smoothing**: SES, Holt, Holt-Winters, ETS framework\n- **Decomposition**: STL, classical decomposition, handling seasonality\n- **Forecasting**: Prediction intervals, multi-step strategies, rolling evaluation\n- **Model Selection**: AIC/BIC, cross-validation for time series, residual diagnostics\n- **Spectral Analysis**: Periodogram, frequency domain basics\n- **State Space Models**: Kalman filter, local level and trend models\n- **Multivariate TS**: VAR, VARMA, Granger causality\n- **Change Detection**: Change-point and anomaly detection methods\n- **Feature Engineering**: Classical pipelines, scaling, missing data handling\n- **Deep Learning**: RNN/LSTM/TCN, Transformers for time series\n- **Practical Modeling**: Backtesting, deployment, common pitfalls", "line_start": 7}, "docs/en/index.md#section_2": {"doc_id": "docs/en/index.md", "heading": "Page Structure", "content": "Every topic page follows a consistent 8-section format:\n\n1. **Interview Summary** — Key points in 3-6 lines\n2. **Core Definitions** — Essential terminology and concepts\n3. **Math and Derivations** — Rigorous mathematical foundations\n4. **Algorithm/Model Sketch** — How the method works\n5. **Common Pitfalls** — Mistakes to avoid\n6. **Mini Example** — Quick illustration\n7. **Quiz** — 5+ questions with hidden answers (click to reveal)\n8. **References** — Further reading", "line_start": 25}, "docs/en/index.md#section_3": {"doc_id": "docs/en/index.md", "heading": "Getting Started", "content": "Choose a topic from the sidebar to begin. Each page is self-contained but builds on foundational concepts.\n\n**Recommended learning path for beginners:**\n\n1. Start with [Stationarity](foundations/stationarity.md) and [Autocorrelation](foundations/autocorrelation.md)\n2. Move to [AR Models](time-domain/ar.md) → [MA Models](time-domain/ma.md) → [ARMA](time-domain/arma.md) → [ARIMA](time-domain/arima.md)\n3. Learn [Model Identification](time-domain/identification.md) and [Residual Diagnostics](model-selection/residual-diagnostics.md)\n4. Explore [Exponential Smoothing](exponential-smoothing/ses.md) and [Decomposition](decomposition/stl.md)\n5. Advance to [State Space Models](state-space/kalman-filter.md) and [Multivariate TS](multivariate/var.md)", "line_start": 38}, "docs/en/index.md#section_4": {"doc_id": "docs/en/index.md", "heading": "Code Examples", "content": "Runnable Python demos are available in the `ts_examples/` directory. Run them with:\n\n```bash\npython -m ts_examples.run --demo <demo_name>\n```\n\nAvailable demos: `arima`, `ets`, `stl`, `kalman`, `var`, `changepoint`, `backtest`, `metrics`", "line_start": 50}, "docs/en/index.md#section_5": {"doc_id": "docs/en/index.md", "heading": "Language Toggle", "content": "Use the language selector in the header to switch between English and 中文. The site maintains parallel content in both languages.\n\n---\n\n*This is an open, extensible knowledge base. See the repository README for instructions on adding new content.*", "line_start": 60}, "docs/en/spectral/spectral-analysis.md#section_0": {"doc_id": "docs/en/spectral/spectral-analysis.md", "heading": "Spectral Analysis", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Spectral analysis decomposes time series into frequency components. The periodogram estimates power at each frequency. Peaks indicate dominant cycles. For stationary series, spectral density is the Fourier transform of autocovariance. Key insight: AR produces smooth spectrum; MA produces peaks. Useful for detecting hidden periodicities and understanding cyclical behavior.\n</div>", "line_start": 1}, "docs/en/spectral/spectral-analysis.md#section_1": {"doc_id": "docs/en/spectral/spectral-analysis.md", "heading": "Core Definitions", "content": "**Spectral Density:** For a stationary process, the spectral density $f(\\omega)$ represents power at frequency $\\omega$:\n$$f(\\omega) = \\frac{1}{2\\pi}\\sum_{h=-\\infty}^{\\infty}\\gamma(h)e^{-i\\omega h}$$\n\n**Periodogram:** Sample estimate of spectral density:\n$$I(\\omega_j) = \\frac{1}{n}\\left|\\sum_{t=1}^{n}y_t e^{-i\\omega_j t}\\right|^2$$\n\nat Fourier frequencies $\\omega_j = 2\\pi j/n$ for $j = 0, 1, \\ldots, n/2$.\n\n**Key Frequencies:**\n- $\\omega = 0$: Mean level (zero frequency)\n- $\\omega = 2\\pi/m$: Period of m time units\n- $\\omega = \\pi$: Nyquist frequency (fastest observable cycle = 2 time units)", "line_start": 7}, "docs/en/spectral/spectral-analysis.md#section_2": {"doc_id": "docs/en/spectral/spectral-analysis.md", "heading": "Fourier Transform Relationship", "content": "The autocovariance and spectral density are Fourier transform pairs:\n$$f(\\omega) = \\frac{1}{2\\pi}\\sum_{h=-\\infty}^{\\infty}\\gamma(h)e^{-i\\omega h}$$\n$$\\gamma(h) = \\int_{-\\pi}^{\\pi}f(\\omega)e^{i\\omega h}d\\omega$$\n\n**Parseval's relation:**\n$$\\gamma(0) = \\text{Var}(y_t) = \\int_{-\\pi}^{\\pi}f(\\omega)d\\omega$$\n\nTotal variance decomposes across frequencies.", "line_start": 24}, "docs/en/spectral/spectral-analysis.md#section_3": {"doc_id": "docs/en/spectral/spectral-analysis.md", "heading": "Spectral Density of AR(1)", "content": "For $y_t = \\phi y_{t-1} + \\epsilon_t$:\n$$f(\\omega) = \\frac{\\sigma^2}{2\\pi|1-\\phi e^{-i\\omega}|^2} = \\frac{\\sigma^2}{2\\pi(1+\\phi^2-2\\phi\\cos\\omega)}$$\n\nProperties:\n- $\\phi > 0$: Peak at $\\omega = 0$ (low-frequency dominance)\n- $\\phi < 0$: Peak at $\\omega = \\pi$ (high-frequency dominance)", "line_start": 35}, "docs/en/spectral/spectral-analysis.md#section_4": {"doc_id": "docs/en/spectral/spectral-analysis.md", "heading": "Spectral Density of MA(1)", "content": "For $y_t = \\epsilon_t + \\theta\\epsilon_{t-1}$:\n$$f(\\omega) = \\frac{\\sigma^2}{2\\pi}|1+\\theta e^{-i\\omega}|^2 = \\frac{\\sigma^2}{2\\pi}(1+\\theta^2+2\\theta\\cos\\omega)$$", "line_start": 44}, "docs/en/spectral/spectral-analysis.md#section_5": {"doc_id": "docs/en/spectral/spectral-analysis.md", "heading": "Period Detection", "content": "If periodogram has peak at $\\omega_j$, the dominant period is:\n$$T = \\frac{2\\pi}{\\omega_j} = \\frac{n}{j}$$\n\nFor monthly data with annual cycle: peak at $\\omega = 2\\pi/12 \\approx 0.524$.", "line_start": 49}, "docs/en/spectral/spectral-analysis.md#section_6": {"doc_id": "docs/en/spectral/spectral-analysis.md", "heading": "Algorithm/Model Sketch", "content": "**Spectral Analysis Procedure:**\n\n```\n1. Remove mean (and trend if necessary)\n   y_centered = y - mean(y)\n\n2. Apply window (optional, reduces leakage)\n   Common: Hanning, Hamming, Blackman\n\n3. Compute FFT\n   Y = FFT(y_centered)\n\n4. Compute periodogram\n   I[j] = |Y[j]|² / n\n\n5. Smooth periodogram (optional)\n   - Daniell kernel\n   - Log-smoothing\n   - Welch's method\n\n6. Identify peaks\n   - Compare to red/white noise baseline\n   - Test significance (F-test against continuum)\n\n7. Interpret\n   - Peaks → dominant periods\n   - Smooth decay → AR-like behavior\n   - Flat → white noise\n```\n\n**Frequency to Period Conversion:**\n$$\\text{Period} = \\frac{n}{\\text{index}} = \\frac{2\\pi}{\\omega}$$", "line_start": 56}, "docs/en/spectral/spectral-analysis.md#section_7": {"doc_id": "docs/en/spectral/spectral-analysis.md", "heading": "Common Pitfalls", "content": "1. **Spectral leakage**: Sharp peaks in true spectrum appear spread out due to finite sample. Use windowing to reduce.\n\n2. **Confusing periodogram with spectral density**: Periodogram is inconsistent (doesn't converge). Smooth it for density estimation.\n\n3. **Ignoring aliasing**: Cycles faster than Nyquist (period < 2) appear at wrong frequencies. Ensure adequate sampling.\n\n4. **Non-stationarity**: Spectral analysis assumes stationarity. Trend causes low-frequency blow-up. Detrend first.\n\n5. **Over-interpreting peaks**: Random fluctuations create spurious peaks. Test significance against noise baseline.\n\n6. **Wrong frequency interpretation**: $\\omega = 0.5$ doesn't mean period = 0.5. Period = $2\\pi/0.5 \\approx 12.6$.", "line_start": 91}, "docs/en/spectral/spectral-analysis.md#section_8": {"doc_id": "docs/en/spectral/spectral-analysis.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom scipy import signal\nimport matplotlib.pyplot as plt", "line_start": 105}, "docs/en/spectral/spectral-analysis.md#section_9": {"doc_id": "docs/en/spectral/spectral-analysis.md", "heading": "Generate signal with known frequencies", "content": "np.random.seed(42)\nn = 500\nt = np.arange(n)", "line_start": 112}, "docs/en/spectral/spectral-analysis.md#section_10": {"doc_id": "docs/en/spectral/spectral-analysis.md", "heading": "Components: trend + period 50 + period 12 + noise", "content": "y = (0.01 * t +                          # trend\n     5 * np.sin(2 * np.pi * t / 50) +    # period 50\n     3 * np.sin(2 * np.pi * t / 12) +    # period 12\n     np.random.randn(n))                  # noise", "line_start": 117}, "docs/en/spectral/spectral-analysis.md#section_11": {"doc_id": "docs/en/spectral/spectral-analysis.md", "heading": "Detrend", "content": "y_detrended = signal.detrend(y)", "line_start": 123}, "docs/en/spectral/spectral-analysis.md#section_12": {"doc_id": "docs/en/spectral/spectral-analysis.md", "heading": "Compute periodogram", "content": "freqs, psd = signal.periodogram(y_detrended, fs=1.0)", "line_start": 126}, "docs/en/spectral/spectral-analysis.md#section_13": {"doc_id": "docs/en/spectral/spectral-analysis.md", "heading": "Find peaks", "content": "peaks, _ = signal.find_peaks(psd, height=np.percentile(psd, 90))\n\nprint(\"Detected periods:\")\nfor p in peaks:\n    if freqs[p] > 0:\n        period = 1 / freqs[p]\n        print(f\"  Frequency {freqs[p]:.4f} → Period {period:.1f}\")", "line_start": 129}, "docs/en/spectral/spectral-analysis.md#section_14": {"doc_id": "docs/en/spectral/spectral-analysis.md", "heading": "Expected: peaks near period 50 and period 12", "content": "```", "line_start": 138}, "docs/en/spectral/spectral-analysis.md#section_15": {"doc_id": "docs/en/spectral/spectral-analysis.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What is the relationship between the autocovariance function and spectral density?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> They are Fourier transform pairs.\n\n$$f(\\omega) = \\frac{1}{2\\pi}\\sum_{h=-\\infty}^{\\infty}\\gamma(h)e^{-i\\omega h}$$\n\n$$\\gamma(h) = \\int_{-\\pi}^{\\pi}f(\\omega)e^{i\\omega h}d\\omega$$\n\n**Interpretation:**\n- ACF describes correlation in time domain\n- Spectral density describes power in frequency domain\n- Same information, different representation\n\n**Key insight:** At $h=0$:\n$$\\gamma(0) = \\text{Var}(y_t) = \\int f(\\omega)d\\omega$$\n\nTotal variance = integral of spectral density (power across all frequencies).\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Thinking frequency and time domain analyses give different information. They're equivalent representations—choose based on what's easier to interpret for your problem.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Why does an AR(1) with positive φ have peak at frequency zero?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Positive AR(1) coefficient creates persistence—values tend to stay above or below mean for extended periods. This translates to slow oscillations (low frequency).\n\n**Mathematical explanation:**\nSpectral density: $f(\\omega) \\propto \\frac{1}{1+\\phi^2-2\\phi\\cos\\omega}$\n\nAt $\\omega = 0$: $f(0) \\propto \\frac{1}{(1-\\phi)^2}$ (maximum for $\\phi > 0$)\nAt $\\omega = \\pi$: $f(\\pi) \\propto \\frac{1}{(1+\\phi)^2}$ (minimum for $\\phi > 0$)\n\n**Intuition:**\n- $\\phi > 0$: Today's value predicts tomorrow's → smooth, low-frequency behavior\n- $\\phi < 0$: Today's value predicts opposite tomorrow → choppy, high-frequency behavior\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Expecting all AR processes to look similar in spectrum. The sign and magnitude of φ drastically change spectral shape.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Derive the Nyquist frequency and explain why frequencies above it cannot be detected.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Nyquist frequency = $\\pi$ rad/sample = 0.5 cycles/sample.\n\n**Derivation:**\nTo observe a cycle, we need at least 2 samples per period (sample at peak and trough).\n\nMinimum period detectable = 2 sample intervals\nMaximum frequency = 1/(2 sample intervals) = 0.5 cycles/sample\n\nIn angular frequency: $\\omega_{Nyquist} = 2\\pi \\times 0.5 = \\pi$\n\n**Aliasing:**\nA signal with frequency $\\omega > \\pi$ appears as frequency $2\\pi - \\omega$ (reflected).\n\nExample: True frequency 0.6 cycles/sample appears as 0.4 cycles/sample.\n\n**Consequence:** Without higher sampling rate, we cannot distinguish $\\omega$ from $2\\pi - \\omega$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Trying to detect daily cycles from monthly data. Monthly sampling (Nyquist = 2 months period) cannot see anything faster than bimonthly oscillations.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Why is the raw periodogram an inconsistent estimator of spectral density?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The periodogram variance doesn't decrease as sample size increases.\n\n**Technical explanation:**\n$$\\text{Var}(I(\\omega)) \\approx f(\\omega)^2$$\n\nfor $\\omega \\neq 0, \\pi$. The variance equals the squared mean—relative error stays constant!\n\n**Why this happens:**\n- Periodogram at each frequency uses information from the entire series\n- But at each Fourier frequency, we essentially have one \"observation\"\n- More data → more frequencies, but still one estimate per frequency\n\n**Solution:** Smooth the periodogram\n- Average nearby frequencies (reduces variance)\n- Or use multitaper methods\n- Trade bias for variance\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Treating raw periodogram peaks as definitive. Peaks can be noise; always smooth or test significance.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You analyze hourly temperature data and see a strong peak at period 24 hours, but also unexpected peaks at periods 12, 8, 6 hours. What's happening?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> These are **harmonics** of the fundamental daily cycle.\n\n**Explanation:**\nA pure 24-hour cycle would show only one peak at period 24. But real temperature patterns aren't pure sinusoids—they have:\n- Sharp morning rise\n- Gradual afternoon decline\n- These non-sinusoidal shapes require multiple frequencies to represent\n\n**Fourier's theorem:** Any periodic signal is a sum of harmonics:\n$$y(t) = \\sum_{k=1}^{\\infty} a_k \\cos(2\\pi kt/24) + b_k \\sin(2\\pi kt/24)$$\n\nHarmonics at periods 24/2=12, 24/3=8, 24/4=6, etc.\n\n**Interpretation:**\n- Period 24: Fundamental daily cycle\n- Period 12: Asymmetry (morning ≠ evening)\n- Period 8, 6: Further shape details\n\n**Action:** This is normal for non-sinusoidal cycles. Focus on fundamental; harmonics indicate shape.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Interpreting harmonics as separate physical phenomena. They're mathematical artifacts of non-sinusoidal shape.\n</div>\n</div>\n</details>", "line_start": 141}, "docs/en/spectral/spectral-analysis.md#section_16": {"doc_id": "docs/en/spectral/spectral-analysis.md", "heading": "References", "content": "1. Shumway, R. H., & Stoffer, D. S. (2017). *Time Series Analysis and Its Applications*. Springer. Chapter 4.\n2. Brockwell, P. J., & Davis, R. A. (2016). *Introduction to Time Series and Forecasting*. Springer. Chapter 4.\n3. Priestley, M. B. (1981). *Spectral Analysis and Time Series*. Academic Press.\n4. Percival, D. B., & Walden, A. T. (1993). *Spectral Analysis for Physical Applications*. Cambridge University Press.", "line_start": 275}, "docs/en/multivariate/granger-causality.md#section_0": {"doc_id": "docs/en/multivariate/granger-causality.md", "heading": "Granger Causality", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Granger causality tests whether past values of X help predict Y beyond Y's own history. It's about predictive precedence, not true causation. Test via F-test on VAR coefficients. X Granger-causes Y if coefficients on lagged X in Y's equation are jointly significant. Bidirectional causality is possible. Sensitive to omitted variables and lag selection.\n</div>", "line_start": 1}, "docs/en/multivariate/granger-causality.md#section_1": {"doc_id": "docs/en/multivariate/granger-causality.md", "heading": "Core Definitions", "content": "**Granger Causality:** X Granger-causes Y if:\n$$E[Y_t | Y_{t-1}, Y_{t-2}, \\ldots, X_{t-1}, X_{t-2}, \\ldots] \\neq E[Y_t | Y_{t-1}, Y_{t-2}, \\ldots]$$\n\nPast X provides predictive information about Y beyond Y's own past.\n\n**Non-causality:** X does NOT Granger-cause Y if knowing past X doesn't improve prediction of Y.\n\n**Bivariate VAR Test:**\n$$y_t = c + \\sum_{i=1}^{p}\\alpha_i y_{t-i} + \\sum_{i=1}^{p}\\beta_i x_{t-i} + \\epsilon_t$$\n\n$H_0$: $\\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0$ (X does not Granger-cause Y)", "line_start": 7}, "docs/en/multivariate/granger-causality.md#section_2": {"doc_id": "docs/en/multivariate/granger-causality.md", "heading": "F-Test for Granger Causality", "content": "**Restricted model:** AR(p) for Y only\n$$y_t = c + \\sum_{i=1}^{p}\\alpha_i y_{t-i} + u_t$$\n\n**Unrestricted model:** VAR including X\n$$y_t = c + \\sum_{i=1}^{p}\\alpha_i y_{t-i} + \\sum_{i=1}^{p}\\beta_i x_{t-i} + \\epsilon_t$$\n\n**F-statistic:**\n$$F = \\frac{(RSS_R - RSS_U)/p}{RSS_U/(T-2p-1)}$$\n\nUnder $H_0$: $F \\sim F_{p, T-2p-1}$", "line_start": 23}, "docs/en/multivariate/granger-causality.md#section_3": {"doc_id": "docs/en/multivariate/granger-causality.md", "heading": "Wald Test (for VAR)", "content": "In VAR framework, test:\n$$H_0: \\mathbf{R}\\boldsymbol{\\beta} = \\mathbf{0}$$\n\nwhere $\\mathbf{R}$ selects the coefficients on lagged X in Y's equation.\n\nWald statistic: $W = (\\mathbf{R}\\hat{\\boldsymbol{\\beta}})'[\\mathbf{R}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{R}'\\hat{\\sigma}^2]^{-1}(\\mathbf{R}\\hat{\\boldsymbol{\\beta}})$\n\nUnder $H_0$: $W \\sim \\chi^2_p$", "line_start": 36}, "docs/en/multivariate/granger-causality.md#section_4": {"doc_id": "docs/en/multivariate/granger-causality.md", "heading": "Instantaneous Causality", "content": "Tests whether current X helps predict current Y (beyond lagged effects):\n\n$$\\text{Cov}(\\epsilon_{yt}, \\epsilon_{xt}) \\neq 0$$\n\nThis tests contemporaneous correlation, not temporal precedence.", "line_start": 47}, "docs/en/multivariate/granger-causality.md#section_5": {"doc_id": "docs/en/multivariate/granger-causality.md", "heading": "Block Exogeneity", "content": "In multivariate system, test whether a group of variables Granger-causes another group.\n\nJoint test on all relevant coefficient matrices.", "line_start": 55}, "docs/en/multivariate/granger-causality.md#section_6": {"doc_id": "docs/en/multivariate/granger-causality.md", "heading": "Algorithm/Model Sketch", "content": "**Granger Causality Test Procedure:**\n\n```\n1. Determine if series are stationary\n   - If I(1), difference or use Toda-Yamamoto approach\n   - Standard GC tests require stationarity\n\n2. Select optimal lag order\n   - Use AIC/BIC on bivariate VAR\n   - Or use same p for all tests (consistency)\n\n3. Estimate unrestricted VAR(p)\n\n4. Perform Wald/F test:\n   - H0: Coefficients on lagged X = 0 (in Y equation)\n   - Reject → X Granger-causes Y\n\n5. Test reverse direction:\n   - H0: Coefficients on lagged Y = 0 (in X equation)\n   - Reject → Y Granger-causes X\n\n6. Interpret:\n   - Both reject: bidirectional causality\n   - One rejects: unidirectional causality\n   - Neither rejects: no Granger causality\n```\n\n**Toda-Yamamoto Approach (for I(1) series):**\n1. Determine maximum integration order d_max\n2. Fit VAR(p + d_max)\n3. Test only coefficients on first p lags\n4. Avoids issues with pretesting for unit roots", "line_start": 61}, "docs/en/multivariate/granger-causality.md#section_7": {"doc_id": "docs/en/multivariate/granger-causality.md", "heading": "Common Pitfalls", "content": "1. **Confusing with true causation**: Granger causality is predictive precedence, not causal mechanism. Correlation can arise from common causes.\n\n2. **Omitted variable bias**: If Z causes both X and Y with different lags, you may find spurious GC between X and Y.\n\n3. **Wrong lag selection**: Too few lags → miss true effects. Too many → lose power and introduce noise.\n\n4. **Non-stationary data**: Standard F-tests have wrong distribution with unit roots. Use augmented lag approach or error correction.\n\n5. **Multiple testing**: Testing many pairs inflates Type I error. Adjust significance level.\n\n6. **Contemporaneous effects only**: If X and Y move together within a period but not across periods, GC won't detect it.", "line_start": 96}, "docs/en/multivariate/granger-causality.md#section_8": {"doc_id": "docs/en/multivariate/granger-causality.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.stattools import grangercausalitytests\nfrom statsmodels.tsa.api import VAR", "line_start": 110}, "docs/en/multivariate/granger-causality.md#section_9": {"doc_id": "docs/en/multivariate/granger-causality.md", "heading": "Generate data where X Granger-causes Y but not vice versa", "content": "np.random.seed(42)\nn = 200", "line_start": 117}, "docs/en/multivariate/granger-causality.md#section_10": {"doc_id": "docs/en/multivariate/granger-causality.md", "heading": "X is independent AR(1)", "content": "x = np.zeros(n)\nfor t in range(1, n):\n    x[t] = 0.7 * x[t-1] + np.random.randn()", "line_start": 121}, "docs/en/multivariate/granger-causality.md#section_11": {"doc_id": "docs/en/multivariate/granger-causality.md", "heading": "Y depends on own lag and lagged X", "content": "y = np.zeros(n)\nfor t in range(1, n):\n    y[t] = 0.5 * y[t-1] + 0.4 * x[t-1] + np.random.randn()", "line_start": 126}, "docs/en/multivariate/granger-causality.md#section_12": {"doc_id": "docs/en/multivariate/granger-causality.md", "heading": "Stack data", "content": "data = np.column_stack([y, x])", "line_start": 131}, "docs/en/multivariate/granger-causality.md#section_13": {"doc_id": "docs/en/multivariate/granger-causality.md", "heading": "Granger causality tests", "content": "print(\"=== Does X Granger-cause Y? ===\")\ngc_x_to_y = grangercausalitytests(data, maxlag=4, verbose=True)\n\nprint(\"\\n=== Does Y Granger-cause X? ===\")\ngc_y_to_x = grangercausalitytests(data[:, ::-1], maxlag=4, verbose=True)", "line_start": 134}, "docs/en/multivariate/granger-causality.md#section_14": {"doc_id": "docs/en/multivariate/granger-causality.md", "heading": "Using VAR", "content": "model = VAR(data)\nresults = model.fit(2)\nprint(\"\\n=== VAR-based Granger Causality ===\")\nprint(results.test_causality('y1', 'y2', kind='f'))  # X → Y\nprint(results.test_causality('y2', 'y1', kind='f'))  # Y → X\n```", "line_start": 141}, "docs/en/multivariate/granger-causality.md#section_15": {"doc_id": "docs/en/multivariate/granger-causality.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why is Granger causality not the same as true causation?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Granger causality only measures predictive precedence—whether past X helps predict Y. It doesn't establish causal mechanism.\n\n**Why they differ:**\n1. **Omitted variables**: Z may cause both X and Y with different lags, creating spurious GC\n2. **Common causes**: X and Y may both respond to unobserved factor\n3. **Spurious correlation**: Can find GC even in independent series by chance\n4. **Measurement timing**: If X and Y are measured at different times, GC reflects measurement, not causation\n\n**Example:** Ice cream sales Granger-cause drownings (both caused by summer heat with different lags).\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Claiming X causes Y based on GC test. Always say \"X Granger-causes Y\" or \"X has predictive power for Y\"—not \"X causes Y.\"\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> What does bidirectional Granger causality mean? Is this common?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Bidirectional GC means both X → Y and Y → X (each helps predict the other). This is common in economics.\n\n**Examples:**\n- GDP ↔ Employment (economic activity and labor market interact)\n- Prices ↔ Wages (wage-price spiral)\n- Interest rates ↔ Exchange rates (monetary policy and currency markets)\n\n**Interpretation:**\n- Feedback relationship\n- Both variables contain unique predictive information\n- System is interdependent\n\n**Caution:** Bidirectional GC doesn't mean simultaneous causation—it means mutual predictive value across time.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Expecting one-way causality. In complex systems, feedback is the rule rather than exception.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> What are the degrees of freedom for the Granger causality F-test with p lags and T observations?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> $F_{p, T-2p-1}$ (numerator df = p, denominator df = T - 2p - 1)\n\n**Derivation:**\n- Restricted model: p parameters (p lags of Y + constant)\n- Unrestricted model: 2p + 1 parameters (p lags of Y + p lags of X + constant)\n- Restriction: p parameters set to zero\n- Observations used: T - p (lose p for lags)\n\nNumerator df = number of restrictions = p\nDenominator df = T - p - (2p + 1) = T - 3p - 1\n\n(Some formulations differ slightly depending on whether constant is counted.)\n\n**Practical:** Use software; these details are handled automatically.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> With short series and many lags, degrees of freedom are low, reducing test power. Balance p against sample size.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> How does the Toda-Yamamoto approach handle non-stationary series in Granger causality testing?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Toda-Yamamoto (1995) avoids pretesting for unit roots:\n\n1. Determine maximum integration order $d_{max}$ (usually 1 or 2)\n2. Fit VAR(p + $d_{max}$) in levels (don't difference)\n3. Test Granger causality on first p lags only\n4. Extra $d_{max}$ lags absorb non-stationarity\n\n**Why it works:**\n- VAR in levels with extra lags has standard asymptotic distribution for Wald test\n- No need to pretest for cointegration\n- Robust to I(1) or I(0) series\n\n**Test:**\n$$H_0: \\beta_1 = \\cdots = \\beta_p = 0$$\n\n(Ignore $\\beta_{p+1}, \\ldots, \\beta_{p+d_{max}}$)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Testing all coefficients including extra lags. Only test first p; the extra $d_{max}$ are \"nuisance\" parameters.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You test Granger causality between oil prices and stock returns using lags 1-8. Results vary: significant at lags 2, 4, 5 but not others. How do you interpret?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> This pattern suggests:\n\n1. **Lag selection matters**: Results are sensitive to specification\n2. **Possible weak relationship**: Significance appears at some lags by chance\n3. **Multiple testing**: Testing 8 specifications inflates false positives\n\n**Recommended approach:**\n1. Select lag order FIRST using information criteria (not GC results)\n2. Report single test at optimal lag\n3. If sensitivity analysis needed, report all results and acknowledge instability\n4. Consider Bonferroni correction for multiple tests\n5. Validate on out-of-sample data\n\n**If results are inconsistent:**\n- Weak evidence for Granger causality\n- Relationship may be nonlinear or time-varying\n- Consider threshold VAR or regime-switching model\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Selecting lag that gives desired result (\"lag shopping\"). This is p-hacking; report pre-specified lag or all results.\n</div>\n</div>\n</details>", "line_start": 149}, "docs/en/multivariate/granger-causality.md#section_16": {"doc_id": "docs/en/multivariate/granger-causality.md", "heading": "References", "content": "1. Granger, C. W. J. (1969). Investigating causal relations by econometric models and cross-spectral methods. *Econometrica*, 37(3), 424-438.\n2. Toda, H. Y., & Yamamoto, T. (1995). Statistical inference in vector autoregressions with possibly integrated processes. *Journal of Econometrics*, 66(1-2), 225-250.\n3. Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press. Chapter 11.\n4. Lütkepohl, H. (2005). *New Introduction to Multiple Time Series Analysis*. Springer. Chapter 2.", "line_start": 275}, "docs/en/multivariate/var.md#section_0": {"doc_id": "docs/en/multivariate/var.md", "heading": "Vector Autoregression (VAR)", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> VAR models multiple time series jointly, where each variable depends on its own lags and lags of others. VAR(p): $\\mathbf{y}_t = \\mathbf{c} + \\mathbf{A}_1\\mathbf{y}_{t-1} + \\cdots + \\mathbf{A}_p\\mathbf{y}_{t-p} + \\boldsymbol{\\epsilon}_t$. Useful for forecasting interrelated series and analyzing dynamic relationships. Estimate by OLS equation-by-equation. Select order with AIC/BIC. Check stability via eigenvalues.\n</div>", "line_start": 1}, "docs/en/multivariate/var.md#section_1": {"doc_id": "docs/en/multivariate/var.md", "heading": "Core Definitions", "content": "**VAR(p) Model:**\n$$\\mathbf{y}_t = \\mathbf{c} + \\mathbf{A}_1\\mathbf{y}_{t-1} + \\mathbf{A}_2\\mathbf{y}_{t-2} + \\cdots + \\mathbf{A}_p\\mathbf{y}_{t-p} + \\boldsymbol{\\epsilon}_t$$\n\nwhere:\n- $\\mathbf{y}_t$: k×1 vector of variables\n- $\\mathbf{c}$: k×1 constant vector\n- $\\mathbf{A}_i$: k×k coefficient matrices\n- $\\boldsymbol{\\epsilon}_t \\sim N(\\mathbf{0}, \\boldsymbol{\\Sigma})$: k×1 error vector\n\n**Compact Form:**\n$$\\mathbf{A}(L)\\mathbf{y}_t = \\mathbf{c} + \\boldsymbol{\\epsilon}_t$$\n\nwhere $\\mathbf{A}(L) = \\mathbf{I} - \\mathbf{A}_1 L - \\cdots - \\mathbf{A}_p L^p$\n\n**Stationarity Condition:** All eigenvalues of companion matrix inside unit circle.", "line_start": 7}, "docs/en/multivariate/var.md#section_2": {"doc_id": "docs/en/multivariate/var.md", "heading": "Bivariate VAR(1) Example", "content": "For variables $(y_{1t}, y_{2t})$:\n$$\\begin{pmatrix} y_{1t} \\\\ y_{2t} \\end{pmatrix} = \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} + \\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{pmatrix}\\begin{pmatrix} y_{1,t-1} \\\\ y_{2,t-1} \\end{pmatrix} + \\begin{pmatrix} \\epsilon_{1t} \\\\ \\epsilon_{2t} \\end{pmatrix}$$\n\nWritten out:\n$$y_{1t} = c_1 + a_{11}y_{1,t-1} + a_{12}y_{2,t-1} + \\epsilon_{1t}$$\n$$y_{2t} = c_2 + a_{21}y_{1,t-1} + a_{22}y_{2,t-1} + \\epsilon_{2t}$$\n\nCross-coefficients $a_{12}, a_{21}$ capture dynamic spillovers.", "line_start": 27}, "docs/en/multivariate/var.md#section_3": {"doc_id": "docs/en/multivariate/var.md", "heading": "Companion Form", "content": "VAR(p) can be written as VAR(1) in higher dimension:\n$$\\boldsymbol{\\xi}_t = \\mathbf{F}\\boldsymbol{\\xi}_{t-1} + \\mathbf{v}_t$$\n\nwhere $\\boldsymbol{\\xi}_t = (\\mathbf{y}_t', \\mathbf{y}_{t-1}', \\ldots, \\mathbf{y}_{t-p+1}')'$ and $\\mathbf{F}$ is the companion matrix.\n\nStationarity: eigenvalues of $\\mathbf{F}$ have modulus < 1.", "line_start": 38}, "docs/en/multivariate/var.md#section_4": {"doc_id": "docs/en/multivariate/var.md", "heading": "MA(∞) Representation", "content": "Stationary VAR has moving average form:\n$$\\mathbf{y}_t = \\boldsymbol{\\mu} + \\sum_{i=0}^{\\infty}\\boldsymbol{\\Phi}_i\\boldsymbol{\\epsilon}_{t-i}$$\n\n$\\boldsymbol{\\Phi}_i$ are impulse response matrices: $\\boldsymbol{\\Phi}_i^{jk}$ = response of variable j to shock in variable k at lag i.", "line_start": 47}, "docs/en/multivariate/var.md#section_5": {"doc_id": "docs/en/multivariate/var.md", "heading": "Forecast Error Variance Decomposition", "content": "Variance of h-step forecast error for variable j:\n$$\\sigma_j^2(h) = \\sum_{i=0}^{h-1}\\sum_{k=1}^{K}(\\Phi_i^{jk})^2\\sigma_k^2$$\n\nContribution of variable k to variance of j at horizon h.", "line_start": 54}, "docs/en/multivariate/var.md#section_6": {"doc_id": "docs/en/multivariate/var.md", "heading": "Algorithm/Model Sketch", "content": "**VAR Estimation:**\n\n```\n1. Determine optimal lag order p:\n   - Fit VAR(1), VAR(2), ..., VAR(p_max)\n   - Select p minimizing AIC or BIC\n\n2. Estimate by OLS:\n   - Each equation can be estimated separately\n   - OLS is consistent and efficient (same regressors)\n\n3. Check stability:\n   - Compute eigenvalues of companion matrix\n   - All |λᵢ| < 1 for stationarity\n\n4. Diagnostics:\n   - Test residuals for autocorrelation (multivariate LB)\n   - Test for normality\n   - Check for heteroskedasticity\n\n5. Analysis:\n   - Impulse responses\n   - Forecast error variance decomposition\n   - Granger causality tests\n```", "line_start": 61}, "docs/en/multivariate/var.md#section_7": {"doc_id": "docs/en/multivariate/var.md", "heading": "Common Pitfalls", "content": "1. **Too many parameters**: VAR(p) with k variables has k + k²p parameters. Overfitting is easy with high k or p.\n\n2. **Non-stationary variables**: VAR requires stationarity. Use differences or VECM for I(1) variables.\n\n3. **Structural interpretation**: Reduced-form VAR shows correlations, not causation. Use structural VAR (SVAR) for causal claims.\n\n4. **Ignoring cointegration**: If variables are cointegrated, restricted VECM is more efficient than unrestricted VAR in differences.\n\n5. **Over-interpreting IRFs**: Impulse responses depend on ordering (Cholesky) or identification assumptions.\n\n6. **Forgetting contemporaneous correlation**: $\\boldsymbol{\\Sigma}$ is not diagonal; shocks are correlated across equations.", "line_start": 89}, "docs/en/multivariate/var.md#section_8": {"doc_id": "docs/en/multivariate/var.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.api import VAR", "line_start": 103}, "docs/en/multivariate/var.md#section_9": {"doc_id": "docs/en/multivariate/var.md", "heading": "Generate bivariate VAR(1) data", "content": "np.random.seed(42)\nn = 200\nA = np.array([[0.5, 0.3],\n              [0.2, 0.4]])\nc = np.array([1, 2])\n\ny = np.zeros((n, 2))\nfor t in range(1, n):\n    y[t] = c + A @ y[t-1] + np.random.randn(2) * 0.5", "line_start": 109}, "docs/en/multivariate/var.md#section_10": {"doc_id": "docs/en/multivariate/var.md", "heading": "Fit VAR", "content": "model = VAR(y)", "line_start": 120}, "docs/en/multivariate/var.md#section_11": {"doc_id": "docs/en/multivariate/var.md", "heading": "Select lag order", "content": "lag_order = model.select_order(maxlags=8)\nprint(\"Lag selection:\")\nprint(lag_order.summary())", "line_start": 123}, "docs/en/multivariate/var.md#section_12": {"doc_id": "docs/en/multivariate/var.md", "heading": "Fit VAR(1)", "content": "results = model.fit(1)\nprint(\"\\nCoefficient matrix A:\")\nprint(results.coefs[0])\nprint(f\"\\nTrue A:\\n{A}\")", "line_start": 128}, "docs/en/multivariate/var.md#section_13": {"doc_id": "docs/en/multivariate/var.md", "heading": "Impulse response", "content": "irf = results.irf(10)\nprint(f\"\\nIRF: Response of y1 to y2 shock at lag 5: {irf.irfs[5, 0, 1]:.3f}\")", "line_start": 134}, "docs/en/multivariate/var.md#section_14": {"doc_id": "docs/en/multivariate/var.md", "heading": "Forecast", "content": "forecast = results.forecast(y[-1:], steps=5)\nprint(f\"\\n5-step forecast:\\n{forecast}\")\n```", "line_start": 138}, "docs/en/multivariate/var.md#section_15": {"doc_id": "docs/en/multivariate/var.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What is the advantage of VAR over fitting separate univariate models?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> VAR captures cross-variable dynamics:\n\n1. **Dynamic interactions**: How $y_1$ affects future $y_2$ and vice versa\n2. **Joint forecasting**: Uses information from all variables\n3. **Correlated errors**: Accounts for contemporaneous shocks\n4. **Policy analysis**: Impulse responses show system-wide effects\n\n**Example:** GDP and inflation. VAR captures:\n- Past GDP affecting future inflation (demand effects)\n- Past inflation affecting future GDP (real balance effects)\n- Correlated supply shocks hitting both\n\nSeparate ARIMAs miss these interactions.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using VAR when variables are unrelated. With k variables, you estimate k² coefficients per lag—wasteful if many are zero. Consider sparse VAR or variable selection.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Why is the ordering of variables important for impulse response analysis?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Standard IRFs use Cholesky decomposition of $\\boldsymbol{\\Sigma}$, which depends on variable ordering.\n\n**Cholesky:** $\\boldsymbol{\\Sigma} = \\mathbf{PP}'$ where $\\mathbf{P}$ is lower triangular.\n\nThis implies:\n- First variable's shock is \"structural\" (not affected by others contemporaneously)\n- Later variables respond to earlier ones within same period\n\n**Different orderings → different IRFs**\n\n**Example:** Order (GDP, Inflation) vs (Inflation, GDP)\n- First ordering: GDP shock affects inflation immediately\n- Second ordering: Inflation shock affects GDP immediately\n\n**Solutions:**\n- Use theory to justify ordering\n- Use structural VAR with explicit identification\n- Report sensitivity to ordering\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Reporting IRFs without stating ordering or justifying identification. Results may be driven by arbitrary ordering choice.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> For VAR(1) with coefficient matrix $\\mathbf{A}$, what is the stationarity condition?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> All eigenvalues of $\\mathbf{A}$ must have modulus less than 1.\n\n**Why:**\nVAR(1): $\\mathbf{y}_t = \\mathbf{c} + \\mathbf{A}\\mathbf{y}_{t-1} + \\boldsymbol{\\epsilon}_t$\n\nIterating backward:\n$$\\mathbf{y}_t = (\\mathbf{I} + \\mathbf{A} + \\mathbf{A}^2 + \\cdots)\\mathbf{c} + \\sum_{j=0}^{\\infty}\\mathbf{A}^j\\boldsymbol{\\epsilon}_{t-j}$$\n\nThis converges iff $\\mathbf{A}^j \\to 0$, which requires all eigenvalues inside unit circle.\n\n**For bivariate:**\nIf $\\mathbf{A} = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$:\n\nEigenvalues: $\\lambda = \\frac{(a+d) \\pm \\sqrt{(a+d)^2 - 4(ad-bc)}}{2}$\n\nNeed $|\\lambda_1| < 1$ and $|\\lambda_2| < 1$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Checking only diagonal elements. Even if $|a|, |d| < 1$, off-diagonal terms can make system unstable.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> How many parameters does a VAR(p) model with k variables have?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> $k + k^2 p + \\frac{k(k+1)}{2}$\n\n**Breakdown:**\n- $k$ constant terms (vector $\\mathbf{c}$)\n- $k^2 \\times p$ coefficients (p matrices of size k×k)\n- $\\frac{k(k+1)}{2}$ variance-covariance parameters (symmetric $\\boldsymbol{\\Sigma}$)\n\n**Example:** k=3 variables, p=4 lags:\n- Constants: 3\n- AR coefficients: 9 × 4 = 36\n- Covariance: 6\n- Total: 45 parameters\n\n**Implications:**\n- Parameters grow as $k^2$\n- With limited data, overfitting is severe\n- Consider restricted VAR, BVAR, or variable selection\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Fitting large VAR with small samples. Rule of thumb: need at least 10-20 observations per parameter.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You fit VAR(2) to 3 quarterly macro variables. The residual autocorrelation test rejects at lag 4. What do you do?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Lag 4 autocorrelation with quarterly data suggests annual seasonality. Options:\n\n1. **Increase lag order**: Try VAR(4) or VAR(5) to capture annual dynamics\n\n2. **Add seasonal dummies**: Include Q1, Q2, Q3 indicators as exogenous variables\n\n3. **Seasonally adjust**: Pre-filter data to remove seasonality\n\n4. **VARX**: Add seasonal Fourier terms as exogenous regressors\n\n**Diagnostic process:**\n1. Check if all three residuals show lag-4 pattern\n2. Fit VAR(4) and re-test\n3. Compare AIC: VAR(2) with seasonals vs VAR(4)\n4. Verify residuals now pass tests\n\n**Consideration:** More lags = more parameters. If sample is small, prefer seasonal dummies over VAR(4).\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Ignoring seasonal patterns in macro data. Annual effects are common; quarterly VAR should capture them explicitly.\n</div>\n</div>\n</details>", "line_start": 143}, "docs/en/multivariate/var.md#section_16": {"doc_id": "docs/en/multivariate/var.md", "heading": "References", "content": "1. Lütkepohl, H. (2005). *New Introduction to Multiple Time Series Analysis*. Springer.\n2. Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press. Chapters 10-11.\n3. Sims, C. A. (1980). Macroeconomics and reality. *Econometrica*, 48(1), 1-48.\n4. Stock, J. H., & Watson, M. W. (2001). Vector autoregressions. *Journal of Economic Perspectives*, 15(4), 101-115.", "line_start": 281}, "docs/en/time-domain/ma.md#section_0": {"doc_id": "docs/en/time-domain/ma.md", "heading": "Moving Average (MA) Models", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> MA(q) models express current value as a linear combination of current and past q noise terms. MA processes are always stationary (finite linear combination of white noise). ACF cuts off after lag q; PACF decays exponentially. Estimation requires nonlinear optimization (MLE). Invertibility requires roots outside unit circle.\n</div>", "line_start": 1}, "docs/en/time-domain/ma.md#section_1": {"doc_id": "docs/en/time-domain/ma.md", "heading": "Core Definitions", "content": "**MA(q) Model**:\n$$X_t = \\mu + \\epsilon_t + \\theta_1\\epsilon_{t-1} + \\theta_2\\epsilon_{t-2} + \\cdots + \\theta_q\\epsilon_{t-q}$$\n\nwhere $\\epsilon_t \\sim WN(0, \\sigma^2)$.\n\n**Lag Operator Form**:\n$$X_t = \\mu + \\Theta(L)\\epsilon_t$$\n\nwhere $\\Theta(L) = 1 + \\theta_1 L + \\theta_2 L^2 + \\cdots + \\theta_q L^q$.\n\n**Characteristic Polynomial**:\n$$\\Theta(z) = 1 + \\theta_1 z + \\theta_2 z^2 + \\cdots + \\theta_q z^q$$\n\n**Invertibility Condition**: All roots of $\\Theta(z) = 0$ must lie outside the unit circle.", "line_start": 7}, "docs/en/time-domain/ma.md#section_2": {"doc_id": "docs/en/time-domain/ma.md", "heading": "MA(1) Model: $X_t = \\mu + \\epsilon_t + \\theta\\epsilon_{t-1}$", "content": "**Mean**: $E[X_t] = \\mu$\n\n**Variance**:\n$$\\gamma(0) = \\text{Var}(X_t) = \\sigma^2(1 + \\theta^2)$$\n\n**Autocovariance at lag 1**:\n$$\\gamma(1) = E[(\\epsilon_t + \\theta\\epsilon_{t-1})(\\epsilon_{t+1} + \\theta\\epsilon_t)] = \\theta\\sigma^2$$\n\n**Autocovariance at lag $h \\geq 2$**: $\\gamma(h) = 0$\n\n**ACF**:\n$$\\rho(1) = \\frac{\\theta}{1+\\theta^2}, \\quad \\rho(h) = 0 \\text{ for } h \\geq 2$$\n\n**Note**: Maximum $|\\rho(1)| = 0.5$ at $\\theta = \\pm 1$.", "line_start": 26}, "docs/en/time-domain/ma.md#section_3": {"doc_id": "docs/en/time-domain/ma.md", "heading": "MA(q) General ACF", "content": "$$\\gamma(h) = \\begin{cases} \\sigma^2 \\sum_{j=0}^{q-h} \\theta_j \\theta_{j+h} & 0 \\leq h \\leq q \\\\ 0 & h > q \\end{cases}$$\n\nwhere $\\theta_0 = 1$.\n\n$$\\rho(h) = \\frac{\\sum_{j=0}^{q-h} \\theta_j \\theta_{j+h}}{\\sum_{j=0}^{q} \\theta_j^2}$$", "line_start": 43}, "docs/en/time-domain/ma.md#section_4": {"doc_id": "docs/en/time-domain/ma.md", "heading": "Invertibility and the AR(∞) Representation", "content": "An invertible MA(1) can be written as AR(∞):\n$$X_t = \\mu + \\epsilon_t + \\theta\\epsilon_{t-1}$$\n\nIf $|\\theta| < 1$:\n$$\\epsilon_t = \\sum_{j=0}^{\\infty}(-\\theta)^j(X_{t-j} - \\mu)$$\n\nThis gives:\n$$X_t = \\mu(1+\\theta) - \\theta X_{t-1} + \\theta^2 X_{t-2} - \\theta^3 X_{t-3} + \\cdots + \\epsilon_t$$\n\n**Why invertibility matters**: Allows expressing shocks in terms of observables. Required for proper forecasting and model interpretation.", "line_start": 51}, "docs/en/time-domain/ma.md#section_5": {"doc_id": "docs/en/time-domain/ma.md", "heading": "PACF of MA(1)", "content": "The PACF of MA(1) decays exponentially:\n$$\\phi_{hh} = \\frac{-(-\\theta)^h(1-\\theta^2)}{1-\\theta^{2(h+1)}}$$\n\nFor large $h$: $\\phi_{hh} \\approx -(-\\theta)^h$", "line_start": 64}, "docs/en/time-domain/ma.md#section_6": {"doc_id": "docs/en/time-domain/ma.md", "heading": "Algorithm/Model Sketch", "content": "**Estimation Methods:**\n\n1. **Innovation Algorithm**: Recursive method to compute MA coefficients from autocovariances.\n\n2. **Conditional Sum of Squares (CSS)**:\n   - Set pre-sample $\\epsilon$ values to zero\n   - Minimize $\\sum \\epsilon_t^2$\n   - Fast but may be biased\n\n3. **Exact Maximum Likelihood (MLE)**:\n   - Accounts for initial conditions\n   - Uses Kalman filter or direct likelihood\n   - Most efficient asymptotically\n\n**Estimation Challenges:**\n- MA estimation is nonlinear (unlike AR)\n- Multiple local optima possible\n- Need good starting values\n- Invertibility constraints must be enforced\n\n**Order Selection:**\n```\n1. Examine ACF - cutoff suggests MA order\n2. If ACF cuts off after lag q, start with MA(q)\n3. Fit candidate models\n4. Compare AIC/BIC\n5. Check residual ACF/PACF\n```", "line_start": 71}, "docs/en/time-domain/ma.md#section_7": {"doc_id": "docs/en/time-domain/ma.md", "heading": "Common Pitfalls", "content": "1. **Parameter identification**: MA(1) with $\\theta$ and MA(1) with $1/\\theta$ give the same ACF! Always enforce invertibility to get unique solution.\n\n2. **Estimation difficulty**: MA models are harder to estimate than AR. Poor starting values lead to convergence issues. Use method=\"innovations\" or CSS for initial estimates.\n\n3. **Confusing MA order with differencing**: Large negative spike at lag 1 in ACF after differencing often indicates over-differencing, not MA(1).\n\n4. **Misinterpreting ACF cutoff**: \"Cutoff\" means abrupt drop to zero, not just decay. AR processes also show ACF patterns—check PACF to distinguish.\n\n5. **Non-invertible estimates**: If estimated $|\\theta| > 1$, the model is non-invertible. Either flip to $1/\\theta$ or reconsider model specification.\n\n6. **Ignoring the unit root boundary**: $\\theta = -1$ or $\\theta = 1$ are non-invertible. Near these values, standard inference breaks down.", "line_start": 102}, "docs/en/time-domain/ma.md#section_8": {"doc_id": "docs/en/time-domain/ma.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.stattools import acf", "line_start": 116}, "docs/en/time-domain/ma.md#section_9": {"doc_id": "docs/en/time-domain/ma.md", "heading": "Generate MA(2) process", "content": "np.random.seed(42)\nn = 300\ntheta1, theta2 = 0.6, 0.3\neps = np.random.randn(n + 2)\nX = np.zeros(n)\n\nfor t in range(n):\n    X[t] = eps[t+2] + theta1*eps[t+1] + theta2*eps[t]", "line_start": 123}, "docs/en/time-domain/ma.md#section_10": {"doc_id": "docs/en/time-domain/ma.md", "heading": "Check ACF (should cut off after lag 2)", "content": "acf_values = acf(X, nlags=10)\nprint(\"ACF:\", np.round(acf_values, 3))", "line_start": 133}, "docs/en/time-domain/ma.md#section_11": {"doc_id": "docs/en/time-domain/ma.md", "heading": "Fit MA(2) model", "content": "model = ARIMA(X, order=(0, 0, 2)).fit()\nprint(f\"True: theta1={theta1}, theta2={theta2}\")\nprint(f\"Estimated: theta1={model.maparams[0]:.3f}, theta2={model.maparams[1]:.3f}\")", "line_start": 138}, "docs/en/time-domain/ma.md#section_12": {"doc_id": "docs/en/time-domain/ma.md", "heading": "Theoretical ACF for comparison", "content": "gamma0 = 1 + theta1**2 + theta2**2\nrho1 = (theta1 + theta1*theta2) / gamma0\nrho2 = theta2 / gamma0\nprint(f\"Theoretical rho(1)={rho1:.3f}, rho(2)={rho2:.3f}\")\n```", "line_start": 143}, "docs/en/time-domain/ma.md#section_13": {"doc_id": "docs/en/time-domain/ma.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why are MA processes always stationary regardless of parameter values?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> MA(q) is a finite linear combination of white noise: $X_t = \\mu + \\sum_{j=0}^{q}\\theta_j\\epsilon_{t-j}$. The mean is constant ($\\mu$), variance is $\\sigma^2\\sum\\theta_j^2$ (constant), and autocovariance depends only on lag (not time).\n\n<strong>Explanation:</strong>\nStationarity requires:\n1. Constant mean: $E[X_t] = \\mu$ ✓\n2. Constant variance: $\\text{Var}(X_t) = \\sigma^2(1+\\theta_1^2+\\cdots+\\theta_q^2)$ ✓\n3. Autocovariance depends only on lag: $\\gamma(h)$ doesn't depend on $t$ ✓\n\nAll conditions are satisfied for any finite $\\theta$ values because white noise is stationary and finite linear combinations preserve stationarity.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Confusing stationarity with invertibility. MA is always stationary but not always invertible. Invertibility is about the AR(∞) representation, not stationarity.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Explain the concept of invertibility. Why do we care about it?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Invertibility means we can express the unobservable shocks $\\epsilon_t$ as a convergent function of past observables $X_t, X_{t-1}, \\ldots$. We care because:\n1. It ensures unique model identification\n2. It enables computing residuals for diagnostics\n3. It's needed for proper forecasting updates\n\n<strong>Technical detail:</strong> For MA(1): $X_t = \\epsilon_t + \\theta\\epsilon_{t-1}$\n\nIf $|\\theta| < 1$: $\\epsilon_t = X_t - \\theta X_{t-1} + \\theta^2 X_{t-2} - \\cdots$ (converges)\nIf $|\\theta| > 1$: the expansion diverges\n\n**Key equation:** MA(q) is invertible iff all roots of $\\Theta(z) = 0$ lie outside the unit circle.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Models with $\\theta$ and $1/\\theta$ produce identical ACFs but different forecasts. Without enforcing invertibility, you might get the \"wrong\" model that performs poorly.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Show that for MA(1), the ACF satisfies $|\\rho(1)| \\leq 0.5$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> We have $\\rho(1) = \\frac{\\theta}{1+\\theta^2}$. Taking the derivative and setting to zero finds the maximum.\n\n<strong>Derivation:</strong>\n$$\\frac{d\\rho(1)}{d\\theta} = \\frac{(1+\\theta^2) - \\theta(2\\theta)}{(1+\\theta^2)^2} = \\frac{1-\\theta^2}{(1+\\theta^2)^2}$$\n\nSetting equal to zero: $\\theta = \\pm 1$\n\nAt $\\theta = 1$: $\\rho(1) = \\frac{1}{1+1} = 0.5$\nAt $\\theta = -1$: $\\rho(1) = \\frac{-1}{1+1} = -0.5$\n\nAs $\\theta \\to 0$: $\\rho(1) \\to 0$\nAs $|\\theta| \\to \\infty$: $\\rho(1) \\to 0$\n\nTherefore $|\\rho(1)| \\leq 0.5$ with equality at $\\theta = \\pm 1$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> If you observe sample $|\\hat{\\rho}(1)| > 0.5$, it's unlikely to be pure MA(1). Consider AR(1) (which can have any $|\\rho(1)| < 1$) or mixed ARMA.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Derive the variance $\\gamma(0)$ for MA(2): $X_t = \\epsilon_t + \\theta_1\\epsilon_{t-1} + \\theta_2\\epsilon_{t-2}$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> $\\gamma(0) = \\sigma^2(1 + \\theta_1^2 + \\theta_2^2)$\n\n<strong>Derivation:</strong>\n$$\\gamma(0) = \\text{Var}(X_t) = \\text{Var}(\\epsilon_t + \\theta_1\\epsilon_{t-1} + \\theta_2\\epsilon_{t-2})$$\n\nSince $\\epsilon_t$, $\\epsilon_{t-1}$, $\\epsilon_{t-2}$ are independent:\n$$= \\text{Var}(\\epsilon_t) + \\theta_1^2\\text{Var}(\\epsilon_{t-1}) + \\theta_2^2\\text{Var}(\\epsilon_{t-2})$$\n$$= \\sigma^2 + \\theta_1^2\\sigma^2 + \\theta_2^2\\sigma^2$$\n$$= \\sigma^2(1 + \\theta_1^2 + \\theta_2^2)$$\n\n**General formula for MA(q):**\n$$\\gamma(0) = \\sigma^2\\sum_{j=0}^{q}\\theta_j^2 \\text{ where } \\theta_0 = 1$$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Forgetting that $\\theta_0 = 1$ by convention. The \"1\" in the formula comes from the $\\epsilon_t$ term (coefficient 1).\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You fit an MA(1) model and get $\\hat{\\theta} = 1.2$. What should you do?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The estimate $\\hat{\\theta} = 1.2$ is outside the invertibility region. Options:\n1. Flip to the invertible form: use $\\theta' = 1/1.2 = 0.833$\n2. Re-fit with invertibility constraints enforced\n3. Reconsider model specification (maybe ARMA is better)\n\n<strong>Explanation:</strong>\nMA(1) with $\\theta = 1.2$ and MA(1) with $\\theta = 0.833$ produce identical ACFs:\n- $\\rho(1) = \\frac{1.2}{1+1.44} = \\frac{0.833}{1+0.694} = 0.492$\n\nBut only $\\theta = 0.833$ is invertible. When computing forecasts or residuals, the invertible form is needed.\n\n**Action plan:**\n1. Check if software enforces invertibility automatically\n2. If not, manually transform: $\\theta_{new} = 1/\\hat{\\theta}$\n3. Adjust variance estimate: $\\sigma^2_{new} = \\hat{\\sigma}^2 \\cdot \\hat{\\theta}^2$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Ignoring non-invertibility warnings. The non-invertible model will give poor forecasts because the AR(∞) expansion diverges.\n</div>\n</div>\n</details>", "line_start": 150}, "docs/en/time-domain/ma.md#section_14": {"doc_id": "docs/en/time-domain/ma.md", "heading": "References", "content": "1. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis*. Wiley. Chapter 4.\n2. Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press. Chapter 4.\n3. Brockwell, P. J., & Davis, R. A. (2016). *Introduction to Time Series and Forecasting*. Springer. Chapter 3.\n4. Shumway, R. H., & Stoffer, D. S. (2017). *Time Series Analysis and Its Applications*. Springer. Chapter 3.", "line_start": 268}, "docs/en/time-domain/arma.md#section_0": {"doc_id": "docs/en/time-domain/arma.md", "heading": "ARMA Models", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> ARMA(p,q) combines AR and MA components: $\\Phi(L)X_t = \\Theta(L)\\epsilon_t$. Both ACF and PACF tail off (decay). Stationarity depends on AR part; invertibility depends on MA part. Estimation via MLE. More parsimonious than pure AR or MA when both patterns present.\n</div>", "line_start": 1}, "docs/en/time-domain/arma.md#section_1": {"doc_id": "docs/en/time-domain/arma.md", "heading": "Core Definitions", "content": "**ARMA(p,q) Model**:\n$$X_t = c + \\phi_1 X_{t-1} + \\cdots + \\phi_p X_{t-p} + \\epsilon_t + \\theta_1\\epsilon_{t-1} + \\cdots + \\theta_q\\epsilon_{t-q}$$\n\n**Lag Operator Form**:\n$$\\Phi(L)X_t = c + \\Theta(L)\\epsilon_t$$\n\nwhere:\n- $\\Phi(L) = 1 - \\phi_1 L - \\cdots - \\phi_p L^p$ (AR polynomial)\n- $\\Theta(L) = 1 + \\theta_1 L + \\cdots + \\theta_q L^q$ (MA polynomial)\n\n**Mean**: $\\mu = \\frac{c}{1 - \\phi_1 - \\cdots - \\phi_p}$\n\n**Stationarity**: Roots of $\\Phi(z) = 0$ outside unit circle\n\n**Invertibility**: Roots of $\\Theta(z) = 0$ outside unit circle", "line_start": 7}, "docs/en/time-domain/arma.md#section_2": {"doc_id": "docs/en/time-domain/arma.md", "heading": "ARMA(1,1): $X_t = c + \\phi X_{t-1} + \\epsilon_t + \\theta\\epsilon_{t-1}$", "content": "**Stationarity**: $|\\phi| < 1$\n\n**Invertibility**: $|\\theta| < 1$\n\n**Mean**: $\\mu = \\frac{c}{1-\\phi}$\n\n**Variance** (assuming $\\mu = 0$ for simplicity):\n$$\\gamma(0) = \\phi\\gamma(1) + \\sigma^2(1 + \\theta\\phi + \\theta^2)$$\n\nSolving with $\\gamma(1) = \\phi\\gamma(0) + \\theta\\sigma^2$:\n$$\\gamma(0) = \\sigma^2 \\frac{1 + 2\\theta\\phi + \\theta^2}{1-\\phi^2}$$\n\n**ACF**:\n$$\\rho(1) = \\frac{(1+\\theta\\phi)(\\phi+\\theta)}{1 + 2\\theta\\phi + \\theta^2}$$\n$$\\rho(h) = \\phi\\rho(h-1) \\text{ for } h \\geq 2$$\n\nNote: ACF decays like AR(1) after lag 1, but $\\rho(1)$ differs from AR(1).", "line_start": 27}, "docs/en/time-domain/arma.md#section_3": {"doc_id": "docs/en/time-domain/arma.md", "heading": "General ARMA(p,q) ACF", "content": "For $h > q$:\n$$\\gamma(h) = \\phi_1\\gamma(h-1) + \\phi_2\\gamma(h-2) + \\cdots + \\phi_p\\gamma(h-p)$$\n\nThe ACF satisfies the same recursion as AR(p) for lags beyond q. Initial values $\\gamma(0), \\ldots, \\gamma(q)$ depend on both AR and MA parameters.", "line_start": 47}, "docs/en/time-domain/arma.md#section_4": {"doc_id": "docs/en/time-domain/arma.md", "heading": "Causal and Invertible Representations", "content": "**Causal (MA(∞)) form**: If stationary:\n$$X_t = \\mu + \\sum_{j=0}^{\\infty}\\psi_j\\epsilon_{t-j}$$\n\nwhere $\\psi_j$ coefficients come from $\\Psi(L) = \\Theta(L)/\\Phi(L)$.\n\n**Invertible (AR(∞)) form**: If invertible:\n$$\\Pi(L)(X_t - \\mu) = \\epsilon_t$$\n\nwhere $\\Pi(L) = \\Phi(L)/\\Theta(L)$.", "line_start": 54}, "docs/en/time-domain/arma.md#section_5": {"doc_id": "docs/en/time-domain/arma.md", "heading": "Parameter Redundancy", "content": "**Critical**: Ensure AR and MA polynomials share no common roots (factors).\n\nExample: $X_t = 0.5X_{t-1} + \\epsilon_t - 0.5\\epsilon_{t-1}$\n\nHere $(1-0.5L)X_t = (1-0.5L)\\epsilon_t$, which simplifies to $X_t = \\epsilon_t$ (white noise!).\n\nThis is called a **common factor** or **parameter redundancy**.", "line_start": 66}, "docs/en/time-domain/arma.md#section_6": {"doc_id": "docs/en/time-domain/arma.md", "heading": "Algorithm/Model Sketch", "content": "**Identification:**\n```\n1. Check stationarity; difference if needed\n2. Examine ACF and PACF:\n   - Both tail off → ARMA (not pure AR or MA)\n   - ACF cuts off → likely MA\n   - PACF cuts off → likely AR\n3. Use EACF (Extended ACF) or information criteria\n4. Fit candidate models\n5. Compare AIC/BIC, check residuals\n```\n\n**Extended ACF (EACF) Method:**\n\nEACF simplifies identification by iteratively removing AR structure. The resulting table shows \"O\" pattern indicating (p,q) order.\n\n**Estimation:**\n\n1. **Conditional MLE**: Condition on initial values, maximize likelihood\n2. **Exact MLE**: Properly accounts for initial conditions\n3. **CSS (Conditional Sum of Squares)**: Minimize squared residuals\n\nMost software uses exact MLE by default.", "line_start": 76}, "docs/en/time-domain/arma.md#section_7": {"doc_id": "docs/en/time-domain/arma.md", "heading": "Common Pitfalls", "content": "1. **Over-parameterization**: ARMA(2,2) often not better than ARMA(1,1). Parsimony matters for forecasting.\n\n2. **Common factor problem**: ARMA(p,q) may reduce to ARMA(p-1,q-1) if polynomials share a root. Check for parameter redundancy.\n\n3. **Local optima**: ARMA likelihood can have multiple modes. Try different starting values.\n\n4. **Near-cancellation**: Parameters close to canceling (e.g., $\\phi \\approx \\theta$) cause estimation instability and inflated standard errors.\n\n5. **Identification confusion**: Both ACF and PACF tail off, but the patterns differ. Focus on overall decay rate and compare with theoretical patterns.\n\n6. **Forgetting conditions**: Need both stationarity (AR roots) AND invertibility (MA roots) outside unit circle.", "line_start": 102}, "docs/en/time-domain/arma.md#section_8": {"doc_id": "docs/en/time-domain/arma.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.stattools import acf, pacf", "line_start": 116}, "docs/en/time-domain/arma.md#section_9": {"doc_id": "docs/en/time-domain/arma.md", "heading": "Generate ARMA(1,1) process", "content": "np.random.seed(42)\nn = 500\nphi, theta = 0.7, 0.4\neps = np.random.randn(n + 1)\nX = np.zeros(n)\n\nX[0] = eps[1] + theta * eps[0]\nfor t in range(1, n):\n    X[t] = phi * X[t-1] + eps[t+1] + theta * eps[t]", "line_start": 123}, "docs/en/time-domain/arma.md#section_10": {"doc_id": "docs/en/time-domain/arma.md", "heading": "Both ACF and PACF should tail off", "content": "print(\"ACF (first 6):\", np.round(acf(X, nlags=5), 3))\nprint(\"PACF (first 6):\", np.round(pacf(X, nlags=5), 3))", "line_start": 134}, "docs/en/time-domain/arma.md#section_11": {"doc_id": "docs/en/time-domain/arma.md", "heading": "Fit ARMA(1,1)", "content": "model = ARIMA(X, order=(1, 0, 1)).fit()\nprint(f\"\\nTrue: phi={phi}, theta={theta}\")\nprint(f\"Estimated: phi={model.arparams[0]:.3f}, theta={model.maparams[0]:.3f}\")", "line_start": 138}, "docs/en/time-domain/arma.md#section_12": {"doc_id": "docs/en/time-domain/arma.md", "heading": "Check for parameter redundancy", "content": "print(f\"\\nPhi - Theta = {abs(model.arparams[0] - model.maparams[0]):.3f}\")", "line_start": 143}, "docs/en/time-domain/arma.md#section_13": {"doc_id": "docs/en/time-domain/arma.md", "heading": "Compare with pure AR and pure MA", "content": "ar_aic = ARIMA(X, order=(2, 0, 0)).fit().aic\nma_aic = ARIMA(X, order=(0, 0, 2)).fit().aic\narma_aic = model.aic\nprint(f\"\\nAIC: AR(2)={ar_aic:.1f}, MA(2)={ma_aic:.1f}, ARMA(1,1)={arma_aic:.1f}\")\n```", "line_start": 147}, "docs/en/time-domain/arma.md#section_14": {"doc_id": "docs/en/time-domain/arma.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why can ARMA models be more parsimonious than pure AR or MA models?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Many real processes have both autoregressive dynamics (momentum/persistence) and shock effects that dissipate over time. Modeling this with pure AR or MA requires many parameters, while ARMA captures both with fewer parameters.\n\n<strong>Example:</strong> A process requiring AR(10) or MA(10) might be well-approximated by ARMA(1,1) with just 2 parameters.\n\n**Key insight:** ARMA(1,1) has infinite ACF decay (like AR(∞)) and infinite PACF decay (like MA(∞)), achieving complex correlation structure parsimoniously.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Assuming more parameters is better. ARMA(3,3) often overfits. Start simple—ARMA(1,1) is frequently sufficient.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> What is parameter redundancy in ARMA models and why is it problematic?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Parameter redundancy (common factor problem) occurs when AR and MA polynomials share a root, causing them to cancel. The model reduces to a lower-order ARMA.\n\n<strong>Example:</strong>\n$$(1 - 0.5L)X_t = (1 - 0.5L)\\epsilon_t$$\n\nBoth sides have factor $(1-0.5L)$. Canceling gives $X_t = \\epsilon_t$.\n\n**Problems:**\n1. Extra parameters don't improve fit\n2. Estimation becomes unstable (nearly singular Hessian)\n3. Standard errors explode\n4. Misleading model complexity\n\n**Detection:** Check if AR and MA roots are close. Large standard errors suggest near-redundancy.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Not checking for common factors. Software may fit ARMA(2,2) when ARMA(1,1) suffices, leading to unstable estimates.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> For ARMA(1,1), show that the ACF follows $\\rho(h) = \\phi^{h-1}\\rho(1)$ for $h \\geq 1$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> For $h \\geq 2$, the autocovariance satisfies the AR(1) recursion: $\\gamma(h) = \\phi\\gamma(h-1)$.\n\n<strong>Derivation:</strong>\nMultiply both sides of $X_t = \\phi X_{t-1} + \\epsilon_t + \\theta\\epsilon_{t-1}$ by $X_{t-h}$ and take expectations:\n\nFor $h \\geq 2$:\n$$E[X_t X_{t-h}] = \\phi E[X_{t-1}X_{t-h}] + E[\\epsilon_t X_{t-h}] + \\theta E[\\epsilon_{t-1}X_{t-h}]$$\n\nSince $\\epsilon_t$ and $\\epsilon_{t-1}$ are uncorrelated with $X_{t-h}$ when $h \\geq 2$:\n$$\\gamma(h) = \\phi\\gamma(h-1)$$\n\nTherefore:\n$$\\gamma(h) = \\phi^{h-1}\\gamma(1)$$\n$$\\rho(h) = \\phi^{h-1}\\rho(1)$$\n\n**Note:** $\\rho(1)$ itself is not simply $\\phi$ — it depends on both $\\phi$ and $\\theta$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Assuming ARMA(1,1) ACF at lag 1 equals $\\phi$. The MA component modifies $\\rho(1)$; only subsequent lags follow pure AR(1) decay.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Derive the condition for an ARMA(1,1) to be both stationary and invertible.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Stationarity requires $|\\phi| < 1$; invertibility requires $|\\theta| < 1$.\n\n<strong>Derivation:</strong>\n\n**Stationarity:**\nAR polynomial: $\\Phi(z) = 1 - \\phi z$\nRoot: $z = 1/\\phi$\nOutside unit circle: $|1/\\phi| > 1 \\Rightarrow |\\phi| < 1$\n\n**Invertibility:**\nMA polynomial: $\\Theta(z) = 1 + \\theta z$\nRoot: $z = -1/\\theta$\nOutside unit circle: $|{-1/\\theta}| > 1 \\Rightarrow |\\theta| < 1$\n\n**Combined:** The process is stationary and invertible iff $|\\phi| < 1$ AND $|\\theta| < 1$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Checking only stationarity. A stationary but non-invertible ARMA has improper AR(∞) representation, causing forecasting and diagnostic issues.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You fit several models and get: AR(2) AIC=520, MA(2) AIC=525, ARMA(1,1) AIC=515, ARMA(2,1) AIC=514. Which model do you choose and why?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Choose ARMA(1,1) despite ARMA(2,1) having slightly lower AIC.\n\n<strong>Reasoning:</strong>\n1. AIC difference of 1 is negligible (within noise)\n2. Parsimony principle: simpler model preferred when performance similar\n3. ARMA(1,1) is more stable and interpretable\n4. Additional AR parameter unlikely to improve forecasts\n\n**Decision framework:**\n- AIC difference < 2: models essentially equivalent\n- AIC difference 2-7: some evidence for lower AIC model\n- AIC difference > 10: strong evidence for lower AIC model\n\nAlso consider:\n- BIC (penalizes complexity more)\n- Out-of-sample forecast accuracy\n- Residual diagnostics for all candidates\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Blindly choosing lowest AIC. Small AIC differences are not meaningful. Always consider parsimony and validate with holdout data.\n</div>\n</div>\n</details>", "line_start": 154}, "docs/en/time-domain/arma.md#section_15": {"doc_id": "docs/en/time-domain/arma.md", "heading": "References", "content": "1. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis*. Wiley. Chapter 4.\n2. Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press. Chapter 4.\n3. Tsay, R. S. (2010). *Analysis of Financial Time Series*. Wiley. Chapter 2.\n4. Shumway, R. H., & Stoffer, D. S. (2017). *Time Series Analysis and Its Applications*. Springer. Chapter 3.", "line_start": 278}, "docs/en/time-domain/ar.md#section_0": {"doc_id": "docs/en/time-domain/ar.md", "heading": "Autoregressive (AR) Models", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> AR(p) models express current value as a linear combination of p past values plus noise. Stationarity requires roots of characteristic polynomial outside unit circle (equivalently, $|\\phi| < 1$ for AR(1)). ACF decays exponentially/sinusoidally; PACF cuts off after lag p. Estimate via Yule-Walker, OLS, or MLE.\n</div>", "line_start": 1}, "docs/en/time-domain/ar.md#section_1": {"doc_id": "docs/en/time-domain/ar.md", "heading": "Core Definitions", "content": "**AR(p) Model**:\n$$X_t = c + \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + \\cdots + \\phi_p X_{t-p} + \\epsilon_t$$\n\nwhere $\\epsilon_t \\sim WN(0, \\sigma^2)$ (white noise).\n\n**Lag Operator Form**:\n$$\\Phi(L)X_t = c + \\epsilon_t$$\n\nwhere $\\Phi(L) = 1 - \\phi_1 L - \\phi_2 L^2 - \\cdots - \\phi_p L^p$ and $LX_t = X_{t-1}$.\n\n**Characteristic Polynomial**:\n$$\\Phi(z) = 1 - \\phi_1 z - \\phi_2 z^2 - \\cdots - \\phi_p z^p$$\n\n**Stationarity Condition**: All roots of $\\Phi(z) = 0$ must lie outside the unit circle (|z| > 1).\n\n**Mean of Stationary AR(p)**:\n$$\\mu = E[X_t] = \\frac{c}{1 - \\phi_1 - \\phi_2 - \\cdots - \\phi_p}$$", "line_start": 7}, "docs/en/time-domain/ar.md#section_2": {"doc_id": "docs/en/time-domain/ar.md", "heading": "AR(1) Model: $X_t = c + \\phi X_{t-1} + \\epsilon_t$", "content": "**Stationarity condition**: $|\\phi| < 1$\n\n**Mean**: $\\mu = \\frac{c}{1-\\phi}$\n\n**Variance**:\n$$\\gamma(0) = \\text{Var}(X_t) = \\frac{\\sigma^2}{1-\\phi^2}$$\n\n**Autocovariance**:\n$$\\gamma(h) = \\phi^{|h|} \\gamma(0) = \\frac{\\phi^{|h|} \\sigma^2}{1-\\phi^2}$$\n\n**ACF**: $\\rho(h) = \\phi^{|h|}$\n\n**PACF**: $\\phi_{11} = \\phi$, $\\phi_{hh} = 0$ for $h > 1$", "line_start": 29}, "docs/en/time-domain/ar.md#section_3": {"doc_id": "docs/en/time-domain/ar.md", "heading": "AR(2) Model: $X_t = c + \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + \\epsilon_t$", "content": "**Stationarity conditions** (all must hold):\n1. $\\phi_1 + \\phi_2 < 1$\n2. $\\phi_2 - \\phi_1 < 1$\n3. $|\\phi_2| < 1$\n\n**Characteristic roots**: Solutions to $1 - \\phi_1 z - \\phi_2 z^2 = 0$\n- If roots are real: monotonic decay in ACF\n- If roots are complex: damped sinusoidal ACF\n\n**Yule-Walker equations for AR(2)**:\n$$\\rho(1) = \\phi_1 + \\phi_2\\rho(1) \\Rightarrow \\rho(1) = \\frac{\\phi_1}{1-\\phi_2}$$\n$$\\rho(2) = \\phi_1\\rho(1) + \\phi_2$$", "line_start": 45}, "docs/en/time-domain/ar.md#section_4": {"doc_id": "docs/en/time-domain/ar.md", "heading": "General AR(p) Yule-Walker Equations", "content": "$$\\gamma(h) = \\phi_1\\gamma(h-1) + \\phi_2\\gamma(h-2) + \\cdots + \\phi_p\\gamma(h-p) \\text{ for } h > 0$$\n\nIn matrix form:\n$$\\begin{pmatrix} \\gamma(0) & \\gamma(1) & \\cdots & \\gamma(p-1) \\\\ \\gamma(1) & \\gamma(0) & \\cdots & \\gamma(p-2) \\\\ \\vdots & & \\ddots & \\vdots \\\\ \\gamma(p-1) & \\cdots & \\gamma(1) & \\gamma(0) \\end{pmatrix} \\begin{pmatrix} \\phi_1 \\\\ \\phi_2 \\\\ \\vdots \\\\ \\phi_p \\end{pmatrix} = \\begin{pmatrix} \\gamma(1) \\\\ \\gamma(2) \\\\ \\vdots \\\\ \\gamma(p) \\end{pmatrix}$$\n\nOr in terms of autocorrelations:\n$$\\mathbf{R}\\boldsymbol{\\phi} = \\boldsymbol{\\rho}$$", "line_start": 60}, "docs/en/time-domain/ar.md#section_5": {"doc_id": "docs/en/time-domain/ar.md", "heading": "Infinite MA Representation", "content": "A stationary AR(p) can be written as an infinite MA:\n$$X_t = \\mu + \\sum_{j=0}^{\\infty} \\psi_j \\epsilon_{t-j}$$\n\nFor AR(1): $\\psi_j = \\phi^j$\n\nThis shows AR processes have infinite memory but with exponentially decaying weights.", "line_start": 70}, "docs/en/time-domain/ar.md#section_6": {"doc_id": "docs/en/time-domain/ar.md", "heading": "Algorithm/Model Sketch", "content": "**Estimation Methods:**\n\n1. **Yule-Walker (Method of Moments)**:\n   - Replace $\\gamma(h)$ with $\\hat{\\gamma}(h)$\n   - Solve linear system for $\\hat{\\phi}$\n   - Always yields stationary estimates\n   - May be inefficient for small samples\n\n2. **Ordinary Least Squares (OLS)**:\n   - Regress $X_t$ on $X_{t-1}, \\ldots, X_{t-p}$\n   - Simple but loses first $p$ observations\n   - May give non-stationary estimates\n\n3. **Maximum Likelihood (MLE)**:\n   - Most efficient asymptotically\n   - Accounts for initial conditions\n   - Requires distributional assumption (usually Gaussian)\n   - Use numerical optimization\n\n**Order Selection:**\n```\n1. Examine PACF - significant spikes suggest AR order\n2. Fit AR(1), AR(2), ..., AR(p_max)\n3. Compare AIC/BIC values\n4. Select model with lowest information criterion\n5. Verify residuals are white noise\n```", "line_start": 79}, "docs/en/time-domain/ar.md#section_7": {"doc_id": "docs/en/time-domain/ar.md", "heading": "Common Pitfalls", "content": "1. **Ignoring stationarity check**: Always verify estimated parameters satisfy stationarity conditions. Non-stationary AR leads to explosive forecasts.\n\n2. **Over-fitting with too many lags**: AIC may favor larger models. BIC penalizes complexity more and often gives better forecasts.\n\n3. **Assuming causality**: AR models capture correlation, not causation. $X_{t-1}$ predicting $X_t$ doesn't mean past causes future.\n\n4. **Neglecting seasonality**: Standard AR doesn't capture seasonal patterns at lag $s$. Consider SARIMA or include $X_{t-s}$ explicitly.\n\n5. **Using OLS without correction**: Standard OLS standard errors are invalid for time series with autocorrelation. Use HAC standard errors or proper likelihood-based inference.\n\n6. **Confusing AR(1) coefficient sign**: Positive $\\phi$ gives positive autocorrelation (momentum). Negative $\\phi$ gives alternating signs (mean reversion).", "line_start": 109}, "docs/en/time-domain/ar.md#section_8": {"doc_id": "docs/en/time-domain/ar.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom statsmodels.tsa.stattools import pacf", "line_start": 123}, "docs/en/time-domain/ar.md#section_9": {"doc_id": "docs/en/time-domain/ar.md", "heading": "Generate AR(2) data", "content": "np.random.seed(42)\nn = 300\nphi1, phi2 = 0.6, -0.3\nX = np.zeros(n)\neps = np.random.randn(n)\n\nfor t in range(2, n):\n    X[t] = phi1 * X[t-1] + phi2 * X[t-2] + eps[t]", "line_start": 130}, "docs/en/time-domain/ar.md#section_10": {"doc_id": "docs/en/time-domain/ar.md", "heading": "Check PACF (should cut off after lag 2)", "content": "pacf_values = pacf(X, nlags=10)\nprint(\"PACF:\", np.round(pacf_values, 3))", "line_start": 140}, "docs/en/time-domain/ar.md#section_11": {"doc_id": "docs/en/time-domain/ar.md", "heading": "Fit AR model using AIC to select order", "content": "from statsmodels.tsa.ar_model import ar_select_order\nsel = ar_select_order(X, maxlag=10, ic='aic')\nprint(f\"Selected order: {sel.ar_lags}\")", "line_start": 144}, "docs/en/time-domain/ar.md#section_12": {"doc_id": "docs/en/time-domain/ar.md", "heading": "Fit AR(2) and check estimates", "content": "model = AutoReg(X, lags=2).fit()\nprint(f\"True: phi1={phi1}, phi2={phi2}\")\nprint(f\"Estimated: phi1={model.params[1]:.3f}, phi2={model.params[2]:.3f}\")", "line_start": 149}, "docs/en/time-domain/ar.md#section_13": {"doc_id": "docs/en/time-domain/ar.md", "heading": "Forecast", "content": "forecast = model.forecast(steps=5)\nprint(\"5-step forecast:\", forecast)\n```", "line_start": 154}, "docs/en/time-domain/ar.md#section_14": {"doc_id": "docs/en/time-domain/ar.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Explain intuitively why the stationarity condition for AR(1) is $|\\phi| < 1$. What happens when $\\phi = 1$ or $\\phi > 1$?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> When $|\\phi| < 1$, shocks decay over time, keeping variance bounded. When $\\phi = 1$, we have a random walk where shocks persist forever (unit root). When $|\\phi| > 1$, the process explodes exponentially.\n\n<strong>Explanation:</strong>\nThe AR(1) can be written as:\n$$X_t = \\phi^t X_0 + \\sum_{j=0}^{t-1} \\phi^j \\epsilon_{t-j}$$\n\n- If $|\\phi| < 1$: $\\phi^t \\to 0$ and the MA representation converges (bounded variance)\n- If $\\phi = 1$: $X_t = X_0 + \\sum \\epsilon_j$ (random walk, variance $\\to \\infty$)\n- If $|\\phi| > 1$: $\\phi^t \\to \\infty$ (explosive)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Thinking $\\phi = 0.99$ is \"close enough\" to stationary. While technically stationary, near-unit-root processes behave like random walks in finite samples. Predictions degrade quickly.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Why does the PACF of an AR(p) process cut off after lag p while the ACF decays gradually?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> PACF measures direct correlation after controlling for intermediate lags. AR(p) by definition only has direct dependence on $p$ past values, so PACF is zero beyond lag $p$. ACF includes indirect effects through intermediate values, causing gradual decay.\n\n<strong>Explanation:</strong>\nFor AR(2): $X_t = \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + \\epsilon_t$\n- Direct effects: only from $X_{t-1}$ and $X_{t-2}$\n- PACF at lag 3: After controlling for $X_{t-1}, X_{t-2}$, $X_{t-3}$ has no additional predictive power\n- But ACF at lag 3: $X_t$ correlates with $X_{t-3}$ through the chain $X_{t-1} \\to X_{t-2} \\to X_{t-3}$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Expecting perfectly zero PACF beyond lag $p$ in samples. Due to estimation error, you'll see small nonzero values. Use confidence bands to judge significance.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Derive the variance of a stationary AR(1) process: $\\text{Var}(X_t) = \\frac{\\sigma^2}{1-\\phi^2}$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Starting from $X_t = \\phi X_{t-1} + \\epsilon_t$, take variance of both sides and use stationarity.\n\n<strong>Derivation:</strong>\n$$\\text{Var}(X_t) = \\text{Var}(\\phi X_{t-1} + \\epsilon_t)$$\n$$= \\phi^2 \\text{Var}(X_{t-1}) + \\text{Var}(\\epsilon_t) + 2\\phi\\text{Cov}(X_{t-1}, \\epsilon_t)$$\n\nSince $\\epsilon_t$ is independent of $X_{t-1}$:\n$$\\gamma(0) = \\phi^2 \\gamma(0) + \\sigma^2$$\n\nBy stationarity, $\\text{Var}(X_t) = \\text{Var}(X_{t-1}) = \\gamma(0)$:\n$$\\gamma(0) - \\phi^2\\gamma(0) = \\sigma^2$$\n$$\\gamma(0)(1 - \\phi^2) = \\sigma^2$$\n$$\\gamma(0) = \\frac{\\sigma^2}{1-\\phi^2}$$\n\n**Note:** Requires $|\\phi| < 1$ for positive variance.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Forgetting that variance increases as $|\\phi| \\to 1$. Near-unit-root processes have large variance, making them look more volatile than white noise with the same $\\sigma^2$.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> For AR(2), show that the stationarity region in the $(\\phi_1, \\phi_2)$ plane is triangular with vertices at $(2, -1)$, $(-2, -1)$, and $(0, 1)$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The three stationarity conditions $\\phi_1 + \\phi_2 < 1$, $\\phi_2 - \\phi_1 < 1$, and $|\\phi_2| < 1$ define a triangular region.\n\n<strong>Derivation:</strong>\nThe characteristic equation $1 - \\phi_1 z - \\phi_2 z^2 = 0$ must have roots outside unit circle.\n\nSetting $z = 1$: $1 - \\phi_1 - \\phi_2 > 0 \\Rightarrow \\phi_1 + \\phi_2 < 1$\nSetting $z = -1$: $1 + \\phi_1 - \\phi_2 > 0 \\Rightarrow \\phi_2 - \\phi_1 < 1$\n\nFor complex roots, discriminant analysis gives: $|\\phi_2| < 1$\n\nBoundary lines:\n- $\\phi_1 + \\phi_2 = 1$ (passes through $(2, -1)$ and $(0, 1)$)\n- $\\phi_2 - \\phi_1 = 1$ (passes through $(-2, -1)$ and $(0, 1)$)\n- $\\phi_2 = -1$ (connects $(2, -1)$ and $(-2, -1)$)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Checking only one condition. All three must hold simultaneously. A model can satisfy two conditions but fail the third and still be non-stationary.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You fit an AR(3) model and get estimates $\\hat{\\phi}_1 = 0.5$, $\\hat{\\phi}_2 = 0.3$, $\\hat{\\phi}_3 = 0.25$. The residual ACF shows a significant spike at lag 1. What might be wrong?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> A significant residual autocorrelation at lag 1 indicates model misspecification. Possible causes:\n1. MA component is needed (ARMA instead of pure AR)\n2. Structural break in the data\n3. Outliers affecting estimation\n4. Non-stationarity not fully addressed\n\n<strong>Diagnostic steps:</strong>\n1. Perform Ljung-Box test on residuals\n2. Try fitting ARMA(3,1) or ARMA(3,2)\n3. Plot residuals over time to check for patterns\n4. Check for outliers or level shifts\n5. Verify original series was stationary\n\n**Key insight:** Pure AR residuals should be white noise. Significant residual autocorrelation means the model hasn't captured all temporal dependence.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Adding more AR lags to fix residual correlation. Sometimes an MA term is more parsimonious. Compare AIC between AR(4), AR(5) and ARMA(3,1).\n</div>\n</div>\n</details>", "line_start": 159}, "docs/en/time-domain/ar.md#section_15": {"doc_id": "docs/en/time-domain/ar.md", "heading": "References", "content": "1. Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press. Chapter 3.\n2. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis*. Wiley. Chapter 3.\n3. Brockwell, P. J., & Davis, R. A. (2016). *Introduction to Time Series and Forecasting*. Springer. Chapter 3.\n4. Lütkepohl, H. (2005). *New Introduction to Multiple Time Series Analysis*. Springer. Chapter 2.", "line_start": 275}, "docs/en/time-domain/arima.md#section_0": {"doc_id": "docs/en/time-domain/arima.md", "heading": "ARIMA Models", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> ARIMA(p,d,q) extends ARMA to non-stationary series by including differencing. The \"I\" stands for \"integrated\" — meaning the series needs d differences to become stationary. After differencing d times, fit ARMA(p,q) to the differenced series. Most commonly d=1 (first difference) or d=2 (second difference).\n</div>", "line_start": 1}, "docs/en/time-domain/arima.md#section_1": {"doc_id": "docs/en/time-domain/arima.md", "heading": "Core Definitions", "content": "**ARIMA(p,d,q) Model**:\n\nApply ARMA(p,q) to the d-th difference of $X_t$:\n$$\\Phi(L)(1-L)^d X_t = c + \\Theta(L)\\epsilon_t$$\n\n**Components:**\n- $p$: AR order (autoregressive lags)\n- $d$: Degree of differencing (integration order)\n- $q$: MA order (moving average lags)\n\n**Difference Operator**:\n$$\\nabla X_t = (1-L)X_t = X_t - X_{t-1}$$\n$$\\nabla^2 X_t = X_t - 2X_{t-1} + X_{t-2}$$\n\n**Integrated Process**: A process is integrated of order d, written $I(d)$, if it requires d differences to become stationary.", "line_start": 7}, "docs/en/time-domain/arima.md#section_2": {"doc_id": "docs/en/time-domain/arima.md", "heading": "ARIMA(0,1,0): Random Walk", "content": "$$X_t = X_{t-1} + \\epsilon_t$$\n\nOr equivalently: $(1-L)X_t = \\epsilon_t$\n\nThe first difference $\\nabla X_t = \\epsilon_t$ is white noise (stationary).", "line_start": 27}, "docs/en/time-domain/arima.md#section_3": {"doc_id": "docs/en/time-domain/arima.md", "heading": "ARIMA(0,1,0) with Drift", "content": "$$X_t = c + X_{t-1} + \\epsilon_t$$\n\nThe drift $c$ creates a linear trend in levels:\n$$E[X_t] = X_0 + ct$$", "line_start": 35}, "docs/en/time-domain/arima.md#section_4": {"doc_id": "docs/en/time-domain/arima.md", "heading": "ARIMA(1,1,0): Differenced AR(1)", "content": "$$\\nabla X_t = \\phi \\nabla X_{t-1} + \\epsilon_t$$\n\nExpanding:\n$$(X_t - X_{t-1}) = \\phi(X_{t-1} - X_{t-2}) + \\epsilon_t$$\n$$X_t = (1+\\phi)X_{t-1} - \\phi X_{t-2} + \\epsilon_t$$\n\nThis is AR(2) in levels with a unit root.", "line_start": 42}, "docs/en/time-domain/arima.md#section_5": {"doc_id": "docs/en/time-domain/arima.md", "heading": "ARIMA(0,1,1): IMA(1,1)", "content": "$$\\nabla X_t = \\epsilon_t + \\theta\\epsilon_{t-1}$$\n\nAlso known as an **exponentially weighted moving average (EWMA)** process. Forms the basis for simple exponential smoothing.", "line_start": 52}, "docs/en/time-domain/arima.md#section_6": {"doc_id": "docs/en/time-domain/arima.md", "heading": "General ARIMA(p,d,q)", "content": "In lag operator notation:\n$$\\Phi(L)(1-L)^d X_t = c + \\Theta(L)\\epsilon_t$$\n\nWhere:\n- $\\Phi(L) = 1 - \\phi_1 L - \\cdots - \\phi_p L^p$ has roots outside unit circle (stationary AR)\n- $\\Theta(L) = 1 + \\theta_1 L + \\cdots + \\theta_q L^q$ has roots outside unit circle (invertible MA)\n- $(1-L)^d$ contributes $d$ unit roots", "line_start": 58}, "docs/en/time-domain/arima.md#section_7": {"doc_id": "docs/en/time-domain/arima.md", "heading": "Forecasting with ARIMA", "content": "For ARIMA(p,1,q), the h-step forecast:\n$$\\hat{X}_{T+h|T} = E[X_{T+h} | X_T, X_{T-1}, \\ldots]$$\n\nKey property: forecasts revert to a linear trend (if drift) or constant growth for $d \\geq 1$.\n\n**Prediction intervals** widen with horizon due to accumulated uncertainty.", "line_start": 68}, "docs/en/time-domain/arima.md#section_8": {"doc_id": "docs/en/time-domain/arima.md", "heading": "Algorithm/Model Sketch", "content": "**Box-Jenkins Methodology:**\n\n```\n1. IDENTIFICATION\n   - Plot series; check for trend/non-stationarity\n   - Apply ADF/KPSS tests\n   - Difference until stationary (determine d)\n   - Examine ACF/PACF of differenced series\n   - Identify candidate (p, q) orders\n\n2. ESTIMATION\n   - Fit candidate ARIMA models\n   - Use MLE (or CSS for initial values)\n   - Check parameter significance\n\n3. DIAGNOSTICS\n   - Examine residuals: ACF should show no pattern\n   - Ljung-Box test for residual autocorrelation\n   - Check residual normality (Q-Q plot)\n   - Look for outliers\n\n4. FORECASTING\n   - Generate point forecasts\n   - Compute prediction intervals\n   - Validate on holdout data if possible\n```\n\n**Determining d:**\n\n| Symptom | Likely d |\n|---------|----------|\n| Series wanders, slow ACF decay | d = 1 |\n| Trend in differenced series | d = 2 |\n| Seasonal pattern persists | Need seasonal differencing |\n| Already fluctuates around mean | d = 0 |", "line_start": 77}, "docs/en/time-domain/arima.md#section_9": {"doc_id": "docs/en/time-domain/arima.md", "heading": "Common Pitfalls", "content": "1. **Over-differencing**: If the original series is stationary, differencing introduces MA(1) with $\\theta = -1$. Check: if ACF of differenced series has large negative spike at lag 1, you may have over-differenced.\n\n2. **Under-differencing**: ACF that doesn't decay or stays significant at high lags suggests more differencing needed. Also check KPSS test.\n\n3. **Ignoring drift**: ARIMA(0,1,0) without constant is pure random walk. With drift, there's a trend. Misspecifying this affects long-term forecasts.\n\n4. **d > 2 rarely needed**: If you need d > 2, reconsider—series might have other issues (outliers, structural breaks, wrong transformation).\n\n5. **Confusing trend types**: Deterministic trend (detrend with regression) vs. stochastic trend (difference). Using wrong approach gives poor results.\n\n6. **Negative forecasts**: For positive series (prices, counts), ARIMA may forecast negatives. Consider log transform or constrained models.", "line_start": 115}, "docs/en/time-domain/arima.md#section_10": {"doc_id": "docs/en/time-domain/arima.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.stattools import adfuller", "line_start": 129}, "docs/en/time-domain/arima.md#section_11": {"doc_id": "docs/en/time-domain/arima.md", "heading": "Generate ARIMA(1,1,1) process", "content": "np.random.seed(42)\nn = 300\nphi, theta = 0.5, 0.3\neps = np.random.randn(n + 2)", "line_start": 136}, "docs/en/time-domain/arima.md#section_12": {"doc_id": "docs/en/time-domain/arima.md", "heading": "First generate the differenced series as ARMA(1,1)", "content": "dX = np.zeros(n)\ndX[0] = eps[1] + theta * eps[0]\nfor t in range(1, n):\n    dX[t] = phi * dX[t-1] + eps[t+1] + theta * eps[t]", "line_start": 142}, "docs/en/time-domain/arima.md#section_13": {"doc_id": "docs/en/time-domain/arima.md", "heading": "Integrate to get X", "content": "X = np.cumsum(dX)", "line_start": 148}, "docs/en/time-domain/arima.md#section_14": {"doc_id": "docs/en/time-domain/arima.md", "heading": "Test stationarity", "content": "adf_X = adfuller(X)\nadf_dX = adfuller(np.diff(X))\nprint(f\"ADF p-value (levels): {adf_X[1]:.4f}\")  # Should be high (non-stationary)\nprint(f\"ADF p-value (differenced): {adf_dX[1]:.4f}\")  # Should be low (stationary)", "line_start": 151}, "docs/en/time-domain/arima.md#section_15": {"doc_id": "docs/en/time-domain/arima.md", "heading": "Fit ARIMA(1,1,1)", "content": "model = ARIMA(X, order=(1, 1, 1)).fit()\nprint(f\"\\nTrue: phi={phi}, theta={theta}\")\nprint(f\"Estimated: phi={model.arparams[0]:.3f}, theta={model.maparams[0]:.3f}\")", "line_start": 157}, "docs/en/time-domain/arima.md#section_16": {"doc_id": "docs/en/time-domain/arima.md", "heading": "Forecast", "content": "forecast = model.forecast(steps=10)\nconf_int = model.get_forecast(10).conf_int()\nprint(f\"\\n10-step forecast: {forecast[-1]:.2f}\")\nprint(f\"95% CI: [{conf_int.iloc[-1, 0]:.2f}, {conf_int.iloc[-1, 1]:.2f}]\")\n```", "line_start": 162}, "docs/en/time-domain/arima.md#section_17": {"doc_id": "docs/en/time-domain/arima.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What does the \"I\" in ARIMA stand for, and what does it mean for a process to be \"integrated of order d\"?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> \"I\" stands for \"Integrated.\" A process is integrated of order d, denoted I(d), if it requires exactly d differences to become stationary. Integration is the inverse of differencing — if you sum (integrate) a stationary series, you get an I(1) process.\n\n<strong>Explanation:</strong>\n- I(0): Stationary (no differencing needed)\n- I(1): First difference is stationary (e.g., random walk)\n- I(2): Second difference is stationary (e.g., random walk with drift in levels)\n\n**Key insight:** \"Integrated\" comes from continuous-time analogy. In discrete time: $X_t = \\sum_{s=1}^t \\epsilon_s$ (integrated/summed white noise) is I(1).\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Confusing integration order with polynomial degree. I(1) is not about linear trends—it's about the type of non-stationarity (stochastic vs. deterministic).\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> How can you tell if a series has been over-differenced?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Signs of over-differencing:\n1. ACF of differenced series shows large negative spike at lag 1 (often near -0.5)\n2. Variance increases after differencing (should decrease or stay similar)\n3. The differenced series looks \"over-corrected\" with excessive alternation\n\n<strong>Explanation:</strong>\nDifferencing a stationary series adds MA(1) structure with $\\theta \\approx -1$:\n$$(1-L)X_t = X_t - X_{t-1}$$\n\nIf $X_t$ was already stationary, the difference behaves like $\\epsilon_t - \\epsilon_{t-1}$, which is MA(1) with $\\theta = -1$ and $\\rho(1) = -0.5$.\n\n**Test:** If $d=1$ differencing gives ACF(1) ≈ -0.5 and all other ACF ≈ 0, try $d=0$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Automatically differencing because \"everyone does it.\" Always test stationarity first. Many series (especially returns) are already stationary.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Show that ARIMA(0,1,1) with parameter $\\theta$ produces forecasts equivalent to exponential smoothing with $\\alpha = 1/(1+\\theta)$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> ARIMA(0,1,1): $(1-L)X_t = (1+\\theta L)\\epsilon_t$\n\nThe optimal forecast can be written recursively as:\n$$\\hat{X}_{t+1|t} = \\hat{X}_{t|t-1} + (1+\\theta)^{-1}(X_t - \\hat{X}_{t|t-1})$$\n\nThis is exactly exponential smoothing: $\\hat{X}_{t+1} = \\alpha X_t + (1-\\alpha)\\hat{X}_t$ with $\\alpha = \\frac{1}{1+\\theta}$.\n\n<strong>Derivation:</strong>\nFrom ARIMA(0,1,1): $X_t = X_{t-1} + \\epsilon_t + \\theta\\epsilon_{t-1}$\n\nThe forecast error is:\n$$e_t = X_t - \\hat{X}_{t|t-1} = \\epsilon_t$$\n\nThe forecast update:\n$$\\hat{X}_{t+1|t} = X_t + \\theta\\hat{\\epsilon}_t = X_t + \\theta e_t$$\n\nRearranging:\n$$\\hat{X}_{t+1|t} = X_t + \\theta(X_t - \\hat{X}_{t|t-1})/(1+\\theta) \\cdot (1+\\theta)$$\n\nWith $\\alpha = 1/(1+\\theta)$: this gives the exponential smoothing recursion.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> For invertibility, need $|\\theta| < 1$, which means $\\alpha \\in (0.5, 1)$ for IMA(1,1). Values $\\alpha < 0.5$ correspond to non-invertible MA.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Why do prediction intervals for ARIMA models widen as the forecast horizon increases?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Future shocks $\\epsilon_{T+1}, \\epsilon_{T+2}, \\ldots$ are unknown, and their accumulated effect grows with horizon. For I(d) processes, shocks have permanent effects, causing variance to grow without bound.\n\n<strong>Derivation for random walk (ARIMA(0,1,0)):</strong>\n$$X_{T+h} = X_T + \\sum_{j=1}^{h}\\epsilon_{T+j}$$\n\nForecast: $\\hat{X}_{T+h|T} = X_T$\n\nError: $X_{T+h} - \\hat{X}_{T+h|T} = \\sum_{j=1}^{h}\\epsilon_{T+j}$\n\nVariance: $\\text{Var}(X_{T+h} - \\hat{X}_{T+h|T}) = h\\sigma^2$\n\n**95% PI:** $X_T \\pm 1.96\\sigma\\sqrt{h}$\n\nThe interval width grows like $\\sqrt{h}$, becoming arbitrarily wide.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Expecting narrow long-horizon intervals. ARIMA cannot provide tight long-range forecasts—uncertainty is fundamental. This is why judgment and scenarios matter for long-term planning.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You have monthly sales data showing a clear upward trend. After first differencing, the ACF shows significant spikes at lags 1, 12, and 13. What model structure might be appropriate?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The pattern suggests SARIMA with:\n- d=1 (first differencing handles trend)\n- Significant lag 1 suggests AR(1) or MA(1)\n- Significant lag 12 suggests seasonal component (monthly data, annual pattern)\n- Lag 13 = 12+1 is interaction of seasonal and non-seasonal\n\n**Candidate models:**\n- SARIMA(1,1,0)(1,0,0)[12]\n- SARIMA(0,1,1)(0,1,1)[12] (airline model)\n- SARIMA(1,1,1)(1,1,0)[12]\n\n**Next steps:**\n1. Apply seasonal differencing and re-check ACF\n2. Fit candidates and compare AIC\n3. Check residuals for remaining patterns\n4. Validate on holdout data\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Ignoring the seasonal spike and fitting non-seasonal ARIMA. The lag-12 autocorrelation will persist in residuals, degrading forecasts.\n</div>\n</div>\n</details>", "line_start": 169}, "docs/en/time-domain/arima.md#section_18": {"doc_id": "docs/en/time-domain/arima.md", "heading": "References", "content": "1. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis: Forecasting and Control*. Wiley. Chapters 4-6.\n2. Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press. Chapters 15, 17.\n3. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 9.\n4. Brockwell, P. J., & Davis, R. A. (2016). *Introduction to Time Series and Forecasting*. Springer. Chapter 5.", "line_start": 296}, "docs/en/time-domain/identification.md#section_0": {"doc_id": "docs/en/time-domain/identification.md", "heading": "Model Identification", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Model identification determines (p,d,q) orders using ACF/PACF patterns, unit root tests, and information criteria. AR(p): PACF cuts off at p. MA(q): ACF cuts off at q. ARMA: both tail off. Use ADF/KPSS for d. For complex cases, use AIC/BIC to compare candidates. Always validate with residual diagnostics.\n</div>", "line_start": 1}, "docs/en/time-domain/identification.md#section_1": {"doc_id": "docs/en/time-domain/identification.md", "heading": "Core Definitions", "content": "**Model Identification**: The process of determining:\n1. Transformation needed (log, Box-Cox)\n2. Order of differencing (d, D)\n3. AR order (p, P)\n4. MA order (q, Q)\n\n**ACF/PACF Patterns for Pure Models:**\n\n| Model | ACF | PACF |\n|-------|-----|------|\n| AR(p) | Tails off (exponential/sinusoidal decay) | Cuts off after lag p |\n| MA(q) | Cuts off after lag q | Tails off (exponential/sinusoidal decay) |\n| ARMA(p,q) | Tails off | Tails off |\n| Non-stationary | Very slow decay | Large spike at lag 1 |\n| White noise | All near zero | All near zero |\n\n**Significance Threshold**: Under white noise, $\\hat{\\rho}(h) \\sim N(0, 1/n)$, so use $\\pm 1.96/\\sqrt{n}$ for 95% bands.", "line_start": 7}, "docs/en/time-domain/identification.md#section_2": {"doc_id": "docs/en/time-domain/identification.md", "heading": "Information Criteria", "content": "**AIC (Akaike Information Criterion)**:\n$$\\text{AIC} = -2\\ln(\\hat{L}) + 2k$$\n\nwhere $\\hat{L}$ is maximum likelihood and $k$ is number of parameters.\n\nFor ARIMA with Gaussian errors:\n$$\\text{AIC} = n\\ln(\\hat{\\sigma}^2) + 2(p+q+1)$$\n\n**BIC (Bayesian/Schwarz Information Criterion)**:\n$$\\text{BIC} = -2\\ln(\\hat{L}) + k\\ln(n)$$\n\nBIC penalizes complexity more heavily than AIC.\n\n**AICc (Corrected AIC)**:\n$$\\text{AICc} = \\text{AIC} + \\frac{2k(k+1)}{n-k-1}$$\n\nPreferred for small samples.", "line_start": 29}, "docs/en/time-domain/identification.md#section_3": {"doc_id": "docs/en/time-domain/identification.md", "heading": "Extended ACF (EACF)", "content": "The EACF method iteratively removes AR structure to identify MA order:\n\n1. Fit AR(0), AR(1), AR(2), ... up to max\n2. For each AR(j), compute ACF of residuals\n3. Create table: rows = AR order, columns = MA order\n4. \"O\" indicates insignificant, \"X\" indicates significant\n5. Top-left corner of \"O\" triangle suggests (p,q)", "line_start": 49}, "docs/en/time-domain/identification.md#section_4": {"doc_id": "docs/en/time-domain/identification.md", "heading": "Unit Root Testing Strategy", "content": "**Combined ADF + KPSS approach:**\n\n| ADF result | KPSS result | Conclusion |\n|------------|-------------|------------|\n| Reject (p < 0.05) | Fail to reject | Stationary (d=0) |\n| Fail to reject | Reject | Non-stationary (d≥1) |\n| Reject | Reject | Possible structural break |\n| Fail to reject | Fail to reject | Inconclusive; try differencing |", "line_start": 59}, "docs/en/time-domain/identification.md#section_5": {"doc_id": "docs/en/time-domain/identification.md", "heading": "Ljung-Box Test for Residual Autocorrelation", "content": "$$Q(m) = n(n+2)\\sum_{k=1}^{m}\\frac{\\hat{\\rho}_k^2}{n-k}$$\n\nUnder null (white noise residuals): $Q(m) \\sim \\chi^2_{m-p-q}$\n\nReject if Q is large (residuals are not white noise).", "line_start": 70}, "docs/en/time-domain/identification.md#section_6": {"doc_id": "docs/en/time-domain/identification.md", "heading": "Algorithm/Model Sketch", "content": "**Complete Identification Procedure:**\n\n```\nSTEP 1: PRELIMINARY ANALYSIS\n- Plot the series\n- Check for obvious trend, seasonality, outliers\n- Apply transformation if variance changes with level (log, sqrt)\n\nSTEP 2: DETERMINE DIFFERENCING ORDER\n- ADF test: if p > 0.05, difference\n- KPSS test: if p < 0.05, difference\n- After differencing, re-test\n- Usually d ≤ 2; rarely d > 2\n\nSTEP 3: EXAMINE ACF/PACF OF STATIONARY SERIES\n- ACF cuts off at lag q → try MA(q)\n- PACF cuts off at lag p → try AR(p)\n- Both tail off → try ARMA\n\nSTEP 4: FIT CANDIDATE MODELS\n- Start simple: AR(1), MA(1), ARMA(1,1)\n- Add complexity as needed\n- Compare AIC/BIC\n\nSTEP 5: DIAGNOSTIC CHECKING\n- Residual ACF/PACF (should be white noise)\n- Ljung-Box test\n- Residual normality (Q-Q plot)\n- Parameter significance\n\nSTEP 6: SELECT FINAL MODEL\n- Lowest AIC/BIC among adequate models\n- Parsimony when AIC/BIC are close\n- Good residual diagnostics\n```\n\n**Auto-Selection Tools:**\n- `auto.arima()` in R (forecast package)\n- `pmdarima.auto_arima()` in Python\n- Uses stepwise search with AIC", "line_start": 78}, "docs/en/time-domain/identification.md#section_7": {"doc_id": "docs/en/time-domain/identification.md", "heading": "Common Pitfalls", "content": "1. **Mechanical ACF/PACF reading**: Patterns aren't always clean. Real data is noisy. Consider multiple interpretations.\n\n2. **Ignoring parsimony**: If ARMA(1,1) and ARMA(2,1) have similar AIC, prefer simpler model.\n\n3. **Over-relying on automated selection**: `auto.arima` is a good start but may miss important features. Always verify manually.\n\n4. **Forgetting seasonal patterns**: Check ACF at seasonal lags (12, 24, ... for monthly). Standard ARIMA won't capture these.\n\n5. **Ignoring residual diagnostics**: A model with lowest AIC can still have autocorrelated residuals. Always check.\n\n6. **Sample size issues**: With small samples (n < 50), ACF/PACF estimates are unreliable. Use simpler models and be conservative.", "line_start": 121}, "docs/en/time-domain/identification.md#section_8": {"doc_id": "docs/en/time-domain/identification.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.tsa.stattools import acf, pacf, adfuller, kpss\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.stats.diagnostic import acorr_ljungbox", "line_start": 135}, "docs/en/time-domain/identification.md#section_9": {"doc_id": "docs/en/time-domain/identification.md", "heading": "Generate ARIMA(2,1,1) data for identification exercise", "content": "np.random.seed(42)\nn = 250\nphi1, phi2, theta = 0.5, -0.2, 0.3\neps = np.random.randn(n + 3)", "line_start": 144}, "docs/en/time-domain/identification.md#section_10": {"doc_id": "docs/en/time-domain/identification.md", "heading": "Differenced series as ARMA(2,1)", "content": "dX = np.zeros(n)\nfor t in range(2, n):\n    dX[t] = phi1*dX[t-1] + phi2*dX[t-2] + eps[t+1] + theta*eps[t]\n\nX = np.cumsum(dX) + 50  # Integrate and add level", "line_start": 150}, "docs/en/time-domain/identification.md#section_11": {"doc_id": "docs/en/time-domain/identification.md", "heading": "Step 1: Test stationarity", "content": "print(\"Step 1: Stationarity Tests\")\nprint(f\"ADF p-value (levels): {adfuller(X)[1]:.4f}\")\nprint(f\"KPSS p-value (levels): {kpss(X, regression='c')[1]:.4f}\")", "line_start": 157}, "docs/en/time-domain/identification.md#section_12": {"doc_id": "docs/en/time-domain/identification.md", "heading": "Step 2: Difference and re-test", "content": "dX_obs = np.diff(X)\nprint(f\"\\nADF p-value (differenced): {adfuller(dX_obs)[1]:.4f}\")\nprint(f\"KPSS p-value (differenced): {kpss(dX_obs, regression='c')[1]:.4f}\")", "line_start": 162}, "docs/en/time-domain/identification.md#section_13": {"doc_id": "docs/en/time-domain/identification.md", "heading": "Step 3: ACF/PACF of differenced series", "content": "print(\"\\nStep 3: ACF/PACF\")\nacf_vals = acf(dX_obs, nlags=10)\npacf_vals = pacf(dX_obs, nlags=10)\nprint(f\"ACF: {np.round(acf_vals[1:6], 3)}\")\nprint(f\"PACF: {np.round(pacf_vals[1:6], 3)}\")", "line_start": 167}, "docs/en/time-domain/identification.md#section_14": {"doc_id": "docs/en/time-domain/identification.md", "heading": "Step 4: Compare candidate models", "content": "print(\"\\nStep 4: Model Comparison\")\nmodels = {\n    'ARIMA(1,1,1)': (1,1,1),\n    'ARIMA(2,1,0)': (2,1,0),\n    'ARIMA(2,1,1)': (2,1,1),\n    'ARIMA(1,1,2)': (1,1,2),\n}\n\nfor name, order in models.items():\n    try:\n        model = ARIMA(X, order=order).fit()\n        lb = acorr_ljungbox(model.resid, lags=[10], return_df=True)\n        print(f\"{name}: AIC={model.aic:.1f}, BIC={model.bic:.1f}, LB p-value={lb['lb_pvalue'].values[0]:.3f}\")\n    except:\n        print(f\"{name}: Failed to converge\")\n```", "line_start": 174}, "docs/en/time-domain/identification.md#section_15": {"doc_id": "docs/en/time-domain/identification.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why do we say ACF \"cuts off\" for MA(q) but \"tails off\" for AR(p)?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n- MA(q): $X_t$ only involves $\\epsilon_t, \\epsilon_{t-1}, \\ldots, \\epsilon_{t-q}$. Beyond lag q, $X_t$ and $X_{t-h}$ share no common $\\epsilon$ terms, so $\\gamma(h) = 0$ exactly.\n- AR(p): $X_t$ depends on all past values through the recursive structure. Even though only p lags appear explicitly, the chain of dependencies means $X_t$ correlates with $X_{t-h}$ for all $h$.\n\n<strong>Key insight:</strong> AR has infinite memory (gradually decaying); MA has finite memory (exactly q lags).\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> In practice, \"cutoff\" doesn't mean exactly zero — sample ACF will have small nonzero values beyond q due to estimation error. Look for sharp drop versus gradual decay.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> What's the difference between AIC and BIC? When would you prefer one over the other?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n- AIC: $-2\\ln(L) + 2k$; penalizes parameters lightly\n- BIC: $-2\\ln(L) + k\\ln(n)$; penalty grows with sample size\n\n**BIC preference situations:**\n- Large sample sizes (BIC penalty more appropriate)\n- When true model is among candidates (BIC is consistent)\n- For inference/explanation (simpler models)\n\n**AIC preference situations:**\n- Forecasting focus (AIC optimizes prediction)\n- Small samples (use AICc)\n- When true model may be complex\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Blindly using one criterion. For forecasting, cross-validation often beats both AIC and BIC. Use information criteria as guides, not final arbiters.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Show that for a white noise process, the sample ACF has approximate variance $1/n$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> For white noise with $\\rho(h) = 0$ for $h \\neq 0$:\n\n<strong>Derivation (Bartlett's approximation):</strong>\n\nFor large n, sample ACF is approximately normal:\n$$\\hat{\\rho}(h) \\sim N(\\rho(h), V_h/n)$$\n\nwhere $V_h = \\sum_{j=-\\infty}^{\\infty} [\\rho(j)\\rho(j+h) + \\rho(j+h)\\rho(j-h) - 2\\rho(h)\\rho(j)\\rho(j+h)]$\n\nFor white noise ($\\rho(j) = 0$ for $j \\neq 0$):\n$$V_h = 1 \\text{ for all } h \\neq 0$$\n\nTherefore: $\\text{Var}(\\hat{\\rho}(h)) \\approx 1/n$\n\n**95% confidence interval:** $\\hat{\\rho}(h) \\pm 1.96/\\sqrt{n}$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> This formula only holds under white noise. For non-white-noise series, the variance is different (Bartlett's formula gives different expressions).\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> The Ljung-Box test statistic is $Q(m) = n(n+2)\\sum_{k=1}^{m}\\frac{\\hat{\\rho}_k^2}{n-k}$. Why is there a $(n-k)$ denominator?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The $(n-k)$ term is a small-sample correction. At lag $k$, only $n-k$ pairs of observations contribute to $\\hat{\\rho}(k)$, making the estimate less precise.\n\n<strong>Explanation:</strong>\nThe original Box-Pierce statistic used $Q' = n\\sum \\hat{\\rho}_k^2$. Ljung and Box modified it because:\n1. $\\text{Var}(\\hat{\\rho}(k)) \\approx (n-k)/n^2$ rather than $1/n$\n2. The correction $(n+2)/(n-k)$ improves the chi-squared approximation in finite samples\n\nWithout correction, the test is undersized (rejects less than it should), missing autocorrelation.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Choosing m too large. If $m > n/4$, the test loses power. Common choices: $m = 10$ for non-seasonal, $m = 2s$ for seasonal data.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You examine ACF/PACF and see: ACF decays slowly over first 3 lags then drops; PACF has spikes at lags 1, 2, 3 then drops. AIC favors ARIMA(3,0,0) but residuals show significant ACF at lag 1. What's your next step?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The significant residual ACF at lag 1 indicates model inadequacy despite good AIC. Try:\n\n1. **Add MA(1)**: Try ARIMA(3,0,1) — MA term may capture the remaining lag-1 correlation\n2. **Check for near-redundancy**: If AR(3) coefficients are close to forming MA factor, simplify\n3. **Consider ARIMA(2,0,1)**: Sometimes mixed model is more parsimonious\n4. **Check for outliers**: Single outliers can cause lag-1 residual correlation\n5. **Re-examine stationarity**: Maybe $d=1$ differencing is needed\n\n**Key principle:** A model isn't adequate until residuals are white noise. AIC is necessary but not sufficient — must pass diagnostic checks.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Accepting a model just because it has lowest AIC. Always verify residual ACF is within bands at all lags, especially early ones.\n</div>\n</div>\n</details>", "line_start": 192}, "docs/en/time-domain/identification.md#section_16": {"doc_id": "docs/en/time-domain/identification.md", "heading": "References", "content": "1. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis*. Wiley. Chapters 6-8.\n2. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 9.\n3. Tsay, R. S. (2010). *Analysis of Financial Time Series*. Wiley. Chapter 2.\n4. Ljung, G. M., & Box, G. E. P. (1978). On a measure of lack of fit in time series models. *Biometrika*, 65(2), 297-303.", "line_start": 299}, "docs/en/time-domain/sarima.md#section_0": {"doc_id": "docs/en/time-domain/sarima.md", "heading": "Seasonal ARIMA (SARIMA) Models", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> SARIMA(p,d,q)(P,D,Q)[s] adds seasonal AR and MA terms at lag s (e.g., s=12 for monthly). Seasonal differencing $(1-L^s)$ removes seasonal unit roots. The \"airline model\" SARIMA(0,1,1)(0,1,1)[12] is a benchmark for seasonal data. Identification uses ACF/PACF at both regular and seasonal lags.\n</div>", "line_start": 1}, "docs/en/time-domain/sarima.md#section_1": {"doc_id": "docs/en/time-domain/sarima.md", "heading": "Core Definitions", "content": "**SARIMA(p,d,q)(P,D,Q)[s] Model**:\n$$\\Phi(L)\\Phi_s(L^s)(1-L)^d(1-L^s)^D X_t = c + \\Theta(L)\\Theta_s(L^s)\\epsilon_t$$\n\n**Components:**\n- $(p, d, q)$: Non-seasonal AR order, differencing, MA order\n- $(P, D, Q)$: Seasonal AR order, differencing, MA order\n- $s$: Seasonal period (e.g., 12 for monthly, 4 for quarterly)\n\n**Polynomials:**\n- $\\Phi(L) = 1 - \\phi_1 L - \\cdots - \\phi_p L^p$ (non-seasonal AR)\n- $\\Theta(L) = 1 + \\theta_1 L + \\cdots + \\theta_q L^q$ (non-seasonal MA)\n- $\\Phi_s(L^s) = 1 - \\Phi_1 L^s - \\cdots - \\Phi_P L^{Ps}$ (seasonal AR)\n- $\\Theta_s(L^s) = 1 + \\Theta_1 L^s + \\cdots + \\Theta_Q L^{Qs}$ (seasonal MA)\n\n**Seasonal Difference Operator**:\n$$\\nabla_s X_t = (1 - L^s)X_t = X_t - X_{t-s}$$", "line_start": 7}, "docs/en/time-domain/sarima.md#section_2": {"doc_id": "docs/en/time-domain/sarima.md", "heading": "SARIMA(0,0,0)(1,0,0)[12]: Seasonal AR(1)", "content": "$$X_t = \\Phi_1 X_{t-12} + \\epsilon_t$$\n\nACF significant only at lags 12, 24, 36, ... with exponential decay.", "line_start": 28}, "docs/en/time-domain/sarima.md#section_3": {"doc_id": "docs/en/time-domain/sarima.md", "heading": "SARIMA(0,0,0)(0,0,1)[12]: Seasonal MA(1)", "content": "$$X_t = \\epsilon_t + \\Theta_1\\epsilon_{t-12}$$\n\nACF significant only at lag 12, zero elsewhere.", "line_start": 34}, "docs/en/time-domain/sarima.md#section_4": {"doc_id": "docs/en/time-domain/sarima.md", "heading": "SARIMA(0,1,1)(0,1,1)[12]: The Airline Model", "content": "The classic Box-Jenkins airline passenger model:\n$$(1-L)(1-L^{12})X_t = (1+\\theta L)(1+\\Theta L^{12})\\epsilon_t$$\n\nExpanding:\n$$X_t - X_{t-1} - X_{t-12} + X_{t-13} = \\epsilon_t + \\theta\\epsilon_{t-1} + \\Theta\\epsilon_{t-12} + \\theta\\Theta\\epsilon_{t-13}$$\n\n**Key properties:**\n- First difference handles trend\n- Seasonal difference handles annual pattern\n- MA(1) smooths non-seasonal noise\n- Seasonal MA(1) smooths annual noise\n- Cross-term $\\theta\\Theta$ at lag 13", "line_start": 40}, "docs/en/time-domain/sarima.md#section_5": {"doc_id": "docs/en/time-domain/sarima.md", "heading": "ACF/PACF Patterns for SARIMA", "content": "**Pure seasonal AR (0,0,0)(P,0,0)[s]:**\n- ACF: Exponential decay at seasonal lags (s, 2s, 3s, ...)\n- PACF: Cuts off at lag Ps\n\n**Pure seasonal MA (0,0,0)(0,0,Q)[s]:**\n- ACF: Cuts off at lag Qs\n- PACF: Exponential decay at seasonal lags\n\n**Mixed SARIMA:**\n- Non-seasonal patterns at lags 1, 2, 3, ...\n- Seasonal patterns at lags s, 2s, 3s, ...\n- Interaction patterns at lags s±1, s±2, ...", "line_start": 55}, "docs/en/time-domain/sarima.md#section_6": {"doc_id": "docs/en/time-domain/sarima.md", "heading": "Multiplicative Model Structure", "content": "The multiplicative form means:\n$$\\Phi(L)\\Phi_s(L^s) = (1-\\phi_1 L)(1 - \\Phi_1 L^s) = 1 - \\phi_1 L - \\Phi_1 L^s + \\phi_1\\Phi_1 L^{s+1}$$\n\nThis creates interaction terms (e.g., coefficient at lag 13 for monthly data with AR(1) × SAR(1)).", "line_start": 70}, "docs/en/time-domain/sarima.md#section_7": {"doc_id": "docs/en/time-domain/sarima.md", "heading": "Algorithm/Model Sketch", "content": "**Identification Procedure:**\n\n```\n1. Plot series; identify seasonal period s\n2. Check for trend → apply regular differencing (d)\n3. Check for seasonal pattern → apply seasonal differencing (D)\n4. Usually D ≤ 1, d ≤ 2\n\n5. Examine ACF/PACF of stationary series:\n   - At lags 1, 2, ..., s-1: determine p, q\n   - At lags s, 2s, 3s: determine P, Q\n   - Spikes at s±k: interaction effects\n\n6. Fit candidate models\n7. Compare AIC/BIC\n8. Check residuals at both regular and seasonal lags\n```\n\n**Common Seasonal Periods:**\n\n| Data Frequency | Period s |\n|----------------|----------|\n| Monthly | 12 |\n| Quarterly | 4 |\n| Weekly (annual) | 52 |\n| Daily (weekly) | 7 |\n| Hourly (daily) | 24 |", "line_start": 77}, "docs/en/time-domain/sarima.md#section_8": {"doc_id": "docs/en/time-domain/sarima.md", "heading": "Common Pitfalls", "content": "1. **Double seasonal patterns**: Some data has multiple seasonalities (daily + weekly). Standard SARIMA handles one period. Consider multiple seasonal models or alternative methods.\n\n2. **Large s causes issues**: For s=52 or s=365, estimation is difficult. Consider Fourier terms or alternative decomposition methods.\n\n3. **Seasonal differencing with D > 1**: Rarely needed and often causes over-differencing. Check if D=1 suffices.\n\n4. **Ignoring multiplicative structure**: The model is multiplicative, so lag s+1 effects exist when both $\\phi$ and $\\Phi$ are non-zero.\n\n5. **Non-integer periods**: If seasonality isn't at integer lags (e.g., 365.25 days/year), SARIMA doesn't apply directly. Use trigonometric seasonality.\n\n6. **Forgetting trend after seasonal differencing**: Seasonal differencing $(1-L^{12})$ doesn't remove linear trend. May still need $d=1$.", "line_start": 107}, "docs/en/time-domain/sarima.md#section_9": {"doc_id": "docs/en/time-domain/sarima.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.seasonal import seasonal_decompose", "line_start": 121}, "docs/en/time-domain/sarima.md#section_10": {"doc_id": "docs/en/time-domain/sarima.md", "heading": "Generate SARIMA(1,1,1)(1,1,1)[12] data", "content": "np.random.seed(42)\nn = 200\ns = 12", "line_start": 129}, "docs/en/time-domain/sarima.md#section_11": {"doc_id": "docs/en/time-domain/sarima.md", "heading": "Create seasonal + trend + noise", "content": "t = np.arange(n)\nseasonal = 10 * np.sin(2 * np.pi * t / s)\ntrend = 0.1 * t\nnoise = np.random.randn(n) * 2\nX = trend + seasonal + np.cumsum(noise)", "line_start": 134}, "docs/en/time-domain/sarima.md#section_12": {"doc_id": "docs/en/time-domain/sarima.md", "heading": "Fit SARIMA", "content": "model = SARIMAX(X, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\nresults = model.fit(disp=False)\nprint(results.summary().tables[1])", "line_start": 141}, "docs/en/time-domain/sarima.md#section_13": {"doc_id": "docs/en/time-domain/sarima.md", "heading": "Check residuals at seasonal lags", "content": "from statsmodels.stats.diagnostic import acorr_ljungbox\nlb_test = acorr_ljungbox(results.resid, lags=[12, 24], return_df=True)\nprint(\"\\nLjung-Box test at seasonal lags:\")\nprint(lb_test)", "line_start": 146}, "docs/en/time-domain/sarima.md#section_14": {"doc_id": "docs/en/time-domain/sarima.md", "heading": "Forecast", "content": "forecast = results.get_forecast(steps=12)\nprint(f\"\\n12-month forecast mean: {forecast.predicted_mean.values}\")\n```", "line_start": 152}, "docs/en/time-domain/sarima.md#section_15": {"doc_id": "docs/en/time-domain/sarima.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why is the SARIMA model called \"multiplicative\"? How does this affect the lag structure?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> \"Multiplicative\" refers to multiplying AR and MA polynomials: $\\Phi(L) \\times \\Phi_s(L^s)$. This creates interaction terms at combined lags.\n\n<strong>Example:</strong> SARIMA(1,0,0)(1,0,0)[12]:\n$$(1-\\phi L)(1-\\Phi L^{12})X_t = \\epsilon_t$$\n$$X_t - \\phi X_{t-1} - \\Phi X_{t-12} + \\phi\\Phi X_{t-13} = \\epsilon_t$$\n\nThe $\\phi\\Phi$ term at lag 13 is an interaction effect—it wouldn't exist in an additive model.\n\n**Implication:** Check ACF/PACF at lags like 11, 13 (not just 12) for monthly data.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Expecting clean separation of seasonal and non-seasonal effects. The multiplicative structure blends them, which can confuse identification.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> What is the \"airline model\" and why is it a useful benchmark?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The airline model is SARIMA(0,1,1)(0,1,1)[12], originally fit to airline passenger data by Box and Jenkins. It's useful because:\n\n1. Handles trend via first differencing\n2. Handles annual seasonality via seasonal differencing\n3. Uses only 2 parameters ($\\theta$, $\\Theta$) yet fits many seasonal series well\n4. Equivalent to Holt-Winters exponential smoothing\n\n**Model:**\n$$(1-L)(1-L^{12})X_t = (1+\\theta L)(1+\\Theta L^{12})\\epsilon_t$$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using the airline model as the default without checking fit. For some data, AR terms or different differencing may be needed. Always verify with residual diagnostics.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Derive the ACF at lag 12 for the seasonal MA(1) model: $X_t = \\epsilon_t + \\Theta\\epsilon_{t-12}$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> $\\rho(12) = \\frac{\\Theta}{1+\\Theta^2}$, and $\\rho(h) = 0$ for $h \\neq 0, 12$.\n\n<strong>Derivation:</strong>\n\n**Variance:**\n$$\\gamma(0) = \\text{Var}(\\epsilon_t + \\Theta\\epsilon_{t-12}) = \\sigma^2(1 + \\Theta^2)$$\n\n**Autocovariance at lag 12:**\n$$\\gamma(12) = E[(\\epsilon_t + \\Theta\\epsilon_{t-12})(\\epsilon_{t-12} + \\Theta\\epsilon_{t-24})]$$\n$$= E[\\Theta\\epsilon_{t-12}^2] = \\Theta\\sigma^2$$\n\n**ACF:**\n$$\\rho(12) = \\frac{\\gamma(12)}{\\gamma(0)} = \\frac{\\Theta\\sigma^2}{\\sigma^2(1+\\Theta^2)} = \\frac{\\Theta}{1+\\Theta^2}$$\n\nFor other lags, there's no overlap of $\\epsilon$ terms, so $\\gamma(h) = 0$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Note $|\\rho(12)| \\leq 0.5$, same constraint as non-seasonal MA(1). Larger observed seasonal correlations suggest seasonal AR or combined model.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> For SARIMA(0,1,0)(0,1,0)[12], write out the model equation and explain what it represents.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> This is a \"seasonal random walk\":\n$$(1-L)(1-L^{12})X_t = \\epsilon_t$$\n\nExpanding:\n$$X_t - X_{t-1} - X_{t-12} + X_{t-13} = \\epsilon_t$$\n$$X_t = X_{t-1} + X_{t-12} - X_{t-13} + \\epsilon_t$$\n\n**Interpretation:** Today's value = yesterday's value + this month last year − same day last year + noise.\n\nThis is the \"naïve seasonal\" forecast: $\\hat{X}_{t+1} = X_t + (X_{t+1-12} - X_{t-12})$.\n\nIt says: repeat last year's seasonal pattern while continuing yesterday's level.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> This model is a useful benchmark but often too simplistic. Real data usually benefits from MA terms to smooth noise.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You're modeling hourly electricity demand with clear daily (24h) and weekly (168h) patterns. Can you use standard SARIMA? What alternatives exist?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Standard SARIMA handles only one seasonal period. For multiple seasonalities, alternatives include:\n\n1. **Double seasonal models**: SARIMA with s=24 plus external regressors for weekly pattern\n2. **TBATS**: Exponential smoothing with multiple seasonal periods\n3. **Fourier terms**: Include sin/cos terms at both frequencies\n4. **Prophet**: Handles multiple seasonalities via additive decomposition\n5. **Neural approaches**: LSTM or Transformers can learn complex patterns\n\n**Practical approach:**\n- Use s=24 (dominant pattern)\n- Add day-of-week dummies or Fourier terms for weekly pattern\n- Consider: `SARIMAX(p,d,q)(P,D,Q)[24]` with `exog=weekly_dummies`\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Trying to use s=168 for weekly seasonality causes estimation problems (168 is large). Use hierarchical or additive approaches instead.\n</div>\n</div>\n</details>", "line_start": 157}, "docs/en/time-domain/sarima.md#section_16": {"doc_id": "docs/en/time-domain/sarima.md", "heading": "References", "content": "1. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis*. Wiley. Chapter 9.\n2. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 9.\n3. Shumway, R. H., & Stoffer, D. S. (2017). *Time Series Analysis and Its Applications*. Springer. Chapter 3.\n4. De Livera, A. M., Hyndman, R. J., & Snyder, R. D. (2011). Forecasting time series with complex seasonal patterns using exponential smoothing. *JASA*, 106(496), 1513-1527.", "line_start": 271}, "docs/en/deep-learning/deep-learning-ts.md#section_0": {"doc_id": "docs/en/deep-learning/deep-learning-ts.md", "heading": "Deep Learning for Time Series", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Deep learning models (RNN, LSTM, TCN, Transformer) capture complex temporal patterns. LSTM addresses vanishing gradients via gates. TCN uses dilated causal convolutions for long-range dependencies. Transformers use attention mechanisms. DL shines with: large data, multiple series, complex patterns. May underperform classical methods on small data or simple patterns.\n</div>", "line_start": 1}, "docs/en/deep-learning/deep-learning-ts.md#section_1": {"doc_id": "docs/en/deep-learning/deep-learning-ts.md", "heading": "Core Definitions", "content": "**RNN (Recurrent Neural Network):**\n$$h_t = \\tanh(W_h h_{t-1} + W_x x_t + b)$$\n$$\\hat{y}_t = W_y h_t$$\n\n**LSTM (Long Short-Term Memory):**\n- Forget gate: $f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$\n- Input gate: $i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$\n- Cell update: $\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$\n- Cell state: $C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$\n- Output gate: $o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$\n- Hidden state: $h_t = o_t * \\tanh(C_t)$\n\n**TCN (Temporal Convolutional Network):**\nDilated causal convolutions with residual connections.\n\n**Transformer:** Self-attention mechanism for sequence modeling.", "line_start": 7}, "docs/en/deep-learning/deep-learning-ts.md#section_2": {"doc_id": "docs/en/deep-learning/deep-learning-ts.md", "heading": "Vanishing Gradient Problem in RNN", "content": "Gradient of loss with respect to early hidden state:\n$$\\frac{\\partial L}{\\partial h_t} = \\frac{\\partial L}{\\partial h_T}\\prod_{k=t}^{T-1}\\frac{\\partial h_{k+1}}{\\partial h_k}$$\n\nFor tanh activation: $|\\frac{\\partial h_{k+1}}{\\partial h_k}| < 1$ typically\n\nProduct of many small numbers → gradient vanishes.\n\n**LSTM solution:** Cell state path has additive updates (not multiplicative), preserving gradients over long sequences.", "line_start": 28}, "docs/en/deep-learning/deep-learning-ts.md#section_3": {"doc_id": "docs/en/deep-learning/deep-learning-ts.md", "heading": "TCN Dilated Convolutions", "content": "For dilation factor $d$ and filter size $k$:\n$$(F *_d x)_t = \\sum_{i=0}^{k-1} f_i \\cdot x_{t-d \\cdot i}$$\n\n**Receptive field with L layers:**\n$$R = 1 + (k-1)\\sum_{l=0}^{L-1}d_l$$\n\nWith exponential dilation ($d_l = 2^l$): $R = 1 + (k-1)(2^L - 1)$", "line_start": 39}, "docs/en/deep-learning/deep-learning-ts.md#section_4": {"doc_id": "docs/en/deep-learning/deep-learning-ts.md", "heading": "Transformer Self-Attention", "content": "**Attention:**\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n\nFor time series:\n- Q, K, V derived from input sequence\n- Attention weights show which past timesteps are relevant\n- Position encoding added to maintain temporal order", "line_start": 49}, "docs/en/deep-learning/deep-learning-ts.md#section_5": {"doc_id": "docs/en/deep-learning/deep-learning-ts.md", "heading": "Training Strategies", "content": "**Teacher forcing:** During training, use actual values (not predictions) as input.\n\n**Multi-step loss:** Optimize over multiple forecast horizons:\n$$L = \\sum_{h=1}^{H}w_h \\cdot L_h(\\hat{y}_{t+h}, y_{t+h})$$", "line_start": 59}, "docs/en/deep-learning/deep-learning-ts.md#section_6": {"doc_id": "docs/en/deep-learning/deep-learning-ts.md", "heading": "Algorithm/Model Sketch", "content": "**LSTM for Forecasting:**\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass LSTMForecaster(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        # x: (batch, seq_len, input_size)\n        lstm_out, _ = self.lstm(x)\n        # Take last hidden state\n        out = self.fc(lstm_out[:, -1, :])\n        return out", "line_start": 66}, "docs/en/deep-learning/deep-learning-ts.md#section_7": {"doc_id": "docs/en/deep-learning/deep-learning-ts.md", "heading": "Training loop", "content": "model = LSTMForecaster(input_size=1, hidden_size=64, num_layers=2, output_size=1)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.MSELoss()\n\nfor epoch in range(100):\n    for X_batch, y_batch in dataloader:\n        optimizer.zero_grad()\n        y_pred = model(X_batch)\n        loss = criterion(y_pred, y_batch)\n        loss.backward()\n        optimizer.step()\n```\n\n**When to Use Deep Learning:**\n\n| Scenario | Recommendation |\n|----------|---------------|\n| Small data (<1000) | Classical (ARIMA, ETS) |\n| Medium data, simple patterns | Classical or simple NN |\n| Large data, complex patterns | Deep learning |\n| Many related series | Deep learning (transfer) |\n| Real-time, low latency | TCN (parallelizable) |", "line_start": 87}, "docs/en/deep-learning/deep-learning-ts.md#section_8": {"doc_id": "docs/en/deep-learning/deep-learning-ts.md", "heading": "Common Pitfalls", "content": "1. **Too little data:** DL needs thousands+ of observations. With small data, ARIMA often wins.\n\n2. **Over-complicated architecture:** Simple LSTM often beats complex Transformer on univariate forecasting.\n\n3. **Ignoring baselines:** Always compare to naive, seasonal naive, and ARIMA before claiming DL success.\n\n4. **Lookback window too short:** LSTM can learn long patterns only if lookback is long enough.\n\n5. **Not using validation properly:** Use time-aware validation (rolling origin), not random split.\n\n6. **Training instability:** Gradient clipping, learning rate scheduling, and careful initialization matter.", "line_start": 111}, "docs/en/deep-learning/deep-learning-ts.md#section_9": {"doc_id": "docs/en/deep-learning/deep-learning-ts.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset", "line_start": 125}, "docs/en/deep-learning/deep-learning-ts.md#section_10": {"doc_id": "docs/en/deep-learning/deep-learning-ts.md", "heading": "Generate data", "content": "np.random.seed(42)\nn = 2000\nt = np.arange(n)\ny = np.sin(2 * np.pi * t / 50) + 0.5 * np.sin(2 * np.pi * t / 10) + np.random.randn(n) * 0.3", "line_start": 133}, "docs/en/deep-learning/deep-learning-ts.md#section_11": {"doc_id": "docs/en/deep-learning/deep-learning-ts.md", "heading": "Create sequences", "content": "def create_sequences(data, seq_length):\n    X, Y = [], []\n    for i in range(len(data) - seq_length):\n        X.append(data[i:i+seq_length])\n        Y.append(data[i+seq_length])\n    return np.array(X), np.array(Y)\n\nseq_length = 50\nX, Y = create_sequences(y, seq_length)\nX = torch.FloatTensor(X).unsqueeze(-1)  # (N, seq_len, 1)\nY = torch.FloatTensor(Y).unsqueeze(-1)  # (N, 1)", "line_start": 139}, "docs/en/deep-learning/deep-learning-ts.md#section_12": {"doc_id": "docs/en/deep-learning/deep-learning-ts.md", "heading": "Train-test split", "content": "train_size = int(len(X) * 0.8)\nX_train, Y_train = X[:train_size], Y[:train_size]\nX_test, Y_test = X[train_size:], Y[train_size:]", "line_start": 152}, "docs/en/deep-learning/deep-learning-ts.md#section_13": {"doc_id": "docs/en/deep-learning/deep-learning-ts.md", "heading": "Simple LSTM model", "content": "class SimpleLSTM(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lstm = nn.LSTM(1, 32, 1, batch_first=True)\n        self.fc = nn.Linear(32, 1)\n\n    def forward(self, x):\n        out, _ = self.lstm(x)\n        return self.fc(out[:, -1, :])\n\nmodel = SimpleLSTM()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\ncriterion = nn.MSELoss()", "line_start": 157}, "docs/en/deep-learning/deep-learning-ts.md#section_14": {"doc_id": "docs/en/deep-learning/deep-learning-ts.md", "heading": "Train", "content": "train_loader = DataLoader(TensorDataset(X_train, Y_train), batch_size=32, shuffle=True)\nfor epoch in range(20):\n    for X_batch, Y_batch in train_loader:\n        optimizer.zero_grad()\n        loss = criterion(model(X_batch), Y_batch)\n        loss.backward()\n        optimizer.step()", "line_start": 172}, "docs/en/deep-learning/deep-learning-ts.md#section_15": {"doc_id": "docs/en/deep-learning/deep-learning-ts.md", "heading": "Evaluate", "content": "model.eval()\nwith torch.no_grad():\n    y_pred = model(X_test)\n    test_rmse = torch.sqrt(criterion(y_pred, Y_test))\n    print(f\"Test RMSE: {test_rmse:.4f}\")", "line_start": 181}, "docs/en/deep-learning/deep-learning-ts.md#section_16": {"doc_id": "docs/en/deep-learning/deep-learning-ts.md", "heading": "Compare to naive", "content": "naive_rmse = np.sqrt(np.mean((Y_test.numpy() - X_test[:, -1, :].numpy())**2))\nprint(f\"Naive RMSE: {naive_rmse:.4f}\")\n```", "line_start": 188}, "docs/en/deep-learning/deep-learning-ts.md#section_17": {"doc_id": "docs/en/deep-learning/deep-learning-ts.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What is the vanishing gradient problem and how does LSTM address it?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Problem:** In vanilla RNN, gradients are multiplied through each timestep. With many steps, gradients become exponentially small (vanish), preventing learning of long-range dependencies.\n\n**LSTM Solution:**\n- Cell state $C_t$ is updated additively, not multiplicatively\n- Forget gate controls what to keep: $C_t = f_t * C_{t-1} + ...$\n- When $f_t \\approx 1$, gradient flows unchanged\n- Information can persist over hundreds of timesteps\n\n**Key insight:** The cell state acts as a \"highway\" for gradients, bypassing the vanishing problem.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Thinking LSTM completely solves long-range dependencies. Very long sequences (1000+) may still need attention mechanisms or hierarchical structures.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> When would you choose TCN over LSTM for time series?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Prefer TCN when:**\n1. **Parallelization needed:** TCN processes all timesteps simultaneously; LSTM is sequential\n2. **Long sequences:** Dilated convolutions efficiently handle very long range\n3. **Training stability:** TCN gradients don't explode/vanish as easily\n4. **Inference speed:** No hidden state to maintain\n5. **Variable length at inference:** Can process any length\n\n**Prefer LSTM when:**\n1. **Truly sequential processing:** Online/streaming data\n2. **Variable length training:** LSTM naturally handles different lengths\n3. **State tracking needed:** Hidden state captures \"memory\"\n4. **Smaller receptive field sufficient:** LSTM may use parameters more efficiently\n\n**Research finding:** TCN often matches or beats LSTM on standard benchmarks with faster training.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Defaulting to LSTM because it's \"the standard.\" TCN is often simpler and faster with comparable accuracy.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> How does dilated convolution increase receptive field without increasing parameters?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Dilation introduces gaps in the convolution, sampling inputs at regular intervals.\n\n**Standard convolution (dilation=1):**\n$$y_t = \\sum_{i=0}^{k-1} w_i \\cdot x_{t-i}$$\nReceptive field = k\n\n**Dilated convolution (dilation=d):**\n$$y_t = \\sum_{i=0}^{k-1} w_i \\cdot x_{t-d \\cdot i}$$\nReceptive field = 1 + (k-1) × d\n\n**With exponential dilation (d = 2^l):**\n- Layer 0: RF = k\n- Layer 1: RF = k + (k-1)×2\n- Layer L-1: RF = 1 + (k-1)(2^L - 1)\n\n**Example:** k=3, L=8 → RF = 1 + 2×255 = 511\n\nSame number of parameters (k weights per layer), but 500+ timestep receptive field!\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using too many layers. With k=3 and L=10, RF ≈ 2000. Check if you actually need that range.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Explain why Transformers need positional encoding for time series.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Self-attention is permutation invariant—it treats input as a set, not a sequence.\n\n**Problem:**\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n\nThis doesn't change if we permute the input order—attention weights depend only on pairwise similarities.\n\n**Solution: Positional encoding**\nAdd position information to embeddings:\n$$x'_t = x_t + PE(t)$$\n\nCommon encoding:\n$$PE(t, 2i) = \\sin(t / 10000^{2i/d})$$\n$$PE(t, 2i+1) = \\cos(t / 10000^{2i/d})$$\n\nNow the model can distinguish $x_5$ from $x_{50}$ even if content is identical.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Forgetting positional encoding → Transformer treats sequence as bag of vectors, losing temporal structure entirely.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You train an LSTM on energy demand data and it predicts flat lines (always the mean). What's wrong?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Several possible causes:\n\n1. **Learning rate too high:** Weights oscillate, model defaults to mean\n   - Fix: Reduce LR, use scheduler\n\n2. **Vanishing gradients:** Despite LSTM, can still occur\n   - Fix: Gradient clipping, check gradient norms\n\n3. **Data not scaled:** Large values cause saturation\n   - Fix: Standardize inputs and targets\n\n4. **Lookback too short:** Model can't see useful patterns\n   - Fix: Increase sequence length\n\n5. **Too few epochs:** Model hasn't learned yet\n   - Fix: Train longer, check loss curve\n\n6. **Wrong loss function:** MSE on non-stationary data dominated by trend\n   - Fix: Use differenced data or relative errors\n\n7. **Model too small:** Can't capture complexity\n   - Fix: Increase hidden size / layers\n\n**Diagnosis:**\n- Plot training loss: decreasing or flat?\n- Check gradient magnitudes\n- Visualize predictions vs. actuals over training\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Assuming mean prediction is \"failure.\" For high-noise data with no pattern, mean IS optimal. Compare to naive baselines first.\n</div>\n</div>\n</details>", "line_start": 193}, "docs/en/deep-learning/deep-learning-ts.md#section_18": {"doc_id": "docs/en/deep-learning/deep-learning-ts.md", "heading": "References", "content": "1. Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. *Neural Computation*, 9(8), 1735-1780.\n2. Bai, S., Kolter, J. Z., & Koltun, V. (2018). An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. *arXiv:1803.01271*.\n3. Vaswani, A., et al. (2017). Attention is all you need. *NeurIPS*.\n4. Lim, B., & Zohren, S. (2021). Time-series forecasting with deep learning: a survey. *Philosophical Transactions A*, 379(2194).", "line_start": 338}, "docs/en/exponential-smoothing/ets.md#section_0": {"doc_id": "docs/en/exponential-smoothing/ets.md", "heading": "ETS Framework", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> ETS (Error-Trend-Seasonal) is a state space framework unifying exponential smoothing methods. Named by three components: Error (A/M), Trend (N/A/Ad/M/Md), Seasonal (N/A/M). Provides likelihood-based estimation and proper prediction intervals. ETS(A,A,A) = additive Holt-Winters. Total of 30 model variants.\n</div>", "line_start": 1}, "docs/en/exponential-smoothing/ets.md#section_1": {"doc_id": "docs/en/exponential-smoothing/ets.md", "heading": "Core Definitions", "content": "**ETS Taxonomy:**\n\n- **E (Error)**: Additive (A) or Multiplicative (M)\n- **T (Trend)**: None (N), Additive (A), Additive damped (Ad), Multiplicative (M), Multiplicative damped (Md)\n- **S (Seasonal)**: None (N), Additive (A), Multiplicative (M)\n\n**Notation:** ETS(E,T,S)\n\n**Examples:**\n- ETS(A,N,N) = Simple Exponential Smoothing\n- ETS(A,A,N) = Holt's linear\n- ETS(A,Ad,N) = Damped trend\n- ETS(A,A,A) = Additive Holt-Winters\n- ETS(M,A,M) = Multiplicative error, additive trend, multiplicative seasonal", "line_start": 7}, "docs/en/exponential-smoothing/ets.md#section_2": {"doc_id": "docs/en/exponential-smoothing/ets.md", "heading": "State Space Form", "content": "**General form:**\n$$y_t = w(\\mathbf{x}_{t-1}) + r(\\mathbf{x}_{t-1})\\epsilon_t$$\n$$\\mathbf{x}_t = f(\\mathbf{x}_{t-1}) + g(\\mathbf{x}_{t-1})\\epsilon_t$$\n\nwhere $\\mathbf{x}_t$ is the state vector (level, trend, seasonal components).", "line_start": 26}, "docs/en/exponential-smoothing/ets.md#section_3": {"doc_id": "docs/en/exponential-smoothing/ets.md", "heading": "ETS(A,A,N): Additive Error, Additive Trend, No Seasonal", "content": "Measurement: $y_t = \\ell_{t-1} + b_{t-1} + \\epsilon_t$\n\nState transitions:\n$$\\ell_t = \\ell_{t-1} + b_{t-1} + \\alpha\\epsilon_t$$\n$$b_t = b_{t-1} + \\beta\\epsilon_t$$\n\nMatrix form:\n$$\\begin{pmatrix} \\ell_t \\\\ b_t \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} \\ell_{t-1} \\\\ b_{t-1} \\end{pmatrix} + \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix}\\epsilon_t$$", "line_start": 34}, "docs/en/exponential-smoothing/ets.md#section_4": {"doc_id": "docs/en/exponential-smoothing/ets.md", "heading": "ETS(M,A,M): Multiplicative Error and Seasonal", "content": "Measurement: $y_t = (\\ell_{t-1} + b_{t-1})s_{t-m}(1 + \\epsilon_t)$\n\nThis means: $\\epsilon_t = \\frac{y_t - (\\ell_{t-1} + b_{t-1})s_{t-m}}{(\\ell_{t-1} + b_{t-1})s_{t-m}}$\n\nState transitions:\n$$\\ell_t = (\\ell_{t-1} + b_{t-1})(1 + \\alpha\\epsilon_t)$$\n$$b_t = b_{t-1} + \\beta(\\ell_{t-1} + b_{t-1})\\epsilon_t$$\n$$s_t = s_{t-m}(1 + \\gamma\\epsilon_t)$$", "line_start": 45}, "docs/en/exponential-smoothing/ets.md#section_5": {"doc_id": "docs/en/exponential-smoothing/ets.md", "heading": "Likelihood Function", "content": "For additive errors:\n$$L(\\boldsymbol{\\theta}|\\mathbf{y}) = \\prod_{t=1}^{n}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{\\epsilon_t^2}{2\\sigma^2}\\right)$$\n\nFor multiplicative errors:\n$$L(\\boldsymbol{\\theta}|\\mathbf{y}) = \\prod_{t=1}^{n}\\frac{1}{\\sqrt{2\\pi\\sigma^2}\\mu_t}\\exp\\left(-\\frac{\\epsilon_t^2}{2\\sigma^2}\\right)$$\n\nwhere $\\mu_t$ is the one-step-ahead forecast.", "line_start": 56}, "docs/en/exponential-smoothing/ets.md#section_6": {"doc_id": "docs/en/exponential-smoothing/ets.md", "heading": "Prediction Intervals", "content": "State space formulation enables analytical or simulation-based prediction intervals:\n\n**Analytical** (for some models):\n$$\\text{Var}(y_{T+h}|y_{1:T}) = \\sigma^2 \\sum_{j=0}^{h-1}c_j^2$$\n\n**Simulation** (general):\n1. Sample future errors $\\epsilon_{T+1}, \\ldots, \\epsilon_{T+h}$\n2. Generate sample paths using state equations\n3. Compute percentiles of forecast distribution", "line_start": 66}, "docs/en/exponential-smoothing/ets.md#section_7": {"doc_id": "docs/en/exponential-smoothing/ets.md", "heading": "Algorithm/Model Sketch", "content": "**Model Selection with ETS:**\n\n```\n1. Consider all valid ETS combinations:\n   - 30 models total (some multiplicative error combinations unstable)\n   - Stable models: about 15-20 depending on data\n\n2. For each model:\n   - Estimate parameters by MLE\n   - Compute AIC/BIC\n\n3. Select model with lowest AIC (or BIC)\n\n4. Validate:\n   - Check residual diagnostics\n   - Compare forecast accuracy on holdout\n\n5. Generate forecasts with prediction intervals\n```\n\n**Valid/Stable Models:**\n- Multiplicative error requires positive data\n- Some combinations are unstable (e.g., M,Md,M with certain parameters)\n- ETS implementations typically restrict to admissible parameter space", "line_start": 78}, "docs/en/exponential-smoothing/ets.md#section_8": {"doc_id": "docs/en/exponential-smoothing/ets.md", "heading": "Common Pitfalls", "content": "1. **Assuming ETS = Holt-Winters**: ETS is broader—includes multiplicative error variants and provides proper statistical framework.\n\n2. **Ignoring multiplicative error**: For positive data with variance proportional to level, multiplicative error often fits better.\n\n3. **Model averaging**: Instead of selecting one model, averaging forecasts from multiple ETS models can improve accuracy.\n\n4. **Large seasonal period**: ETS with $m > 24$ is often impractical. Use Fourier terms or TBATS instead.\n\n5. **Negative forecasts**: Additive models can forecast negatives. For positive data, prefer multiplicative components.\n\n6. **Prediction interval coverage**: Check that actual coverage matches nominal (e.g., 95% intervals should contain ~95% of observations).", "line_start": 105}, "docs/en/exponential-smoothing/ets.md#section_9": {"doc_id": "docs/en/exponential-smoothing/ets.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing", "line_start": 119}, "docs/en/exponential-smoothing/ets.md#section_10": {"doc_id": "docs/en/exponential-smoothing/ets.md", "heading": "Generate data with trend and multiplicative seasonality", "content": "np.random.seed(42)\nn = 120\nt = np.arange(n)\nlevel = 100 + 0.5 * t\nseasonal = 1 + 0.2 * np.sin(2 * np.pi * t / 12)\ny = level * seasonal * (1 + 0.05 * np.random.randn(n))", "line_start": 125}, "docs/en/exponential-smoothing/ets.md#section_11": {"doc_id": "docs/en/exponential-smoothing/ets.md", "heading": "Fit various ETS models", "content": "models = {\n    'ETS(A,A,A)': {'trend': 'add', 'seasonal': 'add'},\n    'ETS(A,A,M)': {'trend': 'add', 'seasonal': 'mul'},\n    'ETS(A,Ad,A)': {'trend': 'add', 'seasonal': 'add', 'damped_trend': True},\n    'ETS(A,Ad,M)': {'trend': 'add', 'seasonal': 'mul', 'damped_trend': True},\n}\n\nresults = {}\nfor name, params in models.items():\n    try:\n        model = ExponentialSmoothing(\n            y,\n            seasonal_periods=12,\n            **params\n        ).fit()\n        results[name] = {'AIC': model.aic, 'BIC': model.bic}\n    except:\n        results[name] = {'AIC': np.inf, 'BIC': np.inf}\n\nprint(\"Model Comparison:\")\nfor name, metrics in sorted(results.items(), key=lambda x: x[1]['AIC']):\n    print(f\"  {name}: AIC={metrics['AIC']:.1f}, BIC={metrics['BIC']:.1f}\")", "line_start": 133}, "docs/en/exponential-smoothing/ets.md#section_12": {"doc_id": "docs/en/exponential-smoothing/ets.md", "heading": "Best model", "content": "best_model = min(results.items(), key=lambda x: x[1]['AIC'])[0]\nprint(f\"\\nBest model by AIC: {best_model}\")\n```", "line_start": 157}, "docs/en/exponential-smoothing/ets.md#section_13": {"doc_id": "docs/en/exponential-smoothing/ets.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What is the advantage of the ETS framework over traditional exponential smoothing formulations?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> ETS provides:\n\n1. **Statistical foundation**: State space formulation enables proper likelihood inference\n2. **Model selection**: AIC/BIC for comparing all 30 variants systematically\n3. **Proper prediction intervals**: Based on forecast error distribution, not ad-hoc formulas\n4. **Unified framework**: All exponential smoothing methods in one consistent notation\n5. **Automatic selection**: Can search over models algorithmically\n\nTraditional formulations gave point forecasts but lacked principled interval estimation and model comparison.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using traditional Holt-Winters formulas then computing intervals with ETS assumptions. The interval formulas depend on the error structure — must be consistent.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> When would you prefer multiplicative error (M) over additive error (A)?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Use multiplicative error when:\n\n1. **Variance scales with level**: Higher values have larger absolute errors but similar percentage errors\n2. **Positive data only**: Multiplicative error requires $y_t > 0$\n3. **Percentage errors are meaningful**: Business contexts where 10% error is similar regardless of level\n4. **Heteroskedasticity**: Variance is not constant over time\n\n**Diagnostic:** Plot residuals vs. fitted values. If variance increases with fitted values → multiplicative error.\n\n**Mathematical interpretation:**\n- Additive: $y_t = \\mu_t + \\epsilon_t$ (constant variance)\n- Multiplicative: $y_t = \\mu_t(1 + \\epsilon_t)$ (variance proportional to $\\mu_t^2$)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using additive error for sales/financial data where percentage errors are natural. This underestimates uncertainty at high values.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Write the state space equations for ETS(A,N,N) — simple exponential smoothing.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Measurement equation:**\n$$y_t = \\ell_{t-1} + \\epsilon_t$$\n\n**State transition:**\n$$\\ell_t = \\ell_{t-1} + \\alpha\\epsilon_t$$\n\n**Or equivalently:**\n$$\\ell_t = \\alpha y_t + (1-\\alpha)\\ell_{t-1}$$\n\nThis is exactly SES. The state is just the level $\\ell_t$. Forecast: $\\hat{y}_{t+h|t} = \\ell_t$ for all $h$.\n\n**Variance of h-step forecast error:**\n$$\\text{Var}(y_{t+h} - \\hat{y}_{t+h|t}) = \\sigma^2[1 + (h-1)\\alpha^2]$$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Forgetting that ETS(A,N,N) prediction intervals widen with horizon. Flat forecasts don't mean constant uncertainty.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Why are some ETS model combinations inadmissible or unstable?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Certain combinations lead to:\n\n1. **Negative components**: Multiplicative seasonal with additive trend can produce $\\ell_t + b_t < 0$, making $y_t = (\\ell_t + b_t) \\times s_t$ negative even with positive seasonals.\n\n2. **Explosive variance**: Some multiplicative error combinations have variance that grows exponentially with horizon.\n\n3. **Non-identifiability**: Parameter combinations that produce identical forecasts.\n\n**Specifically problematic:**\n- ETS(M,M,*) — multiplicative trend with multiplicative error can explode\n- ETS(M,*,M) — can give negative forecasts or infinite variance\n\n**Admissible region:** Parameters must satisfy constraints to ensure positive forecasts and bounded variance. Software enforces these.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Manually setting parameters outside admissible bounds. Always use constrained optimization or let software handle admissibility.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You run automatic ETS selection and get ETS(M,Ad,M) with AIC much lower than alternatives. What checks should you perform before accepting this model?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Before accepting:\n\n1. **Check residuals:**\n   - Plot standardized residuals — should look like white noise\n   - ACF of residuals — no significant spikes\n   - Ljung-Box test — fail to reject white noise\n\n2. **Verify assumptions:**\n   - Data is positive (required for multiplicative)\n   - Variance scales with level (justifies M error)\n   - Seasonal pattern is proportional (justifies M seasonal)\n\n3. **Compare forecasts:**\n   - Out-of-sample validation if possible\n   - Do forecasts look reasonable?\n   - Check prediction interval coverage\n\n4. **Parameter reasonableness:**\n   - Damping parameter φ — should be 0.8-0.98\n   - α, β, γ — not at boundaries\n\n5. **Compare to simpler models:**\n   - If ETS(A,A,M) is close, prefer simpler\n   - Log-transform + ETS(A,*,A) might be equivalent\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Accepting complex model without validation. ETS(M,Ad,M) has many parameters — risk of overfitting. Always validate on holdout data.\n</div>\n</div>\n</details>", "line_start": 162}, "docs/en/exponential-smoothing/ets.md#section_14": {"doc_id": "docs/en/exponential-smoothing/ets.md", "heading": "References", "content": "1. Hyndman, R. J., Koehler, A. B., Snyder, R. D., & Grose, S. (2002). A state space framework for automatic forecasting using exponential smoothing methods. *IJF*, 18(3), 439-454.\n2. Hyndman, R. J., Koehler, A. B., Ord, J. K., & Snyder, R. D. (2008). *Forecasting with Exponential Smoothing: The State Space Approach*. Springer.\n3. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 8.\n4. De Livera, A. M., Hyndman, R. J., & Snyder, R. D. (2011). Forecasting time series with complex seasonal patterns using exponential smoothing. *JASA*, 106(496), 1513-1527.", "line_start": 292}, "docs/en/exponential-smoothing/holt.md#section_0": {"doc_id": "docs/en/exponential-smoothing/holt.md", "heading": "Holt's Linear Method", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Holt's method extends SES to capture linear trends using two equations: one for level, one for trend. Forecasts follow a linear trajectory. Two parameters: α (level smoothing) and β (trend smoothing). Equivalent to ARIMA(0,2,2). Use when data shows persistent trend but no seasonality.\n</div>", "line_start": 1}, "docs/en/exponential-smoothing/holt.md#section_1": {"doc_id": "docs/en/exponential-smoothing/holt.md", "heading": "Core Definitions", "content": "**Holt's Linear Method**:\n\nLevel equation:\n$$\\ell_t = \\alpha y_t + (1-\\alpha)(\\ell_{t-1} + b_{t-1})$$\n\nTrend equation:\n$$b_t = \\beta(\\ell_t - \\ell_{t-1}) + (1-\\beta)b_{t-1}$$\n\nForecast equation:\n$$\\hat{y}_{t+h|t} = \\ell_t + hb_t$$\n\n**Parameters:**\n- $\\alpha \\in (0,1)$: Level smoothing\n- $\\beta \\in (0,1)$: Trend smoothing\n- $\\ell_0$: Initial level\n- $b_0$: Initial trend\n\n**Damped Trend Variant:**\n$$\\hat{y}_{t+h|t} = \\ell_t + (\\phi + \\phi^2 + \\cdots + \\phi^h)b_t$$\n\nwhere $\\phi \\in (0,1)$ dampens the trend for long horizons.", "line_start": 7}, "docs/en/exponential-smoothing/holt.md#section_2": {"doc_id": "docs/en/exponential-smoothing/holt.md", "heading": "Forecast Trajectory", "content": "For standard Holt:\n$$\\hat{y}_{t+h|t} = \\ell_t + hb_t$$\n\nThis is linear in $h$ with:\n- Intercept: $\\ell_t$ (current level)\n- Slope: $b_t$ (current trend)", "line_start": 33}, "docs/en/exponential-smoothing/holt.md#section_3": {"doc_id": "docs/en/exponential-smoothing/holt.md", "heading": "Connection to ARIMA(0,2,2)", "content": "ARIMA(0,2,2): $(1-L)^2 y_t = (1+\\theta_1 L + \\theta_2 L^2)\\epsilon_t$\n\nThe relationship:\n$$\\alpha = 1 - \\theta_1 - \\theta_2$$\n$$\\beta = \\frac{-\\theta_2}{1-\\theta_1-\\theta_2}$$", "line_start": 42}, "docs/en/exponential-smoothing/holt.md#section_4": {"doc_id": "docs/en/exponential-smoothing/holt.md", "heading": "Damped Trend Forecast", "content": "$$\\hat{y}_{t+h|t} = \\ell_t + \\sum_{j=1}^{h}\\phi^j b_t = \\ell_t + \\frac{\\phi(1-\\phi^h)}{1-\\phi}b_t$$\n\nAs $h \\to \\infty$:\n$$\\hat{y}_{t+h|t} \\to \\ell_t + \\frac{\\phi}{1-\\phi}b_t$$\n\nForecasts asymptote to a constant (trend dies out).", "line_start": 50}, "docs/en/exponential-smoothing/holt.md#section_5": {"doc_id": "docs/en/exponential-smoothing/holt.md", "heading": "Prediction Intervals", "content": "Approximate variance for Holt's method:\n$$\\text{Var}(\\hat{e}_{t+h|t}) \\approx \\sigma^2[1 + (h-1)(\\alpha^2 + \\alpha\\beta h + \\frac{\\beta^2 h(2h-1)}{6})]$$\n\nThis grows faster than SES because trend adds uncertainty.", "line_start": 59}, "docs/en/exponential-smoothing/holt.md#section_6": {"doc_id": "docs/en/exponential-smoothing/holt.md", "heading": "Algorithm/Model Sketch", "content": "**Holt's Method Algorithm:**\n\n```\nInput: y[1:n], α, β (or optimize)\nOutput: level, trend, forecasts\n\n1. Initialize:\n   ℓ[0] = y[1]\n   b[0] = y[2] - y[1]  (or use regression on first few points)\n\n2. For t = 1 to n:\n   ℓ[t] = α * y[t] + (1-α) * (ℓ[t-1] + b[t-1])\n   b[t] = β * (ℓ[t] - ℓ[t-1]) + (1-β) * b[t-1]\n\n3. For h = 1 to H:\n   forecast[n+h] = ℓ[n] + h * b[n]\n\nReturn forecasts\n```\n\n**When to Use Damped Trend:**\n- Long forecast horizons\n- Trend expected to flatten\n- Historical trend reversals\n- Generally safer for production", "line_start": 66}, "docs/en/exponential-smoothing/holt.md#section_7": {"doc_id": "docs/en/exponential-smoothing/holt.md", "heading": "Common Pitfalls", "content": "1. **Extrapolating linear trend too far**: Linear trends rarely continue indefinitely. Use damped trend for long horizons.\n\n2. **Using when trend changes sign**: Holt assumes consistent trend direction. Frequent trend reversals confuse the method.\n\n3. **Over-smoothing trend (low β)**: Makes trend too sticky; slow to recognize trend changes.\n\n4. **Under-smoothing trend (high β)**: Makes trend too volatile; noisy trend estimates.\n\n5. **Ignoring negative forecasts**: For positive series, linear extrapolation can predict negatives. Apply transforms or bounds.\n\n6. **Not comparing to damped**: Damped trend often outperforms linear Holt, especially for h > 4.", "line_start": 94}, "docs/en/exponential-smoothing/holt.md#section_8": {"doc_id": "docs/en/exponential-smoothing/holt.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.holtwinters import Holt", "line_start": 108}, "docs/en/exponential-smoothing/holt.md#section_9": {"doc_id": "docs/en/exponential-smoothing/holt.md", "heading": "Generate trend + noise data", "content": "np.random.seed(42)\nn = 100\ntrend = 0.5\ny = 10 + trend * np.arange(n) + np.random.randn(n) * 3", "line_start": 114}, "docs/en/exponential-smoothing/holt.md#section_10": {"doc_id": "docs/en/exponential-smoothing/holt.md", "heading": "Fit Holt's linear method", "content": "model = Holt(y, initialization_method='estimated')\nfit = model.fit(optimized=True)\n\nprint(f\"Optimal alpha: {fit.params['smoothing_level']:.3f}\")\nprint(f\"Optimal beta: {fit.params['smoothing_trend']:.3f}\")", "line_start": 120}, "docs/en/exponential-smoothing/holt.md#section_11": {"doc_id": "docs/en/exponential-smoothing/holt.md", "heading": "Forecast", "content": "forecast = fit.forecast(20)\nprint(f\"Forecast at h=10: {forecast.iloc[9]:.2f}\")\nprint(f\"Forecast at h=20: {forecast.iloc[19]:.2f}\")", "line_start": 127}, "docs/en/exponential-smoothing/holt.md#section_12": {"doc_id": "docs/en/exponential-smoothing/holt.md", "heading": "Compare with damped trend", "content": "fit_damped = Holt(y, damped_trend=True, initialization_method='estimated').fit()\nforecast_damped = fit_damped.forecast(20)\nprint(f\"\\nDamped phi: {fit_damped.params['damping_trend']:.3f}\")\nprint(f\"Damped forecast at h=20: {forecast_damped.iloc[19]:.2f}\")", "line_start": 132}, "docs/en/exponential-smoothing/holt.md#section_13": {"doc_id": "docs/en/exponential-smoothing/holt.md", "heading": "Note: damped forecast will be lower than linear at h=20", "content": "```", "line_start": 138}, "docs/en/exponential-smoothing/holt.md#section_14": {"doc_id": "docs/en/exponential-smoothing/holt.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why does Holt's method need two smoothing parameters while SES only needs one?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> SES models only the level. Holt's models both level and trend, which are separate components that may need different degrees of smoothing.\n\n<strong>Explanation:</strong>\n- Level may change frequently → need responsive α\n- Trend may be stable → need smooth β (or vice versa)\n\nSeparating the parameters allows:\n- Responsive level tracking (high α) + stable trend (low β)\n- Or stable level (low α) + responsive trend (high β)\n\nOne parameter couldn't capture both behaviors.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Setting α = β. These control different aspects; optimizing them independently usually improves forecasts.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Why is damped trend often preferred in practice?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Linear trends rarely persist indefinitely. Damped trend is more realistic because:\n\n1. **Bounded growth**: Real quantities (sales, populations) don't grow linearly forever\n2. **Mean reversion**: Many series return toward long-run average\n3. **Forecast safety**: Prevents extreme predictions at long horizons\n4. **Empirical success**: Often wins forecasting competitions\n\n**Key insight:** Damped trend hedges between \"trend continues\" and \"trend stops,\" which is often closer to reality.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using φ = 1 (no damping) as default. Research shows φ ≈ 0.8-0.98 often optimal. Let optimization choose φ.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> For damped trend with φ = 0.9, derive the long-run forecast limit.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> As h → ∞, the forecast approaches $\\ell_T + \\frac{\\phi}{1-\\phi}b_T = \\ell_T + 9b_T$.\n\n<strong>Derivation:</strong>\n$$\\hat{y}_{T+h|T} = \\ell_T + \\sum_{j=1}^{h}\\phi^j b_T = \\ell_T + b_T\\sum_{j=1}^{h}\\phi^j$$\n\n$$\\sum_{j=1}^{h}\\phi^j = \\phi\\frac{1-\\phi^h}{1-\\phi}$$\n\nAs $h \\to \\infty$ with $|\\phi| < 1$:\n$$\\sum_{j=1}^{\\infty}\\phi^j = \\frac{\\phi}{1-\\phi}$$\n\nFor $\\phi = 0.9$:\n$$\\frac{0.9}{0.1} = 9$$\n\nSo forecast asymptotes to $\\ell_T + 9b_T$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Thinking damped trend means no trend. The trend still contributes; it just doesn't compound indefinitely.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Show that with α = 1 and β = 0, Holt's method reduces to the naive forecast.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> With these parameters:\n\n**Level equation:** $\\ell_t = y_t$ (level = most recent observation)\n\n**Trend equation:** $b_t = b_{t-1}$ (trend never updates from initial value)\n\nIf $b_0 = 0$:\n$$\\hat{y}_{t+h|t} = \\ell_t + h \\cdot 0 = y_t$$\n\nThis is the naive forecast: predict the most recent value.\n\nIf $b_0 \\neq 0$: Still get naive plus a fixed linear trend from initialization.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Extreme parameter values (0 or 1) often indicate model problems. If optimization pushes toward boundaries, reconsider the model or check data quality.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You fit Holt's method and get α = 0.8, β = 0.01. What does this suggest about your data?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n- **High α (0.8)**: Level is volatile; forecasts track recent values closely\n- **Very low β (0.01)**: Trend is very stable; slow to change from initial estimate\n\n**Interpretation:**\nThe data has a persistent, stable trend with volatile fluctuations around it. The model:\n- Quickly adapts level to recent observations\n- Keeps trend nearly constant (essentially using initial trend throughout)\n\n**Consider:**\n1. Is the trend actually constant? Maybe SES + deterministic trend is better\n2. Check if β = 0.01 is at/near boundary → might indicate trend isn't needed\n3. Compare to SES → if similar forecast accuracy, use simpler model\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Very low β might mean Holt's is overfitting — the trend component adds little beyond SES. Compare models with AIC or holdout validation.\n</div>\n</div>\n</details>", "line_start": 141}, "docs/en/exponential-smoothing/holt.md#section_15": {"doc_id": "docs/en/exponential-smoothing/holt.md", "heading": "References", "content": "1. Holt, C. C. (1957). Forecasting seasonals and trends by exponentially weighted moving averages. ONR Research Memorandum 52.\n2. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 8.\n3. Gardner, E. S., & McKenzie, E. (1985). Forecasting trends in time series. *Management Science*, 31(10), 1237-1246.\n4. Makridakis, S., & Hibon, M. (2000). The M3-Competition. *IJF*, 16(4), 451-476.", "line_start": 256}, "docs/en/exponential-smoothing/holt-winters.md#section_0": {"doc_id": "docs/en/exponential-smoothing/holt-winters.md", "heading": "Holt-Winters Method", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Holt-Winters extends Holt's method to handle seasonality. Additive version: $\\hat{y} = \\ell + hb + s_{t+h-m}$. Multiplicative version: $\\hat{y} = (\\ell + hb) \\times s_{t+h-m}$. Three parameters: α (level), β (trend), γ (seasonal). Choose additive when seasonal variation is constant; multiplicative when it scales with level.\n</div>", "line_start": 1}, "docs/en/exponential-smoothing/holt-winters.md#section_1": {"doc_id": "docs/en/exponential-smoothing/holt-winters.md", "heading": "Core Definitions", "content": "**Holt-Winters Additive Method:**\n\nLevel: $\\ell_t = \\alpha(y_t - s_{t-m}) + (1-\\alpha)(\\ell_{t-1} + b_{t-1})$\n\nTrend: $b_t = \\beta(\\ell_t - \\ell_{t-1}) + (1-\\beta)b_{t-1}$\n\nSeasonal: $s_t = \\gamma(y_t - \\ell_{t-1} - b_{t-1}) + (1-\\gamma)s_{t-m}$\n\nForecast: $\\hat{y}_{t+h|t} = \\ell_t + hb_t + s_{t+h-m(k+1)}$\n\nwhere $k = \\lfloor(h-1)/m\\rfloor$ and $m$ is the seasonal period.\n\n**Holt-Winters Multiplicative Method:**\n\nLevel: $\\ell_t = \\alpha\\frac{y_t}{s_{t-m}} + (1-\\alpha)(\\ell_{t-1} + b_{t-1})$\n\nTrend: $b_t = \\beta(\\ell_t - \\ell_{t-1}) + (1-\\beta)b_{t-1}$\n\nSeasonal: $s_t = \\gamma\\frac{y_t}{\\ell_{t-1} + b_{t-1}} + (1-\\gamma)s_{t-m}$\n\nForecast: $\\hat{y}_{t+h|t} = (\\ell_t + hb_t) \\times s_{t+h-m(k+1)}$", "line_start": 7}, "docs/en/exponential-smoothing/holt-winters.md#section_2": {"doc_id": "docs/en/exponential-smoothing/holt-winters.md", "heading": "Additive vs. Multiplicative Seasonality", "content": "**Additive**: Seasonal effect is constant amount\n$$y_t = \\ell_t + b_t + s_t + \\epsilon_t$$\n\n**Multiplicative**: Seasonal effect is proportional to level\n$$y_t = (\\ell_t + b_t) \\times s_t \\times \\epsilon_t$$\n\n**Decision rule:**\n- Plot the series: if seasonal swings grow with level → multiplicative\n- If seasonal swings are constant → additive\n- Ratio test: if std(seasonal) / mean(level) is constant → multiplicative", "line_start": 33}, "docs/en/exponential-smoothing/holt-winters.md#section_3": {"doc_id": "docs/en/exponential-smoothing/holt-winters.md", "heading": "Seasonal Indices", "content": "For a complete seasonal cycle, indices should:\n- Additive: sum to zero ($\\sum_{j=1}^{m} s_j = 0$)\n- Multiplicative: sum to m ($\\sum_{j=1}^{m} s_j = m$)\n\nNormalization is applied after each update.", "line_start": 46}, "docs/en/exponential-smoothing/holt-winters.md#section_4": {"doc_id": "docs/en/exponential-smoothing/holt-winters.md", "heading": "Connection to SARIMA", "content": "Holt-Winters additive with no trend is similar to SARIMA(0,1,m+1)(0,1,0)[m].\n\nThe exact equivalence is:\n$$\\text{ARIMA}(0,1,m+1)(0,1,0)_m: (1-L)(1-L^m)y_t = (1+\\theta_1 L + \\cdots + \\theta_{m+1}L^{m+1})\\epsilon_t$$", "line_start": 54}, "docs/en/exponential-smoothing/holt-winters.md#section_5": {"doc_id": "docs/en/exponential-smoothing/holt-winters.md", "heading": "Damped Seasonal Variants", "content": "Can combine damped trend with seasonal:\n$$\\hat{y}_{t+h|t} = \\ell_t + \\sum_{j=1}^{h}\\phi^j b_t + s_{t+h-m(k+1)}$$\n\nTrend flattens while seasonal pattern continues.", "line_start": 61}, "docs/en/exponential-smoothing/holt-winters.md#section_6": {"doc_id": "docs/en/exponential-smoothing/holt-winters.md", "heading": "Algorithm/Model Sketch", "content": "**Initialization:**\n\n```\nFor first m observations:\n1. Level: ℓ[0] = average of first seasonal cycle\n2. Trend: b[0] = (average of 2nd cycle - average of 1st cycle) / m\n3. Seasonal indices:\n   - Additive: s[j] = y[j] - ℓ[0] for j = 1,...,m\n   - Multiplicative: s[j] = y[j] / ℓ[0] for j = 1,...,m\n4. Normalize seasonal indices\n```\n\n**Parameter Selection:**\n- Start with α = β = γ = 0.2\n- Optimize to minimize SSE or MAE\n- Typical ranges: α ∈ [0.1, 0.5], β ∈ [0, 0.3], γ ∈ [0, 0.5]", "line_start": 68}, "docs/en/exponential-smoothing/holt-winters.md#section_7": {"doc_id": "docs/en/exponential-smoothing/holt-winters.md", "heading": "Common Pitfalls", "content": "1. **Wrong seasonality type**: Using additive when multiplicative is appropriate (or vice versa) degrades forecasts significantly.\n\n2. **Insufficient history**: Need at least 2 full seasonal cycles for reliable initialization. More is better.\n\n3. **Multiple seasonalities**: Holt-Winters handles one seasonal period. For multiple (e.g., daily + weekly), consider TBATS or decomposition.\n\n4. **Non-integer periods**: Seasonal period must be integer. For non-integer (e.g., 365.25 days/year), use Fourier terms.\n\n5. **Over-fitting γ**: High γ makes seasonal indices volatile. If the seasonal pattern is stable, use lower γ.\n\n6. **Forgetting normalization**: Seasonal indices can drift without normalization, causing forecast bias.", "line_start": 87}, "docs/en/exponential-smoothing/holt-winters.md#section_8": {"doc_id": "docs/en/exponential-smoothing/holt-winters.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing", "line_start": 101}, "docs/en/exponential-smoothing/holt-winters.md#section_9": {"doc_id": "docs/en/exponential-smoothing/holt-winters.md", "heading": "Generate seasonal data with trend", "content": "np.random.seed(42)\nn = 96  # 8 years of monthly data\nt = np.arange(n)\ntrend = 0.1 * t\nseasonal = 10 * np.sin(2 * np.pi * t / 12)  # Annual seasonality\nnoise = np.random.randn(n) * 2\ny = 50 + trend + seasonal + noise", "line_start": 107}, "docs/en/exponential-smoothing/holt-winters.md#section_10": {"doc_id": "docs/en/exponential-smoothing/holt-winters.md", "heading": "Fit additive Holt-Winters", "content": "hw_add = ExponentialSmoothing(\n    y,\n    trend='add',\n    seasonal='add',\n    seasonal_periods=12\n).fit()", "line_start": 116}, "docs/en/exponential-smoothing/holt-winters.md#section_11": {"doc_id": "docs/en/exponential-smoothing/holt-winters.md", "heading": "Fit multiplicative Holt-Winters", "content": "hw_mul = ExponentialSmoothing(\n    y,\n    trend='add',\n    seasonal='mul',\n    seasonal_periods=12\n).fit()\n\nprint(\"Additive HW:\")\nprint(f\"  α={hw_add.params['smoothing_level']:.3f}\")\nprint(f\"  β={hw_add.params['smoothing_trend']:.3f}\")\nprint(f\"  γ={hw_add.params['smoothing_seasonal']:.3f}\")\nprint(f\"  AIC={hw_add.aic:.1f}\")\n\nprint(\"\\nMultiplicative HW:\")\nprint(f\"  AIC={hw_mul.aic:.1f}\")", "line_start": 124}, "docs/en/exponential-smoothing/holt-winters.md#section_12": {"doc_id": "docs/en/exponential-smoothing/holt-winters.md", "heading": "Forecast", "content": "forecast = hw_add.forecast(12)\nprint(f\"\\n12-month forecast range: [{forecast.min():.1f}, {forecast.max():.1f}]\")\n```", "line_start": 141}, "docs/en/exponential-smoothing/holt-winters.md#section_13": {"doc_id": "docs/en/exponential-smoothing/holt-winters.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> How do you decide between additive and multiplicative seasonality?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Examine how seasonal variation changes with the level of the series:\n\n**Additive** if:\n- Seasonal swings (peak-to-trough) are roughly constant over time\n- Percentage variation decreases as level increases\n- Log transformation makes pattern multiplicative\n\n**Multiplicative** if:\n- Seasonal swings grow proportionally with level\n- Percentage variation is constant\n- Log transformation makes pattern additive\n\n**Practical test:**\n1. Plot the series — visual inspection often sufficient\n2. Compute seasonal variation in subperiods — if it grows with mean, use multiplicative\n3. Fit both and compare AIC/BIC\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using additive by default. Many business/economic series have multiplicative seasonality (higher sales → larger seasonal swings).\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Why do seasonal indices need to be normalized?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Without normalization, seasonal indices can drift, causing:\n1. Bias in level estimates\n2. Systematic over/under-forecasting\n3. Indices that no longer represent pure seasonal effects\n\n**Normalization constraints:**\n- Additive: $\\sum s_j = 0$ (seasonal effects cancel over a full cycle)\n- Multiplicative: $\\sum s_j = m$ (average seasonal factor is 1)\n\n**When drift occurs:**\nEach update $s_t = \\gamma(\\cdot) + (1-\\gamma)s_{t-m}$ can gradually shift the indices if the constraint isn't enforced, especially with estimation error.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Most software handles normalization automatically. If implementing manually, forget to normalize after updates → forecasts develop systematic bias.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> For additive Holt-Winters, show that with α = γ = 1 and β = 0, the seasonal index becomes $s_t = y_t - y_{t-m}$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> With these parameters:\n\n**Level equation:**\n$$\\ell_t = (y_t - s_{t-m}) + 0 \\cdot (\\ell_{t-1} + b_{t-1}) = y_t - s_{t-m}$$\n\n**Trend equation:** $b_t = b_{t-1}$ (constant, assume $b_0 = 0$)\n\n**Seasonal equation:**\n$$s_t = 1 \\cdot (y_t - \\ell_{t-1} - b_{t-1}) + 0 \\cdot s_{t-m}$$\n$$= y_t - \\ell_{t-1}$$\n\nSince $\\ell_{t-1} = y_{t-1} - s_{t-1-m}$ and with $s_{t-m} = y_{t-m} - \\ell_{t-m-1}$...\n\nAfter simplification:\n$$s_t = y_t - y_{t-m} + s_{t-m} - s_{t-2m} + \\cdots$$\n\nFor the simple case starting from initialization, this reduces to $s_t \\approx y_t - y_{t-m}$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Extreme parameters (α = 1, γ = 1) lead to overfitting — forecasts chase noise. Optimal parameters are usually well inside (0, 1).\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Write the h-step prediction interval formula for additive Holt-Winters (approximate).</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Approximate 95% prediction interval:\n\n$$\\hat{y}_{t+h|t} \\pm 1.96 \\cdot \\hat{\\sigma} \\cdot \\sqrt{1 + \\sum_{j=1}^{h-1}c_j^2}$$\n\nwhere $c_j$ are coefficients from the MA(∞) representation.\n\n**Simplified approximation:**\n$$\\hat{y}_{t+h|t} \\pm 1.96\\hat{\\sigma}\\sqrt{h + \\text{(trend and seasonal variance terms)}}$$\n\nFor practical use, the variance grows approximately linearly with $h$ for short horizons, then the seasonal component adds periodic variation.\n\n**Software approach:** Most implementations use simulation or state-space model variance formulas for accurate intervals.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Assuming constant prediction interval width. Holt-Winters intervals grow with horizon, though seasonal patterns create periodic widening/narrowing within each cycle.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You're forecasting monthly retail sales with clear December peaks. The peak has grown from $100K above average to $200K above average as total sales doubled. Which version of Holt-Winters should you use?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> **Multiplicative** seasonality, because:\n\n- December peak grew proportionally with overall sales level\n- $100K peak when average was (say) $200K → 50% above average\n- $200K peak when average was $400K → still 50% above average\n- The percentage deviation is constant → multiplicative\n\nWith **additive**, you'd assume December is always \"$150K above average\" (or some fixed amount), which doesn't match the pattern.\n\n**Model:** `ExponentialSmoothing(y, trend='add', seasonal='mul', seasonal_periods=12)`\n\n**Verification:** After fitting, check that multiplicative seasonal indices (as percentages) are roughly stable over time, while additive would show growing indices.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Looking only at absolute seasonal deviations. The key question is whether deviations scale with level. Plot deviations/level ratio over time to check.\n</div>\n</div>\n</details>", "line_start": 146}, "docs/en/exponential-smoothing/holt-winters.md#section_14": {"doc_id": "docs/en/exponential-smoothing/holt-winters.md", "heading": "References", "content": "1. Winters, P. R. (1960). Forecasting sales by exponentially weighted moving averages. *Management Science*, 6(3), 324-342.\n2. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 8.\n3. Chatfield, C., & Yar, M. (1988). Holt-Winters forecasting: Some practical issues. *The Statistician*, 129-140.\n4. Gardner, E. S. (2006). Exponential smoothing: The state of the art—Part II. *IJF*, 22(4), 637-666.", "line_start": 271}, "docs/en/exponential-smoothing/ses.md#section_0": {"doc_id": "docs/en/exponential-smoothing/ses.md", "heading": "Simple Exponential Smoothing (SES)", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> SES forecasts using weighted average of past observations with exponentially decaying weights. Formula: $\\hat{y}_{t+1} = \\alpha y_t + (1-\\alpha)\\hat{y}_t$. Parameter $\\alpha \\in (0,1)$ controls responsiveness. Equivalent to ARIMA(0,1,1). Best for series with no trend or seasonality. Point forecasts are flat (constant) for all horizons.\n</div>", "line_start": 1}, "docs/en/exponential-smoothing/ses.md#section_1": {"doc_id": "docs/en/exponential-smoothing/ses.md", "heading": "Core Definitions", "content": "**Simple Exponential Smoothing**:\n$$\\hat{y}_{t+1|t} = \\alpha y_t + (1-\\alpha)\\hat{y}_{t|t-1}$$\n\n**Alternative Forms:**\n\nComponent form:\n$$\\ell_t = \\alpha y_t + (1-\\alpha)\\ell_{t-1}$$\n$$\\hat{y}_{t+h|t} = \\ell_t$$\n\nWeighted average form:\n$$\\hat{y}_{t+1|t} = \\alpha \\sum_{j=0}^{t-1}(1-\\alpha)^j y_{t-j} + (1-\\alpha)^t \\ell_0$$\n\n**Parameters:**\n- $\\alpha \\in (0,1)$: Smoothing parameter\n- $\\ell_0$: Initial level", "line_start": 7}, "docs/en/exponential-smoothing/ses.md#section_2": {"doc_id": "docs/en/exponential-smoothing/ses.md", "heading": "Exponential Weights", "content": "Expanding the recursion:\n$$\\hat{y}_{t+1|t} = \\alpha y_t + \\alpha(1-\\alpha)y_{t-1} + \\alpha(1-\\alpha)^2 y_{t-2} + \\cdots$$\n\nWeights: $\\alpha, \\alpha(1-\\alpha), \\alpha(1-\\alpha)^2, \\ldots$\n\nThese sum to 1: $\\alpha \\sum_{j=0}^{\\infty}(1-\\alpha)^j = \\alpha \\cdot \\frac{1}{1-(1-\\alpha)} = 1$", "line_start": 27}, "docs/en/exponential-smoothing/ses.md#section_3": {"doc_id": "docs/en/exponential-smoothing/ses.md", "heading": "Connection to ARIMA(0,1,1)", "content": "ARIMA(0,1,1): $(1-L)y_t = (1+\\theta L)\\epsilon_t$\n\nThe optimal forecast is:\n$$\\hat{y}_{t+1|t} = \\hat{y}_{t|t-1} + \\frac{1}{1+\\theta}(y_t - \\hat{y}_{t|t-1})$$\n\nWith $\\alpha = \\frac{1}{1+\\theta}$, this is identical to SES.\n\nFor invertibility ($|\\theta| < 1$): $\\alpha \\in (0.5, 1)$", "line_start": 36}, "docs/en/exponential-smoothing/ses.md#section_4": {"doc_id": "docs/en/exponential-smoothing/ses.md", "heading": "Forecast Error Variance", "content": "For ARIMA(0,1,1):\n$$\\text{Var}(y_{t+h} - \\hat{y}_{t+h|t}) = \\sigma^2[1 + (h-1)(1-\\alpha)^2]$$\n\nPrediction interval:\n$$\\hat{y}_{t+h|t} \\pm z_{\\alpha/2}\\sigma\\sqrt{1 + (h-1)(1-\\alpha)^2}$$", "line_start": 47}, "docs/en/exponential-smoothing/ses.md#section_5": {"doc_id": "docs/en/exponential-smoothing/ses.md", "heading": "Optimal Smoothing Parameter", "content": "Choose $\\alpha$ to minimize sum of squared one-step forecast errors:\n$$\\text{SSE} = \\sum_{t=1}^{n}(y_t - \\hat{y}_{t|t-1})^2$$\n\nNo closed form; use numerical optimization.", "line_start": 55}, "docs/en/exponential-smoothing/ses.md#section_6": {"doc_id": "docs/en/exponential-smoothing/ses.md", "heading": "Algorithm/Model Sketch", "content": "**SES Algorithm:**\n\n```\nInput: time series y[1:n], smoothing parameter α\nOutput: forecasts\n\n1. Initialize: ℓ[0] = y[1] (or average of first few values)\n\n2. For t = 1 to n:\n   ℓ[t] = α * y[t] + (1-α) * ℓ[t-1]\n   fitted[t] = ℓ[t-1]  # one-step-ahead\n\n3. For h = 1 to H:\n   forecast[n+h] = ℓ[n]  # flat forecast\n\nReturn forecasts\n```\n\n**Selecting α:**\n- $\\alpha \\to 0$: Heavy smoothing, slow response to changes\n- $\\alpha \\to 1$: Light smoothing, forecasts close to most recent observation\n- Typical range: 0.1 to 0.3", "line_start": 62}, "docs/en/exponential-smoothing/ses.md#section_7": {"doc_id": "docs/en/exponential-smoothing/ses.md", "heading": "Common Pitfalls", "content": "1. **Using SES with trend**: SES produces flat forecasts. For trending data, use Holt's method.\n\n2. **Using SES with seasonality**: SES doesn't capture seasonal patterns. Use Holt-Winters or seasonal decomposition first.\n\n3. **Choosing α arbitrarily**: Always optimize α using historical data or use cross-validation.\n\n4. **Ignoring initialization**: The choice of $\\ell_0$ affects early forecasts. Common choices: $\\ell_0 = y_1$ or $\\ell_0 = \\bar{y}$.\n\n5. **Expecting decreasing forecast intervals**: For SES, prediction intervals grow with horizon (like random walk).\n\n6. **Confusing α interpretation**: High α = less smoothing (more weight on recent data). Some practitioners expect the opposite.", "line_start": 87}, "docs/en/exponential-smoothing/ses.md#section_8": {"doc_id": "docs/en/exponential-smoothing/ses.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing", "line_start": 101}, "docs/en/exponential-smoothing/ses.md#section_9": {"doc_id": "docs/en/exponential-smoothing/ses.md", "heading": "Generate level + noise data (appropriate for SES)", "content": "np.random.seed(42)\nn = 100\nlevel = 50\ny = level + np.random.randn(n) * 5", "line_start": 107}, "docs/en/exponential-smoothing/ses.md#section_10": {"doc_id": "docs/en/exponential-smoothing/ses.md", "heading": "Fit SES with optimization", "content": "model = SimpleExpSmoothing(y, initialization_method='estimated')\nfit = model.fit(optimized=True)\n\nprint(f\"Optimal alpha: {fit.params['smoothing_level']:.3f}\")\nprint(f\"Initial level: {fit.params['initial_level']:.2f}\")", "line_start": 113}, "docs/en/exponential-smoothing/ses.md#section_11": {"doc_id": "docs/en/exponential-smoothing/ses.md", "heading": "Compare different alpha values", "content": "alphas = [0.1, 0.3, 0.5, 0.9]\nfor alpha in alphas:\n    fit_alpha = model.fit(smoothing_level=alpha, optimized=False)\n    sse = np.sum((y - fit_alpha.fittedvalues)**2)\n    print(f\"Alpha={alpha}: SSE={sse:.1f}\")", "line_start": 120}, "docs/en/exponential-smoothing/ses.md#section_12": {"doc_id": "docs/en/exponential-smoothing/ses.md", "heading": "Forecast", "content": "forecast = fit.forecast(10)\nprint(f\"\\n10-step forecast (all same): {forecast.values}\")\n```", "line_start": 127}, "docs/en/exponential-smoothing/ses.md#section_13": {"doc_id": "docs/en/exponential-smoothing/ses.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why are SES forecasts constant (flat) for all future horizons?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> SES models the series as a local level plus noise: $y_t = \\ell_t + \\epsilon_t$. The best estimate of future level is the current level $\\ell_T$. With no trend or seasonality modeled, there's no reason to predict change.\n\n<strong>Explanation:</strong>\nThe forecast equation:\n$$\\hat{y}_{T+h|T} = \\ell_T \\text{ for all } h \\geq 1$$\n\nThis assumes the level stays constant. The uncertainty (prediction interval) grows with h, but the point forecast doesn't change.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using flat forecasts for trending data leads to systematic under/over-prediction. Always check if the series has a trend before choosing SES.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Explain the trade-off when choosing the smoothing parameter α.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n- **High α (close to 1)**: More weight on recent observations. Fast response to changes but noisy forecasts. Good for series with frequent level shifts.\n- **Low α (close to 0)**: More weight on distant observations. Smooth forecasts but slow to adapt. Good for stable series with much noise.\n\n**The trade-off:**\n- Responsiveness vs. stability\n- Bias vs. variance (high α = high variance; low α = potential bias if level changes)\n\n**Optimal α:** Balances these considerations; found by minimizing forecast error on historical data.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Assuming lower α is always \"smoother\" and better. In volatile series, low α causes the forecast to lag behind actual level shifts.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Show that the weights in SES sum to 1.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The forecast is $\\hat{y} = \\alpha \\sum_{j=0}^{\\infty}(1-\\alpha)^j y_{t-j}$.\n\n<strong>Derivation:</strong>\nSum of weights:\n$$\\sum_{j=0}^{\\infty}\\alpha(1-\\alpha)^j = \\alpha \\sum_{j=0}^{\\infty}(1-\\alpha)^j$$\n\nThis is a geometric series with ratio $(1-\\alpha)$, where $|1-\\alpha| < 1$:\n$$= \\alpha \\cdot \\frac{1}{1-(1-\\alpha)} = \\alpha \\cdot \\frac{1}{\\alpha} = 1$$\n\n**Key equation:** $\\sum_{j=0}^{\\infty}r^j = \\frac{1}{1-r}$ for $|r| < 1$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> In practice, we don't have infinite history. The \"missing\" weight goes to the initialization $\\ell_0$, which is why initialization matters for short series.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Derive the relationship between SES parameter α and ARIMA(0,1,1) parameter θ.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> $\\alpha = \\frac{1}{1+\\theta}$ or equivalently $\\theta = \\frac{1-\\alpha}{\\alpha}$.\n\n<strong>Derivation:</strong>\nARIMA(0,1,1): $y_t - y_{t-1} = \\epsilon_t + \\theta\\epsilon_{t-1}$\n\nThe optimal h-step forecast:\n$$\\hat{y}_{t+1|t} = y_t + \\theta\\hat{\\epsilon}_t$$\n\nwhere $\\hat{\\epsilon}_t = y_t - \\hat{y}_{t|t-1}$\n\nThis gives:\n$$\\hat{y}_{t+1|t} = y_t + \\theta(y_t - \\hat{y}_{t|t-1}) = (1+\\theta)y_t - \\theta\\hat{y}_{t|t-1}$$\n\nRearranging:\n$$\\hat{y}_{t+1|t} = \\frac{1}{1+\\theta}(1+\\theta)y_t + \\frac{\\theta}{1+\\theta}\\hat{y}_{t|t-1}$$\n\nWith $\\alpha = \\frac{1}{1+\\theta}$ and $1-\\alpha = \\frac{\\theta}{1+\\theta}$, this matches SES.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> ARIMA(0,1,1) with $\\theta > 0$ gives $\\alpha < 0.5$, which is non-invertible. Standard SES with optimized α usually gives $\\alpha > 0.5$ (invertible range).\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You apply SES to monthly sales data and get optimal α = 0.95. What does this suggest about your data? What should you consider?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> α = 0.95 means almost all weight on the most recent observation. This suggests:\n\n1. **High volatility or frequent level shifts** in the data\n2. **Possible trend** that SES is trying to track by being very responsive\n3. **Potential outliers** pulling the optimization toward high α\n4. **Near-random-walk behavior**\n\n**What to consider:**\n1. Check for trend → use Holt's method instead\n2. Look for outliers → they inflate optimal α\n3. Plot the series and fitted values → see if SES is \"chasing\" the data\n4. Try Holt's or Holt-Winters → may give better forecasts with lower α\n5. Consider ARIMA(0,1,1) → the θ would be near 0, confirming random walk\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Very high α often indicates model misspecification (missing trend or seasonality), not that SES is appropriate. Investigate before accepting.\n</div>\n</div>\n</details>", "line_start": 132}, "docs/en/exponential-smoothing/ses.md#section_14": {"doc_id": "docs/en/exponential-smoothing/ses.md", "heading": "References", "content": "1. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 8.\n2. Hyndman, R. J., Koehler, A. B., Ord, J. K., & Snyder, R. D. (2008). *Forecasting with Exponential Smoothing*. Springer.\n3. Gardner, E. S. (1985). Exponential smoothing: The state of the art. *Journal of Forecasting*, 4(1), 1-28.\n4. Brown, R. G. (1959). *Statistical Forecasting for Inventory Control*. McGraw-Hill.", "line_start": 245}, "docs/en/forecasting/prediction-intervals.md#section_0": {"doc_id": "docs/en/forecasting/prediction-intervals.md", "heading": "Prediction Intervals", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Prediction intervals quantify forecast uncertainty, giving a range where future values will likely fall. For ARIMA: PI width grows with horizon due to accumulated uncertainty. Key formula: $\\hat{y}_{T+h} \\pm z_{\\alpha/2}\\sigma_h$ where $\\sigma_h$ depends on model. Intervals assume normality; bootstrap provides non-parametric alternative.\n</div>", "line_start": 1}, "docs/en/forecasting/prediction-intervals.md#section_1": {"doc_id": "docs/en/forecasting/prediction-intervals.md", "heading": "Core Definitions", "content": "**Point Forecast:** Single best estimate of future value\n$$\\hat{y}_{T+h|T} = E[y_{T+h}|y_1,\\ldots,y_T]$$\n\n**Prediction Interval:** Range containing future value with probability $(1-\\alpha)$\n$$[\\hat{y}_{T+h|T} - z_{\\alpha/2}\\sigma_h, \\hat{y}_{T+h|T} + z_{\\alpha/2}\\sigma_h]$$\n\n**Forecast Error:** $e_{T+h|T} = y_{T+h} - \\hat{y}_{T+h|T}$\n\n**Forecast Variance:** $\\sigma_h^2 = \\text{Var}(e_{T+h|T})$\n\n**Coverage Probability:** Proportion of actual values falling within PI (should match nominal level).", "line_start": 7}, "docs/en/forecasting/prediction-intervals.md#section_2": {"doc_id": "docs/en/forecasting/prediction-intervals.md", "heading": "ARMA(p,q) Forecast Variance", "content": "The h-step forecast error can be written as:\n$$e_{T+h|T} = \\sum_{j=0}^{h-1}\\psi_j\\epsilon_{T+h-j}$$\n\nwhere $\\psi_j$ are MA(∞) coefficients.\n\nForecast variance:\n$$\\sigma_h^2 = \\sigma_\\epsilon^2\\sum_{j=0}^{h-1}\\psi_j^2$$", "line_start": 23}, "docs/en/forecasting/prediction-intervals.md#section_3": {"doc_id": "docs/en/forecasting/prediction-intervals.md", "heading": "Specific Models", "content": "**AR(1):** $y_t = \\phi y_{t-1} + \\epsilon_t$\n$$\\psi_j = \\phi^j$$\n$$\\sigma_h^2 = \\sigma_\\epsilon^2\\frac{1-\\phi^{2h}}{1-\\phi^2}$$\n\nAs $h \\to \\infty$: $\\sigma_h^2 \\to \\sigma_\\epsilon^2/(1-\\phi^2) = \\text{Var}(y_t)$\n\n**MA(1):** $y_t = \\epsilon_t + \\theta\\epsilon_{t-1}$\n$$\\sigma_1^2 = \\sigma_\\epsilon^2$$\n$$\\sigma_h^2 = \\sigma_\\epsilon^2(1+\\theta^2) \\text{ for } h \\geq 2$$\n\n**Random Walk (ARIMA(0,1,0)):**\n$$\\sigma_h^2 = h\\sigma_\\epsilon^2$$\n\nVariance grows linearly; PI width grows as $\\sqrt{h}$.", "line_start": 33}, "docs/en/forecasting/prediction-intervals.md#section_4": {"doc_id": "docs/en/forecasting/prediction-intervals.md", "heading": "Gaussian Prediction Intervals", "content": "Under normality:\n$$y_{T+h}|y_{1:T} \\sim N(\\hat{y}_{T+h|T}, \\sigma_h^2)$$\n\n95% PI: $\\hat{y}_{T+h|T} \\pm 1.96\\sigma_h$\n80% PI: $\\hat{y}_{T+h|T} \\pm 1.28\\sigma_h$", "line_start": 50}, "docs/en/forecasting/prediction-intervals.md#section_5": {"doc_id": "docs/en/forecasting/prediction-intervals.md", "heading": "Accounting for Parameter Uncertainty", "content": "When parameters are estimated, additional uncertainty:\n$$\\text{Var}(e_{T+h|T}) \\approx \\sigma_h^2 + \\frac{\\sigma_h^2}{n}\\sum_{j=0}^{h-1}\\left(\\frac{\\partial\\psi_j}{\\partial\\theta}\\right)^2\\text{Var}(\\hat{\\theta})$$\n\nFor large samples, parameter uncertainty is small relative to intrinsic forecast uncertainty.", "line_start": 58}, "docs/en/forecasting/prediction-intervals.md#section_6": {"doc_id": "docs/en/forecasting/prediction-intervals.md", "heading": "Algorithm/Model Sketch", "content": "**Computing Prediction Intervals:**\n\n```\n1. Fit model, estimate parameters θ̂ and σ̂²\n2. For each horizon h = 1, ..., H:\n   a. Compute point forecast ŷ_{T+h|T}\n   b. Calculate ψ₀, ψ₁, ..., ψ_{h-1} (MA coefficients)\n   c. Compute σ̂ₕ² = σ̂² × Σψⱼ²\n   d. Form interval: ŷ_{T+h|T} ± z_{α/2} × σ̂ₕ\n\n3. For non-normal data, use bootstrap:\n   a. Generate B bootstrap samples\n   b. Refit model on each\n   c. Generate forecasts\n   d. Take percentiles of forecast distribution\n```\n\n**Bootstrap Prediction Intervals:**\n```\nFor b = 1 to B:\n   1. Sample residuals with replacement: ε*[1:n]\n   2. Generate bootstrap series y* using model\n   3. Refit model to y*\n   4. Generate forecasts ŷ*_{T+1:T+H}\n\nTake 2.5% and 97.5% percentiles → 95% PI\n```", "line_start": 65}, "docs/en/forecasting/prediction-intervals.md#section_7": {"doc_id": "docs/en/forecasting/prediction-intervals.md", "heading": "Common Pitfalls", "content": "1. **Ignoring PI widening**: For I(d) processes, PIs grow without bound. Don't expect tight long-range forecasts.\n\n2. **Assuming constant width**: Only stationary AR(∞) processes have bounded PI. Most models have widening intervals.\n\n3. **Undercoverage**: If actual coverage < nominal, model may be misspecified or variance underestimated.\n\n4. **Overcoverage**: If actual coverage >> nominal, model may be over-conservative or wrong distributional assumption.\n\n5. **Non-normality**: For skewed or heavy-tailed data, Gaussian PIs may be too narrow. Use bootstrap.\n\n6. **Ignoring parameter uncertainty**: Small sample → parameter estimates uncertain → PIs wider than formula suggests.", "line_start": 95}, "docs/en/forecasting/prediction-intervals.md#section_8": {"doc_id": "docs/en/forecasting/prediction-intervals.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.arima.model import ARIMA", "line_start": 109}, "docs/en/forecasting/prediction-intervals.md#section_9": {"doc_id": "docs/en/forecasting/prediction-intervals.md", "heading": "Generate AR(1) data", "content": "np.random.seed(42)\nphi = 0.7\nsigma = 1.0\nn = 200\ny = np.zeros(n)\nfor t in range(1, n):\n    y[t] = phi * y[t-1] + np.random.randn() * sigma", "line_start": 115}, "docs/en/forecasting/prediction-intervals.md#section_10": {"doc_id": "docs/en/forecasting/prediction-intervals.md", "heading": "Fit model", "content": "model = ARIMA(y, order=(1, 0, 0)).fit()", "line_start": 124}, "docs/en/forecasting/prediction-intervals.md#section_11": {"doc_id": "docs/en/forecasting/prediction-intervals.md", "heading": "Get forecasts with prediction intervals", "content": "forecast_obj = model.get_forecast(steps=20)\nforecast = forecast_obj.predicted_mean\nconf_int = forecast_obj.conf_int(alpha=0.05)  # 95% PI\n\nprint(\"Forecast with 95% PI:\")\nfor h in [1, 5, 10, 20]:\n    print(f\"  h={h}: {forecast.iloc[h-1]:.2f} \"\n          f\"[{conf_int.iloc[h-1, 0]:.2f}, {conf_int.iloc[h-1, 1]:.2f}]\")", "line_start": 127}, "docs/en/forecasting/prediction-intervals.md#section_12": {"doc_id": "docs/en/forecasting/prediction-intervals.md", "heading": "Theoretical width for AR(1)", "content": "phi_hat = model.arparams[0]\nsigma_hat = np.sqrt(model.scale)\nfor h in [1, 5, 10, 20]:\n    var_h = sigma_hat**2 * (1 - phi_hat**(2*h)) / (1 - phi_hat**2)\n    width_theory = 2 * 1.96 * np.sqrt(var_h)\n    width_actual = conf_int.iloc[h-1, 1] - conf_int.iloc[h-1, 0]\n    print(f\"h={h}: Theory width={width_theory:.2f}, Actual={width_actual:.2f}\")\n```", "line_start": 137}, "docs/en/forecasting/prediction-intervals.md#section_13": {"doc_id": "docs/en/forecasting/prediction-intervals.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why do prediction intervals widen with forecast horizon for most time series models?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Future shocks are unknown and accumulate over time.\n\nFor h-step forecast, we don't know $\\epsilon_{T+1}, \\ldots, \\epsilon_{T+h}$. The forecast error:\n$$e_{T+h|T} = \\sum_{j=0}^{h-1}\\psi_j\\epsilon_{T+h-j}$$\n\nMore unknown shocks → more variance → wider interval.\n\n**Exceptions:**\n- Mean-reverting processes (stationary AR) converge to unconditional variance\n- But for random walk (unit root), variance grows linearly with h\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Expecting tight long-range forecasts. For I(1) processes, 1-year-ahead PI is much wider than 1-day-ahead. This is fundamental, not a modeling failure.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> What does it mean if your prediction intervals have 85% coverage when they're supposed to have 95%?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> **Undercoverage** — actual values fall outside the PI more often than expected.\n\n**Possible causes:**\n1. **Model misspecification**: True process not captured (missing components)\n2. **Non-normality**: Heavy tails cause more extreme values\n3. **Variance underestimated**: $\\hat{\\sigma}$ too small\n4. **Structural changes**: Model fit to stable period, tested on volatile period\n5. **Parameter uncertainty ignored**: Especially problematic in small samples\n\n**Remedies:**\n- Use bootstrap PIs\n- Check residual diagnostics\n- Consider heavier-tailed distributions\n- Widen intervals manually (e.g., use 99% for conservative 95%)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Trusting nominal coverage without validation. Always check empirical coverage on holdout data.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Derive the h-step forecast variance for ARIMA(0,1,0) (random walk).</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> $\\sigma_h^2 = h\\sigma_\\epsilon^2$\n\n**Derivation:**\nRandom walk: $y_t = y_{t-1} + \\epsilon_t$\n\nThe h-step forecast:\n$$y_{T+h} = y_T + \\sum_{j=1}^{h}\\epsilon_{T+j}$$\n\nBest forecast: $\\hat{y}_{T+h|T} = y_T$ (current value)\n\nForecast error:\n$$e_{T+h|T} = y_{T+h} - y_T = \\sum_{j=1}^{h}\\epsilon_{T+j}$$\n\nSince $\\epsilon_j$ are independent:\n$$\\text{Var}(e_{T+h|T}) = \\sum_{j=1}^{h}\\sigma_\\epsilon^2 = h\\sigma_\\epsilon^2$$\n\n**PI:** $y_T \\pm 1.96\\sigma_\\epsilon\\sqrt{h}$\n\nWidth grows as $\\sqrt{h}$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> For random walk, long-horizon PIs become very wide. A 100-step-ahead 95% PI is 10× wider than 1-step-ahead.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Why does a stationary AR(1) have bounded prediction interval width as $h \\to \\infty$?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> For stationary AR(1) with $|\\phi| < 1$:\n\n$$\\sigma_h^2 = \\sigma_\\epsilon^2\\frac{1-\\phi^{2h}}{1-\\phi^2}$$\n\nAs $h \\to \\infty$: $\\phi^{2h} \\to 0$\n\n$$\\lim_{h\\to\\infty}\\sigma_h^2 = \\frac{\\sigma_\\epsilon^2}{1-\\phi^2} = \\text{Var}(y_t)$$\n\n**Intuition:** For stationary processes, distant future values are independent of current observation. The forecast converges to unconditional mean, and uncertainty converges to unconditional variance.\n\nThe PI width converges to $2 \\times 1.96 \\times \\sqrt{\\text{Var}(y_t)}$ — the interval you'd give without any data.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Bounded PI doesn't mean narrow PI. The unconditional variance can still be large, especially for $\\phi$ near 1.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> Your model produces 95% prediction intervals, but stakeholders want to know the \"worst case.\" How do you translate PIs for business use?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Several approaches:\n\n1. **Use higher confidence level**: 99% PI gives more conservative bounds\n   - Upper 99% PI ≈ upper bound for planning\n\n2. **Report specific percentiles**:\n   - \"95% chance demand is below X\"\n   - \"5% chance it exceeds Y\"\n\n3. **Scenario analysis**:\n   - Best case: lower 80% bound\n   - Expected: point forecast\n   - Worst case: upper 95% or 99% bound\n\n4. **Distribution summary**:\n   - Most likely range: 50% PI\n   - Reasonable range: 80% PI\n   - Extreme scenarios: 95% PI\n\n5. **Risk quantiles**: \"10% chance of loss exceeding $Z\"\n\n**Key message:** PIs are probability statements. \"Worst case\" depends on acceptable risk level.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using upper 95% bound as \"worst case\" — this still has 2.5% chance of being exceeded. For true tail risk, use higher percentiles or extreme value methods.\n</div>\n</div>\n</details>", "line_start": 147}, "docs/en/forecasting/prediction-intervals.md#section_14": {"doc_id": "docs/en/forecasting/prediction-intervals.md", "heading": "References", "content": "1. Chatfield, C. (1993). Calculating interval forecasts. *Journal of Business & Economic Statistics*, 11(2), 121-135.\n2. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 5.\n3. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis*. Wiley. Chapter 5.\n4. Thombs, L. A., & Schucany, W. R. (1990). Bootstrap prediction intervals for autoregression. *JASA*, 85(410), 486-492.", "line_start": 280}, "docs/en/forecasting/multi-step.md#section_0": {"doc_id": "docs/en/forecasting/multi-step.md", "heading": "Multi-step Forecasting", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Multi-step forecasting predicts multiple future values. Three strategies: recursive (iterate 1-step), direct (separate model per horizon), MIMO (multiple-input-multiple-output). Recursive accumulates errors but uses single model; direct avoids error accumulation but needs h models. For ARIMA, recursive is standard. For ML, direct often preferred.\n</div>", "line_start": 1}, "docs/en/forecasting/multi-step.md#section_1": {"doc_id": "docs/en/forecasting/multi-step.md", "heading": "Core Definitions", "content": "**Multi-step Forecast**: Predict $y_{T+1}, y_{T+2}, \\ldots, y_{T+H}$ given $y_1, \\ldots, y_T$.\n\n**Forecast Horizon (H)**: Number of steps ahead to predict.\n\n**Strategies:**\n\n1. **Recursive (Iterated)**: Use 1-step model repeatedly, feeding predictions as inputs\n2. **Direct**: Train separate model for each horizon h\n3. **MIMO**: Single model outputs all horizons simultaneously\n4. **DirRec**: Hybrid of direct and recursive", "line_start": 7}, "docs/en/forecasting/multi-step.md#section_2": {"doc_id": "docs/en/forecasting/multi-step.md", "heading": "Recursive Strategy", "content": "Train model: $\\hat{y}_{t+1} = f(y_t, y_{t-1}, \\ldots)$\n\nFor h-step forecast:\n$$\\hat{y}_{T+1} = f(y_T, y_{T-1}, \\ldots)$$\n$$\\hat{y}_{T+2} = f(\\hat{y}_{T+1}, y_T, \\ldots)$$\n$$\\hat{y}_{T+h} = f(\\hat{y}_{T+h-1}, \\hat{y}_{T+h-2}, \\ldots)$$\n\n**Properties:**\n- Uses single model\n- Consistent with underlying DGP\n- Error accumulates through iterations", "line_start": 22}, "docs/en/forecasting/multi-step.md#section_3": {"doc_id": "docs/en/forecasting/multi-step.md", "heading": "Direct Strategy", "content": "Train h separate models:\n$$\\hat{y}_{t+h}^{(h)} = f_h(y_t, y_{t-1}, \\ldots)$$\n\nEach model directly predicts h steps ahead.\n\n**Properties:**\n- No error propagation\n- Requires h models\n- Each model trained on different target\n- May violate consistency across horizons", "line_start": 36}, "docs/en/forecasting/multi-step.md#section_4": {"doc_id": "docs/en/forecasting/multi-step.md", "heading": "Error Analysis", "content": "**Recursive error:**\n$$e_{T+h}^{rec} = \\sum_{j=1}^{h}\\alpha_j\\epsilon_{T+j} + O(\\text{model error})$$\n\nError compounds through iterations.\n\n**Direct error:**\n$$e_{T+h}^{dir} = \\epsilon_{T+h}^{(h)} + O(\\text{model error}_h)$$\n\nNo compounding, but model $f_h$ may be less efficient.", "line_start": 49}, "docs/en/forecasting/multi-step.md#section_5": {"doc_id": "docs/en/forecasting/multi-step.md", "heading": "Theoretical Comparison", "content": "**Theorem (Ben Taieb & Hyndman):**\nUnder correct model specification:\n- Recursive is optimal (MSFE-minimizing)\n- Direct is consistent but less efficient\n\nUnder misspecification:\n- Direct may outperform recursive\n- Recursive compounds misspecification errors", "line_start": 61}, "docs/en/forecasting/multi-step.md#section_6": {"doc_id": "docs/en/forecasting/multi-step.md", "heading": "Algorithm/Model Sketch", "content": "**Strategy Selection Guidelines:**\n\n```\nIF model is well-specified (ARIMA, ETS):\n   USE recursive\n   - Theoretical optimality\n   - Proper uncertainty quantification\n\nELIF using ML/nonparametric methods:\n   USE direct\n   - Avoids error accumulation\n   - Each horizon optimized separately\n\nELIF forecast horizons are related:\n   USE MIMO\n   - Single model, multiple outputs\n   - Can capture horizon dependencies\n\nFOR robust approach:\n   COMBINE recursive and direct\n   - Average forecasts\n   - Often improves accuracy\n```\n\n**MIMO Implementation:**\n```python", "line_start": 72}, "docs/en/forecasting/multi-step.md#section_7": {"doc_id": "docs/en/forecasting/multi-step.md", "heading": "Output: [y_{t+1}, y_{t+2}, ..., y_{t+H}]", "content": "X_train, Y_train = create_mimo_data(y, lags=p, horizon=H)\nmodel = MultiOutputRegressor(base_model)\nmodel.fit(X_train, Y_train)", "line_start": 102}, "docs/en/forecasting/multi-step.md#section_8": {"doc_id": "docs/en/forecasting/multi-step.md", "heading": "Forecast", "content": "X_new = get_features(y[-p:])\nforecasts = model.predict(X_new)  # Returns [ŷ_{T+1}, ..., ŷ_{T+H}]\n```", "line_start": 108}, "docs/en/forecasting/multi-step.md#section_9": {"doc_id": "docs/en/forecasting/multi-step.md", "heading": "Common Pitfalls", "content": "1. **Using recursive with ML**: Tree-based models don't extrapolate well; recursive strategy can produce flat or exploding forecasts.\n\n2. **Ignoring error accumulation**: For long horizons, recursive ARIMA uncertainty grows. Don't trust tight intervals at h=100.\n\n3. **Direct model inconsistency**: Direct models for h=5 and h=6 may give $\\hat{y}_{T+6} < \\hat{y}_{T+5}$ (non-monotonic when trend expected).\n\n4. **Computational cost**: Direct requires H models. For H=365 (daily data, 1 year), this is expensive.\n\n5. **Different targets, same features**: Direct models at different horizons have different optimal features. Using same features for all h is suboptimal.\n\n6. **Ignoring seasonality in direct**: Direct model for h=12 on monthly data should capture annual pattern, but training data may not provide enough signal.", "line_start": 113}, "docs/en/forecasting/multi-step.md#section_10": {"doc_id": "docs/en/forecasting/multi-step.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.multioutput import MultiOutputRegressor\n\ndef create_lagged_data(y, lags, horizon):\n    \"\"\"Create dataset for direct/MIMO forecasting.\"\"\"\n    X, Y = [], []\n    for t in range(lags, len(y) - horizon):\n        X.append(y[t-lags:t][::-1])  # [y_{t-1}, y_{t-2}, ...]\n        Y.append(y[t:t+horizon])      # [y_t, y_{t+1}, ...]\n    return np.array(X), np.array(Y)", "line_start": 127}, "docs/en/forecasting/multi-step.md#section_11": {"doc_id": "docs/en/forecasting/multi-step.md", "heading": "Generate AR(2) data", "content": "np.random.seed(42)\nn = 500\ny = np.zeros(n)\nfor t in range(2, n):\n    y[t] = 0.5 * y[t-1] + 0.3 * y[t-2] + np.random.randn()", "line_start": 142}, "docs/en/forecasting/multi-step.md#section_12": {"doc_id": "docs/en/forecasting/multi-step.md", "heading": "Split", "content": "train, test = y[:400], y[400:]\nH = 10  # Forecast horizon", "line_start": 149}, "docs/en/forecasting/multi-step.md#section_13": {"doc_id": "docs/en/forecasting/multi-step.md", "heading": "Recursive strategy", "content": "from statsmodels.tsa.ar_model import AutoReg\nmodel_rec = AutoReg(train, lags=2).fit()\nforecast_rec = model_rec.forecast(H)", "line_start": 153}, "docs/en/forecasting/multi-step.md#section_14": {"doc_id": "docs/en/forecasting/multi-step.md", "heading": "Direct strategy", "content": "X_train, Y_train = create_lagged_data(train, lags=5, horizon=H)\ndirect_models = [Ridge().fit(X_train, Y_train[:, h]) for h in range(H)]\nX_new = train[-5:][::-1].reshape(1, -1)\nforecast_dir = np.array([m.predict(X_new)[0] for m in direct_models])", "line_start": 158}, "docs/en/forecasting/multi-step.md#section_15": {"doc_id": "docs/en/forecasting/multi-step.md", "heading": "MIMO strategy", "content": "mimo_model = MultiOutputRegressor(Ridge()).fit(X_train, Y_train)\nforecast_mimo = mimo_model.predict(X_new)[0]", "line_start": 164}, "docs/en/forecasting/multi-step.md#section_16": {"doc_id": "docs/en/forecasting/multi-step.md", "heading": "Compare", "content": "print(\"Forecasts comparison:\")\nprint(f\"Recursive: {np.round(forecast_rec[:5], 2)}\")\nprint(f\"Direct:    {np.round(forecast_dir[:5], 2)}\")\nprint(f\"MIMO:      {np.round(forecast_mimo[:5], 2)}\")\nprint(f\"Actual:    {np.round(test[:5], 2)}\")\n```", "line_start": 168}, "docs/en/forecasting/multi-step.md#section_17": {"doc_id": "docs/en/forecasting/multi-step.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why does the recursive strategy accumulate errors while direct does not?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Recursive:** At each step, uses predicted values as inputs:\n$$\\hat{y}_{T+2} = f(\\hat{y}_{T+1}, y_T, \\ldots)$$\n\nError in $\\hat{y}_{T+1}$ affects $\\hat{y}_{T+2}$, which affects $\\hat{y}_{T+3}$, etc.\n\n**Direct:** Each horizon uses only actual observed values:\n$$\\hat{y}_{T+h} = f_h(y_T, y_{T-1}, \\ldots)$$\n\nErrors at different horizons are independent (given the data).\n\n**Trade-off:**\n- Recursive: consistent but error compounds\n- Direct: no compounding but less efficient (separate models)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Thinking direct is always better. For well-specified models, recursive is theoretically optimal. Direct wins mainly under misspecification.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> When would you prefer direct over recursive forecasting?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Prefer direct when:\n\n1. **Model misspecification**: If 1-step model is wrong, recursive compounds errors\n2. **ML methods**: Trees/NNs often perform poorly in recursive mode\n3. **Horizon-specific patterns**: Different dynamics at different horizons\n4. **Long horizons**: Recursive uncertainty explodes; direct stays bounded\n5. **Non-stationary features**: Recursive may drift; direct anchors to data\n\nPrefer recursive when:\n- Model is well-specified (ARIMA, ETS)\n- Need consistent probability framework\n- Computational efficiency matters\n- Understanding model dynamics is important\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using recursive with gradient boosting — trees don't extrapolate, leading to flat/constant long-horizon forecasts.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> For AR(1) with $y_t = \\phi y_{t-1} + \\epsilon_t$, derive the h-step recursive forecast and show it equals the direct optimal forecast.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Recursive:**\n$$\\hat{y}_{T+1|T} = \\phi y_T$$\n$$\\hat{y}_{T+2|T} = \\phi \\hat{y}_{T+1|T} = \\phi^2 y_T$$\n$$\\hat{y}_{T+h|T} = \\phi^h y_T$$\n\n**Direct (optimal):**\nThe conditional expectation:\n$$E[y_{T+h}|y_T] = E[\\phi^h y_T + \\sum_{j=0}^{h-1}\\phi^j\\epsilon_{T+h-j}|y_T]$$\n$$= \\phi^h y_T + 0 = \\phi^h y_T$$\n\nThey're identical! For correctly specified linear models, recursive = direct optimal.\n\n**Key insight:** When model is correct, feeding $\\hat{y}_{T+j}$ in place of $y_{T+j}$ gives the same result as computing $E[y_{T+h}|y_{1:T}]$ directly.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> This equivalence holds for linear models. For nonlinear models, recursive ≠ direct even when correctly specified.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> How does the forecast error variance differ between recursive and direct strategies?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Recursive (correct model):**\n$$\\text{Var}(e_{T+h}^{rec}) = \\sigma^2\\sum_{j=0}^{h-1}\\psi_j^2$$\n\nThis is the theoretical minimum (Cramér-Rao bound for linear prediction).\n\n**Direct (correct model):**\n$$\\text{Var}(e_{T+h}^{dir}) = \\text{Var}(e_{T+h}^{rec}) + \\text{estimation variance}_h$$\n\nDirect adds variance because model $f_h$ is estimated less efficiently than the 1-step model (less data effectively used).\n\n**Under misspecification:**\n- Recursive: $\\text{Var}(e_{T+h}^{rec}) \\approx h \\times \\text{bias}^2 + \\text{variance}$\n- Direct: $\\text{Var}(e_{T+h}^{dir}) \\approx \\text{bias}_h^2 + \\text{variance}_h$\n\nDirect doesn't compound bias across horizons.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Assuming direct always has larger variance. Under misspecification, direct often wins because it avoids compounding the bias.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You need to forecast daily sales 30 days ahead. You have an XGBoost model. Which strategy do you use?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> **Direct** or **MIMO** strategy.\n\n**Why not recursive:**\n- XGBoost is a tree-based model that doesn't extrapolate\n- Recursive feeding predictions back leads to:\n  - Forecasts that flatten to mean\n  - Or erratic behavior if predictions drift outside training range\n- 30-step recursion compounds errors significantly\n\n**Recommended approach:**\n1. **Direct:** Train 30 separate XGBoost models\n   - Horizon-specific optimization\n   - Can use different features per horizon\n   - Computationally more expensive\n\n2. **MIMO:** Train one multi-output model\n   - Use `MultiOutputRegressor(XGBRegressor())`\n   - Or custom multi-output architecture\n   - More efficient than 30 models\n\n3. **Hybrid:** Use LightGBM/XGBoost for short horizons, average with simpler model for longer horizons\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using recursive XGBoost — forecasts often degrade to constant or oscillate. Always validate multi-step behavior before production.\n</div>\n</div>\n</details>", "line_start": 176}, "docs/en/forecasting/multi-step.md#section_18": {"doc_id": "docs/en/forecasting/multi-step.md", "heading": "References", "content": "1. Ben Taieb, S., & Hyndman, R. J. (2014). A gradient boosting approach to the Kaggle load forecasting competition. *IJF*, 30(2), 382-394.\n2. Chevillon, G. (2007). Direct multi-step estimation and forecasting. *Journal of Economic Surveys*, 21(4), 746-785.\n3. Marcellino, M., Stock, J. H., & Watson, M. W. (2006). A comparison of direct and iterated multistep AR methods for forecasting macroeconomic time series. *Journal of Econometrics*, 135(1-2), 499-526.\n4. Ben Taieb, S., Bontempi, G., Atiya, A. F., & Sorjamaa, A. (2012). A review and comparison of strategies for multi-step ahead time series forecasting based on the NN5 forecasting competition. *Expert Systems with Applications*, 39(8), 7067-7083.", "line_start": 314}, "docs/en/interview/interview-questions.md#section_0": {"doc_id": "docs/en/interview/interview-questions.md", "heading": "Common Interview Questions", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> This section compiles frequently asked time series interview questions with concise answers. Topics span fundamentals (stationarity, ACF/PACF), model selection (ARIMA vs. ETS vs. ML), practical challenges (seasonality, missing data, forecasting strategy), and advanced topics (state space, deep learning). Use this as a quick review before interviews.\n</div>", "line_start": 1}, "docs/en/interview/interview-questions.md#section_1": {"doc_id": "docs/en/interview/interview-questions.md", "heading": "Q1: What is stationarity and why does it matter?", "content": "**Answer:** A stationary process has constant mean, constant variance, and autocovariance that depends only on lag (not time). It matters because:\n- Most classical models (ARIMA, VAR) assume stationarity\n- Non-stationary series have unpredictable long-term behavior\n- Statistical tests and inference require stationarity\n\n**Test for it:** ADF (null: unit root), KPSS (null: stationary). Use both for robust conclusions.\n\n**Make it stationary:** Differencing (stochastic trend), detrending (deterministic trend), log transform (changing variance).", "line_start": 9}, "docs/en/interview/interview-questions.md#section_2": {"doc_id": "docs/en/interview/interview-questions.md", "heading": "Q2: Explain the difference between ACF and PACF.", "content": "**Answer:**\n- **ACF:** Total correlation between $y_t$ and $y_{t-k}$, including indirect effects through intermediate lags\n- **PACF:** Direct correlation between $y_t$ and $y_{t-k}$ after removing effects of $y_{t-1}, \\ldots, y_{t-k+1}$\n\n**Use for identification:**\n- ACF cuts off at lag q → MA(q)\n- PACF cuts off at lag p → AR(p)\n- Both tail off → ARMA", "line_start": 20}, "docs/en/interview/interview-questions.md#section_3": {"doc_id": "docs/en/interview/interview-questions.md", "heading": "Q3: How do you handle seasonality?", "content": "**Answer:** Multiple approaches:\n1. **Seasonal differencing:** $(1-L^s)y_t$ removes seasonal unit root\n2. **SARIMA:** Explicit seasonal AR/MA terms\n3. **Seasonal decomposition:** STL separates trend, seasonal, remainder\n4. **Fourier terms:** Sin/cos features at seasonal frequencies\n5. **Dummy variables:** For short seasonal periods", "line_start": 31}, "docs/en/interview/interview-questions.md#section_4": {"doc_id": "docs/en/interview/interview-questions.md", "heading": "Q4: When would you choose ARIMA over exponential smoothing (ETS)?", "content": "**Answer:**\n- **ARIMA:** When series has complex autocorrelation structure, when you need to include external regressors (ARIMAX), when interpretability of AR/MA structure matters\n- **ETS:** When you want automatic model selection among 30 variants, when forecasting is primary goal, when you need proper state space framework for intervals\n\n**In practice:** Both often give similar forecasts. Use whichever is easier to implement and interpret for your context.", "line_start": 42}, "docs/en/interview/interview-questions.md#section_5": {"doc_id": "docs/en/interview/interview-questions.md", "heading": "Q5: How do you choose between classical methods and machine learning?", "content": "**Answer:**\n\n| Factor | Classical (ARIMA/ETS) | ML (RF, LSTM, etc.) |\n|--------|----------------------|---------------------|\n| Data size | Small-medium | Large |\n| Interpretability | High | Low |\n| Uncertainty | Well-calibrated intervals | Harder to quantify |\n| Multiple series | Separate models | Can share patterns |\n| Complex patterns | Limited | Can learn anything |\n| Computational cost | Low | High |\n\n**Rule of thumb:** Start with classical baselines. Use ML if you have lots of data AND classical methods underperform.", "line_start": 50}, "docs/en/interview/interview-questions.md#section_6": {"doc_id": "docs/en/interview/interview-questions.md", "heading": "Q6: What information criteria do you use for model selection? Explain AIC vs BIC.", "content": "**Answer:**\n- **AIC = -2ln(L) + 2k:** Optimizes prediction error; can overfit with large samples\n- **BIC = -2ln(L) + k·ln(n):** Consistent (selects true model); more parsimonious\n- **AICc:** Corrected AIC for small samples (use when n/k < 40)\n\n**Guideline:** Use AICc for forecasting, BIC for inference/interpretation.", "line_start": 65}, "docs/en/interview/interview-questions.md#section_7": {"doc_id": "docs/en/interview/interview-questions.md", "heading": "Q7: How do you handle missing values in time series?", "content": "**Answer:**\n1. **Linear interpolation:** For sporadic missing values\n2. **Forward/backward fill:** When last known value is reasonable\n3. **Seasonal imputation:** Fill with same period from previous cycle\n4. **Model-based:** Kalman filter or EM algorithm\n5. **Missing indicator:** Add binary feature, let model learn\n\n**Never:** Delete rows (breaks temporal continuity) or use mean imputation globally.", "line_start": 76}, "docs/en/interview/interview-questions.md#section_8": {"doc_id": "docs/en/interview/interview-questions.md", "heading": "Q8: What is data leakage in time series? How do you prevent it?", "content": "**Answer:** Using future information when training or creating features.\n\n**Common sources:**\n- Random train-test split (future in training set)\n- Rolling features without shift (includes current value)\n- Scaling on full data (test statistics in training)\n- External features not available at forecast time\n\n**Prevention:**\n- Always split temporally\n- Use `.shift(1).rolling()` for features\n- Fit scalers on training data only\n- Verify feature availability in production", "line_start": 87}, "docs/en/interview/interview-questions.md#section_9": {"doc_id": "docs/en/interview/interview-questions.md", "heading": "Q9: Explain multi-step forecasting strategies.", "content": "**Answer:**\n1. **Recursive:** Use 1-step model, iterate (feed predictions back)\n2. **Direct:** Train separate model for each horizon\n3. **MIMO:** Single model outputs all horizons\n\n**Trade-offs:**\n- Recursive: One model, but errors accumulate\n- Direct: No error accumulation, but h models needed\n- MIMO: Balance, but needs careful architecture\n\n**For ARIMA:** Recursive is standard and optimal.\n**For ML:** Direct often better (avoids compounding errors).", "line_start": 103}, "docs/en/interview/interview-questions.md#section_10": {"doc_id": "docs/en/interview/interview-questions.md", "heading": "Q10: How do you evaluate forecast accuracy?", "content": "**Answer:**\n\n**Metrics:**\n- MAE: Easy to interpret, robust to outliers\n- RMSE: Penalizes large errors more\n- MAPE: Percentage errors, but undefined at zero\n- MASE: Scale-free, compares to naive forecast (MASE < 1 is good)\n\n**Evaluation:**\n- Use rolling origin cross-validation\n- Match evaluation horizon to business need\n- Always compare to baselines (naive, seasonal naive)\n- Check prediction interval coverage", "line_start": 118}, "docs/en/interview/interview-questions.md#section_11": {"doc_id": "docs/en/interview/interview-questions.md", "heading": "Q11: What is the Kalman filter and when would you use it?", "content": "**Answer:** Recursive algorithm for optimal state estimation in linear Gaussian state space models.\n\n**Use cases:**\n- Tracking unobserved components (level, trend)\n- Online filtering (update as data arrives)\n- Missing data handling (natural framework)\n- Time-varying parameters\n\n**Connection:** ETS, ARIMA, structural time series are all state space models; Kalman filter provides unified estimation.", "line_start": 136}, "docs/en/interview/interview-questions.md#section_12": {"doc_id": "docs/en/interview/interview-questions.md", "heading": "Q12: Explain Granger causality. What are its limitations?", "content": "**Answer:** X Granger-causes Y if past X improves prediction of Y beyond Y's own past.\n\n**Limitations:**\n- **Not true causation:** Correlation due to common causes gives spurious GC\n- **Requires stationarity:** Standard tests need stationary data\n- **Sensitive to lag selection:** Results change with different lags\n- **Omitted variable bias:** Missing Z that causes both X and Y\n- **Contemporaneous effects missed:** Only tests lagged relationships", "line_start": 148}, "docs/en/interview/interview-questions.md#section_13": {"doc_id": "docs/en/interview/interview-questions.md", "heading": "Q13: When should you use deep learning for time series?", "content": "**Answer:** Consider DL when:\n- Large dataset (thousands+ observations)\n- Multiple related series (can share representations)\n- Complex patterns (nonlinear, interaction effects)\n- Long-range dependencies (attention mechanisms help)\n\n**Avoid DL when:**\n- Small dataset (ARIMA usually wins)\n- Interpretability required\n- Simple patterns (Occam's razor)\n- Computational constraints\n\n**Popular architectures:** LSTM, TCN (temporal CNN), Transformers", "line_start": 159}, "docs/en/interview/interview-questions.md#section_14": {"doc_id": "docs/en/interview/interview-questions.md", "heading": "Q14: How do you detect and handle structural breaks?", "content": "**Answer:**\n\n**Detection:**\n- Visual inspection\n- Chow test (tests for break at known point)\n- CUSUM (cumulative sum of residuals)\n- PELT algorithm (multiple change-points)\n\n**Handling:**\n- Regime-switching models (Markov switching)\n- Structural break dummies in regression\n- Train only on post-break data\n- Time-varying parameters", "line_start": 175}, "docs/en/interview/interview-questions.md#section_15": {"doc_id": "docs/en/interview/interview-questions.md", "heading": "Q15: What is forecast reconciliation?", "content": "**Answer:** Ensuring forecasts at different aggregation levels are consistent.\n\n**Example:** Product forecasts should sum to category forecast, which sums to total.\n\n**Approaches:**\n- **Top-down:** Forecast aggregate, distribute\n- **Bottom-up:** Forecast individuals, sum\n- **Optimal reconciliation:** Combine all levels optimally (MinT approach)\n\n**Why it matters:** Inconsistent forecasts confuse planning (inventory, budgets).", "line_start": 191}, "docs/en/interview/interview-questions.md#section_16": {"doc_id": "docs/en/interview/interview-questions.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What are the three components of the Box-Jenkins methodology?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Identification, Estimation, Diagnostic Checking\n\n1. **Identification:** Determine (p, d, q) using ACF/PACF, stationarity tests\n2. **Estimation:** Fit model using MLE or conditional least squares\n3. **Diagnostic Checking:** Verify residuals are white noise (Ljung-Box, ACF plots)\n\nIterate if diagnostics fail: return to identification.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Skipping diagnostics. A model with good AIC can still have autocorrelated residuals, indicating misspecification.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Explain the bias-variance trade-off in time series forecasting.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n- **Bias:** Systematic error from wrong model assumptions (underfitting)\n- **Variance:** Sensitivity to training data (overfitting)\n\n**In time series context:**\n- Simple model (AR(1)): May miss patterns (high bias) but stable (low variance)\n- Complex model (ARIMA(5,1,5)): Captures patterns but unstable (high variance)\n\n**Manifestation:**\n- High bias: Poor training fit, similar performance on test\n- High variance: Great training fit, poor test performance\n\n**Regularization:** AIC/BIC penalize complexity; cross-validation estimates out-of-sample error.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Only looking at training error. Always evaluate on holdout or via cross-validation.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> What is the forecast function for ARIMA(0,1,1)? How does it relate to exponential smoothing?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\nARIMA(0,1,1): $(1-L)y_t = (1+\\theta L)\\epsilon_t$\n\nForecast function:\n$$\\hat{y}_{T+h|T} = y_T + \\theta\\hat{\\epsilon}_T = y_T + \\theta(y_T - \\hat{y}_{T|T-1})$$\n\nThis equals Simple Exponential Smoothing with $\\alpha = 1/(1+\\theta)$:\n$$\\hat{y}_{T+h|T} = \\alpha y_T + (1-\\alpha)\\hat{y}_{T|T-1}$$\n\n**Connection:** SES is optimal for local level model / ARIMA(0,1,1).\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Treating ARIMA and ETS as completely different. They're deeply connected; many ETS models have ARIMA equivalents.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> How would you test for unit root in the presence of seasonality?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\nStandard ADF tests for non-seasonal unit root. For seasonal unit root:\n\n1. **HEGY test:** Tests for unit roots at seasonal and zero frequencies\n2. **Canova-Hansen:** Tests null of stationarity at seasonal frequencies\n3. **OCSB test:** Specifically for seasonal unit roots\n\n**Practical approach:**\n1. First, test for seasonal unit root (OCSB/HEGY)\n2. If present, apply seasonal differencing $(1-L^s)$\n3. Then test seasonally differenced series for regular unit root (ADF)\n4. Apply regular differencing if needed\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using ADF directly on seasonal data. ADF may reject unit root due to seasonality, not because series is stationary.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You're asked to forecast daily sales for a retailer. Walk through your approach.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**1. Understand the data:**\n- Length of history, granularity\n- Missing values, outliers\n- External factors (holidays, promotions)\n\n**2. Explore patterns:**\n- Plot series, decomposition\n- Check for trend, weekly/annual seasonality\n- Examine ACF/PACF\n\n**3. Baseline models:**\n- Naive (yesterday's sales)\n- Seasonal naive (same day last week)\n- Moving average\n\n**4. Candidate models:**\n- SARIMA (captures ARMA + seasonality)\n- ETS (automatic model selection)\n- Prophet (handles holidays, easy to use)\n- XGBoost with lag features (if many series)\n\n**5. Evaluation:**\n- Rolling origin CV (last 8-12 weeks)\n- Metrics: MAE, MAPE, MASE\n- Compare to baselines\n\n**6. Production considerations:**\n- Forecast horizon needed\n- Update frequency\n- Uncertainty communication\n- Monitoring and retraining schedule\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Jumping to complex models. Start simple (seasonal naive is often hard to beat) and add complexity only if justified by evaluation.\n</div>\n</div>\n</details>", "line_start": 204}, "docs/en/interview/interview-questions.md#section_17": {"doc_id": "docs/en/interview/interview-questions.md", "heading": "References", "content": "1. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts.\n2. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis*. Wiley.\n3. Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press.\n4. Shumway, R. H., & Stoffer, D. S. (2017). *Time Series Analysis and Its Applications*. Springer.", "line_start": 336}, "docs/en/model-selection/cross-validation.md#section_0": {"doc_id": "docs/en/model-selection/cross-validation.md", "heading": "Cross-Validation for Time Series", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Standard k-fold CV breaks temporal order and causes data leakage. Time series CV uses rolling/expanding windows: train on past, test on future. Key methods: rolling origin, blocked CV, h-step-ahead CV. Always respect temporal order. CV estimates out-of-sample error; use for model selection and hyperparameter tuning.\n</div>", "line_start": 1}, "docs/en/model-selection/cross-validation.md#section_1": {"doc_id": "docs/en/model-selection/cross-validation.md", "heading": "Core Definitions", "content": "**Time Series Cross-Validation:**\nEvaluate model performance by repeatedly:\n1. Training on past data\n2. Testing on future data (never seen during training)\n3. Rolling the window forward\n\n**Rolling Origin Evaluation:**\n```\nTrain: [1, ..., t]     → Test: [t+1, ..., t+h]\nTrain: [1, ..., t+1]   → Test: [t+2, ..., t+h+1]\n...\nTrain: [1, ..., T-h]   → Test: [T-h+1, ..., T]\n```\n\n**Expanding Window:** Training set grows; uses all past data.\n\n**Sliding Window:** Training set is fixed size; drops oldest data.", "line_start": 7}, "docs/en/model-selection/cross-validation.md#section_2": {"doc_id": "docs/en/model-selection/cross-validation.md", "heading": "Rolling Origin Forecast Error", "content": "For origin $t$ and horizon $h$:\n$$e_{t+h|t} = y_{t+h} - \\hat{y}_{t+h|t}$$\n\nAverage over all origins:\n$$\\text{RMSE}(h) = \\sqrt{\\frac{1}{T-t_0-h+1}\\sum_{t=t_0}^{T-h}e_{t+h|t}^2}$$", "line_start": 29}, "docs/en/model-selection/cross-validation.md#section_3": {"doc_id": "docs/en/model-selection/cross-validation.md", "heading": "Forecast Accuracy Metrics", "content": "**MAE (Mean Absolute Error):**\n$$\\text{MAE} = \\frac{1}{n}\\sum|e_t|$$\n\n**RMSE (Root Mean Squared Error):**\n$$\\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum e_t^2}$$\n\n**MAPE (Mean Absolute Percentage Error):**\n$$\\text{MAPE} = \\frac{100}{n}\\sum\\left|\\frac{e_t}{y_t}\\right|$$\n\n**SMAPE (Symmetric MAPE):**\n$$\\text{sMAPE} = \\frac{200}{n}\\sum\\frac{|e_t|}{|y_t| + |\\hat{y}_t|}$$\n\n**MASE (Mean Absolute Scaled Error):**\n$$\\text{MASE} = \\frac{\\text{MAE}}{\\frac{1}{n-1}\\sum_{t=2}^{n}|y_t - y_{t-1}|}$$\n\nMASE < 1 means better than naive forecast.", "line_start": 37}, "docs/en/model-selection/cross-validation.md#section_4": {"doc_id": "docs/en/model-selection/cross-validation.md", "heading": "Why Standard CV Fails", "content": "Standard k-fold CV:\n- Randomly splits data\n- Training fold may contain future observations\n- Test fold may contain past observations\n\nThis causes **data leakage**: model sees future information during training, giving optimistic error estimates.", "line_start": 56}, "docs/en/model-selection/cross-validation.md#section_5": {"doc_id": "docs/en/model-selection/cross-validation.md", "heading": "Algorithm/Model Sketch", "content": "**Rolling Origin CV:**\n\n```python\ndef rolling_origin_cv(y, model_fn, min_train, horizon, step=1):\n    \"\"\"\n    y: time series\n    model_fn: function that fits model and returns forecasts\n    min_train: minimum training size\n    horizon: forecast horizon\n    step: how much to move origin each iteration\n    \"\"\"\n    errors = []\n\n    for t in range(min_train, len(y) - horizon, step):\n        # Train on [0:t], test on [t:t+horizon]\n        train = y[:t]\n        test = y[t:t+horizon]\n\n        # Fit and forecast\n        forecast = model_fn(train, horizon)\n\n        # Store errors\n        errors.append(test - forecast)\n\n    return np.array(errors)\n```\n\n**Blocked CV (for related series):**\n```\nFold 1: Train [blocks 2,3,4,5] → Test [block 1]\nFold 2: Train [blocks 1,3,4,5] → Test [block 2]\n...\n```\n\nBlocks are contiguous time periods. Less ideal but useful when multiple series share parameters.", "line_start": 65}, "docs/en/model-selection/cross-validation.md#section_6": {"doc_id": "docs/en/model-selection/cross-validation.md", "heading": "Common Pitfalls", "content": "1. **Using standard k-fold CV**: Breaks temporal order, causes leakage. Never use for time series.\n\n2. **Testing on training period**: Even with rolling origin, some implementations accidentally include overlapping data.\n\n3. **Ignoring horizon**: CV for h=1 doesn't guarantee good h=10 performance. Match CV horizon to application.\n\n4. **Fixed origin only**: Testing from single origin underestimates variance. Use multiple origins.\n\n5. **Computation cost**: Full rolling CV with refitting is expensive. Consider step > 1 or fixed models.\n\n6. **Non-representative windows**: If dynamics change, old data may mislead. Consider sliding window.", "line_start": 103}, "docs/en/model-selection/cross-validation.md#section_7": {"doc_id": "docs/en/model-selection/cross-validation.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.arima.model import ARIMA\n\ndef mase(actual, forecast, train):\n    \"\"\"Mean Absolute Scaled Error.\"\"\"\n    mae = np.mean(np.abs(actual - forecast))\n    naive_mae = np.mean(np.abs(np.diff(train)))\n    return mae / naive_mae", "line_start": 117}, "docs/en/model-selection/cross-validation.md#section_8": {"doc_id": "docs/en/model-selection/cross-validation.md", "heading": "Generate data", "content": "np.random.seed(42)\nn = 200\ny = np.cumsum(np.random.randn(n)) + 0.1 * np.arange(n)", "line_start": 129}, "docs/en/model-selection/cross-validation.md#section_9": {"doc_id": "docs/en/model-selection/cross-validation.md", "heading": "Rolling origin CV", "content": "min_train = 100\nhorizon = 5\nstep = 5\n\nresults = {'ARIMA(1,1,0)': [], 'ARIMA(1,1,1)': [], 'ARIMA(2,1,1)': []}\n\nfor t in range(min_train, len(y) - horizon, step):\n    train = y[:t]\n    test = y[t:t+horizon]\n\n    for name, order in [('ARIMA(1,1,0)', (1,1,0)),\n                        ('ARIMA(1,1,1)', (1,1,1)),\n                        ('ARIMA(2,1,1)', (2,1,1))]:\n        try:\n            model = ARIMA(train, order=order).fit()\n            forecast = model.forecast(horizon)\n            error = mase(test, forecast, train)\n            results[name].append(error)\n        except:\n            results[name].append(np.nan)", "line_start": 134}, "docs/en/model-selection/cross-validation.md#section_10": {"doc_id": "docs/en/model-selection/cross-validation.md", "heading": "Compare models", "content": "print(\"Cross-Validation Results (MASE):\")\nfor name, errors in results.items():\n    valid_errors = [e for e in errors if not np.isnan(e)]\n    print(f\"  {name}: Mean={np.mean(valid_errors):.3f}, \"\n          f\"Std={np.std(valid_errors):.3f}\")\n```", "line_start": 156}, "docs/en/model-selection/cross-validation.md#section_11": {"doc_id": "docs/en/model-selection/cross-validation.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why does standard k-fold cross-validation fail for time series?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Standard k-fold CV randomly assigns observations to folds, breaking temporal order. This causes:\n\n1. **Data leakage**: Training data may include future observations relative to test data\n2. **Unrealistic evaluation**: In practice, you never have future data to train on\n3. **Optimistic error estimates**: Model implicitly learns from future, inflating apparent accuracy\n4. **Autocorrelation ignored**: Nearby points in train and test are correlated, reducing effective test independence\n\n**Example:** If test fold contains y[50:60] and train contains y[55:100], the model uses y[55:60] (future!) during training.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using sklearn's `cross_val_score` directly on time series. Always use `TimeSeriesSplit` or custom rolling evaluation.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> What's the difference between expanding window and sliding window CV?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Expanding window:**\n- Training set grows: [1:t], [1:t+1], [1:t+2], ...\n- Uses all historical data\n- Better for stable processes\n- More data → lower variance estimates\n\n**Sliding window:**\n- Training set is fixed size: [t-w:t], [t-w+1:t+1], ...\n- Drops oldest data\n- Better for non-stationary/evolving processes\n- Adapts to recent patterns\n\n**Choice depends on:**\n- Stationarity: non-stationary → sliding\n- Data availability: limited → expanding\n- Concept drift: present → sliding\n- Computational cost: sliding is more expensive (always refits)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using expanding window when dynamics change. Old data misleads the model. Check for structural breaks.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Why is MASE preferred over MAPE for forecast evaluation?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> MASE advantages:\n\n1. **Scale-independent**: Like MAPE, but handles zeros\n2. **No division by zero**: MAPE fails when $y_t = 0$\n3. **Symmetric**: Doesn't favor under/over-prediction asymmetrically (unlike MAPE)\n4. **Benchmark comparison**: MASE < 1 means better than naive\n5. **Well-defined for intermittent series**: Common in demand forecasting\n\n**Formula:**\n$$\\text{MASE} = \\frac{\\text{MAE}}{\\text{MAE}_{\\text{naive}}}$$\n\nwhere MAE_naive uses seasonal naive or 1-step naive as benchmark.\n\n**MAPE problems:**\n- Infinite when $y_t = 0$\n- Asymmetric: 50% error on y=100 (predict 50 or 150) treated differently\n- Scale-dependent interpretation\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using MAPE for intermittent demand or data with zeros — gives undefined or misleading results.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> How do you choose the minimum training size for rolling origin CV?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Balance between:\n\n1. **Statistical requirements:**\n   - Need enough data for reliable estimation\n   - Rule: at least 3-5 observations per parameter\n   - For ARIMA(p,d,q): min ~50 + 10(p+q) observations\n\n2. **Practical considerations:**\n   - More training → better model estimates\n   - But also → fewer CV folds → higher variance of CV estimate\n   - Typical: 60-80% of data for first training set\n\n3. **Domain knowledge:**\n   - If dynamics change, recent data matters more\n   - Full business cycles should be included (e.g., full year for seasonal)\n\n**Formula guidance:**\n$$\\text{min\\_train} = \\max(50, 2 \\times m, 5k + 10)$$\n\nwhere m = seasonal period, k = number of parameters.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using too small min_train gives unreliable early models; using too large leaves few CV folds for variance estimation.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You have 3 years of daily data (1095 observations) and need to select a model for 7-day forecasting. Design a CV scheme.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Recommended scheme:**\n\n```\nmin_train = 365 (1 full year to capture seasonality)\nhorizon = 7\nstep = 7 (weekly, reduces computation)\n```\n\nThis gives: (1095 - 365 - 7) / 7 ≈ 103 CV folds\n\n**Implementation:**\n```python\nfor t in range(365, 1095-7, 7):\n    train = data[:t]\n    test = data[t:t+7]\n    # Fit and evaluate\n```\n\n**Considerations:**\n1. **Include full seasonality**: 365 days captures annual pattern\n2. **Match horizon**: CV horizon = production horizon (7 days)\n3. **Step = horizon**: Non-overlapping test sets for independence\n4. **Metrics**: Use MASE, MAE, RMSE at each horizon h=1,...,7\n\n**Variants:**\n- Sliding window: train on last 365 days only (if non-stationary)\n- Gap: skip 1-2 days between train/test to simulate production delay\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using step=1 with 1095 observations → 723 fits, very slow. Use larger step for efficiency.\n</div>\n</div>\n</details>", "line_start": 164}, "docs/en/model-selection/cross-validation.md#section_12": {"doc_id": "docs/en/model-selection/cross-validation.md", "heading": "References", "content": "1. Bergmeir, C., & Benítez, J. M. (2012). On the use of cross-validation for time series predictor evaluation. *Information Sciences*, 191, 192-213.\n2. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 5.\n3. Tashman, L. J. (2000). Out-of-sample tests of forecasting accuracy: an analysis and review. *IJF*, 16(4), 437-450.\n4. Cerqueira, V., Torgo, L., & Mozetič, I. (2020). Evaluating time series forecasting models: An empirical study on performance estimation methods. *Machine Learning*, 109(11), 1997-2028.", "line_start": 314}, "docs/en/model-selection/information-criteria.md#section_0": {"doc_id": "docs/en/model-selection/information-criteria.md", "heading": "Information Criteria", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Information criteria balance model fit against complexity. AIC = -2log(L) + 2k favors prediction; BIC = -2log(L) + k·log(n) favors true model recovery. Lower is better. AIC tends to select larger models; BIC is more parsimonious. For small samples, use AICc. When criteria disagree, consider your goal: prediction (AIC) vs. inference (BIC).\n</div>", "line_start": 1}, "docs/en/model-selection/information-criteria.md#section_1": {"doc_id": "docs/en/model-selection/information-criteria.md", "heading": "Core Definitions", "content": "**AIC (Akaike Information Criterion):**\n$$\\text{AIC} = -2\\ln(\\hat{L}) + 2k$$\n\n**BIC (Bayesian/Schwarz Information Criterion):**\n$$\\text{BIC} = -2\\ln(\\hat{L}) + k\\ln(n)$$\n\n**AICc (Corrected AIC):**\n$$\\text{AICc} = \\text{AIC} + \\frac{2k(k+1)}{n-k-1}$$\n\n**Components:**\n- $\\hat{L}$: Maximum likelihood\n- $k$: Number of estimated parameters\n- $n$: Sample size", "line_start": 7}, "docs/en/model-selection/information-criteria.md#section_2": {"doc_id": "docs/en/model-selection/information-criteria.md", "heading": "AIC Derivation (Intuition)", "content": "AIC minimizes expected Kullback-Leibler divergence between true and fitted model:\n$$\\text{KL}(f||g_{\\hat{\\theta}}) = E_f[\\ln f(y)] - E_f[\\ln g_{\\hat{\\theta}}(y)]$$\n\nAkaike showed:\n$$E[-2\\ln g_{\\hat{\\theta}}(y_{new})] \\approx -2\\ln g_{\\hat{\\theta}}(y) + 2k$$\n\nMinimizing AIC approximately minimizes out-of-sample prediction error.", "line_start": 25}, "docs/en/model-selection/information-criteria.md#section_3": {"doc_id": "docs/en/model-selection/information-criteria.md", "heading": "BIC Derivation (Intuition)", "content": "BIC approximates log marginal likelihood:\n$$\\ln p(y|M) \\approx \\ln p(y|\\hat{\\theta},M) - \\frac{k}{2}\\ln(n)$$\n\nBIC is consistent: if true model is among candidates, BIC selects it with probability → 1 as n → ∞.", "line_start": 35}, "docs/en/model-selection/information-criteria.md#section_4": {"doc_id": "docs/en/model-selection/information-criteria.md", "heading": "For Gaussian Time Series", "content": "With residual variance $\\hat{\\sigma}^2$:\n$$\\text{AIC} = n\\ln(\\hat{\\sigma}^2) + 2k$$\n$$\\text{BIC} = n\\ln(\\hat{\\sigma}^2) + k\\ln(n)$$", "line_start": 42}, "docs/en/model-selection/information-criteria.md#section_5": {"doc_id": "docs/en/model-selection/information-criteria.md", "heading": "Penalty Comparison", "content": "| n | AIC penalty | BIC penalty |\n|---|-------------|-------------|\n| 8 | 2k | 2.08k |\n| 20 | 2k | 3.00k |\n| 100 | 2k | 4.61k |\n| 1000 | 2k | 6.91k |\n\nBIC penalty grows with n; AIC stays constant.", "line_start": 48}, "docs/en/model-selection/information-criteria.md#section_6": {"doc_id": "docs/en/model-selection/information-criteria.md", "heading": "Algorithm/Model Sketch", "content": "**Model Selection Procedure:**\n\n```\n1. Define candidate models: M₁, M₂, ..., Mₘ\n2. For each model Mᵢ:\n   - Fit model by MLE\n   - Compute AIC and BIC\n\n3. Rank by criterion:\n   - For prediction: prefer AIC (or AICc for small n)\n   - For inference: prefer BIC\n\n4. Compare top candidates:\n   - ΔAIC < 2: essentially equivalent\n   - ΔAIC 2-7: some support for better model\n   - ΔAIC > 10: strong support for better model\n\n5. Validate:\n   - Check residual diagnostics for selected model\n   - Consider out-of-sample testing\n```\n\n**Akaike Weights:**\n$$w_i = \\frac{\\exp(-\\frac{1}{2}\\Delta\\text{AIC}_i)}{\\sum_j\\exp(-\\frac{1}{2}\\Delta\\text{AIC}_j)}$$\n\nGives probability-like weights for model averaging.", "line_start": 59}, "docs/en/model-selection/information-criteria.md#section_7": {"doc_id": "docs/en/model-selection/information-criteria.md", "heading": "Common Pitfalls", "content": "1. **Treating criteria as absolute**: Only relative values matter. AIC = 1000 vs AIC = 1002 is a meaningful comparison.\n\n2. **Ignoring sample size for AIC/BIC choice**: For n < 40, AICc is essential. For large n, BIC may be too parsimonious.\n\n3. **Using wrong likelihood**: Comparing models with different transformations (log vs. level) requires adjusting likelihood.\n\n4. **Overfitting with AIC in large samples**: As n grows, AIC allows increasingly complex models. Consider BIC for parsimony.\n\n5. **Ignoring ties**: If ΔAIC < 2, models are equivalent. Don't over-interpret small differences.\n\n6. **Forgetting model checking**: Lowest IC doesn't guarantee good model. Always check residuals.", "line_start": 88}, "docs/en/model-selection/information-criteria.md#section_8": {"doc_id": "docs/en/model-selection/information-criteria.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.arima.model import ARIMA", "line_start": 102}, "docs/en/model-selection/information-criteria.md#section_9": {"doc_id": "docs/en/model-selection/information-criteria.md", "heading": "Generate ARMA(1,1) data", "content": "np.random.seed(42)\nn = 200\nphi, theta = 0.7, 0.3\neps = np.random.randn(n + 1)\ny = np.zeros(n)\nfor t in range(1, n):\n    y[t] = phi * y[t-1] + eps[t] + theta * eps[t-1]", "line_start": 108}, "docs/en/model-selection/information-criteria.md#section_10": {"doc_id": "docs/en/model-selection/information-criteria.md", "heading": "Fit candidate models", "content": "candidates = [\n    ('AR(1)', (1, 0, 0)),\n    ('AR(2)', (2, 0, 0)),\n    ('MA(1)', (0, 0, 1)),\n    ('MA(2)', (0, 0, 2)),\n    ('ARMA(1,1)', (1, 0, 1)),\n    ('ARMA(2,1)', (2, 0, 1)),\n]\n\nresults = []\nfor name, order in candidates:\n    model = ARIMA(y, order=order).fit()\n    results.append({\n        'Model': name,\n        'AIC': model.aic,\n        'BIC': model.bic,\n        'k': sum(order) + 1  # +1 for variance\n    })", "line_start": 117}, "docs/en/model-selection/information-criteria.md#section_11": {"doc_id": "docs/en/model-selection/information-criteria.md", "heading": "Display sorted by AIC", "content": "import pandas as pd\ndf = pd.DataFrame(results).sort_values('AIC')\ndf['ΔAIC'] = df['AIC'] - df['AIC'].min()\ndf['ΔBIC'] = df['BIC'] - df['BIC'].min()\nprint(df.to_string(index=False))", "line_start": 137}, "docs/en/model-selection/information-criteria.md#section_12": {"doc_id": "docs/en/model-selection/information-criteria.md", "heading": "True model ARMA(1,1) should rank well", "content": "print(f\"\\nBest by AIC: {df.iloc[0]['Model']}\")\nprint(f\"Best by BIC: {df.sort_values('BIC').iloc[0]['Model']}\")\n```", "line_start": 144}, "docs/en/model-selection/information-criteria.md#section_13": {"doc_id": "docs/en/model-selection/information-criteria.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why does BIC tend to select simpler models than AIC?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> BIC has a stronger penalty for parameters: $k\\ln(n)$ vs $2k$.\n\nFor $n > 8$: $\\ln(n) > 2$, so BIC penalizes each parameter more.\n\n**Mathematical comparison:**\n- AIC adds $2k$ regardless of sample size\n- BIC adds $k\\ln(n)$, which grows with n\n\nFor n = 100: BIC adds 4.6k vs AIC's 2k per parameter.\n\n**Consequence:** BIC requires stronger likelihood improvement to justify additional parameters, leading to simpler models.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Thinking simpler is always better. BIC can underfit when true model is complex. For forecasting, AIC often wins because it allows capturing more signal.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> When should you use AICc instead of AIC?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Use AICc when sample size is small relative to number of parameters.\n\n**Rule of thumb:** Use AICc when $n/k < 40$.\n\n**Why AICc?**\nAIC is derived asymptotically. For small samples, it underpenalizes complexity, leading to overfitting.\n\nAICc correction: $\\frac{2k(k+1)}{n-k-1}$\n\nThis additional term is large when n ≈ k but vanishes as n → ∞.\n\n**Example:**\n- n = 50, k = 5\n- AIC penalty: 10\n- AICc penalty: 10 + 2(5)(6)/(50-6) ≈ 10 + 1.4 = 11.4\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using AIC by default without checking n/k ratio. For small samples, AIC systematically selects overly complex models.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Derive the formula for AICc from AIC.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> AICc adds a bias correction term:\n\n$$\\text{AICc} = \\text{AIC} + \\frac{2k(k+1)}{n-k-1}$$\n\n**Derivation sketch:**\nFor regression with Gaussian errors, Hurvich and Tsai (1989) showed:\n\n$$E[\\text{AIC}] = E[-2\\ln L] + 2k$$\n\nhas bias when n is small. The exact expected value:\n\n$$E[-2\\ln L(\\hat{\\theta})] + \\frac{2kn}{n-k-1}$$\n\nleads to:\n$$\\text{AICc} = -2\\ln L + \\frac{2kn}{n-k-1} = \\text{AIC} + \\frac{2k^2 + 2k}{n-k-1}$$\n\nAs $n \\to \\infty$: $\\frac{2k(k+1)}{n-k-1} \\to 0$, so AICc → AIC.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> AICc formula assumes residual variance is estimated. For restricted cases, different corrections apply.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Show that BIC is consistent (selects true model) while AIC is not.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**BIC Consistency:**\nFor nested models, consider true model $M_0$ (k₀ params) vs larger $M_1$ (k₁ > k₀).\n\n$$\\text{BIC}_1 - \\text{BIC}_0 = -2(\\ln L_1 - \\ln L_0) + (k_1 - k_0)\\ln n$$\n\nBy likelihood ratio theory: $-2(\\ln L_1 - \\ln L_0) = O_p(1)$ (bounded)\nBut penalty: $(k_1 - k_0)\\ln n \\to \\infty$\n\nSo $P(\\text{BIC}_1 > \\text{BIC}_0) \\to 1$.\n\n**AIC Inconsistency:**\n$$\\text{AIC}_1 - \\text{AIC}_0 = -2(\\ln L_1 - \\ln L_0) + 2(k_1 - k_0)$$\n\nExtra parameters add fixed penalty 2(k₁-k₀), while likelihood improvement is $O_p(1)$. There's always positive probability that extra parameters improve fit enough to offset the fixed penalty.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Consistency ≠ better forecasts. AIC minimizes prediction error; BIC identifies true model. Different goals.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> AIC selects ARIMA(2,1,2) while BIC selects ARIMA(1,1,1). AIC difference is 4. Which do you choose?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> It depends on your goal and context:\n\n**For forecasting:** Lean toward ARIMA(2,1,2) (AIC's choice)\n- ΔAIC = 4 suggests meaningful predictive improvement\n- Extra complexity may capture real dynamics\n\n**For interpretation:** Lean toward ARIMA(1,1,1) (BIC's choice)\n- Simpler, more interpretable\n- Less risk of overfitting\n\n**Recommended approach:**\n1. Compare out-of-sample forecast accuracy\n2. Check residual diagnostics for both\n3. If similar performance, prefer simpler\n4. Consider ensemble/averaging\n\n**Decision matrix:**\n\n| Factor | Favors (2,1,2) | Favors (1,1,1) |\n|--------|----------------|----------------|\n| Large sample | ✓ | |\n| Short forecast horizon | ✓ | |\n| Complex dynamics expected | ✓ | |\n| Interpretability needed | | ✓ |\n| Small sample | | ✓ |\n| Long forecast horizon | | ✓ |\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Picking one criterion dogmatically. Use domain knowledge, validation, and judgment alongside IC.\n</div>\n</div>\n</details>", "line_start": 149}, "docs/en/model-selection/information-criteria.md#section_14": {"doc_id": "docs/en/model-selection/information-criteria.md", "heading": "References", "content": "1. Akaike, H. (1974). A new look at the statistical model identification. *IEEE Transactions on Automatic Control*, 19(6), 716-723.\n2. Schwarz, G. (1978). Estimating the dimension of a model. *Annals of Statistics*, 6(2), 461-464.\n3. Burnham, K. P., & Anderson, D. R. (2002). *Model Selection and Multimodel Inference*. Springer.\n4. Hurvich, C. M., & Tsai, C. L. (1989). Regression and time series model selection in small samples. *Biometrika*, 76(2), 297-307.", "line_start": 291}, "docs/en/model-selection/residual-diagnostics.md#section_0": {"doc_id": "docs/en/model-selection/residual-diagnostics.md", "heading": "Residual Diagnostics", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Residuals should be white noise if model is adequate. Check via: ACF plot (no significant spikes), Ljung-Box test (p > 0.05), normality (Q-Q plot), and homoskedasticity (constant variance). Patterns in residuals indicate model inadequacy: autocorrelation suggests missing AR/MA terms; changing variance suggests GARCH; trends suggest wrong differencing.\n</div>", "line_start": 1}, "docs/en/model-selection/residual-diagnostics.md#section_1": {"doc_id": "docs/en/model-selection/residual-diagnostics.md", "heading": "Core Definitions", "content": "**Residuals:** $e_t = y_t - \\hat{y}_{t|t-1}$ (one-step-ahead prediction errors)\n\n**Standardized Residuals:** $z_t = e_t / \\hat{\\sigma}$ (should be approximately N(0,1))\n\n**White Noise Properties:**\n- $E[e_t] = 0$\n- $\\text{Var}(e_t) = \\sigma^2$ (constant)\n- $\\text{Cov}(e_t, e_{t-k}) = 0$ for $k \\neq 0$", "line_start": 7}, "docs/en/model-selection/residual-diagnostics.md#section_2": {"doc_id": "docs/en/model-selection/residual-diagnostics.md", "heading": "Ljung-Box Test", "content": "Tests whether autocorrelations are jointly zero.\n\n$$Q(m) = n(n+2)\\sum_{k=1}^{m}\\frac{\\hat{\\rho}_k^2}{n-k}$$\n\nUnder H₀ (white noise): $Q(m) \\sim \\chi^2_{m-p-q}$ (adjusted for estimated parameters)\n\n**Decision:** Reject H₀ if Q > critical value (or p < α)", "line_start": 20}, "docs/en/model-selection/residual-diagnostics.md#section_3": {"doc_id": "docs/en/model-selection/residual-diagnostics.md", "heading": "Jarque-Bera Normality Test", "content": "$$JB = \\frac{n}{6}\\left(S^2 + \\frac{(K-3)^2}{4}\\right)$$\n\nwhere S = skewness, K = kurtosis.\n\nUnder H₀ (normality): $JB \\sim \\chi^2_2$", "line_start": 30}, "docs/en/model-selection/residual-diagnostics.md#section_4": {"doc_id": "docs/en/model-selection/residual-diagnostics.md", "heading": "ARCH-LM Test for Heteroskedasticity", "content": "Test if $e_t^2$ depends on past squared residuals:\n$$e_t^2 = \\alpha_0 + \\alpha_1 e_{t-1}^2 + \\cdots + \\alpha_p e_{t-p}^2 + v_t$$\n\nTest statistic: $nR^2 \\sim \\chi^2_p$ under H₀ (homoskedasticity)", "line_start": 38}, "docs/en/model-selection/residual-diagnostics.md#section_5": {"doc_id": "docs/en/model-selection/residual-diagnostics.md", "heading": "Runs Test for Randomness", "content": "Counts runs (consecutive same-sign residuals). Too few runs suggests autocorrelation; too many suggests over-differencing.", "line_start": 45}, "docs/en/model-selection/residual-diagnostics.md#section_6": {"doc_id": "docs/en/model-selection/residual-diagnostics.md", "heading": "Algorithm/Model Sketch", "content": "**Diagnostic Checklist:**\n\n```\n1. MEAN ZERO\n   □ Mean of residuals ≈ 0\n   □ Plot residuals over time: no trend\n\n2. NO AUTOCORRELATION\n   □ ACF plot: all spikes within ±1.96/√n bands\n   □ PACF plot: no patterns\n   □ Ljung-Box test: p > 0.05 at multiple lags\n\n3. CONSTANT VARIANCE\n   □ Plot residuals vs time: no fanning/clustering\n   □ Plot residuals vs fitted: no pattern\n   □ ARCH test: p > 0.05\n\n4. NORMALITY (less critical)\n   □ Histogram: roughly bell-shaped\n   □ Q-Q plot: points on diagonal\n   □ Jarque-Bera: p > 0.05\n\n5. NO OUTLIERS\n   □ |standardized residuals| < 3 mostly\n   □ Check any points > 3 for data issues\n```\n\n**Interpretation of Violations:**\n\n| Violation | Interpretation | Fix |\n|-----------|---------------|-----|\n| Lag 1 ACF spike | Missing MA(1) | Add MA term |\n| Lag 1 PACF spike | Missing AR(1) | Add AR term |\n| Seasonal spikes | Missing seasonal | Add seasonal terms |\n| Slow ACF decay | Under-differencing | Increase d |\n| Negative ACF at lag 1 | Over-differencing | Decrease d |\n| Changing variance | Heteroskedasticity | GARCH, log-transform |\n| Non-normality | Heavy tails | Robust methods, outlier treatment |", "line_start": 49}, "docs/en/model-selection/residual-diagnostics.md#section_7": {"doc_id": "docs/en/model-selection/residual-diagnostics.md", "heading": "Common Pitfalls", "content": "1. **Over-testing**: With many lags, some will be significant by chance. Focus on early lags and patterns.\n\n2. **Ignoring degrees of freedom**: Ljung-Box df = m - p - q, not m. Wrong df gives wrong p-values.\n\n3. **Choosing m poorly**: Too small m misses long-range dependence; too large has low power. Rule: m ≈ min(10, n/5).\n\n4. **Normality obsession**: Non-normality is often acceptable. Autocorrelation is the critical check.\n\n5. **Missing patterns at seasonal lags**: Always check ACF at lags 12, 24 (monthly), 7, 14 (daily), etc.\n\n6. **Confusing residuals and innovations**: For MA models, residuals ≠ true innovations. Some autocorrelation is expected in finite samples.", "line_start": 90}, "docs/en/model-selection/residual-diagnostics.md#section_8": {"doc_id": "docs/en/model-selection/residual-diagnostics.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nfrom statsmodels.graphics.tsaplots import plot_acf\nimport scipy.stats as stats", "line_start": 104}, "docs/en/model-selection/residual-diagnostics.md#section_9": {"doc_id": "docs/en/model-selection/residual-diagnostics.md", "heading": "Generate data and fit intentionally wrong model", "content": "np.random.seed(42)\nn = 200", "line_start": 113}, "docs/en/model-selection/residual-diagnostics.md#section_10": {"doc_id": "docs/en/model-selection/residual-diagnostics.md", "heading": "True: ARMA(1,1)", "content": "phi, theta = 0.7, 0.4\neps = np.random.randn(n + 1)\ny = np.zeros(n)\nfor t in range(1, n):\n    y[t] = phi * y[t-1] + eps[t] + theta * eps[t-1]", "line_start": 116}, "docs/en/model-selection/residual-diagnostics.md#section_11": {"doc_id": "docs/en/model-selection/residual-diagnostics.md", "heading": "Fit AR(1) only (missing MA term)", "content": "model_wrong = ARIMA(y, order=(1, 0, 0)).fit()\nresid = model_wrong.resid\n\nprint(\"=== Residual Diagnostics ===\\n\")", "line_start": 123}, "docs/en/model-selection/residual-diagnostics.md#section_12": {"doc_id": "docs/en/model-selection/residual-diagnostics.md", "heading": "1. Mean", "content": "print(f\"1. Mean: {np.mean(resid):.4f} (should be ≈ 0)\")", "line_start": 129}, "docs/en/model-selection/residual-diagnostics.md#section_13": {"doc_id": "docs/en/model-selection/residual-diagnostics.md", "heading": "2. Autocorrelation", "content": "print(\"\\n2. Autocorrelation:\")\nlb_test = acorr_ljungbox(resid, lags=[5, 10, 15], return_df=True)\nprint(lb_test)", "line_start": 132}, "docs/en/model-selection/residual-diagnostics.md#section_14": {"doc_id": "docs/en/model-selection/residual-diagnostics.md", "heading": "3. Normality", "content": "jb_stat, jb_p = stats.jarque_bera(resid)\nprint(f\"\\n3. Normality (Jarque-Bera): stat={jb_stat:.2f}, p={jb_p:.4f}\")", "line_start": 137}, "docs/en/model-selection/residual-diagnostics.md#section_15": {"doc_id": "docs/en/model-selection/residual-diagnostics.md", "heading": "4. Check ACF", "content": "acf_vals = np.correlate(resid, resid, mode='full')\nacf_vals = acf_vals[len(acf_vals)//2:] / acf_vals[len(acf_vals)//2]\nprint(f\"\\n4. ACF at lag 1: {acf_vals[1]:.3f} (significant if |.| > {1.96/np.sqrt(n):.3f})\")", "line_start": 141}, "docs/en/model-selection/residual-diagnostics.md#section_16": {"doc_id": "docs/en/model-selection/residual-diagnostics.md", "heading": "Compare with correct model", "content": "model_correct = ARIMA(y, order=(1, 0, 1)).fit()\nresid_correct = model_correct.resid\nlb_correct = acorr_ljungbox(resid_correct, lags=[5, 10, 15], return_df=True)\nprint(\"\\n=== Correct Model (ARMA(1,1)) ===\")\nprint(lb_correct)\n```", "line_start": 146}, "docs/en/model-selection/residual-diagnostics.md#section_17": {"doc_id": "docs/en/model-selection/residual-diagnostics.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why is checking residual autocorrelation more important than checking normality?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Autocorrelation matters more because:**\n1. **Biased forecasts**: Residual autocorrelation means systematic patterns remain unexploited\n2. **Invalid inference**: Standard errors and confidence intervals assume independence\n3. **Model inadequacy**: Autocorrelation directly indicates missing structure\n4. **Fixable**: Can add AR/MA terms to remove autocorrelation\n\n**Normality is less critical because:**\n1. **Robust methods exist**: Point forecasts don't require normality\n2. **CLT helps**: Averages become normal even if residuals aren't\n3. **Only affects intervals**: Normality matters mainly for prediction intervals\n4. **Often ignorable**: Heavy tails don't bias forecasts, just widen intervals\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Spending effort on normality transformations while ignoring autocorrelation. Fix autocorrelation first; normality can often be ignored.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> What does a significant negative spike at lag 1 in the residual ACF suggest?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Likely **over-differencing**.\n\n**Explanation:**\nDifferencing a stationary series introduces MA(1) with θ ≈ -1:\n$$(1-L)y_t = \\epsilon_t - \\epsilon_{t-1} \\text{ approximately}$$\n\nThis has ACF: $\\rho(1) = -1/(1+1) = -0.5$\n\nSo large negative lag-1 ACF (around -0.3 to -0.5) suggests you differenced a series that was already stationary.\n\n**Action:**\n1. Re-test original series for stationarity\n2. Try model without differencing\n3. Compare AIC between d=0 and d=1\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Reflexively differencing because it's \"standard procedure.\" Check stationarity tests and residuals before and after differencing.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> The Ljung-Box test uses degrees of freedom m-p-q. Why subtract p+q?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> We subtract estimated parameters to account for their effect on residuals.\n\n**Explanation:**\nFor a fitted ARMA(p,q), the residuals $e_t = y_t - \\hat{y}_{t|t-1}$ are computed using estimated $\\hat{\\phi}$, $\\hat{\\theta}$.\n\nThe estimation process uses up information from the data, reducing effective degrees of freedom. Specifically:\n- p AR parameters constrain p lagged autocorrelations\n- q MA parameters constrain q lagged autocorrelations\n\nUnder H₀, the test statistic:\n$$Q(m) \\sim \\chi^2_{m-p-q}$$\n\nnot $\\chi^2_m$. Using m degrees of freedom would reject too often (test is oversized).\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Software may not adjust df automatically. Verify that p and q are subtracted; otherwise p-values are wrong.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> How do you interpret the ARCH-LM test for residuals?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> ARCH-LM tests for conditional heteroskedasticity — whether variance depends on past volatility.\n\n**Procedure:**\n1. Compute squared residuals $e_t^2$\n2. Regress: $e_t^2 = \\alpha_0 + \\alpha_1 e_{t-1}^2 + \\cdots + \\alpha_p e_{t-p}^2$\n3. Test: $H_0$: all $\\alpha_i = 0$ (homoskedasticity)\n\n**Test statistic:** $nR^2 \\sim \\chi^2_p$\n\n**Interpretation:**\n- p < 0.05: Evidence of ARCH effects; variance clusters\n- p > 0.05: No evidence; constant variance OK\n\n**If significant:**\n- Consider GARCH model\n- Or variance-stabilizing transform (log)\n- Prediction intervals need adjustment\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Ignoring ARCH effects leads to prediction intervals that are too narrow during volatile periods and too wide during calm periods.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You fit ARIMA(1,1,1) and residual diagnostics show: Ljung-Box p=0.02 at lag 10, but p=0.15 at lags 5 and 15. Q-Q plot shows slight heavy tails. What do you conclude?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Model is likely adequate; don't over-interpret the lag-10 result.\n\n**Analysis:**\n1. **Ljung-Box at lag 10:** p=0.02 is borderline. But lags 5 and 15 are fine.\n   - Could be spurious (multiple testing)\n   - Or minor model inadequacy that doesn't matter for forecasting\n\n2. **Heavy tails:** Common in economic/financial data\n   - Doesn't invalidate forecasts\n   - Affects prediction intervals (may need wider)\n\n**Recommended actions:**\n1. Check ACF visually — isolated spike at lag 10 likely noise\n2. Compare to simpler models (ARIMA(1,1,0)) — if similar forecasts, prefer simpler\n3. For intervals, consider bootstrap or t-distribution\n4. Validate on holdout data — ultimate test\n\n**Conclusion:** Accept model unless holdout validation shows problems. Perfect residuals are unrealistic; \"good enough\" is the standard.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Chasing perfect diagnostics. Adding parameters to fix one borderline test often causes overfitting. Focus on forecast performance.\n</div>\n</div>\n</details>", "line_start": 154}, "docs/en/model-selection/residual-diagnostics.md#section_18": {"doc_id": "docs/en/model-selection/residual-diagnostics.md", "heading": "References", "content": "1. Ljung, G. M., & Box, G. E. P. (1978). On a measure of lack of fit in time series models. *Biometrika*, 65(2), 297-303.\n2. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis*. Wiley. Chapter 8.\n3. Tsay, R. S. (2010). *Analysis of Financial Time Series*. Wiley. Chapter 2.\n4. Engle, R. F. (1982). Autoregressive conditional heteroscedasticity with estimates of the variance of United Kingdom inflation. *Econometrica*, 50(4), 987-1007.", "line_start": 286}, "docs/en/practical/practical-modeling.md#section_0": {"doc_id": "docs/en/practical/practical-modeling.md", "heading": "Practical Time Series Modeling", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Practical modeling involves: proper train/test splits (temporal, never random), backtesting with rolling windows, handling deployment (model updates, monitoring). Key concerns: data leakage, concept drift, uncertainty quantification. Production tips: start simple (naive baselines), document assumptions, monitor forecast accuracy over time, have fallback strategies.\n</div>", "line_start": 1}, "docs/en/practical/practical-modeling.md#section_1": {"doc_id": "docs/en/practical/practical-modeling.md", "heading": "Core Definitions", "content": "**Backtesting:** Historical simulation of how model would have performed.\n\n**Walk-Forward Validation:** Expanding window validation mimicking production.\n\n**Concept Drift:** When relationship between features and target changes over time.\n\n**Model Retraining:** Updating model with new data periodically.\n\n**Forecast Reconciliation:** Ensuring forecasts at different aggregation levels are consistent.", "line_start": 7}, "docs/en/practical/practical-modeling.md#section_2": {"doc_id": "docs/en/practical/practical-modeling.md", "heading": "Rolling Origin Backtest", "content": "For origins $T_1, T_2, \\ldots, T_m$ and horizon h:\n$$\\text{RMSE}(h) = \\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(y_{T_i+h} - \\hat{y}_{T_i+h|T_i})^2}$$", "line_start": 21}, "docs/en/practical/practical-modeling.md#section_3": {"doc_id": "docs/en/practical/practical-modeling.md", "heading": "Forecast Bias", "content": "$$\\text{Bias} = \\frac{1}{n}\\sum_{t=1}^{n}(y_t - \\hat{y}_t)$$\n\n- Positive bias: systematic under-prediction\n- Negative bias: systematic over-prediction", "line_start": 26}, "docs/en/practical/practical-modeling.md#section_4": {"doc_id": "docs/en/practical/practical-modeling.md", "heading": "Tracking Signal (for monitoring)", "content": "$$TS_t = \\frac{\\sum_{i=1}^{t}e_i}{\\text{MAD}}$$\n\nIf |TS| > 4, model may be biased and needs retraining.", "line_start": 33}, "docs/en/practical/practical-modeling.md#section_5": {"doc_id": "docs/en/practical/practical-modeling.md", "heading": "Prediction Interval Coverage", "content": "$$\\text{Coverage} = \\frac{1}{n}\\sum_{t=1}^{n}\\mathbf{1}(y_t \\in PI_t)$$\n\n95% PI should have ~95% coverage; significantly less indicates miscalibration.", "line_start": 39}, "docs/en/practical/practical-modeling.md#section_6": {"doc_id": "docs/en/practical/practical-modeling.md", "heading": "Algorithm/Model Sketch", "content": "**Production Forecasting Pipeline:**\n\n```python\ndef production_pipeline(data, config):\n    \"\"\"\n    Complete forecasting pipeline for production.\n    \"\"\"\n    # 1. Data validation\n    validate_data(data)\n\n    # 2. Feature engineering\n    features = create_features(data)\n\n    # 3. Train-test split (temporal)\n    train, holdout = temporal_split(features, config['holdout_size'])\n\n    # 4. Model selection via cross-validation\n    best_model = None\n    best_score = float('inf')\n\n    for model_class in config['candidate_models']:\n        score = time_series_cv(train, model_class, config['cv_folds'])\n        if score < best_score:\n            best_model = model_class\n            best_score = score\n\n    # 5. Final training on full training set\n    model = best_model.fit(train)\n\n    # 6. Holdout evaluation\n    holdout_metrics = evaluate(model, holdout)\n\n    # 7. Retrain on all data for deployment\n    final_model = best_model.fit(features)\n\n    # 8. Generate forecasts with intervals\n    forecasts = final_model.forecast(config['horizon'])\n    intervals = final_model.prediction_intervals(config['horizon'])\n\n    return {\n        'model': final_model,\n        'forecasts': forecasts,\n        'intervals': intervals,\n        'metrics': holdout_metrics\n    }\n```\n\n**Monitoring Dashboard Metrics:**\n\n| Metric | Good | Warning | Action |\n|--------|------|---------|--------|\n| MAPE | < 10% | 10-20% | > 20%: investigate |\n| Bias | ≈ 0 | |bias| > 1σ | |bias| > 2σ: retrain |\n| PI Coverage | 90-100% | 80-90% | < 80%: recalibrate |\n| Tracking Signal | |TS| < 4 | 4-6 | > 6: retrain |", "line_start": 45}, "docs/en/practical/practical-modeling.md#section_7": {"doc_id": "docs/en/practical/practical-modeling.md", "heading": "Common Pitfalls", "content": "1. **Random train-test split:** Causes data leakage. Always use temporal splits.\n\n2. **Optimizing wrong metric:** Minimize business-relevant loss (e.g., asymmetric cost), not just RMSE.\n\n3. **No baseline comparison:** Claim \"model works\" without comparing to naive/seasonal naive.\n\n4. **Static model:** Not retraining as new data arrives. Monitor performance and retrain regularly.\n\n5. **Ignoring prediction intervals:** Point forecasts without uncertainty mislead decision-makers.\n\n6. **Overfitting to holdout:** If you tune on holdout multiple times, it becomes training data. Use nested CV.", "line_start": 103}, "docs/en/practical/practical-modeling.md#section_8": {"doc_id": "docs/en/practical/practical-modeling.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\ndef mape(y_true, y_pred):\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ndef tracking_signal(errors):\n    cumsum = np.cumsum(errors)\n    mad = np.mean(np.abs(errors))\n    return cumsum[-1] / mad if mad > 0 else 0", "line_start": 117}, "docs/en/practical/practical-modeling.md#section_9": {"doc_id": "docs/en/practical/practical-modeling.md", "heading": "Simulate production monitoring", "content": "np.random.seed(42)\nn_periods = 12  # 12 months of monitoring\n\nactuals = 100 + np.random.randn(n_periods) * 10\nforecasts = actuals + np.random.randn(n_periods) * 5 + 2  # slight bias\n\nerrors = actuals - forecasts", "line_start": 131}, "docs/en/practical/practical-modeling.md#section_10": {"doc_id": "docs/en/practical/practical-modeling.md", "heading": "Calculate monitoring metrics", "content": "print(\"=== Forecast Monitoring Report ===\\n\")\nprint(f\"MAE: {mean_absolute_error(actuals, forecasts):.2f}\")\nprint(f\"RMSE: {np.sqrt(mean_squared_error(actuals, forecasts)):.2f}\")\nprint(f\"MAPE: {mape(actuals, forecasts):.2f}%\")\nprint(f\"Bias: {np.mean(errors):.2f}\")\nprint(f\"Tracking Signal: {tracking_signal(errors):.2f}\")", "line_start": 140}, "docs/en/practical/practical-modeling.md#section_11": {"doc_id": "docs/en/practical/practical-modeling.md", "heading": "PI coverage check (simulated 95% intervals)", "content": "pi_width = 1.96 * np.std(errors)\nlower = forecasts - pi_width\nupper = forecasts + pi_width\ncoverage = np.mean((actuals >= lower) & (actuals <= upper))\nprint(f\"PI Coverage: {coverage*100:.1f}%\")", "line_start": 148}, "docs/en/practical/practical-modeling.md#section_12": {"doc_id": "docs/en/practical/practical-modeling.md", "heading": "Alert check", "content": "print(\"\\n=== Alerts ===\")\nif abs(np.mean(errors)) > 2 * np.std(errors):\n    print(\"WARNING: Significant forecast bias detected!\")\nif abs(tracking_signal(errors)) > 4:\n    print(\"WARNING: Tracking signal exceeds threshold - consider retraining\")\nif coverage < 0.80:\n    print(\"WARNING: Prediction interval coverage too low\")\n```", "line_start": 155}, "docs/en/practical/practical-modeling.md#section_13": {"doc_id": "docs/en/practical/practical-modeling.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why is random train-test split wrong for time series?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Random splits cause data leakage:\n- Future observations appear in training set\n- Past observations appear in test set\n- Model \"sees\" future information during training\n\n**Consequences:**\n- Overoptimistic evaluation metrics\n- Model fails in production where future isn't available\n- Temporal patterns learned incorrectly\n\n**Correct approach:**\n```\nTrain: [1, ..., T]    Test: [T+1, ..., T+h]\n```\nAlways train on past, test on future.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using sklearn's train_test_split() or KFold directly on time series. Use TimeSeriesSplit or manual temporal split.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> What is concept drift and how do you detect it?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Concept drift = the relationship between features and target changes over time.\n\n**Types:**\n- **Sudden:** Abrupt change (e.g., COVID impact)\n- **Gradual:** Slow shift (e.g., customer behavior evolution)\n- **Seasonal:** Recurring pattern changes\n- **Recurring:** Oscillates between states\n\n**Detection methods:**\n1. **Performance monitoring:** Increasing error over time\n2. **Statistical tests:** Compare recent vs historical distributions\n3. **Control charts:** Track forecast errors, flag out-of-control\n4. **Tracking signal:** Cumulative bias indicates drift\n\n**Response:**\n- Retrain on recent data\n- Use adaptive models (exponential smoothing)\n- Reduce lookback window\n- Add regime indicators\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Assuming model stays accurate forever. Schedule regular monitoring and retraining.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> How do you calculate and interpret the tracking signal?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n$$TS_t = \\frac{\\text{RSFE}_t}{\\text{MAD}_t} = \\frac{\\sum_{i=1}^{t}e_i}{\\frac{1}{t}\\sum_{i=1}^{t}|e_i|}$$\n\n**Interpretation:**\n- TS ≈ 0: No systematic bias\n- TS > 0: Systematic under-forecasting\n- TS < 0: Systematic over-forecasting\n- |TS| > 4: Likely significant bias (action needed)\n\n**Why use it:**\n- Normalizes by MAD for comparability\n- Accumulates evidence over time\n- Distinguishes random errors from systematic bias\n\n**Update frequency:**\nCheck monthly or quarterly; daily TS is noisy.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Reacting to every TS fluctuation. Wait for sustained signal (multiple periods with |TS| > 4) before retraining.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> What does it mean if your 95% prediction intervals have 75% coverage?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The model underestimates uncertainty—intervals are too narrow.\n\n**Possible causes:**\n1. **Model misspecification:** True residuals are larger than estimated\n2. **Heavy tails:** Data has outliers not captured by normal assumption\n3. **Non-constant variance:** Heteroskedasticity not modeled\n4. **Missing patterns:** Unmodeled seasonality or trend adds variance\n\n**Solutions:**\n1. Use bootstrap prediction intervals (more robust)\n2. Apply variance adjustment: multiply width by coverage correction factor\n3. Model heteroskedasticity (GARCH) or use quantile regression\n4. Improve base model to capture more patterns\n\n**Adjustment formula:**\nIf coverage = 75% but target = 95%, scale factor ≈ $z_{0.975}/z_{0.875}$ = 1.96/1.15 ≈ 1.7\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Reporting narrow intervals to seem accurate. Stakeholders need truthful uncertainty; too-narrow intervals cause poor decisions.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> Your demand forecasting model works well in testing but production accuracy is much worse. What might be causing this?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Common causes of train-test vs production gap:\n\n1. **Data leakage in testing:**\n   - Features used information not available at forecast time\n   - Train-test split not strictly temporal\n\n2. **Feature availability:**\n   - Features available historically but delayed in production\n   - External data sources not updated in real-time\n\n3. **Distribution shift:**\n   - Test period was unusual/lucky\n   - Production faces different conditions (seasonality, promotions)\n\n4. **Data quality:**\n   - Production data has errors/delays not in historical\n   - Missing values handled differently\n\n5. **Target leakage:**\n   - Test evaluated at easy horizons; production needs longer\n\n**Diagnosis:**\n1. Verify no leakage in feature engineering\n2. Compare feature distributions: test vs production\n3. Backtest over multiple periods (not just one)\n4. Monitor input data quality in production\n5. Check if production horizon matches testing\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Testing on convenient period and assuming it generalizes. Always backtest across multiple train-test splits spanning different conditions.\n</div>\n</div>\n</details>", "line_start": 165}, "docs/en/practical/practical-modeling.md#section_14": {"doc_id": "docs/en/practical/practical-modeling.md", "heading": "References", "content": "1. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapters 5, 12.\n2. Makridakis, S., Spiliotis, E., & Assimakopoulos, V. (2020). The M4 Competition: 100,000 time series and 61 forecasting methods. *IJF*, 36(1), 54-74.\n3. Gama, J., Žliobaitė, I., Bifet, A., Pechenizkiy, M., & Bouchachia, A. (2014). A survey on concept drift adaptation. *ACM Computing Surveys*, 46(4), 1-37.\n4. Kolassa, S. (2016). Evaluating predictive count data distributions in retail sales forecasting. *IJF*, 32(3), 788-803.", "line_start": 315}, "docs/en/features/feature-engineering.md#section_0": {"doc_id": "docs/en/features/feature-engineering.md", "heading": "Time Series Feature Engineering", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Feature engineering transforms raw time series into ML-ready inputs. Key features: lags, rolling statistics, date/time features, Fourier terms for seasonality. Scaling: standardize or min-max, but fit only on training data. Handle missing data via interpolation or indicators. Avoid leakage: never use future information in features.\n</div>", "line_start": 1}, "docs/en/features/feature-engineering.md#section_1": {"doc_id": "docs/en/features/feature-engineering.md", "heading": "Core Definitions", "content": "**Lag Features:** Past values as predictors\n$$X_{lag_k} = y_{t-k}$$\n\n**Rolling Features:** Statistics over windows\n$$X_{roll\\_mean\\_k} = \\frac{1}{k}\\sum_{i=1}^{k}y_{t-i}$$\n\n**Date Features:** Extracted from timestamp\n- Hour, day of week, month, quarter\n- Is_weekend, is_holiday\n- Days since event\n\n**Fourier Features:** Seasonal patterns as sine/cosine\n$$X_{sin_k} = \\sin\\left(\\frac{2\\pi k t}{m}\\right), \\quad X_{cos_k} = \\cos\\left(\\frac{2\\pi k t}{m}\\right)$$", "line_start": 7}, "docs/en/features/feature-engineering.md#section_2": {"doc_id": "docs/en/features/feature-engineering.md", "heading": "Fourier Terms for Seasonality", "content": "For seasonal period m, capture pattern with K harmonics:\n$$s(t) = \\sum_{k=1}^{K}\\left[\\alpha_k\\sin\\left(\\frac{2\\pi kt}{m}\\right) + \\beta_k\\cos\\left(\\frac{2\\pi kt}{m}\\right)\\right]$$\n\n**K selection:**\n- K = m/2: captures all seasonal frequencies\n- K = 2-4: often sufficient for smooth patterns\n- Use AIC to select optimal K\n\n**Why Fourier:**\n- Handles non-integer and long seasonal periods\n- Works with any ML model\n- Parsimonious: 2K features vs m dummy variables", "line_start": 25}, "docs/en/features/feature-engineering.md#section_3": {"doc_id": "docs/en/features/feature-engineering.md", "heading": "Rolling Statistics", "content": "**Rolling mean (simple moving average):**\n$$\\bar{y}_t^{(w)} = \\frac{1}{w}\\sum_{i=0}^{w-1}y_{t-i}$$\n\n**Rolling standard deviation:**\n$$s_t^{(w)} = \\sqrt{\\frac{1}{w-1}\\sum_{i=0}^{w-1}(y_{t-i} - \\bar{y}_t^{(w)})^2}$$\n\n**Exponential moving average:**\n$$\\text{EMA}_t = \\alpha y_t + (1-\\alpha)\\text{EMA}_{t-1}$$", "line_start": 40}, "docs/en/features/feature-engineering.md#section_4": {"doc_id": "docs/en/features/feature-engineering.md", "heading": "Scaling Methods", "content": "**Standardization (z-score):**\n$$y_{scaled} = \\frac{y - \\mu_{train}}{\\sigma_{train}}$$\n\n**Min-max scaling:**\n$$y_{scaled} = \\frac{y - \\min_{train}}{\\max_{train} - \\min_{train}}$$\n\n**Robust scaling:**\n$$y_{scaled} = \\frac{y - \\text{median}_{train}}{\\text{IQR}_{train}}$$\n\n**Critical:** Always fit scaler on training data only!", "line_start": 51}, "docs/en/features/feature-engineering.md#section_5": {"doc_id": "docs/en/features/feature-engineering.md", "heading": "Algorithm/Model Sketch", "content": "**Feature Engineering Pipeline:**\n\n```python\ndef create_features(df, target_col='y', lags=[1,2,3,7],\n                   rolling_windows=[7,14,30]):\n    features = df.copy()\n\n    # Lag features\n    for lag in lags:\n        features[f'lag_{lag}'] = features[target_col].shift(lag)\n\n    # Rolling features\n    for w in rolling_windows:\n        features[f'roll_mean_{w}'] = features[target_col].shift(1).rolling(w).mean()\n        features[f'roll_std_{w}'] = features[target_col].shift(1).rolling(w).std()\n        features[f'roll_min_{w}'] = features[target_col].shift(1).rolling(w).min()\n        features[f'roll_max_{w}'] = features[target_col].shift(1).rolling(w).max()\n\n    # Date features (if datetime index)\n    features['hour'] = features.index.hour\n    features['dayofweek'] = features.index.dayofweek\n    features['month'] = features.index.month\n    features['is_weekend'] = features.index.dayofweek >= 5\n\n    # Fourier features for annual seasonality\n    day_of_year = features.index.dayofyear\n    for k in range(1, 4):\n        features[f'sin_{k}'] = np.sin(2 * np.pi * k * day_of_year / 365.25)\n        features[f'cos_{k}'] = np.cos(2 * np.pi * k * day_of_year / 365.25)\n\n    return features.dropna()\n```\n\n**Train-Test Split for Time Series:**\n```python", "line_start": 64}, "docs/en/features/feature-engineering.md#section_6": {"doc_id": "docs/en/features/feature-engineering.md", "heading": "WRONG: random split", "content": "X_train, X_test = train_test_split(X)  # Data leakage!", "line_start": 101}, "docs/en/features/feature-engineering.md#section_7": {"doc_id": "docs/en/features/feature-engineering.md", "heading": "RIGHT: temporal split", "content": "train_end = int(len(X) * 0.8)\nX_train, X_test = X[:train_end], X[train_end:]\n```", "line_start": 104}, "docs/en/features/feature-engineering.md#section_8": {"doc_id": "docs/en/features/feature-engineering.md", "heading": "Common Pitfalls", "content": "1. **Using future information:** Lag features must use shift(k) where k ≥ 1. shift(0) = leakage.\n\n2. **Scaling on full data:** Fit scaler on training data only. Otherwise test data statistics leak into training.\n\n3. **Rolling windows including current value:** Rolling mean should be `.shift(1).rolling(w)`, not `.rolling(w)`.\n\n4. **Missing values from lags:** First k observations have NaN after creating lag_k. Drop or impute.\n\n5. **Too many features:** With many lags and rolling windows, dimensionality explodes. Use feature selection.\n\n6. **Non-stationarity in features:** If target is non-stationary, lag features inherit it. Consider differencing.", "line_start": 109}, "docs/en/features/feature-engineering.md#section_9": {"doc_id": "docs/en/features/feature-engineering.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error", "line_start": 123}, "docs/en/features/feature-engineering.md#section_10": {"doc_id": "docs/en/features/feature-engineering.md", "heading": "Generate sample data", "content": "np.random.seed(42)\nn = 365\ndates = pd.date_range('2023-01-01', periods=n, freq='D')\ntrend = np.arange(n) * 0.1\nseasonal = 10 * np.sin(2 * np.pi * np.arange(n) / 365)\nnoise = np.random.randn(n) * 2\ny = trend + seasonal + noise\n\ndf = pd.DataFrame({'y': y}, index=dates)", "line_start": 131}, "docs/en/features/feature-engineering.md#section_11": {"doc_id": "docs/en/features/feature-engineering.md", "heading": "Create features", "content": "def make_features(df):\n    features = df.copy()\n    # Lags\n    for lag in [1, 7, 14, 28]:\n        features[f'lag_{lag}'] = features['y'].shift(lag)\n    # Rolling\n    features['roll_mean_7'] = features['y'].shift(1).rolling(7).mean()\n    features['roll_std_7'] = features['y'].shift(1).rolling(7).std()\n    # Calendar\n    features['dayofweek'] = features.index.dayofweek\n    features['month'] = features.index.month\n    # Fourier (annual)\n    doy = features.index.dayofyear\n    features['sin_annual'] = np.sin(2 * np.pi * doy / 365.25)\n    features['cos_annual'] = np.cos(2 * np.pi * doy / 365.25)\n    return features.dropna()\n\nfeatures = make_features(df)", "line_start": 142}, "docs/en/features/feature-engineering.md#section_12": {"doc_id": "docs/en/features/feature-engineering.md", "heading": "Train-test split (temporal)", "content": "train_size = 300\ntrain = features[:train_size]\ntest = features[train_size:]\n\nX_train = train.drop('y', axis=1)\ny_train = train['y']\nX_test = test.drop('y', axis=1)\ny_test = test['y']", "line_start": 162}, "docs/en/features/feature-engineering.md#section_13": {"doc_id": "docs/en/features/feature-engineering.md", "heading": "Fit model", "content": "model = RandomForestRegressor(n_estimators=100, random_state=42)\nmodel.fit(X_train, y_train)", "line_start": 172}, "docs/en/features/feature-engineering.md#section_14": {"doc_id": "docs/en/features/feature-engineering.md", "heading": "Evaluate", "content": "y_pred = model.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(f\"Test RMSE: {rmse:.2f}\")", "line_start": 176}, "docs/en/features/feature-engineering.md#section_15": {"doc_id": "docs/en/features/feature-engineering.md", "heading": "Feature importance", "content": "importance = pd.Series(model.feature_importances_, index=X_train.columns)\nprint(\"\\nTop features:\")\nprint(importance.sort_values(ascending=False).head())\n```", "line_start": 181}, "docs/en/features/feature-engineering.md#section_16": {"doc_id": "docs/en/features/feature-engineering.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why must you fit the scaler only on training data?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Fitting on full data causes data leakage—test data statistics influence training.\n\n**Problem:**\n```python\nscaler.fit(X)  # Uses test data statistics\nX_train_scaled = scaler.transform(X_train)  # Training influenced by test\n```\n\nThe model \"knows\" about test data range/distribution during training, giving optimistic evaluation.\n\n**Correct approach:**\n```python\nscaler.fit(X_train)  # Only training statistics\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)  # Apply same transformation\n```\n\n**Real-world analogy:** In production, you don't have future data to compute statistics.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using sklearn pipelines incorrectly. Always split BEFORE creating pipeline, or use TimeSeriesSplit in cross-validation.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> When would you use Fourier features instead of dummy variables for seasonality?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Use Fourier when:**\n1. **Long periods:** Weekly seasonality = 7 dummies; annual = 365 dummies vs 2-6 Fourier\n2. **Non-integer periods:** 365.25 days/year can't be captured with dummies\n3. **Smooth patterns:** Seasonality follows sinusoidal shape\n4. **Linear models:** Fourier terms capture cycles naturally\n\n**Use dummies when:**\n1. **Short periods:** Day of week (7 levels) is manageable\n2. **Sharp patterns:** \"Monday effect\" is discrete, not smooth\n3. **Interpretability:** Coefficients directly show day effects\n4. **Non-sinusoidal:** Pattern doesn't fit sine/cosine shape\n\n**Hybrid:** Can use both—Fourier for smooth annual, dummies for weekly.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using 52 dummies for weekly seasonality in daily data. Fourier with K=2-4 is more efficient and generalizes better.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Explain why shift(1).rolling(w).mean() is correct but rolling(w).mean() causes leakage.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Without shift:**\n```python\nrolling_mean[t] = mean(y[t-w+1], ..., y[t])  # Includes y[t]!\n```\nWhen predicting y[t], using rolling_mean[t] includes y[t] itself → leakage.\n\n**With shift:**\n```python\nrolling_mean[t] = mean(y[t-w], ..., y[t-1])  # Excludes y[t]\n```\nOnly uses past values → no leakage.\n\n**Mathematical notation:**\n- Wrong: $\\bar{y}_t = \\frac{1}{w}\\sum_{i=0}^{w-1}y_{t-i}$ includes $y_t$\n- Correct: $\\bar{y}_{t-1} = \\frac{1}{w}\\sum_{i=1}^{w}y_{t-i}$ excludes $y_t$\n\nThe shift moves the window back by one time step.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Pandas rolling default includes current value. Always add .shift(1) before .rolling() for features.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> How many Fourier terms (K) do you need to fully represent seasonality of period m?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> K = m/2 terms fully represent any periodic pattern of period m.\n\n**Explanation:**\nBy Fourier's theorem, any periodic function can be represented as:\n$$f(t) = \\sum_{k=1}^{\\infty}\\left[a_k\\sin\\left(\\frac{2\\pi kt}{m}\\right) + b_k\\cos\\left(\\frac{2\\pi kt}{m}\\right)\\right]$$\n\nFor discrete data with period m, frequencies above k = m/2 alias to lower frequencies (Nyquist).\n\n**Practical:**\n- K = m/2: Full representation (2K = m parameters, same as dummies)\n- K = 2-4: Often sufficient; smooth patterns don't need high harmonics\n- Use AIC/BIC: Add terms until no improvement\n\n**Example:** Annual seasonality in daily data\n- Full: K = 365/2 ≈ 182 (overkill)\n- Typical: K = 3-5 (6-10 parameters)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using K = m/2 when K = 3 suffices. Extra terms add noise and reduce interpretability.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> Your lag features have 30% missing values at the start due to the lag window. How do you handle this?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Several options:\n\n1. **Drop rows (simplest):**\n   ```python\n   features = features.dropna()\n   ```\n   - Lose initial observations\n   - OK if plenty of data\n\n2. **Fill with first available:**\n   ```python\n   features = features.fillna(method='bfill')\n   ```\n   - Uses earliest available value\n   - Slight bias but preserves data\n\n3. **Use target mean/median:**\n   ```python\n   features['lag_7'] = features['lag_7'].fillna(features['y'].mean())\n   ```\n   - Neutral imputation\n   - Works for tree models\n\n4. **Missing indicator:**\n   ```python\n   features['lag_7_missing'] = features['lag_7'].isna().astype(int)\n   features['lag_7'] = features['lag_7'].fillna(0)\n   ```\n   - Model learns to handle missing\n   - Most flexible\n\n5. **Shorter warmup lags:**\n   - Use lag_1 at start, add longer lags as available\n   - Complex but maximizes data\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Dropping 30% of data when you have limited observations. Try imputation first; validate on holdout to check impact.\n</div>\n</div>\n</details>", "line_start": 187}, "docs/en/features/feature-engineering.md#section_17": {"doc_id": "docs/en/features/feature-engineering.md", "heading": "References", "content": "1. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 7.\n2. Christ, M., Braun, N., Neuffer, J., & Kempa-Liehr, A. W. (2018). Time series feature extraction on basis of scalable hypothesis tests. *Neurocomputing*, 307, 72-77.\n3. Fulcher, B. D., & Jones, N. S. (2017). hctsa: A computational framework for automated time-series phenotyping. *Journal of Open Research Software*, 5(1).\n4. Brownlee, J. (2018). *Deep Learning for Time Series Forecasting*. Machine Learning Mastery.", "line_start": 346}, "docs/en/state-space/kalman-filter.md#section_0": {"doc_id": "docs/en/state-space/kalman-filter.md", "heading": "State Space Models and Kalman Filter", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> State space models represent time series via hidden states evolving over time. Kalman filter recursively estimates states: prediction step (propagate state forward), update step (incorporate new observation). Optimal for linear Gaussian systems. Core equations: $x_t = Fx_{t-1} + w_t$, $y_t = Hx_t + v_t$. Unifies ARIMA, exponential smoothing, and structural models.\n</div>", "line_start": 1}, "docs/en/state-space/kalman-filter.md#section_1": {"doc_id": "docs/en/state-space/kalman-filter.md", "heading": "Core Definitions", "content": "**State Space Representation:**\n\nState equation: $\\mathbf{x}_t = \\mathbf{F}\\mathbf{x}_{t-1} + \\mathbf{w}_t$, $\\mathbf{w}_t \\sim N(0, \\mathbf{Q})$\n\nObservation equation: $y_t = \\mathbf{H}\\mathbf{x}_t + v_t$, $v_t \\sim N(0, R)$\n\n**Components:**\n- $\\mathbf{x}_t$: State vector (unobserved)\n- $y_t$: Observation (data)\n- $\\mathbf{F}$: State transition matrix\n- $\\mathbf{H}$: Observation matrix\n- $\\mathbf{Q}$: State noise covariance\n- $R$: Observation noise variance\n\n**Local Level Model (simplest):**\n$$\\mu_t = \\mu_{t-1} + \\eta_t, \\quad \\eta_t \\sim N(0, \\sigma^2_\\eta)$$\n$$y_t = \\mu_t + \\epsilon_t, \\quad \\epsilon_t \\sim N(0, \\sigma^2_\\epsilon)$$", "line_start": 7}, "docs/en/state-space/kalman-filter.md#section_2": {"doc_id": "docs/en/state-space/kalman-filter.md", "heading": "Kalman Filter Recursions", "content": "**Prediction Step:**\n$$\\hat{\\mathbf{x}}_{t|t-1} = \\mathbf{F}\\hat{\\mathbf{x}}_{t-1|t-1}$$\n$$\\mathbf{P}_{t|t-1} = \\mathbf{F}\\mathbf{P}_{t-1|t-1}\\mathbf{F}' + \\mathbf{Q}$$\n\n**Update Step:**\n$$\\mathbf{K}_t = \\mathbf{P}_{t|t-1}\\mathbf{H}'(\\mathbf{H}\\mathbf{P}_{t|t-1}\\mathbf{H}' + R)^{-1}$$\n$$\\hat{\\mathbf{x}}_{t|t} = \\hat{\\mathbf{x}}_{t|t-1} + \\mathbf{K}_t(y_t - \\mathbf{H}\\hat{\\mathbf{x}}_{t|t-1})$$\n$$\\mathbf{P}_{t|t} = (\\mathbf{I} - \\mathbf{K}_t\\mathbf{H})\\mathbf{P}_{t|t-1}$$\n\n**Key quantities:**\n- $\\hat{\\mathbf{x}}_{t|t}$: Filtered state estimate (using data up to t)\n- $\\mathbf{P}_{t|t}$: State covariance (uncertainty)\n- $\\mathbf{K}_t$: Kalman gain (weight on new observation)\n- $v_t = y_t - \\mathbf{H}\\hat{\\mathbf{x}}_{t|t-1}$: Innovation (prediction error)", "line_start": 29}, "docs/en/state-space/kalman-filter.md#section_3": {"doc_id": "docs/en/state-space/kalman-filter.md", "heading": "Local Level Model Kalman Filter", "content": "State: $\\mathbf{x}_t = \\mu_t$ (scalar)\nMatrices: $F = 1$, $H = 1$, $Q = \\sigma^2_\\eta$, $R = \\sigma^2_\\epsilon$\n\nRecursions:\n$$\\hat{\\mu}_{t|t-1} = \\hat{\\mu}_{t-1|t-1}$$\n$$P_{t|t-1} = P_{t-1|t-1} + \\sigma^2_\\eta$$\n$$K_t = \\frac{P_{t|t-1}}{P_{t|t-1} + \\sigma^2_\\epsilon}$$\n$$\\hat{\\mu}_{t|t} = \\hat{\\mu}_{t|t-1} + K_t(y_t - \\hat{\\mu}_{t|t-1})$$\n$$P_{t|t} = (1 - K_t)P_{t|t-1}$$\n\nAs $t \\to \\infty$: $K_t \\to K^* = $ steady-state gain (related to SES α).", "line_start": 46}, "docs/en/state-space/kalman-filter.md#section_4": {"doc_id": "docs/en/state-space/kalman-filter.md", "heading": "Local Linear Trend Model", "content": "State: $\\mathbf{x}_t = (\\mu_t, \\beta_t)'$ (level and trend)\n\n$$\\begin{pmatrix} \\mu_t \\\\ \\beta_t \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}\\begin{pmatrix} \\mu_{t-1} \\\\ \\beta_{t-1} \\end{pmatrix} + \\begin{pmatrix} \\eta_t \\\\ \\zeta_t \\end{pmatrix}$$\n\n$$y_t = (1, 0)\\begin{pmatrix} \\mu_t \\\\ \\beta_t \\end{pmatrix} + \\epsilon_t$$", "line_start": 60}, "docs/en/state-space/kalman-filter.md#section_5": {"doc_id": "docs/en/state-space/kalman-filter.md", "heading": "Algorithm/Model Sketch", "content": "**Kalman Filter Algorithm:**\n\n```\nInput: y[1:n], F, H, Q, R, x0, P0\nOutput: filtered states, innovations, likelihood\n\nInitialize:\n  x_hat = x0\n  P = P0\n  log_lik = 0\n\nFor t = 1 to n:\n  # Prediction\n  x_pred = F @ x_hat\n  P_pred = F @ P @ F' + Q\n\n  # Innovation\n  v = y[t] - H @ x_pred\n  S = H @ P_pred @ H' + R\n\n  # Update\n  K = P_pred @ H' @ inv(S)\n  x_hat = x_pred + K @ v\n  P = (I - K @ H) @ P_pred\n\n  # Likelihood contribution\n  log_lik += -0.5 * (log(det(S)) + v' @ inv(S) @ v)\n\nReturn x_hat_history, P_history, log_lik\n```\n\n**Kalman Smoother** (uses all data):\n$$\\hat{\\mathbf{x}}_{t|n} = \\hat{\\mathbf{x}}_{t|t} + \\mathbf{J}_t(\\hat{\\mathbf{x}}_{t+1|n} - \\hat{\\mathbf{x}}_{t+1|t})$$\n\nwhere $\\mathbf{J}_t = \\mathbf{P}_{t|t}\\mathbf{F}'\\mathbf{P}_{t+1|t}^{-1}$", "line_start": 68}, "docs/en/state-space/kalman-filter.md#section_6": {"doc_id": "docs/en/state-space/kalman-filter.md", "heading": "Common Pitfalls", "content": "1. **Numerical instability**: Covariance matrices can become non-positive-definite. Use square-root or UD factorization.\n\n2. **Wrong initialization**: Poor $\\mathbf{x}_0, \\mathbf{P}_0$ affects early estimates. Use diffuse initialization for unknown initial states.\n\n3. **Model misspecification**: Kalman filter is optimal only for true model. Non-Gaussian or nonlinear systems need extensions (EKF, UKF, particle filter).\n\n4. **Forgetting the smoother**: For historical analysis (not real-time), use Kalman smoother to incorporate future observations.\n\n5. **Over-complicated state space**: Can represent many models but simple alternatives (ARIMA) may be easier.\n\n6. **Confusing filter vs. forecast**: Filter = estimate given data up to t. Forecast = prediction beyond observed data.", "line_start": 106}, "docs/en/state-space/kalman-filter.md#section_7": {"doc_id": "docs/en/state-space/kalman-filter.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\n\ndef kalman_filter_local_level(y, sigma_eta, sigma_eps, mu0=None, P0=1000):\n    \"\"\"Kalman filter for local level model.\"\"\"\n    n = len(y)\n\n    # Initialize\n    mu_filt = np.zeros(n)\n    P_filt = np.zeros(n)\n    mu_pred = mu0 if mu0 is not None else y[0]\n    P_pred = P0\n\n    for t in range(n):\n        # Prediction (for t > 0)\n        if t > 0:\n            mu_pred = mu_filt[t-1]\n            P_pred = P_filt[t-1] + sigma_eta**2\n\n        # Update\n        K = P_pred / (P_pred + sigma_eps**2)\n        mu_filt[t] = mu_pred + K * (y[t] - mu_pred)\n        P_filt[t] = (1 - K) * P_pred\n\n    return mu_filt, P_filt", "line_start": 120}, "docs/en/state-space/kalman-filter.md#section_8": {"doc_id": "docs/en/state-space/kalman-filter.md", "heading": "Generate local level data", "content": "np.random.seed(42)\nn = 100\nsigma_eta, sigma_eps = 0.5, 1.0", "line_start": 148}, "docs/en/state-space/kalman-filter.md#section_9": {"doc_id": "docs/en/state-space/kalman-filter.md", "heading": "True states", "content": "mu_true = np.cumsum(np.random.randn(n) * sigma_eta)", "line_start": 153}, "docs/en/state-space/kalman-filter.md#section_10": {"doc_id": "docs/en/state-space/kalman-filter.md", "heading": "Observations", "content": "y = mu_true + np.random.randn(n) * sigma_eps", "line_start": 155}, "docs/en/state-space/kalman-filter.md#section_11": {"doc_id": "docs/en/state-space/kalman-filter.md", "heading": "Run Kalman filter", "content": "mu_hat, P_hat = kalman_filter_local_level(y, sigma_eta, sigma_eps)\n\nprint(\"Kalman Filter Results:\")\nprint(f\"Final state estimate: {mu_hat[-1]:.2f}\")\nprint(f\"True final state: {mu_true[-1]:.2f}\")\nprint(f\"Final state std: {np.sqrt(P_hat[-1]):.3f}\")", "line_start": 158}, "docs/en/state-space/kalman-filter.md#section_12": {"doc_id": "docs/en/state-space/kalman-filter.md", "heading": "Steady-state Kalman gain", "content": "K_steady = (-sigma_eps**2 + np.sqrt(sigma_eps**4 + 4*sigma_eta**2*sigma_eps**2)) / (2*sigma_eps**2)\nprint(f\"\\nSteady-state Kalman gain: {K_steady:.3f}\")\nprint(f\"Equivalent SES alpha: {K_steady:.3f}\")\n```", "line_start": 166}, "docs/en/state-space/kalman-filter.md#section_13": {"doc_id": "docs/en/state-space/kalman-filter.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What is the intuition behind the Kalman gain?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The Kalman gain $K_t$ balances trust in prediction vs. trust in observation.\n\n$$K_t = \\frac{P_{t|t-1}}{P_{t|t-1} + R} = \\frac{\\text{prediction uncertainty}}{\\text{prediction uncertainty} + \\text{observation noise}}$$\n\n**Interpretation:**\n- High $K_t$ (close to 1): Observation is trusted more; state updates significantly\n- Low $K_t$ (close to 0): Prediction is trusted more; observation has little effect\n\n**When is K high?**\n- High state uncertainty ($P$ large)\n- Low observation noise ($R$ small)\n\n**When is K low?**\n- Low state uncertainty ($P$ small)\n- High observation noise ($R$ large)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Thinking gain is fixed. It evolves as uncertainty changes. Early on, gain may be high (uncertain prior); later, it stabilizes as filter \"learns.\"\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> How does the local level model relate to simple exponential smoothing?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> They're equivalent! Local level model with specific noise ratio produces SES.\n\n**Local level:**\n$$\\mu_t = \\mu_{t-1} + \\eta_t, \\quad y_t = \\mu_t + \\epsilon_t$$\n\n**Kalman update:**\n$$\\hat{\\mu}_{t|t} = \\hat{\\mu}_{t|t-1} + K(y_t - \\hat{\\mu}_{t|t-1})$$\n\n**At steady state:** $K \\to K^* = \\alpha$ (SES smoothing parameter)\n\nThe relationship:\n$$\\alpha = \\frac{-\\sigma_\\epsilon^2 + \\sqrt{\\sigma_\\epsilon^4 + 4\\sigma_\\eta^2\\sigma_\\epsilon^2}}{2\\sigma_\\eta^2}$$\n\n**Key insight:** SES is the steady-state Kalman filter for local level model. Kalman provides:\n- Optimal initialization\n- Proper uncertainty quantification\n- Connection to likelihood\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using SES without recognizing it assumes specific state space structure. If dynamics differ, SES may be suboptimal.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Derive the steady-state Kalman gain for the local level model.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> At steady state, $P_{t|t} = P^*$ (constant).\n\n**Recursion:**\n$$P_{t|t-1} = P_{t-1|t-1} + Q = P^* + Q$$\n$$P_{t|t} = (1 - K_t)P_{t|t-1} = P^*$$\n\n**Steady state condition:**\n$$P^* = (1 - K^*)(P^* + Q)$$\n$$P^* = P^* + Q - K^*(P^* + Q)$$\n$$K^* = \\frac{Q}{P^* + Q}$$\n\nAlso: $K^* = \\frac{P^* + Q}{P^* + Q + R}$\n\nSolving for $P^*$:\n$$P^* = \\frac{-R + \\sqrt{R^2 + 4QR}}{2}$$\n\nAnd:\n$$K^* = \\frac{-R + \\sqrt{R^2 + 4QR}}{2Q}$$\n\nFor Q = $\\sigma_\\eta^2$, R = $\\sigma_\\epsilon^2$, this gives the SES α relationship.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Steady state may take many iterations to reach. For short series, time-varying gain matters.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> What is the difference between filtered and smoothed state estimates?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Filtered:** $\\hat{x}_{t|t} = E[x_t | y_1, \\ldots, y_t]$\n- Uses observations up to time t\n- Real-time estimate\n- From forward Kalman pass\n\n**Smoothed:** $\\hat{x}_{t|n} = E[x_t | y_1, \\ldots, y_n]$\n- Uses all observations\n- Retrospective estimate\n- Requires backward pass after forward\n\n**Key difference:** Smoother uses future observations to refine past state estimates.\n\n**When to use which:**\n- Real-time forecasting: Filter\n- Historical analysis: Smoother\n- Parameter estimation: Smoother (better likelihood)\n\n**Variance relationship:**\n$$\\text{Var}(\\hat{x}_{t|n}) \\leq \\text{Var}(\\hat{x}_{t|t})$$\n\nSmoother is never worse than filter.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using filter estimates for historical analysis when smoother is available and more accurate.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You have GPS measurements with known accuracy (R) but unknown vehicle dynamics (Q). How would you tune the Kalman filter?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Approach 1: Maximum Likelihood**\n- Treat Q as parameter\n- Compute likelihood from innovations: $\\log L = -\\frac{1}{2}\\sum(\\log S_t + v_t^2/S_t)$\n- Optimize Q to maximize likelihood\n\n**Approach 2: Innovation-based tuning**\n- Innovations $v_t$ should be white noise with variance $S_t$\n- If innovations autocorrelated: Q too small\n- If innovation variance >> $S_t$: Q too small\n- If innovation variance << $S_t$: Q too large\n\n**Approach 3: Adaptive filtering**\n- Estimate Q online from recent innovation statistics\n- $\\hat{Q}_t = $ sample variance of recent innovations minus R\n\n**Practical steps:**\n1. Start with Q = R (equal trust)\n2. Check innovation statistics\n3. Grid search or gradient optimization\n4. Validate on holdout data\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Setting Q too small makes filter overconfident and slow to track changes. Too large makes it noisy. Cross-validate!\n</div>\n</div>\n</details>", "line_start": 172}, "docs/en/state-space/kalman-filter.md#section_14": {"doc_id": "docs/en/state-space/kalman-filter.md", "heading": "References", "content": "1. Harvey, A. C. (1990). *Forecasting, Structural Time Series Models and the Kalman Filter*. Cambridge University Press.\n2. Durbin, J., & Koopman, S. J. (2012). *Time Series Analysis by State Space Methods*. Oxford University Press.\n3. Kalman, R. E. (1960). A new approach to linear filtering and prediction problems. *Journal of Basic Engineering*, 82(1), 35-45.\n4. Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press. Chapter 13.", "line_start": 326}, "docs/en/anomaly-detection/anomaly-detection.md#section_0": {"doc_id": "docs/en/anomaly-detection/anomaly-detection.md", "heading": "Anomaly Detection", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Anomaly detection identifies unusual observations—point anomalies (single outliers), collective anomalies (unusual sequences), or contextual anomalies (unusual given context). Methods: statistical (z-score, IQR), model-based (forecast residuals), distance-based (LOF), and ML (isolation forest, autoencoders). Key challenge: defining \"normal\" and choosing threshold.\n</div>", "line_start": 1}, "docs/en/anomaly-detection/anomaly-detection.md#section_1": {"doc_id": "docs/en/anomaly-detection/anomaly-detection.md", "heading": "Core Definitions", "content": "**Anomaly Types:**\n- **Point anomaly:** Single unusual value (e.g., spike)\n- **Collective anomaly:** Sequence that's unusual as a group\n- **Contextual anomaly:** Normal value in wrong context (e.g., high AC usage in winter)\n\n**Detection Settings:**\n- **Supervised:** Labeled normal/anomaly data\n- **Semi-supervised:** Only normal data for training\n- **Unsupervised:** No labels, detect statistical outliers", "line_start": 7}, "docs/en/anomaly-detection/anomaly-detection.md#section_2": {"doc_id": "docs/en/anomaly-detection/anomaly-detection.md", "heading": "Statistical Methods", "content": "**Z-score:**\n$$z_t = \\frac{y_t - \\bar{y}}{s}$$\n\nAnomaly if $|z_t| > 3$ (or chosen threshold)\n\n**Modified Z-score (robust):**\n$$M_t = \\frac{0.6745(y_t - \\text{median})}{\\text{MAD}}$$\n\nwhere MAD = median(|y - median(y)|)\n\n**IQR Method:**\n$$\\text{Anomaly if } y_t < Q_1 - 1.5\\times IQR \\text{ or } y_t > Q_3 + 1.5\\times IQR$$", "line_start": 21}, "docs/en/anomaly-detection/anomaly-detection.md#section_3": {"doc_id": "docs/en/anomaly-detection/anomaly-detection.md", "heading": "Model-Based Detection", "content": "Fit time series model, flag large residuals:\n$$e_t = y_t - \\hat{y}_{t|t-1}$$\n\nAnomaly if $|e_t| > k \\times \\hat{\\sigma}$ (typically k = 3)\n\n**Advantages:**\n- Accounts for trend and seasonality\n- Adapts to changing patterns\n- More sensitive to contextual anomalies", "line_start": 36}, "docs/en/anomaly-detection/anomaly-detection.md#section_4": {"doc_id": "docs/en/anomaly-detection/anomaly-detection.md", "heading": "Isolation Forest", "content": "Anomalies are easier to isolate (require fewer splits).\n\n**Algorithm:**\n1. Build random trees by random splits\n2. Anomaly score = average path length to isolate point\n3. Short path → anomaly\n\n**Score:**\n$$s(x, n) = 2^{-E[h(x)]/c(n)}$$\n\nwhere h(x) = path length, c(n) = average path length in random tree.", "line_start": 48}, "docs/en/anomaly-detection/anomaly-detection.md#section_5": {"doc_id": "docs/en/anomaly-detection/anomaly-detection.md", "heading": "Local Outlier Factor (LOF)", "content": "Compares local density to neighbors' density:\n$$LOF(x) = \\frac{\\sum_{o \\in N_k(x)} \\frac{lrd(o)}{lrd(x)}}{|N_k(x)|}$$\n\nLOF >> 1 → anomaly (lower density than neighbors)", "line_start": 62}, "docs/en/anomaly-detection/anomaly-detection.md#section_6": {"doc_id": "docs/en/anomaly-detection/anomaly-detection.md", "heading": "Algorithm/Model Sketch", "content": "**Time Series Anomaly Detection Pipeline:**\n\n```python\ndef detect_anomalies(y, method='model', threshold=3):\n    if method == 'zscore':\n        z = (y - np.mean(y)) / np.std(y)\n        return np.abs(z) > threshold\n\n    elif method == 'model':\n        # Fit model and get residuals\n        model = fit_model(y)\n        residuals = y - model.fittedvalues\n        sigma = np.std(residuals)\n        return np.abs(residuals) > threshold * sigma\n\n    elif method == 'rolling':\n        # Rolling window approach\n        window = 30\n        rolling_mean = y.rolling(window).mean()\n        rolling_std = y.rolling(window).std()\n        z = (y - rolling_mean) / rolling_std\n        return np.abs(z) > threshold\n```\n\n**Threshold Selection:**\n- Fixed (z > 3): Simple but may not fit data\n- Percentile (top 1%): Adapts to distribution\n- Domain-specific: Based on cost of false positives/negatives\n- Extreme Value Theory: For tail events", "line_start": 69}, "docs/en/anomaly-detection/anomaly-detection.md#section_7": {"doc_id": "docs/en/anomaly-detection/anomaly-detection.md", "heading": "Common Pitfalls", "content": "1. **Masking:** One anomaly affects mean/std, hiding others. Use robust statistics or median-based methods.\n\n2. **Swamping:** Normal points flagged due to anomaly influence. Sequential cleaning or robust fitting helps.\n\n3. **Non-stationarity:** Using global statistics when local context matters. Use rolling windows or model residuals.\n\n4. **Wrong threshold:** Fixed threshold may be too sensitive or too conservative. Tune based on validation data.\n\n5. **Ignoring seasonality:** Saturday sales ≠ weekday anomaly. Model seasonal patterns first.\n\n6. **Collective anomalies:** Point methods miss unusual sequences. Use sequence-aware methods.", "line_start": 101}, "docs/en/anomaly-detection/anomaly-detection.md#section_8": {"doc_id": "docs/en/anomaly-detection/anomaly-detection.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom scipy import stats", "line_start": 115}, "docs/en/anomaly-detection/anomaly-detection.md#section_9": {"doc_id": "docs/en/anomaly-detection/anomaly-detection.md", "heading": "Generate data with anomalies", "content": "np.random.seed(42)\nn = 200\ny = np.sin(2 * np.pi * np.arange(n) / 50) + np.random.randn(n) * 0.3", "line_start": 121}, "docs/en/anomaly-detection/anomaly-detection.md#section_10": {"doc_id": "docs/en/anomaly-detection/anomaly-detection.md", "heading": "Insert anomalies", "content": "anomaly_idx = [50, 100, 150]\ny[anomaly_idx[0]] += 5   # Positive spike\ny[anomaly_idx[1]] -= 4   # Negative spike\ny[anomaly_idx[2]] += 3   # Smaller spike", "line_start": 126}, "docs/en/anomaly-detection/anomaly-detection.md#section_11": {"doc_id": "docs/en/anomaly-detection/anomaly-detection.md", "heading": "Method 1: Simple z-score", "content": "z_scores = np.abs(stats.zscore(y))\ndetected_zscore = np.where(z_scores > 3)[0]\nprint(f\"Z-score detected: {detected_zscore}\")", "line_start": 132}, "docs/en/anomaly-detection/anomaly-detection.md#section_12": {"doc_id": "docs/en/anomaly-detection/anomaly-detection.md", "heading": "Method 2: Rolling z-score", "content": "window = 20\nrolling_mean = np.convolve(y, np.ones(window)/window, mode='same')\nrolling_std = np.array([np.std(y[max(0,i-window):i+1]) for i in range(n)])\nrolling_z = np.abs((y - rolling_mean) / rolling_std)\ndetected_rolling = np.where(rolling_z > 3)[0]\nprint(f\"Rolling z-score detected: {detected_rolling[:10]}...\")  # May have more", "line_start": 137}, "docs/en/anomaly-detection/anomaly-detection.md#section_13": {"doc_id": "docs/en/anomaly-detection/anomaly-detection.md", "heading": "Method 3: IQR", "content": "Q1, Q3 = np.percentile(y, [25, 75])\nIQR = Q3 - Q1\nlower, upper = Q1 - 1.5*IQR, Q3 + 1.5*IQR\ndetected_iqr = np.where((y < lower) | (y > upper))[0]\nprint(f\"IQR detected: {detected_iqr}\")", "line_start": 145}, "docs/en/anomaly-detection/anomaly-detection.md#section_14": {"doc_id": "docs/en/anomaly-detection/anomaly-detection.md", "heading": "Method 4: Model-based (simple AR)", "content": "from statsmodels.tsa.ar_model import AutoReg\nmodel = AutoReg(y, lags=5).fit()\nresiduals = model.resid\nres_z = np.abs(stats.zscore(residuals))\ndetected_model = np.where(res_z > 3)[0] + 5  # Adjust for lag offset\nprint(f\"Model-based detected: {detected_model}\")\n\nprint(f\"\\nTrue anomalies: {anomaly_idx}\")\n```", "line_start": 152}, "docs/en/anomaly-detection/anomaly-detection.md#section_15": {"doc_id": "docs/en/anomaly-detection/anomaly-detection.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What is the difference between a point anomaly and a contextual anomaly?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Point anomaly:** Value is unusual regardless of context.\n- Example: Temperature reading of 500°F\n- Detected by: Global statistics, simple thresholds\n\n**Contextual anomaly:** Value is normal in some contexts, unusual in current context.\n- Example: 80°F temperature is normal in summer, anomalous in winter\n- Detected by: Model residuals, conditional distributions\n\n**Why distinction matters:**\n- Point methods (z-score) miss contextual anomalies\n- Need context-aware methods for seasonal/temporal patterns\n- False positives if using wrong method\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using simple z-score on seasonal data. A December value might be normal for December but flagged as anomaly compared to annual mean.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Why is the median absolute deviation (MAD) preferred over standard deviation for anomaly detection?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> MAD is robust to outliers; standard deviation is not.\n\n**Problem with std:**\nSingle large outlier → inflates std → makes all z-scores smaller → outlier \"masks\" itself and others\n\n**MAD robustness:**\n$$\\text{MAD} = \\text{median}(|y_i - \\text{median}(y)|)$$\n\n- Median is not affected by extreme values\n- 50% of data must be outliers to significantly affect MAD\n- Breaking point: 50% vs ~0% for std\n\n**Scale factor:**\nFor normal data: $\\sigma \\approx 1.4826 \\times \\text{MAD}$\n\nUse modified z-score: $M = \\frac{0.6745(y - \\text{median})}{\\text{MAD}}$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using std-based z-scores when data may contain multiple outliers. Masking effect hides true anomalies.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Derive the modified z-score formula using MAD.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The modified z-score normalizes by MAD instead of std:\n\n$$M_i = \\frac{y_i - \\tilde{y}}{\\text{MAD}}$$\n\nwhere $\\tilde{y}$ = median(y).\n\n**For comparison with standard z-score:**\nFor normal distribution: $E[\\text{MAD}] = \\Phi^{-1}(0.75) \\times \\sigma \\approx 0.6745\\sigma$\n\nSo: $\\sigma \\approx \\frac{\\text{MAD}}{0.6745}$\n\n**Modified z-score (scaled):**\n$$M_i = \\frac{0.6745(y_i - \\tilde{y})}{\\text{MAD}}$$\n\nNow M has same scale as standard z-score under normality.\nThreshold M > 3.5 often used (equivalent to |z| > 3).\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using threshold 3 for modified z-score without scaling factor. The raw MAD-based score has different scale than standard z.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> How does Isolation Forest detect anomalies without computing distances?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Isolation Forest uses tree-based isolation:\n\n**Key insight:** Anomalies are few and different → easier to isolate (separate from rest).\n\n**Algorithm:**\n1. Randomly select feature and split value\n2. Recursively partition data\n3. Anomaly score = average path length to isolate point\n\n**Why anomalies have short paths:**\n- Normal points: surrounded by similar points, need many splits\n- Anomalies: isolated, few splits separate them\n\n**Score formula:**\n$$s(x,n) = 2^{-\\frac{E[h(x)]}{c(n)}}$$\n\n- $E[h(x)]$ = expected path length for x\n- $c(n)$ = average path length in binary search tree of n points\n- $s \\to 1$: anomaly; $s \\to 0.5$: normal; $s \\to 0$: very normal\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Isolation Forest assumes anomalies are isolated. Clustered anomalies (collective) may be missed.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You deploy an anomaly detection system and it triggers 50 alerts per day. After investigation, 45 are false positives. How do you improve?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> 10% precision is problematic. Approaches:\n\n1. **Raise threshold:**\n   - Increase z-score cutoff from 3 to 4\n   - Reduces false positives but may miss true anomalies\n\n2. **Add context:**\n   - Use time-of-day, day-of-week features\n   - Model seasonal patterns\n   - Contextual anomalies only within context\n\n3. **Ensemble methods:**\n   - Combine multiple detectors\n   - Flag only if majority agree\n\n4. **Learn from feedback:**\n   - Label false positives as normal\n   - Retrain semi-supervised model\n\n5. **Two-stage detection:**\n   - First stage: sensitive (catch all anomalies)\n   - Second stage: verify (filter false positives)\n\n6. **Domain rules:**\n   - Add business logic filters\n   - Known patterns that aren't anomalies\n\n**Metrics to track:** Precision, recall, F1-score at different thresholds.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Optimizing only for catching anomalies (recall). High false positive rate leads to alert fatigue and ignored warnings.\n</div>\n</div>\n</details>", "line_start": 163}, "docs/en/anomaly-detection/anomaly-detection.md#section_16": {"doc_id": "docs/en/anomaly-detection/anomaly-detection.md", "heading": "References", "content": "1. Chandola, V., Banerjee, A., & Kumar, V. (2009). Anomaly detection: A survey. *ACM Computing Surveys*, 41(3), 1-58.\n2. Liu, F. T., Ting, K. M., & Zhou, Z. H. (2008). Isolation forest. *ICDM*, 413-422.\n3. Breunig, M. M., Kriegel, H. P., Ng, R. T., & Sander, J. (2000). LOF: Identifying density-based local outliers. *SIGMOD*, 93-104.\n4. Hochenbaum, J., Vallis, O. S., & Kejariwal, A. (2017). Automatic anomaly detection in the cloud via statistical learning. *arXiv:1704.07706*.", "line_start": 313}, "docs/en/decomposition/classical.md#section_0": {"doc_id": "docs/en/decomposition/classical.md", "heading": "Classical Decomposition", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Classical decomposition separates a series into trend, seasonal, and irregular components using moving averages. Additive: $y = T + S + I$. Multiplicative: $y = T \\times S \\times I$. Simple but assumes constant seasonal pattern and is sensitive to outliers. Trend extracted via centered moving average; seasonal via period averages.\n</div>", "line_start": 1}, "docs/en/decomposition/classical.md#section_1": {"doc_id": "docs/en/decomposition/classical.md", "heading": "Core Definitions", "content": "**Additive Model:**\n$$y_t = T_t + S_t + I_t$$\n\nUsed when seasonal variation is constant regardless of level.\n\n**Multiplicative Model:**\n$$y_t = T_t \\times S_t \\times I_t$$\n\nUsed when seasonal variation scales with level.\n\n**Components:**\n- $T_t$: Trend-cycle (smooth underlying level)\n- $S_t$: Seasonal (periodic pattern repeating every $m$ periods)\n- $I_t$: Irregular/residual (random noise)\n\n**Seasonal Indices:**\n- Additive: $S_t$ values sum to 0 over one period\n- Multiplicative: $S_t$ values average to 1 over one period", "line_start": 7}, "docs/en/decomposition/classical.md#section_2": {"doc_id": "docs/en/decomposition/classical.md", "heading": "Trend Extraction via Moving Average", "content": "**Centered Moving Average (CMA):**\n\nFor odd $m$ (e.g., $m=7$):\n$$T_t = \\frac{1}{m}\\sum_{j=-(m-1)/2}^{(m-1)/2} y_{t+j}$$\n\nFor even $m$ (e.g., $m=12$):\n$$T_t = \\frac{1}{2m}\\left(y_{t-m/2} + 2\\sum_{j=-(m/2-1)}^{m/2-1} y_{t+j} + y_{t+m/2}\\right)$$\n\nThis is a $2 \\times m$-MA: first $m$-MA, then 2-MA to center.", "line_start": 30}, "docs/en/decomposition/classical.md#section_3": {"doc_id": "docs/en/decomposition/classical.md", "heading": "Seasonal Index Calculation", "content": "**Additive:**\n1. Detrend: $y_t - T_t$\n2. Average detrended values for each season: $\\bar{S}_s = \\frac{1}{k}\\sum_{j} (y_{s+jm} - T_{s+jm})$\n3. Normalize: $S_s = \\bar{S}_s - \\frac{1}{m}\\sum_{s=1}^{m}\\bar{S}_s$\n\n**Multiplicative:**\n1. Detrend: $y_t / T_t$\n2. Average ratios for each season: $\\bar{S}_s = \\frac{1}{k}\\sum_{j} \\frac{y_{s+jm}}{T_{s+jm}}$\n3. Normalize: $S_s = \\bar{S}_s \\times \\frac{m}{\\sum_{s=1}^{m}\\bar{S}_s}$", "line_start": 42}, "docs/en/decomposition/classical.md#section_4": {"doc_id": "docs/en/decomposition/classical.md", "heading": "Properties of Moving Average", "content": "The $m$-point moving average:\n- Removes seasonality of period $m$ (averages over full cycle)\n- Smooths high-frequency noise\n- Introduces lag of $(m-1)/2$ periods\n- Loses $(m-1)/2$ observations at each end\n\n**Frequency response:** MA is a low-pass filter that attenuates frequencies $\\geq 1/m$.", "line_start": 54}, "docs/en/decomposition/classical.md#section_5": {"doc_id": "docs/en/decomposition/classical.md", "heading": "Algorithm/Model Sketch", "content": "**Classical Decomposition Algorithm:**\n\n```\nInput: y[1:n], seasonal period m, type (additive/multiplicative)\nOutput: Trend T, Seasonal S, Irregular I\n\n1. TREND EXTRACTION\n   - Compute centered moving average of y\n   - T[t] = CMA(y, m) for t = m/2+1 to n-m/2\n   - End points: use extrapolation or leave missing\n\n2. DETREND\n   - Additive: D[t] = y[t] - T[t]\n   - Multiplicative: D[t] = y[t] / T[t]\n\n3. SEASONAL INDICES\n   - Group D[t] by season (1 to m)\n   - Average each group: S_raw[s] = mean(D[s], D[s+m], D[s+2m],...)\n   - Normalize:\n     - Additive: S[s] = S_raw[s] - mean(S_raw)\n     - Multiplicative: S[s] = S_raw[s] × m / sum(S_raw)\n\n4. SEASONAL COMPONENT\n   - S[t] = S[t mod m] (replicate indices across series)\n\n5. IRREGULAR\n   - Additive: I[t] = y[t] - T[t] - S[t]\n   - Multiplicative: I[t] = y[t] / (T[t] × S[t])\n\nReturn T, S, I\n```", "line_start": 64}, "docs/en/decomposition/classical.md#section_6": {"doc_id": "docs/en/decomposition/classical.md", "heading": "Common Pitfalls", "content": "1. **Fixed seasonal pattern**: Classical decomposition assumes the same seasonal pattern throughout. Doesn't adapt to evolving seasonality.\n\n2. **Outlier sensitivity**: One outlier affects trend (via MA) and seasonal indices. No robust fitting.\n\n3. **End-point loss**: Lose $m/2$ observations at each end. Problematic for short series.\n\n4. **Wrong model type**: Using additive when data is multiplicative (or vice versa) gives poor decomposition.\n\n5. **Calendar effects**: Doesn't handle trading days, Easter, etc. These appear in irregular component.\n\n6. **Non-integer period**: Requires integer $m$. For 365.25 days/year, need alternative methods.", "line_start": 98}, "docs/en/decomposition/classical.md#section_7": {"doc_id": "docs/en/decomposition/classical.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.seasonal import seasonal_decompose", "line_start": 112}, "docs/en/decomposition/classical.md#section_8": {"doc_id": "docs/en/decomposition/classical.md", "heading": "Generate multiplicative seasonal data", "content": "np.random.seed(42)\nn = 48  # 4 years monthly\nt = np.arange(n)\ntrend = 100 + 2 * t\nseasonal_mult = 1 + 0.3 * np.sin(2 * np.pi * t / 12)\ny = trend * seasonal_mult * (1 + 0.05 * np.random.randn(n))", "line_start": 118}, "docs/en/decomposition/classical.md#section_9": {"doc_id": "docs/en/decomposition/classical.md", "heading": "Classical decomposition (multiplicative)", "content": "result = seasonal_decompose(y, model='multiplicative', period=12)\n\nprint(\"Seasonal indices (should repeat):\")\nprint(np.round(result.seasonal[:12], 3))\n\nprint(\"\\nTrend (first and last available):\")\nprint(f\"  First: {result.trend[~np.isnan(result.trend)][0]:.1f}\")\nprint(f\"  Last: {result.trend[~np.isnan(result.trend)][-1]:.1f}\")", "line_start": 126}, "docs/en/decomposition/classical.md#section_10": {"doc_id": "docs/en/decomposition/classical.md", "heading": "Compare additive (wrong model for this data)", "content": "result_add = seasonal_decompose(y, model='additive', period=12)\nprint(\"\\nCompare residual std:\")\nprint(f\"  Multiplicative: {np.nanstd(result.resid):.3f}\")\nprint(f\"  Additive: {np.nanstd(result_add.resid):.3f}\")", "line_start": 136}, "docs/en/decomposition/classical.md#section_11": {"doc_id": "docs/en/decomposition/classical.md", "heading": "Multiplicative should have smaller residual std", "content": "```", "line_start": 141}, "docs/en/decomposition/classical.md#section_12": {"doc_id": "docs/en/decomposition/classical.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> When should you use additive vs. multiplicative decomposition?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Additive** when:\n- Seasonal fluctuations are roughly constant in absolute terms\n- Low and high values have similar seasonal swings\n- Example: Temperature (±10°F regardless of base temp)\n\n**Multiplicative** when:\n- Seasonal fluctuations are proportional to level\n- Percentage variation is constant\n- Example: Retail sales (December is 20% above average regardless of total sales)\n\n**Decision test:**\n1. Plot series — do seasonal swings grow with level?\n2. Compute: std(seasonal) / mean(level) for different periods\n   - If ratio is constant → multiplicative\n   - If std(seasonal) is constant → additive\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Defaulting to additive. Many economic/business series are multiplicative because growth compounds.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Why does the moving average remove seasonality of period m?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> An $m$-point moving average includes exactly one complete seasonal cycle. Since seasonal components sum to zero (additive) or average to one (multiplicative) over a cycle, they cancel out.\n\n**Mathematical explanation (additive):**\n$$\\text{MA}_t = \\frac{1}{m}\\sum_{j=0}^{m-1}(T_{t+j} + S_{t+j} + I_{t+j})$$\n\nIf trend is locally constant and $\\sum_{j=0}^{m-1}S_{t+j} = 0$:\n$$\\text{MA}_t \\approx T_t + \\frac{1}{m}\\sum_{j=0}^{m-1}I_{t+j}$$\n\nThe seasonal cancels; only trend and smoothed noise remain.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using wrong MA order. If true period is 12 but you use MA(6), seasonality won't be fully removed.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> For a 12-point centered moving average of monthly data, how many observations are lost at each end?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> 6 observations at each end (12 total).\n\n**Derivation:**\nFor even $m=12$, the 2×12-MA formula at time $t$ uses:\n$$\\frac{1}{24}(y_{t-6} + 2y_{t-5} + \\cdots + 2y_{t+5} + y_{t+6})$$\n\nThis requires observations from $t-6$ to $t+6$.\n\nAt the start: Can only compute for $t \\geq 7$ (need $y_1, \\ldots, y_{13}$)\nAt the end: Can only compute for $t \\leq n-6$ (need data through $y_{n}$)\n\nSo positions 1-6 and (n-5)-n are missing → 12 total missing values.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> For short series (say, 2 years = 24 points), losing 12 points means half the data has no trend estimate. Consider STL or parametric trend instead.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Why must seasonal indices be normalized, and what constraint do they satisfy?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Why normalize:**\nWithout normalization, average of raw seasonal indices might not be zero (additive) or one (multiplicative), causing systematic bias in trend or irregular.\n\n**Constraints:**\n\nAdditive: $\\sum_{s=1}^{m} S_s = 0$\n- Ensures seasonality doesn't shift the overall level\n- Positive seasons offset by negative seasons\n\nMultiplicative: $\\sum_{s=1}^{m} S_s = m$ (equivalently, average = 1)\n- Ensures seasonal factors don't inflate/deflate overall level\n- Factors > 1 offset by factors < 1\n\n**Normalization formulas:**\n- Additive: $S_s^{new} = S_s^{raw} - \\bar{S}^{raw}$\n- Multiplicative: $S_s^{new} = S_s^{raw} \\times m / \\sum S^{raw}$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Forgetting normalization leads to decomposition where $T + S + I \\neq y$ due to bias in seasonal.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You decompose monthly sales data and find the irregular component has strong autocorrelation at lag 1. What does this indicate and how would you address it?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Lag-1 autocorrelation in irregular indicates:\n1. The model hasn't captured all systematic patterns\n2. Short-term dynamics exist beyond trend and seasonality\n3. Possibly: month-to-month momentum not in seasonal pattern\n\n**How to address:**\n\n1. **Model the irregular:** Fit ARIMA to irregular component\n   - If AR(1), incorporate into forecasting\n   - STL+ARIMA or similar pipeline\n\n2. **Use better decomposition:** STL can adapt to changing patterns that classical misses\n\n3. **Consider SARIMA directly:** Handles trend, seasonality, AND autocorrelation in one model\n\n4. **Check for calendar effects:** Trading days, holidays may create autocorrelation\n\n5. **Use ETS:** State space models can capture autocorrelated errors\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Ignoring irregular autocorrelation in forecasting. This underestimates forecast uncertainty and may bias predictions.\n</div>\n</div>\n</details>", "line_start": 144}, "docs/en/decomposition/classical.md#section_13": {"doc_id": "docs/en/decomposition/classical.md", "heading": "References", "content": "1. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 3.\n2. Makridakis, S., Wheelwright, S. C., & Hyndman, R. J. (1998). *Forecasting: Methods and Applications*. Wiley. Chapter 4.\n3. Brockwell, P. J., & Davis, R. A. (2016). *Introduction to Time Series and Forecasting*. Springer. Chapter 1.\n4. Census Bureau. (2017). X-13ARIMA-SEATS Reference Manual. U.S. Census Bureau.", "line_start": 275}, "docs/en/decomposition/stl.md#section_0": {"doc_id": "docs/en/decomposition/stl.md", "heading": "STL Decomposition", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> STL (Seasonal and Trend decomposition using Loess) robustly separates a series into trend, seasonal, and remainder components using local regression. Unlike classical decomposition, STL handles any seasonal period and is robust to outliers. Key parameters: seasonal window (odd integer ≥ 7) and trend window. Remainder should be stationary for forecasting.\n</div>", "line_start": 1}, "docs/en/decomposition/stl.md#section_1": {"doc_id": "docs/en/decomposition/stl.md", "heading": "Core Definitions", "content": "**Additive Decomposition:**\n$$y_t = T_t + S_t + R_t$$\n\n- $T_t$: Trend component (smooth, long-term movement)\n- $S_t$: Seasonal component (periodic pattern, sums to ~0 over each cycle)\n- $R_t$: Remainder/residual (everything else)\n\n**Loess (Locally Estimated Scatterplot Smoothing):**\nLocal polynomial regression using weighted least squares. Weights decrease with distance from target point.\n\n**Key STL Parameters:**\n- `seasonal`: Seasonal period (e.g., 12 for monthly)\n- `seasonal_deg`: Degree of seasonal polynomial (0 or 1)\n- `trend`: Trend smoothing window (odd integer, default depends on seasonal)\n- `robust`: Whether to use robust fitting (downweight outliers)", "line_start": 7}, "docs/en/decomposition/stl.md#section_2": {"doc_id": "docs/en/decomposition/stl.md", "heading": "Loess Smoother", "content": "For point $x_0$, fit weighted polynomial:\n$$\\min_{\\beta} \\sum_{i=1}^{n} w_i(x_0)(y_i - \\beta_0 - \\beta_1(x_i - x_0))^2$$\n\nWeights use tricube function:\n$$w(u) = \\begin{cases} (1-|u|^3)^3 & |u| < 1 \\\\ 0 & |u| \\geq 1 \\end{cases}$$\n\nDistance scaled by bandwidth $h$: $u_i = |x_i - x_0|/h$", "line_start": 27}, "docs/en/decomposition/stl.md#section_3": {"doc_id": "docs/en/decomposition/stl.md", "heading": "STL Algorithm (Simplified)", "content": "**Outer loop** (for robustness):\n1. Initialize: $R_t^{(0)} = 0$, $T_t^{(0)} = $ loess smooth of $y_t$\n\n**Inner loop**:\n2. **Detrend**: $y_t - T_t^{(k-1)}$\n3. **Cycle-subseries smoothing**: For each season $s=1,\\ldots,m$, smooth values at positions $s, s+m, s+2m, \\ldots$ using loess\n4. **Low-pass filter**: Remove low-frequency from seasonal\n5. **Deseasonalize**: $y_t - S_t^{(k)}$\n6. **Trend extraction**: Loess smooth of deseasonalized series\n\nRepeat inner loop until convergence; outer loop updates robustness weights.", "line_start": 37}, "docs/en/decomposition/stl.md#section_4": {"doc_id": "docs/en/decomposition/stl.md", "heading": "Robustness Weights", "content": "After each outer iteration, compute residuals and assign weights:\n$$\\rho_t = |R_t|$$\n$$h = 6 \\cdot \\text{median}(|\\rho_t|)$$\n$$w_t = B(\\rho_t/h)$$\n\nwhere $B$ is the bisquare function: $B(u) = (1-u^2)^2$ for $|u| < 1$, else 0.\n\nOutliers get downweighted in subsequent iterations.", "line_start": 51}, "docs/en/decomposition/stl.md#section_5": {"doc_id": "docs/en/decomposition/stl.md", "heading": "Algorithm/Model Sketch", "content": "**STL Decomposition Steps:**\n\n```\nInput: y[1:n], seasonal period m, parameters\nOutput: Trend T, Seasonal S, Remainder R\n\n1. Initialize trend T = loess(y) or moving average\n2. For k = 1 to n_outer:\n\n   For j = 1 to n_inner:\n      a. Detrend: D = y - T\n      b. For each position i in 1...m:\n         - Extract subseries: values at i, i+m, i+2m,...\n         - Smooth subseries with loess\n         - Store smoothed seasonal values\n      c. Low-pass filter seasonal (remove trend leakage)\n      d. Subtract filtered seasonal from raw seasonal → S\n      e. Deseasonalize: y - S\n      f. Smooth deseasonalized → T\n\n   Update robustness weights based on R = y - T - S\n\n3. Return T, S, R = y - T - S\n```", "line_start": 62}, "docs/en/decomposition/stl.md#section_6": {"doc_id": "docs/en/decomposition/stl.md", "heading": "Common Pitfalls", "content": "1. **Wrong seasonal period**: STL requires correct m. If m is wrong, seasonal won't be captured properly.\n\n2. **Over-smoothing trend**: Too large trend window removes real variation. Under-smoothing captures noise.\n\n3. **Seasonal leakage into trend**: If seasonal window too small, trend absorbs some seasonality.\n\n4. **Not using robust mode**: Outliers distort both trend and seasonal. Always try `robust=True` first.\n\n5. **Assuming multiplicative works directly**: STL is additive. For multiplicative, log-transform first, then decompose, then exponentiate.\n\n6. **Ignoring remainder**: Remainder should look like noise. Strong patterns indicate model inadequacy.", "line_start": 89}, "docs/en/decomposition/stl.md#section_7": {"doc_id": "docs/en/decomposition/stl.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.seasonal import STL\nimport matplotlib.pyplot as plt", "line_start": 103}, "docs/en/decomposition/stl.md#section_8": {"doc_id": "docs/en/decomposition/stl.md", "heading": "Generate data with trend, seasonality, and outliers", "content": "np.random.seed(42)\nn = 120\nt = np.arange(n)\ntrend = 50 + 0.3 * t\nseasonal = 10 * np.sin(2 * np.pi * t / 12)\nnoise = np.random.randn(n) * 3\ny = trend + seasonal + noise", "line_start": 110}, "docs/en/decomposition/stl.md#section_9": {"doc_id": "docs/en/decomposition/stl.md", "heading": "Add outliers", "content": "y[50] += 40\ny[80] -= 35", "line_start": 119}, "docs/en/decomposition/stl.md#section_10": {"doc_id": "docs/en/decomposition/stl.md", "heading": "STL decomposition (robust)", "content": "stl = STL(y, period=12, robust=True)\nresult = stl.fit()\n\nprint(\"Component statistics:\")\nprint(f\"Trend range: [{result.trend.min():.1f}, {result.trend.max():.1f}]\")\nprint(f\"Seasonal range: [{result.seasonal.min():.1f}, {result.seasonal.max():.1f}]\")\nprint(f\"Remainder std: {result.resid.std():.2f}\")", "line_start": 123}, "docs/en/decomposition/stl.md#section_11": {"doc_id": "docs/en/decomposition/stl.md", "heading": "Check if outliers are in remainder (they should be)", "content": "print(f\"\\nRemainder at outlier positions:\")\nprint(f\"  t=50: {result.resid[50]:.1f}\")\nprint(f\"  t=80: {result.resid[80]:.1f}\")", "line_start": 132}, "docs/en/decomposition/stl.md#section_12": {"doc_id": "docs/en/decomposition/stl.md", "heading": "Plot", "content": "fig = result.plot()\nplt.tight_layout()\nplt.show()\n```", "line_start": 137}, "docs/en/decomposition/stl.md#section_13": {"doc_id": "docs/en/decomposition/stl.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> Why is STL preferred over classical decomposition in many applications?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> STL advantages:\n\n1. **Flexibility**: Works with any seasonal period, not just 4 or 12\n2. **Robustness**: Outliers don't distort estimates (with robust=True)\n3. **Control**: Adjustable smoothness via window parameters\n4. **Evolving seasonal**: Can capture slowly changing seasonal patterns\n5. **No end-point issues**: Loess handles boundaries better than moving averages\n\nClassical decomposition:\n- Assumes fixed seasonal pattern\n- Sensitive to outliers\n- Moving average loses observations at ends\n- Limited to standard frequencies\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using classical decomposition by default. STL is almost always better, especially with outliers or evolving patterns.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> How does the robustness mechanism in STL work?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Iterative reweighting:\n\n1. First pass: fit STL normally, compute residuals\n2. Identify outliers: large |residuals| relative to median\n3. Assign weights: outliers get weight → 0, normal points → 1\n4. Refit STL with weighted observations\n5. Repeat until convergence\n\n**Weight function (bisquare):**\n$$w = (1 - (r/h)^2)^2$$\n\nwhere $r$ = |residual|, $h$ = 6 × median(|residuals|)\n\nOutliers (large $r$) get near-zero weights and don't influence the fit.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Not using robust mode then wondering why one outlier distorts the entire seasonal pattern. Always start with robust=True.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> In loess smoothing, why is the tricube weight function used?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Tricube function $w(u) = (1-|u|^3)^3$:\n\n1. **Smooth**: Continuous and differentiable, giving smooth fitted curves\n2. **Compact support**: Zero beyond bandwidth, so distant points don't influence fit\n3. **Downweighting**: Smoothly decreases influence with distance\n4. **Computationally nice**: Simple polynomial form\n\n**Properties:**\n- $w(0) = 1$ (full weight at target point)\n- $w(u) \\to 0$ smoothly as $|u| \\to 1$\n- $w(u) = 0$ for $|u| \\geq 1$\n\nAlternative: Gaussian weights have infinite support (all points contribute), which is less local.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Choosing bandwidth too small → jagged fit; too large → over-smoothed. STL uses data-driven defaults but tuning may help.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Why does STL use a low-pass filter on the seasonal component?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The low-pass filter removes trend that leaked into the seasonal during cycle-subseries smoothing.\n\n**Problem:** When smoothing each season's subseries (e.g., all Januaries), if there's trend, the subseries average drifts. This drift appears as low-frequency content in the seasonal.\n\n**Solution:** Apply moving average to the seasonal across full cycles:\n$$L_t = \\frac{1}{m}\\sum_{j=-(m-1)/2}^{(m-1)/2} S^*_{t+j}$$\n\nThen subtract: $S_t = S^*_t - L_t$\n\nThis ensures seasonal averages to zero over each cycle.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Without the low-pass filter, seasonal component captures some trend, leaving residual with trend pattern.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> After STL decomposition, your remainder shows a clear AR(1) pattern. What does this mean and what should you do?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> An AR(1) pattern in remainder means:\n- STL captured trend and seasonality\n- But short-term autocorrelation remains\n- This is common and expected\n\n**What to do:**\n1. **For forecasting**: Model remainder with AR(1) or ARIMA\n   - Forecast trend (extrapolation or drift)\n   - Forecast seasonal (repeat pattern)\n   - Forecast remainder with AR(1)\n   - Combine: $\\hat{y} = \\hat{T} + \\hat{S} + \\hat{R}$\n\n2. **STL + ARIMA pipeline:**\n   ```python\n   stl_result = STL(y, period=12).fit()\n   remainder = stl_result.resid\n   arima_model = ARIMA(remainder, order=(1,0,0)).fit()\n   ```\n\n3. **Consider ETS/SARIMA directly**: They handle all components in one model.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Ignoring remainder autocorrelation in forecasting. This underestimates short-term uncertainty and biases predictions.\n</div>\n</div>\n</details>", "line_start": 143}, "docs/en/decomposition/stl.md#section_14": {"doc_id": "docs/en/decomposition/stl.md", "heading": "References", "content": "1. Cleveland, R. B., Cleveland, W. S., McRae, J. E., & Terpenning, I. (1990). STL: A seasonal-trend decomposition procedure based on loess. *Journal of Official Statistics*, 6(1), 3-73.\n2. Cleveland, W. S., & Devlin, S. J. (1988). Locally weighted regression: An approach to regression analysis by local fitting. *JASA*, 83(403), 596-610.\n3. Hyndman, R. J., & Athanasopoulos, G. (2021). *Forecasting: Principles and Practice*. OTexts. Chapter 3.\n4. Dokumentov, A., & Hyndman, R. J. (2015). STR: A seasonal-trend decomposition procedure based on regression. *Monash Econometrics Working Papers*.", "line_start": 270}, "docs/en/change-detection/change-point.md#section_0": {"doc_id": "docs/en/change-detection/change-point.md", "heading": "Change-Point Detection", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Change-point detection identifies times when statistical properties (mean, variance, trend) shift. Methods: CUSUM (cumulative sum), PELT (penalized exact linear time), Bayesian online detection. Key trade-off: sensitivity vs. false positives. Applications: process monitoring, regime detection, structural breaks in economics. For offline detection, use dynamic programming; for online, use sequential methods.\n</div>", "line_start": 1}, "docs/en/change-detection/change-point.md#section_1": {"doc_id": "docs/en/change-detection/change-point.md", "heading": "Core Definitions", "content": "**Change-Point:** Time $\\tau$ where distribution parameters change:\n$$y_t \\sim \\begin{cases} F_1(\\theta_1) & t < \\tau \\\\ F_2(\\theta_2) & t \\geq \\tau \\end{cases}$$\n\n**Types:**\n- Mean shift: $\\mu_1 \\neq \\mu_2$\n- Variance change: $\\sigma_1^2 \\neq \\sigma_2^2$\n- Trend break: Slope changes\n- Multiple change-points: $\\tau_1 < \\tau_2 < \\cdots < \\tau_k$\n\n**Settings:**\n- Offline: All data available, find all change-points\n- Online: Sequential, detect changes as they occur", "line_start": 7}, "docs/en/change-detection/change-point.md#section_2": {"doc_id": "docs/en/change-detection/change-point.md", "heading": "CUSUM (Cumulative Sum)", "content": "For mean detection with known parameters:\n$$S_t = \\sum_{i=1}^{t}(y_i - \\mu_0)$$\n\nUnder H₀ (no change): $S_t$ fluctuates around 0\nUnder H₁ (mean shift at τ): $S_t$ trends away from 0 after τ\n\n**Page's CUSUM:**\n$$C_t^+ = \\max(0, C_{t-1}^+ + y_t - \\mu_0 - k)$$\n$$C_t^- = \\max(0, C_{t-1}^- - y_t + \\mu_0 - k)$$\n\nSignal change when $C_t^+ > h$ or $C_t^- > h$\n\nParameters: $k$ (allowance), $h$ (threshold)", "line_start": 24}, "docs/en/change-detection/change-point.md#section_3": {"doc_id": "docs/en/change-detection/change-point.md", "heading": "Binary Segmentation", "content": "Greedy algorithm for multiple change-points:\n\n1. Test for one change-point in [1, T]\n2. If found at τ₁, recursively search [1, τ₁) and [τ₁, T]\n3. Continue until no more significant changes\n\nCost function (e.g., RSS):\n$$C(y_{s:t}) = \\sum_{i=s}^{t}(y_i - \\bar{y}_{s:t})^2$$", "line_start": 40}, "docs/en/change-detection/change-point.md#section_4": {"doc_id": "docs/en/change-detection/change-point.md", "heading": "PELT (Pruned Exact Linear Time)", "content": "Optimal partitioning via dynamic programming:\n$$F(t) = \\min_{s < t}\\{F(s) + C(y_{s+1:t}) + \\beta\\}$$\n\nwhere β is penalty per change-point.\n\n**Pruning:** Eliminate suboptimal segmentations to achieve O(n) complexity.", "line_start": 51}, "docs/en/change-detection/change-point.md#section_5": {"doc_id": "docs/en/change-detection/change-point.md", "heading": "Bayesian Online Change-Point Detection", "content": "Maintain probability distribution over run length $r_t$ (time since last change):\n$$P(r_t | y_{1:t}) \\propto P(y_t | r_t, y_{1:t-1}) P(r_t | r_{t-1})$$\n\nGrowth probability: $P(r_t = r_{t-1} + 1)$\nChange probability: $P(r_t = 0)$", "line_start": 60}, "docs/en/change-detection/change-point.md#section_6": {"doc_id": "docs/en/change-detection/change-point.md", "heading": "Algorithm/Model Sketch", "content": "**Offline Detection (PELT):**\n\n```python\ndef pelt(y, penalty, min_size=2):\n    n = len(y)\n    F = [0]  # F[t] = min cost for y[0:t]\n    cp = [[]]  # change-points for optimal segmentation\n\n    for t in range(1, n + 1):\n        candidates = []\n        for s in range(max(0, t - max_segments), t):\n            if t - s >= min_size:\n                cost = F[s] + segment_cost(y[s:t]) + penalty\n                candidates.append((cost, s))\n\n        best_cost, best_s = min(candidates)\n        F.append(best_cost)\n        cp.append(cp[best_s] + [best_s] if best_s > 0 else [])\n\n    return cp[-1]\n```\n\n**Online Detection (CUSUM):**\n\n```python\ndef cusum_online(y, mu0, k, h):\n    n = len(y)\n    C_plus, C_minus = 0, 0\n    alarms = []\n\n    for t in range(n):\n        C_plus = max(0, C_plus + y[t] - mu0 - k)\n        C_minus = max(0, C_minus - y[t] + mu0 - k)\n\n        if C_plus > h or C_minus > h:\n            alarms.append(t)\n            C_plus, C_minus = 0, 0  # Reset\n\n    return alarms\n```", "line_start": 68}, "docs/en/change-detection/change-point.md#section_7": {"doc_id": "docs/en/change-detection/change-point.md", "heading": "Common Pitfalls", "content": "1. **Penalty selection**: Too small → over-segmentation; too large → miss changes. Use BIC-based penalties or cross-validation.\n\n2. **Minimum segment length**: Very short segments are often noise. Enforce minimum size constraint.\n\n3. **Multiple testing**: Testing many potential change-points inflates false positives. Adjust thresholds.\n\n4. **Model misspecification**: Assuming wrong distribution (e.g., normal for heavy-tailed data) affects detection.\n\n5. **Gradual vs. abrupt changes**: Most methods assume sudden shifts. Gradual changes may appear as multiple small changes.\n\n6. **Online delay**: Online methods detect changes with delay. Trade-off between speed and accuracy.", "line_start": 111}, "docs/en/change-detection/change-point.md#section_8": {"doc_id": "docs/en/change-detection/change-point.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\nimport ruptures as rpt", "line_start": 125}, "docs/en/change-detection/change-point.md#section_9": {"doc_id": "docs/en/change-detection/change-point.md", "heading": "Generate data with 2 change-points", "content": "np.random.seed(42)\nn = 300", "line_start": 131}, "docs/en/change-detection/change-point.md#section_10": {"doc_id": "docs/en/change-detection/change-point.md", "heading": "Three segments with different means", "content": "y = np.concatenate([\n    np.random.randn(100) + 0,      # mean 0\n    np.random.randn(100) + 3,      # mean 3\n    np.random.randn(100) + 1       # mean 1\n])", "line_start": 135}, "docs/en/change-detection/change-point.md#section_11": {"doc_id": "docs/en/change-detection/change-point.md", "heading": "PELT detection", "content": "algo = rpt.Pelt(model=\"l2\", min_size=10).fit(y)\nchange_points = algo.predict(pen=10)\nprint(f\"Detected change-points: {change_points[:-1]}\")\nprint(f\"True change-points: [100, 200]\")", "line_start": 142}, "docs/en/change-detection/change-point.md#section_12": {"doc_id": "docs/en/change-detection/change-point.md", "heading": "Binary segmentation", "content": "algo_binseg = rpt.Binseg(model=\"l2\", min_size=10).fit(y)\nchange_points_bs = algo_binseg.predict(n_bkps=2)\nprint(f\"BinSeg change-points: {change_points_bs[:-1]}\")", "line_start": 148}, "docs/en/change-detection/change-point.md#section_13": {"doc_id": "docs/en/change-detection/change-point.md", "heading": "Using online Bayesian detection", "content": "from scipy.stats import norm\n\ndef bocpd_simple(y, hazard=0.01, mu0=0, sigma=1):\n    \"\"\"Simplified Bayesian Online CPD.\"\"\"\n    n = len(y)\n    R = np.zeros((n + 1, n + 1))  # Run length probabilities\n    R[0, 0] = 1\n\n    for t in range(1, n + 1):\n        # Predictive probability under each run length\n        predprob = norm.pdf(y[t-1], mu0, sigma)  # Simplified\n\n        # Growth probability\n        R[t, 1:t+1] = R[t-1, :t] * predprob * (1 - hazard)\n        # Change probability\n        R[t, 0] = np.sum(R[t-1, :t] * predprob * hazard)\n        # Normalize\n        R[t, :] /= R[t, :].sum()\n\n    return R\n\nR = bocpd_simple(y, hazard=0.01, mu0=1, sigma=1.5)\nprint(f\"Max probability run length at end: {np.argmax(R[-1])}\")\n```", "line_start": 154}, "docs/en/change-detection/change-point.md#section_14": {"doc_id": "docs/en/change-detection/change-point.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What is the trade-off between online and offline change-point detection?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Online detection:**\n- ✓ Real-time alerts\n- ✓ No need for full data\n- ✗ Detection delay (need evidence to accumulate)\n- ✗ Can't revise past decisions\n- Use case: Process monitoring, fraud detection\n\n**Offline detection:**\n- ✓ Uses all data for optimal segmentation\n- ✓ Can find exact change locations\n- ✓ More accurate (global optimization)\n- ✗ Not real-time\n- Use case: Historical analysis, model building\n\n**Hybrid:** Some methods (e.g., Bayesian online) can be run offline for retrospective analysis while also providing online capability.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using online methods for offline analysis. If all data is available, use PELT or exact methods for better accuracy.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> How do you choose the penalty parameter in PELT or similar methods?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Several approaches:\n\n1. **BIC-style penalty:** $\\beta = \\log(n) \\times k$ where k = parameters per segment\n   - Theoretically justified\n   - Can be conservative\n\n2. **Cross-validation:**\n   - Hold out data, select penalty minimizing prediction error\n   - Computationally intensive\n\n3. **Elbow method:**\n   - Plot cost vs. number of change-points\n   - Select \"elbow\" where diminishing returns begin\n\n4. **Domain knowledge:**\n   - Expected number of changes\n   - Cost of false positives vs. missed detections\n\n**SIC/MBIC:** Modified BIC for change-point specific context.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using fixed penalty across different datasets. Optimal penalty depends on signal-to-noise ratio, segment lengths, and number of observations.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Derive the expected value of CUSUM under H₀ (no change) and H₁ (mean shift).</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong>\n\n**Under H₀:** $y_t \\sim N(\\mu_0, \\sigma^2)$\n$$E[S_t] = E\\left[\\sum_{i=1}^{t}(y_i - \\mu_0)\\right] = \\sum_{i=1}^{t}E[y_i - \\mu_0] = 0$$\n\nCUSUM fluctuates around 0 (random walk behavior).\n\n**Under H₁:** Mean shifts to $\\mu_1$ at time $\\tau$\n$$E[S_t] = \\sum_{i=1}^{\\tau-1}(\\mu_0 - \\mu_0) + \\sum_{i=\\tau}^{t}(\\mu_1 - \\mu_0) = (t - \\tau + 1)(\\mu_1 - \\mu_0)$$\n\nAfter change, CUSUM drifts linearly away from 0 at rate $(\\mu_1 - \\mu_0)$.\n\n**Key insight:** The drift rate equals the mean shift magnitude, making CUSUM sensitive to sustained changes.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> CUSUM assumes known pre-change mean $\\mu_0$. If estimated, use standardized CUSUM or adjust thresholds.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Why does binary segmentation not guarantee finding the optimal solution?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Binary segmentation is greedy—it makes locally optimal choices without considering global structure.\n\n**Problem scenario:**\nConsider data with two close change-points:\n- Segment 1: mean 0, length 80\n- Segment 2: mean 2, length 20\n- Segment 3: mean 0, length 100\n\nBinary segmentation first finds the \"best\" single split, which might be around position 100 (between segments 2 and 3). Then it searches [0,100] and [100,200] separately.\n\nBut the small segment 2 might be better detected by finding BOTH change-points (80 and 100) together.\n\n**Solution:** Use exact methods (PELT, optimal partitioning) that consider all possible segmentations.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Assuming binary segmentation is \"good enough.\" For complex signals with varying segment sizes, exact methods can be significantly better.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You're monitoring server response times. You detect a \"change-point\" but it turns out to be a single outlier. How do you prevent false alarms from outliers?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Several strategies:\n\n1. **Robust cost functions:**\n   - Use L1 (absolute) instead of L2 (squared)\n   - Huber loss\n   - Median-based detection\n\n2. **Minimum segment length:**\n   - Require at least k observations in each segment\n   - Single outliers can't form segments\n\n3. **Pre-filtering:**\n   - Apply median filter or outlier removal first\n   - Then detect changes\n\n4. **Multi-scale detection:**\n   - Detect at multiple resolutions\n   - True changes appear at all scales; outliers don't\n\n5. **Confirmation period:**\n   - Don't alarm on first deviation\n   - Require sustained change (e.g., CUSUM with appropriate k)\n\n6. **Model-based:**\n   - Use models that explicitly include outlier component\n   - Separate outliers from level shifts\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using squared error cost with heavy-tailed data. Single large values dominate and trigger false change-points.\n</div>\n</div>\n</details>", "line_start": 180}, "docs/en/change-detection/change-point.md#section_15": {"doc_id": "docs/en/change-detection/change-point.md", "heading": "References", "content": "1. Truong, C., Oudre, L., & Vayatis, N. (2020). Selective review of offline change point detection methods. *Signal Processing*, 167, 107299.\n2. Killick, R., Fearnhead, P., & Eckley, I. A. (2012). Optimal detection of changepoints with a linear computational cost. *JASA*, 107(500), 1590-1598.\n3. Adams, R. P., & MacKay, D. J. (2007). Bayesian online changepoint detection. *arXiv:0710.3742*.\n4. Page, E. S. (1954). Continuous inspection schemes. *Biometrika*, 41(1/2), 100-115.", "line_start": 325}, "docs/en/foundations/autocorrelation.md#section_0": {"doc_id": "docs/en/foundations/autocorrelation.md", "heading": "Autocorrelation and Partial Autocorrelation", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> ACF measures correlation between a series and its lagged values. PACF measures correlation at lag $k$ after removing effects of intermediate lags. ACF/PACF patterns identify AR/MA orders: AR(p) has PACF cutoff at lag p; MA(q) has ACF cutoff at lag q. Sample ACF/PACF have approximate standard error $1/\\sqrt{n}$ under white noise.\n</div>", "line_start": 1}, "docs/en/foundations/autocorrelation.md#section_1": {"doc_id": "docs/en/foundations/autocorrelation.md", "heading": "Core Definitions", "content": "**Autocovariance Function (ACVF)**: For a stationary process with mean $\\mu$:\n$$\\gamma(h) = \\text{Cov}(X_t, X_{t+h}) = E[(X_t - \\mu)(X_{t+h} - \\mu)]$$\n\n**Autocorrelation Function (ACF)**: Normalized autocovariance:\n$$\\rho(h) = \\frac{\\gamma(h)}{\\gamma(0)} = \\text{Corr}(X_t, X_{t+h})$$\n\n**Partial Autocorrelation Function (PACF)**: Correlation between $X_t$ and $X_{t+h}$ after removing the linear dependence on $X_{t+1}, \\ldots, X_{t+h-1}$:\n$$\\phi_{hh} = \\text{Corr}(X_t - \\hat{X}_t, X_{t+h} - \\hat{X}_{t+h})$$\n\nwhere $\\hat{X}_t$ and $\\hat{X}_{t+h}$ are best linear predictors from intermediate values.\n\n**Sample ACF**: Estimated from data:\n$$\\hat{\\rho}(h) = \\frac{\\hat{\\gamma}(h)}{\\hat{\\gamma}(0)} = \\frac{\\sum_{t=1}^{n-h}(X_t - \\bar{X})(X_{t+h} - \\bar{X})}{\\sum_{t=1}^{n}(X_t - \\bar{X})^2}$$", "line_start": 7}, "docs/en/foundations/autocorrelation.md#section_2": {"doc_id": "docs/en/foundations/autocorrelation.md", "heading": "Properties of ACF", "content": "For any stationary process:\n\n1. $\\rho(0) = 1$\n2. $\\rho(h) = \\rho(-h)$ (symmetry)\n3. $|\\rho(h)| \\leq 1$ for all $h$\n4. $\\rho(h)$ is positive semi-definite: for any $a_1, \\ldots, a_n$:\n   $$\\sum_{i=1}^{n}\\sum_{j=1}^{n} a_i a_j \\rho(i-j) \\geq 0$$", "line_start": 25}, "docs/en/foundations/autocorrelation.md#section_3": {"doc_id": "docs/en/foundations/autocorrelation.md", "heading": "PACF via Yule-Walker Equations", "content": "The PACF at lag $k$ is the last coefficient $\\phi_{kk}$ in the AR(k) regression:\n$$X_t = \\phi_{k1}X_{t-1} + \\phi_{k2}X_{t-2} + \\cdots + \\phi_{kk}X_{t-k} + \\epsilon_t$$\n\nYule-Walker equations in matrix form:\n$$\\begin{pmatrix} 1 & \\rho(1) & \\cdots & \\rho(k-1) \\\\ \\rho(1) & 1 & \\cdots & \\rho(k-2) \\\\ \\vdots & & \\ddots & \\vdots \\\\ \\rho(k-1) & \\cdots & \\rho(1) & 1 \\end{pmatrix} \\begin{pmatrix} \\phi_{k1} \\\\ \\phi_{k2} \\\\ \\vdots \\\\ \\phi_{kk} \\end{pmatrix} = \\begin{pmatrix} \\rho(1) \\\\ \\rho(2) \\\\ \\vdots \\\\ \\rho(k) \\end{pmatrix}$$", "line_start": 35}, "docs/en/foundations/autocorrelation.md#section_4": {"doc_id": "docs/en/foundations/autocorrelation.md", "heading": "ACF of AR(1): $X_t = \\phi X_{t-1} + \\epsilon_t$", "content": "$$\\rho(h) = \\phi^{|h|}$$\n\nACF decays exponentially (geometrically) for $|\\phi| < 1$.", "line_start": 43}, "docs/en/foundations/autocorrelation.md#section_5": {"doc_id": "docs/en/foundations/autocorrelation.md", "heading": "ACF of MA(1): $X_t = \\epsilon_t + \\theta\\epsilon_{t-1}$", "content": "$$\\rho(1) = \\frac{\\theta}{1+\\theta^2}, \\quad \\rho(h) = 0 \\text{ for } h > 1$$\n\nACF cuts off after lag 1.", "line_start": 49}, "docs/en/foundations/autocorrelation.md#section_6": {"doc_id": "docs/en/foundations/autocorrelation.md", "heading": "ACF of MA(q)", "content": "$$\\rho(h) = 0 \\text{ for } h > q$$", "line_start": 55}, "docs/en/foundations/autocorrelation.md#section_7": {"doc_id": "docs/en/foundations/autocorrelation.md", "heading": "PACF of AR(p)", "content": "$$\\phi_{hh} = 0 \\text{ for } h > p$$", "line_start": 59}, "docs/en/foundations/autocorrelation.md#section_8": {"doc_id": "docs/en/foundations/autocorrelation.md", "heading": "Variance of Sample ACF", "content": "Under the null hypothesis that the true process is white noise:\n$$\\text{Var}(\\hat{\\rho}(h)) \\approx \\frac{1}{n}$$\n\nSo approximate 95% confidence bands are $\\pm 1.96/\\sqrt{n}$.\n\n**Bartlett's formula** (for MA(q) process):\n$$\\text{Var}(\\hat{\\rho}(h)) \\approx \\frac{1}{n}\\left(1 + 2\\sum_{k=1}^{q}\\rho(k)^2\\right) \\text{ for } h > q$$", "line_start": 63}, "docs/en/foundations/autocorrelation.md#section_9": {"doc_id": "docs/en/foundations/autocorrelation.md", "heading": "Algorithm/Model Sketch", "content": "**Using ACF/PACF for Model Identification:**\n\n| Pattern | ACF | PACF | Model |\n|---------|-----|------|-------|\n| AR(p) | Exponential/sinusoidal decay | Cuts off after lag p | AR(p) |\n| MA(q) | Cuts off after lag q | Exponential/sinusoidal decay | MA(q) |\n| ARMA(p,q) | Tails off | Tails off | ARMA(p,q) |\n| White noise | All near zero | All near zero | No model needed |\n| Non-stationary | Very slow decay | Large spike at lag 1 | Difference first |\n\n**Interpretation procedure:**\n\n```\n1. Plot series - check for stationarity\n2. If non-stationary, difference until stationary\n3. Compute sample ACF and PACF\n4. Check for significant spikes (outside ±1.96/√n bands)\n5. Identify patterns:\n   - ACF cuts off, PACF decays → MA(q) where q = cutoff lag\n   - PACF cuts off, ACF decays → AR(p) where p = cutoff lag\n   - Both decay → ARMA (use information criteria)\n6. Fit candidate models\n7. Check residual ACF/PACF (should be white noise)\n```", "line_start": 73}, "docs/en/foundations/autocorrelation.md#section_10": {"doc_id": "docs/en/foundations/autocorrelation.md", "heading": "Common Pitfalls", "content": "1. **Ignoring confidence bands**: Not all spikes are significant. Use $\\pm 1.96/\\sqrt{n}$ bands and expect ~5% of spikes to exceed by chance.\n\n2. **Confusing \"cuts off\" vs \"tails off\"**: Cutoff means abrupt drop to zero after lag q. Tails off means gradual decay. This distinction determines AR vs MA.\n\n3. **Applying ACF/PACF to non-stationary data**: Results are meaningless for non-stationary series. Always check stationarity first.\n\n4. **Over-interpreting high-lag correlations**: For small samples, high-lag estimates have high variance. Focus on early lags.\n\n5. **Forgetting seasonal lags**: In seasonal data, check lags at seasonal period (e.g., lag 12 for monthly data with annual seasonality).\n\n6. **Neglecting theoretical ACF/PACF**: When validating models, compare sample functions to theoretical ones, not just residuals.", "line_start": 100}, "docs/en/foundations/autocorrelation.md#section_11": {"doc_id": "docs/en/foundations/autocorrelation.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.arima_process import ArmaProcess", "line_start": 114}, "docs/en/foundations/autocorrelation.md#section_12": {"doc_id": "docs/en/foundations/autocorrelation.md", "heading": "Simulate AR(2) process", "content": "np.random.seed(42)\nar_params = np.array([1, -0.75, 0.25])  # 1 - 0.75L + 0.25L^2\nma_params = np.array([1])\nar2_process = ArmaProcess(ar_params, ma_params)\nar2_data = ar2_process.generate_sample(nsample=300)", "line_start": 122}, "docs/en/foundations/autocorrelation.md#section_13": {"doc_id": "docs/en/foundations/autocorrelation.md", "heading": "Plot ACF and PACF", "content": "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\nplot_acf(ar2_data, ax=axes[0], lags=20, title='ACF of AR(2)')\nplot_pacf(ar2_data, ax=axes[1], lags=20, title='PACF of AR(2)')\nplt.tight_layout()\nplt.show()", "line_start": 129}, "docs/en/foundations/autocorrelation.md#section_14": {"doc_id": "docs/en/foundations/autocorrelation.md", "heading": "Check significant PACF values", "content": "from statsmodels.tsa.stattools import pacf\npacf_values = pacf(ar2_data, nlags=5)\nprint(\"PACF values:\", pacf_values)", "line_start": 137}, "docs/en/foundations/autocorrelation.md#section_15": {"doc_id": "docs/en/foundations/autocorrelation.md", "heading": "Expect: significant at lags 1,2; near zero after", "content": "```", "line_start": 141}, "docs/en/foundations/autocorrelation.md#section_16": {"doc_id": "docs/en/foundations/autocorrelation.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What is the fundamental difference between ACF and PACF? Why do we need both?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> ACF measures total correlation between $X_t$ and $X_{t+h}$, including indirect effects through intermediate lags. PACF measures direct correlation after removing intermediate effects.\n\n<strong>Explanation:</strong> Consider AR(1) with $\\phi = 0.8$. The ACF shows $\\rho(2) = 0.64$ because $X_t$ and $X_{t+2}$ are correlated through $X_{t+1}$. But the PACF at lag 2 is near zero because once we account for $X_{t+1}$, there's no additional direct relationship.\n\nWe need both because:\n- ACF identifies MA order (cuts off at lag q)\n- PACF identifies AR order (cuts off at lag p)\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using only ACF for identification. Without PACF, you cannot distinguish AR from MA patterns—both can show decaying ACF, but only AR shows PACF cutoff.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> How would you interpret an ACF that shows a very slow decay with values staying significant past lag 20?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> This strongly suggests non-stationarity. A stationary process should have ACF that decays relatively quickly to zero. Very slow decay indicates a unit root or near-unit root.\n\n<strong>Recommended action:</strong>\n1. Formally test with ADF/KPSS\n2. Difference the series\n3. Re-compute ACF after differencing\n4. The differenced series should show faster decay\n\n**Key insight:** For a random walk, $\\rho(h) \\approx 1$ for all $h$ in finite samples because successive values are highly dependent.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Trying to fit ARMA models to non-stationary data. The resulting parameters will be misleading, and forecasts will be poor. Always ensure stationarity first.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Derive the ACF for MA(1): $X_t = \\epsilon_t + \\theta\\epsilon_{t-1}$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> $\\rho(0) = 1$, $\\rho(1) = \\frac{\\theta}{1+\\theta^2}$, $\\rho(h) = 0$ for $h \\geq 2$.\n\n<strong>Derivation:</strong>\n\n**Variance (lag 0):**\n$$\\gamma(0) = \\text{Var}(X_t) = \\text{Var}(\\epsilon_t + \\theta\\epsilon_{t-1}) = \\sigma^2 + \\theta^2\\sigma^2 = (1+\\theta^2)\\sigma^2$$\n\n**Autocovariance at lag 1:**\n$$\\gamma(1) = \\text{Cov}(X_t, X_{t+1}) = \\text{Cov}(\\epsilon_t + \\theta\\epsilon_{t-1}, \\epsilon_{t+1} + \\theta\\epsilon_t)$$\n$$= \\text{Cov}(\\epsilon_t, \\theta\\epsilon_t) = \\theta\\sigma^2$$\n\n**Autocovariance at lag $h \\geq 2$:**\n$$\\gamma(h) = \\text{Cov}(\\epsilon_t + \\theta\\epsilon_{t-1}, \\epsilon_{t+h} + \\theta\\epsilon_{t+h-1}) = 0$$\n\n(No overlapping $\\epsilon$ terms when $h \\geq 2$)\n\n**ACF:**\n$$\\rho(1) = \\frac{\\gamma(1)}{\\gamma(0)} = \\frac{\\theta\\sigma^2}{(1+\\theta^2)\\sigma^2} = \\frac{\\theta}{1+\\theta^2}$$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Note that $|\\rho(1)| \\leq 0.5$ for MA(1). The maximum occurs at $\\theta = \\pm 1$. If you observe $|\\hat{\\rho}(1)| > 0.5$, it might be AR, not MA.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> For an AR(2) process $X_t = \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + \\epsilon_t$, show that the PACF is zero for all lags $h > 2$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> For AR(p), the PACF $\\phi_{hh} = 0$ for $h > p$ because once $X_{t-1}, \\ldots, X_{t-p}$ are included, adding more lags provides no additional predictive information.\n\n<strong>Explanation:</strong>\n\nThe AR(2) model states that $X_t$ depends only on $X_{t-1}$ and $X_{t-2}$ (plus noise). Therefore:\n\nFor $h = 3$: We regress $X_t$ on $X_{t-1}, X_{t-2}, X_{t-3}$.\n- $X_t = \\phi_1 X_{t-1} + \\phi_2 X_{t-2} + \\epsilon_t$\n- $X_{t-3}$ only affects $X_t$ through $X_{t-2}$ and $X_{t-1}$\n- After conditioning on $X_{t-1}$ and $X_{t-2}$, $X_{t-3}$ adds no information\n- Thus $\\phi_{33} = 0$\n\nBy induction, this holds for all $h > 2$.\n\n**Key equation:** PACF at lag $k$ is the coefficient $\\phi_{kk}$ in the best linear predictor using exactly $k$ lags. For AR(p), the $k$-th coefficient becomes zero when $k > p$.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Expecting exact zeros in sample PACF. Due to sampling variability, $\\hat{\\phi}_{hh}$ will be nonzero but should fall within confidence bands for $h > p$.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You compute the sample ACF for a time series with $n=100$ observations. You see that lags 1, 2, 7, and 15 exceed the 95% confidence bands. What is your interpretation?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> With 95% bands and ~15-20 lags tested, expect about 1 spurious significant lag by chance. Lags 1 and 2 are likely real signal. Lag 7 might be real (check for weekly patterns if relevant). Lag 15 is likely spurious.\n\n<strong>Interpretation process:</strong>\n1. Focus on early lags (1, 2, 3) — most likely real\n2. Consider domain knowledge (lag 7 = weekly? lag 12 = monthly?)\n3. Isolated high-lag spikes are often noise\n4. Pattern of significant lags matters more than individual spikes\n5. Lag 15 with $n=100$ has high variance: $\\text{SE} \\approx 1/\\sqrt{100} = 0.1$, and only ~85 pairs contribute\n\n**Confidence band:** $\\pm 1.96/\\sqrt{100} = \\pm 0.196$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Treating every significant lag as meaningful. With many lags, false positives occur. Use sequential testing corrections or focus on meaningful patterns, not isolated spikes.\n</div>\n</div>\n</details>", "line_start": 144}, "docs/en/foundations/autocorrelation.md#section_17": {"doc_id": "docs/en/foundations/autocorrelation.md", "heading": "References", "content": "1. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). *Time Series Analysis: Forecasting and Control*. Wiley. Chapter 2.\n2. Shumway, R. H., & Stoffer, D. S. (2017). *Time Series Analysis and Its Applications*. Springer. Chapter 3.\n3. Brockwell, P. J., & Davis, R. A. (2016). *Introduction to Time Series and Forecasting*. Springer. Chapters 2-3.\n4. Bartlett, M. S. (1946). On the theoretical specification and sampling properties of autocorrelated time-series. *JRSS B*, 8(1), 27-41.", "line_start": 260}, "docs/en/foundations/stationarity.md#section_0": {"doc_id": "docs/en/foundations/stationarity.md", "heading": "Stationarity", "content": "<div class=\"interview-summary\">\n<strong>Interview Summary:</strong> Stationarity is the foundational assumption for most classical time series models. A stationary process has constant mean, constant variance, and autocovariance that depends only on lag, not time. Weak (covariance) stationarity is usually sufficient. Test with ADF, KPSS, or PP tests. Non-stationary series can often be made stationary through differencing.\n</div>", "line_start": 1}, "docs/en/foundations/stationarity.md#section_1": {"doc_id": "docs/en/foundations/stationarity.md", "heading": "Core Definitions", "content": "**Strict (Strong) Stationarity**: A process $\\{X_t\\}$ is strictly stationary if the joint distribution of $(X_{t_1}, X_{t_2}, \\ldots, X_{t_k})$ is identical to $(X_{t_1+h}, X_{t_2+h}, \\ldots, X_{t_k+h})$ for all $k$, all time points $t_1, \\ldots, t_k$, and all shifts $h$.\n\n**Weak (Covariance/Second-Order) Stationarity**: A process is weakly stationary if:\n\n1. $E[X_t] = \\mu$ (constant mean, finite)\n2. $\\text{Var}(X_t) = \\sigma^2 < \\infty$ (constant variance, finite)\n3. $\\text{Cov}(X_t, X_{t+h}) = \\gamma(h)$ (autocovariance depends only on lag $h$)\n\n**Ergodicity** (high-level): An ergodic process allows time averages to converge to ensemble averages. This justifies estimating population parameters from a single realization. Most stationary processes encountered in practice are ergodic.\n\n**Trend Stationarity vs. Difference Stationarity**:\n\n- **Trend stationary**: $X_t = \\mu_t + Y_t$ where $Y_t$ is stationary; remove trend by regression\n- **Difference stationary**: $\\Delta X_t = X_t - X_{t-1}$ is stationary; remove unit root by differencing", "line_start": 7}, "docs/en/foundations/stationarity.md#section_2": {"doc_id": "docs/en/foundations/stationarity.md", "heading": "Autocovariance Function", "content": "For a weakly stationary process:\n\n$$\\gamma(h) = \\text{Cov}(X_t, X_{t+h}) = E[(X_t - \\mu)(X_{t+h} - \\mu)]$$\n\nProperties:\n- $\\gamma(0) = \\text{Var}(X_t) = \\sigma^2$\n- $\\gamma(h) = \\gamma(-h)$ (symmetry)\n- $|\\gamma(h)| \\leq \\gamma(0)$ (Cauchy-Schwarz)", "line_start": 26}, "docs/en/foundations/stationarity.md#section_3": {"doc_id": "docs/en/foundations/stationarity.md", "heading": "Autocorrelation Function (ACF)", "content": "$$\\rho(h) = \\frac{\\gamma(h)}{\\gamma(0)} = \\frac{\\text{Cov}(X_t, X_{t+h})}{\\text{Var}(X_t)}$$\n\nProperties:\n- $\\rho(0) = 1$\n- $|\\rho(h)| \\leq 1$\n- $\\rho(h) = \\rho(-h)$", "line_start": 37}, "docs/en/foundations/stationarity.md#section_4": {"doc_id": "docs/en/foundations/stationarity.md", "heading": "Unit Root and Integration", "content": "A process has a unit root if $(1-L)X_t$ is stationary where $L$ is the lag operator ($LX_t = X_{t-1}$).\n\n**Random Walk** (unit root example):\n$$X_t = X_{t-1} + \\epsilon_t$$\n\nThis is non-stationary: $\\text{Var}(X_t) = t\\sigma^2_\\epsilon \\to \\infty$.\n\nAfter differencing: $\\Delta X_t = \\epsilon_t$ which is stationary.", "line_start": 46}, "docs/en/foundations/stationarity.md#section_5": {"doc_id": "docs/en/foundations/stationarity.md", "heading": "Augmented Dickey-Fuller (ADF) Test", "content": "Tests for unit root. Model:\n$$\\Delta X_t = \\alpha + \\beta t + \\gamma X_{t-1} + \\sum_{i=1}^{p} \\delta_i \\Delta X_{t-i} + \\epsilon_t$$\n\n- $H_0$: $\\gamma = 0$ (unit root exists, non-stationary)\n- $H_1$: $\\gamma < 0$ (no unit root, stationary)\n\n**KPSS Test** (complementary):\n- $H_0$: Series is stationary\n- $H_1$: Series has a unit root\n\nUse both ADF and KPSS for robust conclusions.", "line_start": 57}, "docs/en/foundations/stationarity.md#section_6": {"doc_id": "docs/en/foundations/stationarity.md", "heading": "Algorithm/Model Sketch", "content": "**Testing for Stationarity:**\n\n```\n1. Visual inspection: plot series, look for trends/changing variance\n2. ACF plot: stationary series have ACF that decays to zero\n3. ADF test: reject H0 → stationary\n4. KPSS test: fail to reject H0 → stationary\n5. If non-stationary:\n   - Try differencing (for unit root)\n   - Try detrending (for trend stationarity)\n   - Check if seasonal differencing needed\n```\n\n**Making a Series Stationary:**\n\n| Symptom | Solution |\n|---------|----------|\n| Trend (linear) | First difference or detrend |\n| Trend (quadratic) | Second difference |\n| Seasonality | Seasonal difference |\n| Changing variance | Log transform, then difference |\n| Both trend and seasonality | Combine transformations |", "line_start": 71}, "docs/en/foundations/stationarity.md#section_7": {"doc_id": "docs/en/foundations/stationarity.md", "heading": "Common Pitfalls", "content": "1. **Confusing strict and weak stationarity**: Weak is usually sufficient for ARIMA modeling. Strict is rarely tested directly.\n\n2. **Over-differencing**: Differencing a stationary series introduces unnecessary MA structure. Check ACF—if it's already decaying, don't difference.\n\n3. **Ignoring structural breaks**: A series with a structural break may appear non-stationary but differencing won't help. Consider regime-switching models.\n\n4. **Misinterpreting ADF p-values**: ADF tests for unit root, not stationarity. Low p-value rejects unit root (suggests stationarity). Also use KPSS for confirmation.\n\n5. **Neglecting variance stationarity**: A series can have constant mean but changing variance (heteroskedasticity). Consider GARCH models or transformations.\n\n6. **Seasonal unit roots**: Standard ADF doesn't detect seasonal unit roots. Use HEGY test or seasonal differencing.", "line_start": 96}, "docs/en/foundations/stationarity.md#section_8": {"doc_id": "docs/en/foundations/stationarity.md", "heading": "Mini Example", "content": "```python\nimport numpy as np\nfrom statsmodels.tsa.stattools import adfuller, kpss", "line_start": 110}, "docs/en/foundations/stationarity.md#section_9": {"doc_id": "docs/en/foundations/stationarity.md", "heading": "Generate random walk (non-stationary)", "content": "np.random.seed(42)\nrandom_walk = np.cumsum(np.random.randn(200))", "line_start": 116}, "docs/en/foundations/stationarity.md#section_10": {"doc_id": "docs/en/foundations/stationarity.md", "heading": "Generate stationary AR(1)", "content": "ar1 = np.zeros(200)\nfor t in range(1, 200):\n    ar1[t] = 0.7 * ar1[t-1] + np.random.randn()", "line_start": 120}, "docs/en/foundations/stationarity.md#section_11": {"doc_id": "docs/en/foundations/stationarity.md", "heading": "ADF test", "content": "adf_rw = adfuller(random_walk)\nprint(f\"Random Walk ADF p-value: {adf_rw[1]:.4f}\")  # High → non-stationary\n\nadf_ar = adfuller(ar1)\nprint(f\"AR(1) ADF p-value: {adf_ar[1]:.4f}\")  # Low → stationary", "line_start": 125}, "docs/en/foundations/stationarity.md#section_12": {"doc_id": "docs/en/foundations/stationarity.md", "heading": "First difference of random walk", "content": "diff_rw = np.diff(random_walk)\nadf_diff = adfuller(diff_rw)\nprint(f\"Differenced RW ADF p-value: {adf_diff[1]:.4f}\")  # Low → stationary\n```", "line_start": 132}, "docs/en/foundations/stationarity.md#section_13": {"doc_id": "docs/en/foundations/stationarity.md", "heading": "Quiz", "content": "<details class=\"quiz\">\n<summary><strong>Q1 (Conceptual):</strong> What is the difference between strict and weak stationarity? When is weak stationarity sufficient?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Strict stationarity requires the entire joint distribution to be invariant to time shifts. Weak stationarity only requires constant mean, constant variance, and autocovariance depending only on lag.\n\n<strong>Explanation:</strong> Weak stationarity is sufficient for ARIMA-type models because these models only use first and second moments (mean and covariances). The full distributional properties aren't needed for parameter estimation or forecasting.\n\n**Key point:** If a process is strictly stationary with finite second moments, it is also weakly stationary. The reverse is not always true.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Assuming weak stationarity implies Gaussianity. A weakly stationary process can have any marginal distribution—it only constrains the first two moments.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q2 (Conceptual):</strong> Explain the difference between trend stationarity and difference stationarity. How do you handle each?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> Trend stationary: the series has a deterministic trend; subtract the trend to get a stationary residual. Difference stationary: the series has a stochastic trend (unit root); differencing removes the non-stationarity.\n\n<strong>Explanation:</strong>\n- Trend stationary: $X_t = \\alpha + \\beta t + Y_t$ where $Y_t$ is stationary. Fit trend and subtract.\n- Difference stationary: $X_t = X_{t-1} + \\epsilon_t$. First difference: $\\Delta X_t = \\epsilon_t$.\n\nApplying the wrong transformation is inefficient—differencing a trend-stationary series adds MA(1) structure; detrending a difference-stationary series leaves autocorrelation.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Using differencing for everything. Always plot the series and consider whether the trend looks deterministic or stochastic.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q3 (Math):</strong> Show that for a random walk $X_t = X_{t-1} + \\epsilon_t$, the variance grows linearly with time.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> $\\text{Var}(X_t) = t \\cdot \\sigma^2_\\epsilon$\n\n<strong>Derivation:</strong>\nStarting from $X_0 = 0$:\n$$X_t = \\sum_{i=1}^{t} \\epsilon_i$$\n\nSince $\\epsilon_i$ are independent with variance $\\sigma^2_\\epsilon$:\n$$\\text{Var}(X_t) = \\text{Var}\\left(\\sum_{i=1}^{t} \\epsilon_i\\right) = \\sum_{i=1}^{t} \\text{Var}(\\epsilon_i) = t \\cdot \\sigma^2_\\epsilon$$\n\nThis shows variance grows without bound, violating weak stationarity.\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Forgetting that cumulative sums of stationary processes are generally non-stationary. Integration (summation) and differentiation have opposite effects on stationarity.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q4 (Math):</strong> Derive the autocorrelation function for an AR(1) process $X_t = \\phi X_{t-1} + \\epsilon_t$ where $|\\phi| < 1$.</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> $\\rho(h) = \\phi^{|h|}$\n\n<strong>Derivation:</strong>\nFor stationarity, multiply both sides by $X_{t-h}$ and take expectations:\n$$E[X_t X_{t-h}] = \\phi E[X_{t-1} X_{t-h}] + E[\\epsilon_t X_{t-h}]$$\n\nSince $\\epsilon_t$ is uncorrelated with past values:\n$$\\gamma(h) = \\phi \\gamma(h-1)$$ for $h \\geq 1$\n\nThis is a first-order recurrence with solution:\n$$\\gamma(h) = \\phi^h \\gamma(0)$$\n\nTherefore:\n$$\\rho(h) = \\frac{\\gamma(h)}{\\gamma(0)} = \\phi^h$$\n\nFor $h < 0$, use symmetry: $\\rho(h) = \\phi^{|h|}$\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Forgetting the stationarity condition $|\\phi| < 1$. If $|\\phi| \\geq 1$, the process explodes and has no finite variance.\n</div>\n</div>\n</details>\n\n<details class=\"quiz\">\n<summary><strong>Q5 (Practical):</strong> You run an ADF test and get p-value = 0.08, and a KPSS test with p-value = 0.03. What do you conclude? What should you do?</summary>\n\n<div class=\"answer\">\n<strong>Answer:</strong> The results are contradictory. ADF suggests possible stationarity (p = 0.08 is borderline), but KPSS rejects stationarity (p = 0.03). This often indicates the series is near-unit-root or has a structural break.\n\n<strong>Recommended actions:</strong>\n1. Plot the series and ACF to visually inspect\n2. Try differencing and re-test\n3. Check for structural breaks (Chow test, CUSUM)\n4. Consider that the series may be fractionally integrated\n5. Use domain knowledge—is non-stationarity expected?\n\n**Key equation:** For KPSS, low p-value rejects stationarity. For ADF, low p-value rejects unit root (supports stationarity).\n\n<div class=\"pitfall\">\n<strong>Common pitfall:</strong> Relying on a single test. ADF has low power against near-unit-root alternatives. KPSS can reject stationarity due to serial correlation. Always use multiple approaches and visual inspection.\n</div>\n</div>\n</details>", "line_start": 138}, "docs/en/foundations/stationarity.md#section_14": {"doc_id": "docs/en/foundations/stationarity.md", "heading": "References", "content": "1. Hamilton, J. D. (1994). *Time Series Analysis*. Princeton University Press. Chapters 3, 17.\n2. Brockwell, P. J., & Davis, R. A. (2016). *Introduction to Time Series and Forecasting*. Springer. Chapter 1.\n3. Dickey, D. A., & Fuller, W. A. (1979). Distribution of the estimators for autoregressive time series with a unit root. *JASA*, 74(366), 427-431.\n4. Kwiatkowski, D., Phillips, P. C., Schmidt, P., & Shin, Y. (1992). Testing the null hypothesis of stationarity. *Journal of Econometrics*, 54(1-3), 159-178.", "line_start": 243}}, "bm25": {"doc_lengths": {"docs/en/index.md": 314, "docs/en/index.md#section_0": 30, "docs/en/index.md#section_1": 101, "docs/en/index.md#section_2": 45, "docs/en/index.md#section_3": 87, "docs/en/index.md#section_4": 22, "docs/en/index.md#section_5": 25, "docs/en/spectral/spectral-analysis.md": 1032, "docs/en/spectral/spectral-analysis.md#section_0": 50, "docs/en/spectral/spectral-analysis.md#section_1": 53, "docs/en/spectral/spectral-analysis.md#section_2": 36, "docs/en/spectral/spectral-analysis.md#section_3": 31, "docs/en/spectral/spectral-analysis.md#section_4": 17, "docs/en/spectral/spectral-analysis.md#section_5": 19, "docs/en/spectral/spectral-analysis.md#section_6": 69, "docs/en/spectral/spectral-analysis.md#section_7": 78, "docs/en/spectral/spectral-analysis.md#section_8": 13, "docs/en/spectral/spectral-analysis.md#section_9": 11, "docs/en/spectral/spectral-analysis.md#section_10": 27, "docs/en/spectral/spectral-analysis.md#section_11": 3, "docs/en/spectral/spectral-analysis.md#section_12": 7, "docs/en/spectral/spectral-analysis.md#section_13": 24, "docs/en/spectral/spectral-analysis.md#section_14": 7, "docs/en/spectral/spectral-analysis.md#section_15": 546, "docs/en/spectral/spectral-analysis.md#section_16": 37, "docs/en/multivariate/granger-causality.md": 975, "docs/en/multivariate/granger-causality.md#section_0": 47, "docs/en/multivariate/granger-causality.md#section_1": 33, "docs/en/multivariate/granger-causality.md#section_2": 16, "docs/en/multivariate/granger-causality.md#section_3": 34, "docs/en/multivariate/granger-causality.md#section_4": 21, "docs/en/multivariate/granger-causality.md#section_5": 17, "docs/en/multivariate/granger-causality.md#section_6": 86, "docs/en/multivariate/granger-causality.md#section_7": 74, "docs/en/multivariate/granger-causality.md#section_8": 16, "docs/en/multivariate/granger-causality.md#section_9": 11, "docs/en/multivariate/granger-causality.md#section_10": 8, "docs/en/multivariate/granger-causality.md#section_11": 10, "docs/en/multivariate/granger-causality.md#section_12": 4, "docs/en/multivariate/granger-causality.md#section_13": 19, "docs/en/multivariate/granger-causality.md#section_14": 23, "docs/en/multivariate/granger-causality.md#section_15": 503, "docs/en/multivariate/granger-causality.md#section_16": 49, "docs/en/multivariate/var.md": 979, "docs/en/multivariate/var.md#section_0": 53, "docs/en/multivariate/var.md#section_1": 52, "docs/en/multivariate/var.md#section_2": 51, "docs/en/multivariate/var.md#section_3": 26, "docs/en/multivariate/var.md#section_4": 28, "docs/en/multivariate/var.md#section_5": 14, "docs/en/multivariate/var.md#section_6": 53, "docs/en/multivariate/var.md#section_7": 64, "docs/en/multivariate/var.md#section_8": 11, "docs/en/multivariate/var.md#section_9": 19, "docs/en/multivariate/var.md#section_10": 4, "docs/en/multivariate/var.md#section_11": 10, "docs/en/multivariate/var.md#section_12": 13, "docs/en/multivariate/var.md#section_13": 16, "docs/en/multivariate/var.md#section_14": 10, "docs/en/multivariate/var.md#section_15": 512, "docs/en/multivariate/var.md#section_16": 38, "docs/en/time-domain/ma.md": 969, "docs/en/time-domain/ma.md#section_0": 52, "docs/en/time-domain/ma.md#section_1": 28, "docs/en/time-domain/ma.md#section_2": 37, "docs/en/time-domain/ma.md#section_3": 13, "docs/en/time-domain/ma.md#section_4": 32, "docs/en/time-domain/ma.md#section_5": 15, "docs/en/time-domain/ma.md#section_6": 84, "docs/en/time-domain/ma.md#section_7": 96, "docs/en/time-domain/ma.md#section_8": 17, "docs/en/time-domain/ma.md#section_9": 22, "docs/en/time-domain/ma.md#section_10": 13, "docs/en/time-domain/ma.md#section_11": 23, "docs/en/time-domain/ma.md#section_12": 22, "docs/en/time-domain/ma.md#section_13": 465, "docs/en/time-domain/ma.md#section_14": 38, "docs/en/time-domain/arma.md": 1017, "docs/en/time-domain/arma.md#section_0": 40, "docs/en/time-domain/arma.md#section_1": 35, "docs/en/time-domain/arma.md#section_2": 60, "docs/en/time-domain/arma.md#section_3": 23, "docs/en/time-domain/arma.md#section_4": 23, "docs/en/time-domain/arma.md#section_5": 22, "docs/en/time-domain/arma.md#section_6": 84, "docs/en/time-domain/arma.md#section_7": 74, "docs/en/time-domain/arma.md#section_8": 18, "docs/en/time-domain/arma.md#section_9": 24, "docs/en/time-domain/arma.md#section_10": 18, "docs/en/time-domain/arma.md#section_11": 22, "docs/en/time-domain/arma.md#section_12": 12, "docs/en/time-domain/arma.md#section_13": 23, "docs/en/time-domain/arma.md#section_14": 494, "docs/en/time-domain/arma.md#section_15": 37, "docs/en/time-domain/ar.md": 1048, "docs/en/time-domain/ar.md#section_0": 49, "docs/en/time-domain/ar.md#section_1": 34, "docs/en/time-domain/ar.md#section_2": 32, "docs/en/time-domain/ar.md#section_3": 29, "docs/en/time-domain/ar.md#section_4": 52, "docs/en/time-domain/ar.md#section_5": 20, "docs/en/time-domain/ar.md#section_6": 81, "docs/en/time-domain/ar.md#section_7": 96, "docs/en/time-domain/ar.md#section_8": 15, "docs/en/time-domain/ar.md#section_9": 20, "docs/en/time-domain/ar.md#section_10": 13, "docs/en/time-domain/ar.md#section_11": 19, "docs/en/time-domain/ar.md#section_12": 24, "docs/en/time-domain/ar.md#section_13": 9, "docs/en/time-domain/ar.md#section_14": 512, "docs/en/time-domain/ar.md#section_15": 38, "docs/en/time-domain/arima.md": 1028, "docs/en/time-domain/arima.md#section_0": 39, "docs/en/time-domain/arima.md#section_1": 38, "docs/en/time-domain/arima.md#section_2": 10, "docs/en/time-domain/arima.md#section_3": 8, "docs/en/time-domain/arima.md#section_4": 14, "docs/en/time-domain/arima.md#section_5": 16, "docs/en/time-domain/arima.md#section_6": 26, "docs/en/time-domain/arima.md#section_7": 24, "docs/en/time-domain/arima.md#section_8": 91, "docs/en/time-domain/arima.md#section_9": 93, "docs/en/time-domain/arima.md#section_10": 17, "docs/en/time-domain/arima.md#section_11": 14, "docs/en/time-domain/arima.md#section_12": 19, "docs/en/time-domain/arima.md#section_13": 5, "docs/en/time-domain/arima.md#section_14": 21, "docs/en/time-domain/arima.md#section_15": 22, "docs/en/time-domain/arima.md#section_16": 21, "docs/en/time-domain/arima.md#section_17": 505, "docs/en/time-domain/arima.md#section_18": 41, "docs/en/time-domain/identification.md": 1209, "docs/en/time-domain/identification.md#section_0": 49, "docs/en/time-domain/identification.md#section_1": 77, "docs/en/time-domain/identification.md#section_2": 51, "docs/en/time-domain/identification.md#section_3": 39, "docs/en/time-domain/identification.md#section_4": 35, "docs/en/time-domain/identification.md#section_5": 20, "docs/en/time-domain/identification.md#section_6": 124, "docs/en/time-domain/identification.md#section_7": 78, "docs/en/time-domain/identification.md#section_8": 27, "docs/en/time-domain/identification.md#section_9": 17, "docs/en/time-domain/identification.md#section_10": 22, "docs/en/time-domain/identification.md#section_11": 20, "docs/en/time-domain/identification.md#section_12": 19, "docs/en/time-domain/identification.md#section_13": 23, "docs/en/time-domain/identification.md#section_14": 49, "docs/en/time-domain/identification.md#section_15": 515, "docs/en/time-domain/identification.md#section_16": 40, "docs/en/time-domain/sarima.md": 1021, "docs/en/time-domain/sarima.md#section_0": 42, "docs/en/time-domain/sarima.md#section_1": 47, "docs/en/time-domain/sarima.md#section_2": 13, "docs/en/time-domain/sarima.md#section_3": 11, "docs/en/time-domain/sarima.md#section_4": 50, "docs/en/time-domain/sarima.md#section_5": 46, "docs/en/time-domain/sarima.md#section_6": 17, "docs/en/time-domain/sarima.md#section_7": 64, "docs/en/time-domain/sarima.md#section_8": 88, "docs/en/time-domain/sarima.md#section_9": 19, "docs/en/time-domain/sarima.md#section_10": 10, "docs/en/time-domain/sarima.md#section_11": 22, "docs/en/time-domain/sarima.md#section_12": 15, "docs/en/time-domain/sarima.md#section_13": 21, "docs/en/time-domain/sarima.md#section_14": 12, "docs/en/time-domain/sarima.md#section_15": 491, "docs/en/time-domain/sarima.md#section_16": 47, "docs/en/deep-learning/deep-learning-ts.md": 1144, "docs/en/deep-learning/deep-learning-ts.md#section_0": 57, "docs/en/deep-learning/deep-learning-ts.md#section_1": 51, "docs/en/deep-learning/deep-learning-ts.md#section_2": 44, "docs/en/deep-learning/deep-learning-ts.md#section_3": 14, "docs/en/deep-learning/deep-learning-ts.md#section_4": 30, "docs/en/deep-learning/deep-learning-ts.md#section_5": 21, "docs/en/deep-learning/deep-learning-ts.md#section_6": 43, "docs/en/deep-learning/deep-learning-ts.md#section_7": 64, "docs/en/deep-learning/deep-learning-ts.md#section_8": 67, "docs/en/deep-learning/deep-learning-ts.md#section_9": 18, "docs/en/deep-learning/deep-learning-ts.md#section_10": 22, "docs/en/deep-learning/deep-learning-ts.md#section_11": 23, "docs/en/deep-learning/deep-learning-ts.md#section_12": 5, "docs/en/deep-learning/deep-learning-ts.md#section_13": 44, "docs/en/deep-learning/deep-learning-ts.md#section_14": 17, "docs/en/deep-learning/deep-learning-ts.md#section_15": 12, "docs/en/deep-learning/deep-learning-ts.md#section_16": 12, "docs/en/deep-learning/deep-learning-ts.md#section_17": 547, "docs/en/deep-learning/deep-learning-ts.md#section_18": 47, "docs/en/exponential-smoothing/ets.md": 1047, "docs/en/exponential-smoothing/ets.md#section_0": 45, "docs/en/exponential-smoothing/ets.md#section_1": 46, "docs/en/exponential-smoothing/ets.md#section_2": 17, "docs/en/exponential-smoothing/ets.md#section_3": 31, "docs/en/exponential-smoothing/ets.md#section_4": 12, "docs/en/exponential-smoothing/ets.md#section_5": 34, "docs/en/exponential-smoothing/ets.md#section_6": 32, "docs/en/exponential-smoothing/ets.md#section_7": 68, "docs/en/exponential-smoothing/ets.md#section_8": 78, "docs/en/exponential-smoothing/ets.md#section_9": 11, "docs/en/exponential-smoothing/ets.md#section_10": 26, "docs/en/exponential-smoothing/ets.md#section_11": 78, "docs/en/exponential-smoothing/ets.md#section_12": 12, "docs/en/exponential-smoothing/ets.md#section_13": 495, "docs/en/exponential-smoothing/ets.md#section_14": 58, "docs/en/exponential-smoothing/holt.md": 846, "docs/en/exponential-smoothing/holt.md#section_0": 44, "docs/en/exponential-smoothing/holt.md#section_1": 40, "docs/en/exponential-smoothing/holt.md#section_2": 12, "docs/en/exponential-smoothing/holt.md#section_3": 7, "docs/en/exponential-smoothing/holt.md#section_4": 20, "docs/en/exponential-smoothing/holt.md#section_5": 24, "docs/en/exponential-smoothing/holt.md#section_6": 35, "docs/en/exponential-smoothing/holt.md#section_7": 72, "docs/en/exponential-smoothing/holt.md#section_8": 11, "docs/en/exponential-smoothing/holt.md#section_9": 17, "docs/en/exponential-smoothing/holt.md#section_10": 24, "docs/en/exponential-smoothing/holt.md#section_11": 18, "docs/en/exponential-smoothing/holt.md#section_12": 21, "docs/en/exponential-smoothing/holt.md#section_13": 6, "docs/en/exponential-smoothing/holt.md#section_14": 446, "docs/en/exponential-smoothing/holt.md#section_15": 44, "docs/en/exponential-smoothing/holt-winters.md": 923, "docs/en/exponential-smoothing/holt-winters.md#section_0": 43, "docs/en/exponential-smoothing/holt-winters.md#section_1": 39, "docs/en/exponential-smoothing/holt-winters.md#section_2": 37, "docs/en/exponential-smoothing/holt-winters.md#section_3": 15, "docs/en/exponential-smoothing/holt-winters.md#section_4": 13, "docs/en/exponential-smoothing/holt-winters.md#section_5": 15, "docs/en/exponential-smoothing/holt-winters.md#section_6": 34, "docs/en/exponential-smoothing/holt-winters.md#section_7": 75, "docs/en/exponential-smoothing/holt-winters.md#section_8": 11, "docs/en/exponential-smoothing/holt-winters.md#section_9": 32, "docs/en/exponential-smoothing/holt-winters.md#section_10": 11, "docs/en/exponential-smoothing/holt-winters.md#section_11": 34, "docs/en/exponential-smoothing/holt-winters.md#section_12": 15, "docs/en/exponential-smoothing/holt-winters.md#section_13": 500, "docs/en/exponential-smoothing/holt-winters.md#section_14": 44, "docs/en/exponential-smoothing/ses.md": 858, "docs/en/exponential-smoothing/ses.md#section_0": 43, "docs/en/exponential-smoothing/ses.md#section_1": 29, "docs/en/exponential-smoothing/ses.md#section_2": 26, "docs/en/exponential-smoothing/ses.md#section_3": 19, "docs/en/exponential-smoothing/ses.md#section_4": 17, "docs/en/exponential-smoothing/ses.md#section_5": 20, "docs/en/exponential-smoothing/ses.md#section_6": 41, "docs/en/exponential-smoothing/ses.md#section_7": 70, "docs/en/exponential-smoothing/ses.md#section_8": 11, "docs/en/exponential-smoothing/ses.md#section_9": 17, "docs/en/exponential-smoothing/ses.md#section_10": 23, "docs/en/exponential-smoothing/ses.md#section_11": 22, "docs/en/exponential-smoothing/ses.md#section_12": 11, "docs/en/exponential-smoothing/ses.md#section_13": 468, "docs/en/exponential-smoothing/ses.md#section_14": 35, "docs/en/forecasting/prediction-intervals.md": 922, "docs/en/forecasting/prediction-intervals.md#section_0": 45, "docs/en/forecasting/prediction-intervals.md#section_1": 41, "docs/en/forecasting/prediction-intervals.md#section_2": 12, "docs/en/forecasting/prediction-intervals.md#section_3": 33, "docs/en/forecasting/prediction-intervals.md#section_4": 17, "docs/en/forecasting/prediction-intervals.md#section_5": 30, "docs/en/forecasting/prediction-intervals.md#section_6": 56, "docs/en/forecasting/prediction-intervals.md#section_7": 68, "docs/en/forecasting/prediction-intervals.md#section_8": 12, "docs/en/forecasting/prediction-intervals.md#section_9": 18, "docs/en/forecasting/prediction-intervals.md#section_10": 6, "docs/en/forecasting/prediction-intervals.md#section_11": 26, "docs/en/forecasting/prediction-intervals.md#section_12": 22, "docs/en/forecasting/prediction-intervals.md#section_13": 489, "docs/en/forecasting/prediction-intervals.md#section_14": 43, "docs/en/forecasting/multi-step.md": 1054, "docs/en/forecasting/multi-step.md#section_0": 53, "docs/en/forecasting/multi-step.md#section_1": 40, "docs/en/forecasting/multi-step.md#section_2": 28, "docs/en/forecasting/multi-step.md#section_3": 25, "docs/en/forecasting/multi-step.md#section_4": 22, "docs/en/forecasting/multi-step.md#section_5": 27, "docs/en/forecasting/multi-step.md#section_6": 57, "docs/en/forecasting/multi-step.md#section_7": 7, "docs/en/forecasting/multi-step.md#section_8": 5, "docs/en/forecasting/multi-step.md#section_9": 82, "docs/en/forecasting/multi-step.md#section_10": 34, "docs/en/forecasting/multi-step.md#section_11": 14, "docs/en/forecasting/multi-step.md#section_12": 8, "docs/en/forecasting/multi-step.md#section_13": 11, "docs/en/forecasting/multi-step.md#section_14": 13, "docs/en/forecasting/multi-step.md#section_15": 6, "docs/en/forecasting/multi-step.md#section_16": 21, "docs/en/forecasting/multi-step.md#section_17": 516, "docs/en/forecasting/multi-step.md#section_18": 73, "docs/en/interview/interview-questions.md": 1299, "docs/en/interview/interview-questions.md#section_0": 53, "docs/en/interview/interview-questions.md#section_1": 57, "docs/en/interview/interview-questions.md#section_2": 40, "docs/en/interview/interview-questions.md#section_3": 37, "docs/en/interview/interview-questions.md#section_4": 51, "docs/en/interview/interview-questions.md#section_5": 57, "docs/en/interview/interview-questions.md#section_6": 42, "docs/en/interview/interview-questions.md#section_7": 49, "docs/en/interview/interview-questions.md#section_8": 56, "docs/en/interview/interview-questions.md#section_9": 53, "docs/en/interview/interview-questions.md#section_10": 49, "docs/en/interview/interview-questions.md#section_11": 49, "docs/en/interview/interview-questions.md#section_12": 49, "docs/en/interview/interview-questions.md#section_13": 51, "docs/en/interview/interview-questions.md#section_14": 41, "docs/en/interview/interview-questions.md#section_15": 43, "docs/en/interview/interview-questions.md#section_16": 479, "docs/en/interview/interview-questions.md#section_17": 33, "docs/en/model-selection/cross-validation.md": 1092, "docs/en/model-selection/cross-validation.md#section_0": 56, "docs/en/model-selection/cross-validation.md#section_1": 49, "docs/en/model-selection/cross-validation.md#section_2": 14, "docs/en/model-selection/cross-validation.md#section_3": 60, "docs/en/model-selection/cross-validation.md#section_4": 32, "docs/en/model-selection/cross-validation.md#section_5": 79, "docs/en/model-selection/cross-validation.md#section_6": 70, "docs/en/model-selection/cross-validation.md#section_7": 37, "docs/en/model-selection/cross-validation.md#section_8": 14, "docs/en/model-selection/cross-validation.md#section_9": 48, "docs/en/model-selection/cross-validation.md#section_10": 24, "docs/en/model-selection/cross-validation.md#section_11": 548, "docs/en/model-selection/cross-validation.md#section_12": 55, "docs/en/model-selection/information-criteria.md": 982, "docs/en/model-selection/information-criteria.md#section_0": 52, "docs/en/model-selection/information-criteria.md#section_1": 39, "docs/en/model-selection/information-criteria.md#section_2": 40, "docs/en/model-selection/information-criteria.md#section_3": 24, "docs/en/model-selection/information-criteria.md#section_4": 19, "docs/en/model-selection/information-criteria.md#section_5": 23, "docs/en/model-selection/information-criteria.md#section_6": 68, "docs/en/model-selection/information-criteria.md#section_7": 74, "docs/en/model-selection/information-criteria.md#section_8": 12, "docs/en/model-selection/information-criteria.md#section_9": 21, "docs/en/model-selection/information-criteria.md#section_10": 32, "docs/en/model-selection/information-criteria.md#section_11": 27, "docs/en/model-selection/information-criteria.md#section_12": 18, "docs/en/model-selection/information-criteria.md#section_13": 483, "docs/en/model-selection/information-criteria.md#section_14": 46, "docs/en/model-selection/residual-diagnostics.md": 1037, "docs/en/model-selection/residual-diagnostics.md#section_0": 50, "docs/en/model-selection/residual-diagnostics.md#section_1": 24, "docs/en/model-selection/residual-diagnostics.md#section_2": 23, "docs/en/model-selection/residual-diagnostics.md#section_3": 16, "docs/en/model-selection/residual-diagnostics.md#section_4": 17, "docs/en/model-selection/residual-diagnostics.md#section_5": 15, "docs/en/model-selection/residual-diagnostics.md#section_6": 122, "docs/en/model-selection/residual-diagnostics.md#section_7": 71, "docs/en/model-selection/residual-diagnostics.md#section_8": 24, "docs/en/model-selection/residual-diagnostics.md#section_9": 11, "docs/en/model-selection/residual-diagnostics.md#section_10": 15, "docs/en/model-selection/residual-diagnostics.md#section_11": 13, "docs/en/model-selection/residual-diagnostics.md#section_12": 7, "docs/en/model-selection/residual-diagnostics.md#section_13": 10, "docs/en/model-selection/residual-diagnostics.md#section_14": 11, "docs/en/model-selection/residual-diagnostics.md#section_15": 20, "docs/en/model-selection/residual-diagnostics.md#section_16": 16, "docs/en/model-selection/residual-diagnostics.md#section_17": 522, "docs/en/model-selection/residual-diagnostics.md#section_18": 46, "docs/en/practical/practical-modeling.md": 1079, "docs/en/practical/practical-modeling.md#section_0": 54, "docs/en/practical/practical-modeling.md#section_1": 39, "docs/en/practical/practical-modeling.md#section_2": 11, "docs/en/practical/practical-modeling.md#section_3": 16, "docs/en/practical/practical-modeling.md#section_4": 11, "docs/en/practical/practical-modeling.md#section_5": 15, "docs/en/practical/practical-modeling.md#section_6": 107, "docs/en/practical/practical-modeling.md#section_7": 65, "docs/en/practical/practical-modeling.md#section_8": 34, "docs/en/practical/practical-modeling.md#section_9": 27, "docs/en/practical/practical-modeling.md#section_10": 36, "docs/en/practical/practical-modeling.md#section_11": 27, "docs/en/practical/practical-modeling.md#section_12": 35, "docs/en/practical/practical-modeling.md#section_13": 541, "docs/en/practical/practical-modeling.md#section_14": 55, "docs/en/features/feature-engineering.md": 1138, "docs/en/features/feature-engineering.md#section_0": 54, "docs/en/features/feature-engineering.md#section_1": 43, "docs/en/features/feature-engineering.md#section_2": 51, "docs/en/features/feature-engineering.md#section_3": 24, "docs/en/features/feature-engineering.md#section_4": 32, "docs/en/features/feature-engineering.md#section_5": 100, "docs/en/features/feature-engineering.md#section_6": 5, "docs/en/features/feature-engineering.md#section_7": 5, "docs/en/features/feature-engineering.md#section_8": 67, "docs/en/features/feature-engineering.md#section_9": 16, "docs/en/features/feature-engineering.md#section_10": 39, "docs/en/features/feature-engineering.md#section_11": 65, "docs/en/features/feature-engineering.md#section_12": 17, "docs/en/features/feature-engineering.md#section_13": 8, "docs/en/features/feature-engineering.md#section_14": 11, "docs/en/features/feature-engineering.md#section_15": 16, "docs/en/features/feature-engineering.md#section_16": 528, "docs/en/features/feature-engineering.md#section_17": 51, "docs/en/state-space/kalman-filter.md": 978, "docs/en/state-space/kalman-filter.md#section_0": 53, "docs/en/state-space/kalman-filter.md#section_1": 52, "docs/en/state-space/kalman-filter.md#section_2": 62, "docs/en/state-space/kalman-filter.md#section_3": 35, "docs/en/state-space/kalman-filter.md#section_4": 28, "docs/en/state-space/kalman-filter.md#section_5": 44, "docs/en/state-space/kalman-filter.md#section_6": 83, "docs/en/state-space/kalman-filter.md#section_7": 31, "docs/en/state-space/kalman-filter.md#section_8": 9, "docs/en/state-space/kalman-filter.md#section_9": 7, "docs/en/state-space/kalman-filter.md#section_10": 4, "docs/en/state-space/kalman-filter.md#section_11": 24, "docs/en/state-space/kalman-filter.md#section_12": 17, "docs/en/state-space/kalman-filter.md#section_13": 473, "docs/en/state-space/kalman-filter.md#section_14": 49, "docs/en/anomaly-detection/anomaly-detection.md": 1078, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 50, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 42, "docs/en/anomaly-detection/anomaly-detection.md#section_2": 29, "docs/en/anomaly-detection/anomaly-detection.md#section_3": 26, "docs/en/anomaly-detection/anomaly-detection.md#section_4": 32, "docs/en/anomaly-detection/anomaly-detection.md#section_5": 19, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 84, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 72, "docs/en/anomaly-detection/anomaly-detection.md#section_8": 9, "docs/en/anomaly-detection/anomaly-detection.md#section_9": 18, "docs/en/anomaly-detection/anomaly-detection.md#section_10": 11, "docs/en/anomaly-detection/anomaly-detection.md#section_11": 11, "docs/en/anomaly-detection/anomaly-detection.md#section_12": 27, "docs/en/anomaly-detection/anomaly-detection.md#section_13": 23, "docs/en/anomaly-detection/anomaly-detection.md#section_14": 32, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 539, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 50, "docs/en/decomposition/classical.md": 937, "docs/en/decomposition/classical.md#section_0": 44, "docs/en/decomposition/classical.md#section_1": 48, "docs/en/decomposition/classical.md#section_2": 23, "docs/en/decomposition/classical.md#section_3": 32, "docs/en/decomposition/classical.md#section_4": 32, "docs/en/decomposition/classical.md#section_5": 56, "docs/en/decomposition/classical.md#section_6": 71, "docs/en/decomposition/classical.md#section_7": 10, "docs/en/decomposition/classical.md#section_8": 25, "docs/en/decomposition/classical.md#section_9": 41, "docs/en/decomposition/classical.md#section_10": 26, "docs/en/decomposition/classical.md#section_11": 4, "docs/en/decomposition/classical.md#section_12": 485, "docs/en/decomposition/classical.md#section_13": 36, "docs/en/decomposition/stl.md": 966, "docs/en/decomposition/stl.md#section_0": 49, "docs/en/decomposition/stl.md#section_1": 65, "docs/en/decomposition/stl.md#section_2": 19, "docs/en/decomposition/stl.md#section_3": 48, "docs/en/decomposition/stl.md#section_4": 20, "docs/en/decomposition/stl.md#section_5": 52, "docs/en/decomposition/stl.md#section_6": 71, "docs/en/decomposition/stl.md#section_7": 15, "docs/en/decomposition/stl.md#section_8": 28, "docs/en/decomposition/stl.md#section_9": 6, "docs/en/decomposition/stl.md#section_10": 44, "docs/en/decomposition/stl.md#section_11": 19, "docs/en/decomposition/stl.md#section_12": 7, "docs/en/decomposition/stl.md#section_13": 464, "docs/en/decomposition/stl.md#section_14": 55, "docs/en/change-detection/change-point.md": 1047, "docs/en/change-detection/change-point.md#section_0": 59, "docs/en/change-detection/change-point.md#section_1": 44, "docs/en/change-detection/change-point.md#section_2": 26, "docs/en/change-detection/change-point.md#section_3": 22, "docs/en/change-detection/change-point.md#section_4": 21, "docs/en/change-detection/change-point.md#section_5": 20, "docs/en/change-detection/change-point.md#section_6": 54, "docs/en/change-detection/change-point.md#section_7": 76, "docs/en/change-detection/change-point.md#section_8": 9, "docs/en/change-detection/change-point.md#section_9": 9, "docs/en/change-detection/change-point.md#section_10": 21, "docs/en/change-detection/change-point.md#section_11": 23, "docs/en/change-detection/change-point.md#section_12": 13, "docs/en/change-detection/change-point.md#section_13": 60, "docs/en/change-detection/change-point.md#section_14": 532, "docs/en/change-detection/change-point.md#section_15": 50, "docs/en/foundations/autocorrelation.md": 1145, "docs/en/foundations/autocorrelation.md#section_0": 53, "docs/en/foundations/autocorrelation.md#section_1": 63, "docs/en/foundations/autocorrelation.md#section_2": 19, "docs/en/foundations/autocorrelation.md#section_3": 53, "docs/en/foundations/autocorrelation.md#section_4": 10, "docs/en/foundations/autocorrelation.md#section_5": 15, "docs/en/foundations/autocorrelation.md#section_6": 4, "docs/en/foundations/autocorrelation.md#section_7": 4, "docs/en/foundations/autocorrelation.md#section_8": 37, "docs/en/foundations/autocorrelation.md#section_9": 106, "docs/en/foundations/autocorrelation.md#section_10": 96, "docs/en/foundations/autocorrelation.md#section_11": 18, "docs/en/foundations/autocorrelation.md#section_12": 18, "docs/en/foundations/autocorrelation.md#section_13": 26, "docs/en/foundations/autocorrelation.md#section_14": 14, "docs/en/foundations/autocorrelation.md#section_15": 6, "docs/en/foundations/autocorrelation.md#section_16": 547, "docs/en/foundations/autocorrelation.md#section_17": 43, "docs/en/foundations/stationarity.md": 1008, "docs/en/foundations/stationarity.md#section_0": 45, "docs/en/foundations/stationarity.md#section_1": 83, "docs/en/foundations/stationarity.md#section_2": 23, "docs/en/foundations/stationarity.md#section_3": 18, "docs/en/foundations/stationarity.md#section_4": 25, "docs/en/foundations/stationarity.md#section_5": 37, "docs/en/foundations/stationarity.md#section_6": 71, "docs/en/foundations/stationarity.md#section_7": 92, "docs/en/foundations/stationarity.md#section_8": 12, "docs/en/foundations/stationarity.md#section_9": 15, "docs/en/foundations/stationarity.md#section_10": 14, "docs/en/foundations/stationarity.md#section_11": 21, "docs/en/foundations/stationarity.md#section_12": 15, "docs/en/foundations/stationarity.md#section_13": 485, "docs/en/foundations/stationarity.md#section_14": 49}, "avg_doc_length": 118.20628683693516, "doc_count": 509, "term_doc_freq": {"time": 112, "series": 126, "study": 4, "notes": 2, "div": 89, "class": 91, "interview": 61, "summary": 92, "strong": 92, "welcome": 2, "comprehensive": 2, "bilingual": 2, "english": 3, "resource": 2, "learning": 20, "analysis": 53, "forecasting": 82, "designed": 2, "preparation": 2, "practical": 64, "application": 4, "included": 6, "knowledge": 13, "base": 7, "covers": 2, "foundations": 4, "stationarity": 61, "autocorrelation": 36, "partial": 9, "domain": 18, "models": 100, "ar": 102, "ma": 85, "arma": 44, "arima": 92, "sarima": 26, "identification": 31, "estimation": 45, "exponential": 40, "smoothing": 44, "ses": 23, "holt": 33, "winters": 21, "ets": 27, "framework": 18, "decomposition": 30, "stl": 18, "classical": 19, "handling": 7, "seasonality": 55, "prediction": 66, "intervals": 42, "multi": 13, "step": 55, "strategies": 15, "rolling": 31, "evaluation": 16, "model": 200, "selection": 44, "aic": 60, "bic": 49, "cross": 31, "validation": 37, "residual": 46, "diagnostics": 30, "spectral": 14, "periodogram": 10, "frequency": 21, "basics": 2, "state": 38, "space": 24, "kalman": 20, "filter": 32, "local": 26, "level": 57, "trend": 98, "multivariate": 7, "ts": 7, "var": 70, "varma": 2, "granger": 18, "causality": 18, "change": 33, "detection": 27, "point": 43, "anomaly": 13, "methods": 52, "feature": 16, "engineering": 10, "pipelines": 4, "scaling": 10, "missing": 29, "data": 131, "deep": 11, "rnn": 7, "lstm": 15, "tcn": 10, "transformers": 9, "modeling": 15, "backtesting": 5, "deployment": 5, "common": 95, "pitfalls": 59, "page": 6, "structure": 29, "topic": 3, "follows": 6, "consistent": 22, "section": 4, "format": 2, "key": 57, "points": 29, "lines": 6, "core": 59, "definitions": 58, "essential": 4, "terminology": 2, "concepts": 3, "math": 60, "derivations": 30, "rigorous": 2, "mathematical": 12, "algorithm": 66, "sketch": 59, "method": 39, "works": 13, "mistakes": 2, "avoid": 6, "mini": 58, "example": 74, "quick": 4, "illustration": 2, "quiz": 60, "questions": 4, "hidden": 11, "answers": 4, "click": 2, "reveal": 2, "references": 60, "further": 4, "reading": 6, "getting": 2, "started": 2, "choose": 23, "sidebar": 2, "begin": 20, "self": 8, "contained": 2, "builds": 2, "foundational": 4, "recommended": 16, "path": 7, "beginners": 2, "start": 24, "md": 6, "move": 6, "learn": 11, "explore": 4, "advance": 2, "code": 2, "examples": 6, "runnable": 2, "python": 69, "demos": 2, "available": 15, "directory": 2, "run": 14, "bash": 2, "demo": 2, "changepoint": 4, "backtest": 5, "metrics": 19, "language": 2, "toggle": 2, "use": 115, "selector": 2, "header": 2, "switch": 2, "between": 44, "site": 2, "maintains": 2, "parallel": 2, "content": 6, "languages": 2, "open": 4, "extensible": 2, "see": 14, "repository": 2, "readme": 2, "instructions": 2, "adding": 8, "new": 22, "decomposes": 3, "into": 12, "components": 32, "estimates": 34, "power": 15, "peaks": 9, "indicate": 11, "dominant": 6, "cycles": 12, "stationary": 75, "density": 11, "fourier": 23, "transform": 21, "autocovariance": 23, "insight": 25, "produces": 10, "smooth": 21, "spectrum": 4, "useful": 10, "detecting": 2, "periodicities": 2, "understanding": 4, "cyclical": 2, "behavior": 14, "process": 42, "omega": 9, "represents": 4, "frac": 100, "pi": 49, "infty": 35, "gamma": 35, "sample": 44, "estimate": 34, "left": 27, "right": 26, "frequencies": 18, "ldots": 46, "mean": 82, "zero": 40, "period": 55, "units": 2, "nyquist": 6, "fastest": 2, "observable": 2, "cycle": 18, "relationship": 16, "pairs": 9, "parseval": 2, "relation": 2, "text": 89, "total": 17, "variance": 73, "across": 19, "phi": 60, "sigma": 64, "cos": 16, "properties": 22, "peak": 6, "low": 36, "dominance": 2, "high": 39, "theta": 56, "monthly": 32, "annual": 26, "12": 58, "approx": 32, "524": 2, "procedure": 16, "remove": 15, "necessary": 4, "apply": 27, "window": 21, "optional": 2, "reduces": 13, "leakage": 22, "hanning": 2, "hamming": 2, "blackman": 2, "compute": 35, "fft": 2, "daniell": 2, "kernel": 2, "log": 29, "welch": 2, "identify": 13, "compare": 54, "red": 2, "white": 33, "noise": 70, "baseline": 7, "test": 85, "significance": 14, "against": 9, "continuum": 2, "interpret": 17, "periods": 28, "decay": 26, "like": 21, "flat": 14, "conversion": 2, "index": 13, "sharp": 7, "true": 65, "appear": 13, "spread": 2, "out": 32, "due": 23, "finite": 16, "windowing": 2, "reduce": 10, "confusing": 22, "inconsistent": 7, "doesn": 43, "converge": 8, "ignoring": 49, "aliasing": 3, "faster": 9, "wrong": 49, "ensure": 8, "adequate": 8, "sampling": 8, "non": 83, "assumes": 19, "causes": 36, "blow": 2, "up": 12, "detrend": 13, "first": 58, "over": 79, "interpreting": 7, "random": 90, "fluctuations": 6, "create": 16, "spurious": 11, "interpretation": 33, "import": 64, "numpy": 57, "np": 133, "scipy": 8, "signal": 23, "matplotlib": 6, "pyplot": 6, "plt": 8, "generate": 60, "known": 14, "seed": 56, "42": 57, "500": 12, "arange": 22, "50": 31, "01": 10, "sin": 28, "randn": 57, "freqs": 3, "psd": 3, "fs": 2, "find": 10, "height": 2, "percentile": 5, "90": 4, "print": 101, "detected": 14, "4f": 16, "1f": 20, "expected": 22, "near": 28, "details": 58, "q1": 60, "conceptual": 58, "function": 26, "answer": 73, "acf": 75, "describes": 2, "correlation": 24, "information": 36, "different": 42, "representation": 18, "int": 7, "integral": 2, "pitfall": 58, "thinking": 14, "analyses": 2, "give": 17, "re": 32, "equivalent": 22, "representations": 6, "based": 33, "easier": 9, "problem": 16, "q2": 59, "positive": 26, "coefficient": 16, "creates": 7, "persistence": 4, "values": 76, "tend": 4, "stay": 4, "above": 8, "below": 4, "extended": 6, "translates": 2, "slow": 23, "oscillations": 2, "explanation": 24, "propto": 4, "maximum": 19, "minimum": 10, "intuition": 9, "today": 4, "value": 42, "predicts": 7, "tomorrow": 2, "opposite": 6, "choppy": 2, "expecting": 16, "processes": 25, "look": 18, "similar": 27, "sign": 8, "magnitude": 4, "drastically": 2, "shape": 4, "q3": 60, "derive": 30, "explain": 22, "cannot": 6, "rad": 2, "derivation": 32, "observe": 6, "need": 49, "least": 16, "samples": 27, "per": 18, "trough": 4, "detectable": 2, "angular": 2, "times": 37, "appears": 6, "reflected": 2, "consequence": 4, "without": 49, "higher": 12, "rate": 11, "distinguish": 8, "trying": 8, "detect": 17, "daily": 21, "months": 4, "anything": 4, "bimonthly": 2, "q4": 59, "raw": 10, "estimator": 2, "decrease": 8, "size": 24, "increases": 10, "technical": 4, "neq": 17, "equals": 10, "squared": 15, "relative": 13, "error": 62, "stays": 12, "constant": 50, "happens": 4, "uses": 38, "entire": 6, "essentially": 8, "one": 47, "observation": 14, "still": 18, "solution": 14, "average": 41, "nearby": 4, "multitaper": 2, "trade": 13, "bias": 29, "treating": 8, "definitive": 2, "always": 61, "q5": 59, "analyze": 2, "hourly": 5, "temperature": 6, "24": 15, "hours": 2, "unexpected": 2, "happening": 2, "harmonics": 5, "fundamental": 8, "pure": 19, "hour": 5, "show": 39, "real": 31, "patterns": 66, "aren": 10, "sinusoids": 2, "morning": 2, "rise": 2, "gradual": 12, "afternoon": 2, "decline": 2, "sinusoidal": 10, "shapes": 2, "require": 12, "multiple": 53, "represent": 9, "theorem": 6, "any": 20, "periodic": 10, "sum": 38, "kt": 5, "etc": 10, "asymmetry": 2, "evening": 2, "action": 11, "normal": 19, "focus": 12, "separate": 18, "physical": 3, "phenomena": 2, "artifacts": 2, "shumway": 12, "stoffer": 12, "2017": 20, "applications": 20, "springer": 30, "chapter": 42, "brockwell": 14, "davis": 14, "2016": 16, "introduction": 18, "priestley": 2, "1981": 2, "academic": 2, "press": 20, "percival": 2, "walden": 2, "1993": 4, "cambridge": 4, "university": 20, "tests": 33, "whether": 14, "past": 41, "help": 8, "predict": 21, "beyond": 20, "own": 8, "history": 8, "about": 17, "predictive": 15, "precedence": 5, "causation": 10, "via": 34, "coefficients": 21, "lagged": 12, "equation": 28, "jointly": 6, "significant": 34, "bidirectional": 4, "possible": 26, "sensitive": 15, "omitted": 6, "variables": 14, "lag": 80, "provides": 14, "cause": 14, "knowing": 2, "improve": 12, "bivariate": 7, "cdots": 37, "restricted": 8, "unrestricted": 6, "including": 11, "statistic": 8, "2p": 3, "under": 42, "sim": 25, "wald": 4, "mathbf": 22, "boldsymbol": 13, "beta": 20, "selects": 7, "hat": 83, "chi": 10, "instantaneous": 2, "current": 19, "helps": 7, "effects": 33, "cov": 13, "yt": 2, "xt": 2, "contemporaneous": 8, "temporal": 26, "block": 4, "exogeneity": 2, "system": 9, "group": 6, "another": 2, "joint": 7, "relevant": 8, "matrices": 9, "determine": 13, "difference": 39, "toda": 4, "yamamoto": 4, "approach": 35, "standard": 47, "gc": 6, "select": 25, "optimal": 39, "order": 56, "consistency": 6, "perform": 8, "h0": 4, "reject": 15, "reverse": 4, "direction": 4, "rejects": 9, "unidirectional": 2, "neither": 2, "integration": 9, "fit": 105, "lags": 59, "avoids": 9, "issues": 18, "pretesting": 3, "unit": 44, "roots": 26, "causal": 11, "mechanism": 7, "arise": 2, "variable": 11, "miss": 13, "many": 45, "lose": 7, "introduce": 2, "distribution": 26, "augmented": 4, "correction": 11, "testing": 22, "inflates": 7, "type": 11, "adjust": 13, "together": 4, "within": 18, "won": 10, "statsmodels": 47, "tsa": 46, "stattools": 16, "grangercausalitytests": 3, "api": 4, "vice": 10, "versa": 10, "200": 23, "independent": 15, "zeros": 33, "range": 63, "depends": 33, "stack": 2, "maxlag": 4, "verbose": 2, "using": 81, "results": 32, "y1": 4, "y2": 4, "kind": 2, "measures": 7, "establish": 2, "differ": 8, "creating": 7, "respond": 4, "unobserved": 6, "factor": 21, "even": 19, "chance": 9, "measurement": 6, "timing": 2, "measured": 2, "reflects": 2, "ice": 2, "cream": 2, "sales": 23, "drownings": 2, "caused": 2, "summer": 4, "heat": 2, "claiming": 4, "say": 8, "means": 32, "economics": 4, "gdp": 4, "employment": 2, "economic": 14, "activity": 2, "labor": 2, "market": 2, "interact": 2, "prices": 4, "wages": 2, "wage": 2, "price": 2, "spiral": 2, "interest": 2, "rates": 2, "exchange": 2, "monetary": 2, "policy": 4, "currency": 2, "markets": 2, "feedback": 4, "contain": 8, "unique": 5, "interdependent": 2, "caution": 2, "simultaneous": 2, "mutual": 2, "way": 2, "complex": 32, "systems": 7, "rule": 14, "rather": 4, "exception": 2, "degrees": 7, "freedom": 5, "observations": 45, "numerator": 2, "df": 12, "denominator": 4, "parameters": 62, "restriction": 2, "set": 14, "used": 14, "number": 15, "restrictions": 2, "3p": 2, "formulations": 4, "slightly": 4, "depending": 6, "counted": 2, "software": 15, "handled": 4, "automatically": 10, "short": 31, "reducing": 6, "balance": 10, "handle": 21, "1995": 3, "max": 20, "usually": 20, "levels": 15, "don": 32, "extra": 8, "absorb": 2, "asymptotic": 2, "pretest": 2, "cointegration": 4, "robust": 30, "ignore": 2, "nuisance": 2, "oil": 2, "stock": 6, "returns": 10, "vary": 2, "others": 8, "pattern": 43, "suggests": 26, "matters": 25, "specification": 9, "weak": 7, "specifications": 2, "false": 19, "positives": 11, "criteria": 15, "report": 8, "single": 24, "sensitivity": 10, "needed": 34, "acknowledge": 2, "instability": 8, "consider": 54, "bonferroni": 2, "validate": 22, "evidence": 10, "nonlinear": 11, "varying": 9, "threshold": 14, "regime": 10, "switching": 6, "selecting": 6, "gives": 32, "desired": 2, "result": 15, "shopping": 2, "hacking": 2, "pre": 8, "specified": 5, "1969": 2, "investigating": 2, "relations": 2, "econometric": 2, "econometrica": 6, "37": 4, "424": 2, "438": 2, "statistical": 22, "inference": 21, "vector": 11, "autoregressions": 4, "possibly": 4, "integrated": 8, "journal": 18, "econometrics": 8, "66": 2, "225": 2, "250": 4, "hamilton": 18, "1994": 18, "princeton": 18, "11": 15, "2005": 6, "autoregression": 4, "epsilon": 30, "interrelated": 2, "analyzing": 2, "dynamic": 7, "relationships": 4, "ols": 7, "check": 82, "stability": 7, "eigenvalues": 6, "compact": 4, "form": 30, "condition": 16, "companion": 4, "matrix": 16, "inside": 5, "circle": 17, "1t": 2, "2t": 2, "pmatrix": 11, "end": 24, "21": 4, "22": 4, "written": 13, "capture": 32, "spillovers": 2, "dimension": 4, "xi": 2, "modulus": 3, "moving": 24, "mu": 23, "impulse": 6, "response": 13, "jk": 3, "shock": 6, "forecast": 95, "contribution": 4, "horizon": 40, "minimizing": 10, "estimated": 35, "separately": 6, "efficient": 13, "regressors": 7, "residuals": 52, "lb": 4, "normality": 20, "heteroskedasticity": 12, "responses": 4, "overfitting": 18, "easy": 7, "requires": 35, "differences": 10, "vecm": 2, "structural": 24, "reduced": 2, "shows": 20, "correlations": 6, "svar": 2, "claims": 2, "cointegrated": 2, "irfs": 4, "depend": 10, "ordering": 3, "cholesky": 3, "assumptions": 8, "forgetting": 28, "diagonal": 5, "shocks": 12, "correlated": 7, "equations": 14, "array": 13, "maxlags": 2, "ncoefficient": 2, "coefs": 2, "ntrue": 8, "irf": 2, "10": 65, "nirf": 2, "3f": 30, "steps": 21, "n5": 2, "advantage": 4, "fitting": 19, "univariate": 4, "captures": 15, "dynamics": 15, "interactions": 2, "affects": 20, "future": 32, "errors": 41, "accounts": 10, "wide": 8, "inflation": 4, "affecting": 4, "demand": 12, "supply": 2, "hitting": 2, "arimas": 2, "unrelated": 2, "wasteful": 2, "sparse": 2, "important": 8, "pp": 4, "lower": 25, "triangular": 4, "implies": 4, "affected": 4, "contemporaneously": 2, "later": 4, "earlier": 2, "ones": 8, "orderings": 2, "vs": 42, "immediately": 2, "second": 11, "solutions": 6, "theory": 8, "justify": 4, "explicit": 4, "reporting": 4, "stating": 2, "justifying": 2, "driven": 4, "arbitrary": 2, "choice": 9, "less": 21, "iterating": 2, "backward": 9, "converges": 8, "iff": 6, "lambda": 5, "pm": 22, "sqrt": 43, "ad": 7, "bc": 2, "checking": 17, "elements": 2, "off": 38, "terms": 33, "make": 4, "unstable": 9, "breakdown": 2, "covariance": 9, "symmetric": 5, "constants": 2, "36": 6, "45": 6, "implications": 2, "grow": 15, "limited": 10, "severe": 2, "bvar": 2, "large": 54, "small": 45, "thumb": 6, "20": 26, "parameter": 42, "quarterly": 7, "macro": 2, "options": 6, "increase": 8, "try": 28, "add": 32, "seasonal": 97, "dummies": 8, "include": 13, "indicators": 6, "exogenous": 2, "seasonally": 4, "varx": 2, "diagnostic": 19, "three": 14, "seasonals": 6, "verify": 22, "pass": 12, "consideration": 2, "prefer": 17, "explicitly": 8, "chapters": 12, "sims": 2, "1980": 2, "macroeconomics": 2, "reality": 4, "48": 4, "watson": 4, "2001": 2, "perspectives": 2, "15": 14, "101": 2, "115": 4, "express": 5, "linear": 45, "combination": 5, "cuts": 21, "after": 48, "pacf": 50, "decays": 16, "exponentially": 21, "optimization": 18, "mle": 17, "invertibility": 17, "outside": 22, "wn": 4, "operator": 13, "characteristic": 7, "polynomial": 14, "lie": 5, "geq": 24, "rho": 35, "quad": 9, "note": 13, "general": 12, "cases": 12, "leq": 15, "invertible": 12, "allows": 9, "expressing": 2, "observables": 3, "required": 6, "proper": 17, "hh": 8, "innovation": 6, "recursive": 18, "autocovariances": 2, "conditional": 13, "squares": 10, "css": 7, "minimize": 10, "fast": 4, "biased": 6, "exact": 14, "likelihood": 22, "initial": 20, "conditions": 13, "direct": 22, "asymptotically": 6, "challenges": 4, "unlike": 6, "optima": 4, "good": 21, "starting": 11, "constraints": 11, "enforced": 5, "examine": 17, "cutoff": 12, "candidate": 19, "enforce": 4, "get": 25, "difficulty": 2, "harder": 4, "poor": 15, "lead": 6, "convergence": 5, "innovations": 7, "differencing": 39, "negative": 21, "spike": 17, "often": 53, "indicates": 20, "misinterpreting": 4, "abrupt": 8, "drop": 10, "either": 2, "flip": 3, "reconsider": 7, "root": 32, "boundary": 6, "breaks": 16, "down": 4, "300": 12, "theta1": 4, "theta2": 4, "eps": 16, "cut": 6, "nlags": 10, "round": 12, "maparams": 7, "theoretical": 13, "comparison": 21, "gamma0": 2, "rho1": 2, "rho2": 2, "regardless": 11, "satisfied": 2, "because": 30, "combinations": 5, "preserve": 2, "concept": 9, "care": 2, "unobservable": 2, "convergent": 2, "ensures": 6, "enables": 5, "computing": 13, "updates": 14, "detail": 2, "expansion": 2, "diverges": 2, "produce": 6, "identical": 12, "acfs": 2, "forecasts": 59, "enforcing": 2, "performs": 2, "poorly": 6, "satisfies": 5, "taking": 2, "derivative": 2, "setting": 10, "finds": 4, "equal": 4, "therefore": 10, "equality": 2, "unlikely": 4, "mixed": 6, "since": 18, "formula": 23, "convention": 2, "comes": 4, "term": 27, "region": 6, "833": 2, "maybe": 6, "better": 36, "44": 2, "694": 2, "492": 4, "plan": 2, "enforces": 4, "manually": 10, "cdot": 20, "warnings": 4, "box": 37, "jenkins": 24, "reinsel": 20, "ljung": 33, "2015": 22, "wiley": 22, "combines": 2, "tail": 14, "part": 4, "parsimonious": 14, "present": 6, "assuming": 26, "simplicity": 2, "solving": 4, "differs": 2, "recursion": 11, "come": 2, "psi": 2, "redundancy": 7, "critical": 9, "polynomials": 7, "share": 11, "factors": 7, "5l": 3, "simplifies": 3, "called": 4, "likely": 13, "eacf": 4, "iteratively": 4, "removing": 8, "resulting": 4, "table": 4, "indicating": 4, "maximize": 4, "properly": 6, "default": 15, "parameterization": 2, "parsimony": 8, "modes": 2, "cancellation": 2, "close": 17, "canceling": 3, "inflated": 2, "confusion": 2, "overall": 6, "arparams": 7, "nphi": 2, "abs": 12, "naic": 2, "autoregressive": 10, "momentum": 6, "dissipate": 2, "while": 19, "fewer": 6, "requiring": 2, "well": 18, "approximated": 2, "infinite": 14, "achieving": 2, "parsimoniously": 2, "overfits": 2, "simple": 40, "frequently": 6, "sufficient": 15, "problematic": 10, "occurs": 6, "causing": 13, "cancel": 6, "sides": 6, "problems": 12, "becomes": 8, "nearly": 4, "singular": 2, "hessian": 2, "explode": 6, "misleading": 6, "complexity": 16, "suggest": 13, "suffices": 6, "leading": 6, "multiply": 6, "take": 12, "expectations": 4, "uncorrelated": 4, "itself": 6, "simply": 2, "component": 22, "modifies": 2, "subsequent": 4, "follow": 4, "rightarrow": 5, "combined": 6, "improper": 2, "several": 10, "520": 2, "525": 2, "515": 2, "514": 2, "despite": 6, "having": 2, "reasoning": 2, "negligible": 2, "principle": 4, "simpler": 17, "preferred": 14, "performance": 15, "stable": 17, "interpretable": 4, "additional": 10, "decision": 12, "penalizes": 11, "accuracy": 27, "candidates": 13, "blindly": 4, "choosing": 13, "lowest": 12, "meaningful": 9, "holdout": 23, "tsay": 6, "2010": 6, "financial": 9, "plus": 10, "equivalently": 10, "sinusoidally": 2, "yule": 7, "walker": 7, "hold": 5, "monotonic": 4, "damped": 14, "vdots": 4, "ddots": 4, "autocorrelations": 5, "memory": 8, "decaying": 10, "weights": 18, "moments": 4, "replace": 2, "solve": 2, "yields": 2, "inefficient": 4, "ordinary": 2, "regress": 6, "loses": 8, "distributional": 6, "assumption": 8, "gaussian": 17, "numerical": 6, "spikes": 17, "criterion": 14, "satisfy": 7, "leads": 14, "explosive": 5, "favor": 4, "larger": 17, "predicting": 4, "neglecting": 6, "invalid": 4, "hac": 2, "alternating": 2, "signs": 4, "reversion": 4, "autoreg": 7, "phi1": 6, "phi2": 6, "sel": 2, "ic": 5, "selected": 4, "params": 13, "intuitively": 2, "keeping": 2, "bounded": 13, "walk": 28, "persist": 8, "forever": 6, "explodes": 8, "99": 4, "enough": 14, "technically": 2, "behave": 2, "walks": 2, "predictions": 16, "degrade": 4, "quickly": 6, "gradually": 6, "controlling": 2, "intermediate": 8, "definition": 2, "dependence": 6, "includes": 10, "indirect": 6, "through": 18, "correlates": 4, "chain": 4, "perfectly": 2, "ll": 2, "nonzero": 6, "confidence": 12, "bands": 12, "judge": 2, "making": 10, "volatile": 13, "plane": 2, "vertices": 2, "define": 4, "discriminant": 2, "passes": 2, "connects": 2, "simultaneously": 6, "two": 11, "fail": 12, "third": 2, "25": 16, "misspecification": 17, "instead": 18, "break": 11, "outliers": 35, "fully": 6, "addressed": 2, "plot": 35, "shifts": 10, "original": 8, "hasn": 6, "captured": 13, "fix": 7, "sometimes": 4, "extends": 6, "stands": 3, "meaning": 2, "needs": 13, "become": 12, "differenced": 22, "commonly": 2, "th": 4, "degree": 5, "nabla": 5, "drift": 21, "ct": 2, "expanding": 13, "ima": 3, "weighted": 14, "ewma": 2, "forms": 4, "basis": 4, "notation": 7, "contributes": 4, "property": 2, "revert": 2, "growth": 9, "widen": 9, "accumulated": 5, "uncertainty": 32, "methodology": 4, "adf": 21, "kpss": 21, "until": 13, "orders": 6, "determining": 4, "symptom": 4, "wanders": 2, "persists": 2, "already": 7, "fluctuates": 5, "around": 9, "introduces": 10, "misspecifying": 2, "long": 33, "rarely": 11, "transformation": 11, "types": 8, "deterministic": 9, "regression": 22, "stochastic": 7, "counts": 4, "negatives": 8, "constrained": 4, "adfuller": 11, "dx": 5, "integrate": 5, "cumsum": 14, "diff": 8, "n10": 4, "2f": 20, "95": 24, "ci": 2, "iloc": 10, "stand": 2, "denoted": 2, "exactly": 10, "inverse": 2, "continuous": 6, "analogy": 4, "discrete": 4, "summed": 2, "trends": 13, "tell": 2, "looks": 4, "corrected": 8, "excessive": 2, "alternation": 2, "adds": 19, "behaves": 2, "everyone": 2, "especially": 12, "alpha": 38, "recursively": 8, "update": 20, "rearranging": 4, "correspond": 2, "unknown": 7, "effect": 14, "grows": 27, "permanent": 2, "bound": 9, "96": 23, "interval": 22, "width": 12, "becoming": 2, "arbitrarily": 4, "narrow": 9, "provide": 4, "tight": 7, "judgment": 4, "scenarios": 4, "matter": 10, "planning": 6, "showing": 2, "clear": 8, "upward": 2, "13": 10, "appropriate": 11, "handles": 22, "interaction": 9, "airline": 6, "next": 4, "remaining": 4, "degrading": 2, "control": 14, "17": 4, "hyndman": 31, "athanasopoulos": 28, "2021": 30, "principles": 28, "practice": 35, "otexts": 28, "determines": 4, "cox": 2, "tails": 13, "akaike": 7, "ln": 11, "2k": 15, "bayesian": 10, "schwarz": 7, "heavily": 2, "aicc": 11, "removes": 13, "rows": 6, "columns": 4, "insignificant": 2, "top": 8, "corner": 2, "triangle": 2, "strategy": 13, "conclusion": 4, "05": 13, "inconclusive": 2, "null": 9, "complete": 8, "preliminary": 2, "obvious": 2, "changes": 21, "final": 7, "among": 7, "auto": 3, "tools": 2, "package": 2, "pmdarima": 2, "stepwise": 2, "search": 10, "mechanical": 2, "clean": 4, "noisy": 10, "interpretations": 2, "relying": 4, "automated": 4, "features": 25, "autocorrelated": 10, "unreliable": 4, "conservative": 9, "pandas": 9, "pd": 10, "stats": 13, "exercise": 2, "nadf": 2, "nstep": 3, "name": 9, "items": 7, "resid": 19, "except": 6, "failed": 2, "involves": 4, "though": 4, "dependencies": 9, "versus": 2, "lightly": 2, "penalty": 10, "preference": 2, "situations": 2, "sizes": 4, "optimizes": 4, "beats": 5, "guides": 2, "arbiters": 2, "approximate": 9, "bartlett": 5, "approximation": 4, "approximately": 9, "holds": 6, "expressions": 2, "contribute": 6, "precise": 2, "pierce": 2, "modified": 7, "improves": 8, "undersized": 2, "choices": 6, "2s": 5, "slowly": 4, "drops": 5, "favors": 5, "inadequacy": 7, "forming": 2, "simplify": 2, "isn": 10, "checks": 4, "accepting": 6, "early": 16, "1978": 6, "measure": 4, "lack": 4, "biometrika": 8, "65": 4, "297": 6, "303": 4, "benchmark": 5, "regular": 9, "ps": 3, "qs": 3, "elsewhere": 2, "classic": 2, "passenger": 3, "smooths": 4, "3s": 3, "multiplicative": 36, "sar": 2, "weekly": 14, "52": 7, "double": 3, "seasonalities": 5, "alternative": 10, "365": 15, "difficult": 2, "exist": 7, "integer": 12, "days": 14, "year": 15, "directly": 22, "trigonometric": 2, "statespace": 2, "sarimax": 4, "disp": 2, "tables": 2, "nljung": 2, "n12": 4, "month": 11, "affect": 4, "refers": 2, "multiplying": 2, "wouldn": 2, "additive": 33, "implication": 2, "separation": 2, "blends": 2, "confuse": 6, "originally": 2, "yet": 4, "fits": 7, "overlap": 2, "constraint": 8, "observed": 6, "write": 6, "yesterday": 4, "last": 15, "day": 13, "says": 2, "repeat": 7, "continuing": 2, "simplistic": 2, "benefits": 2, "electricity": 2, "24h": 2, "168h": 2, "alternatives": 8, "external": 8, "tbats": 6, "prophet": 4, "neural": 5, "approaches": 15, "week": 9, "exog": 2, "168": 2, "hierarchical": 4, "de": 4, "livera": 4, "snyder": 6, "2011": 4, "jasa": 12, "106": 4, "496": 4, "1513": 4, "1527": 4, "transformer": 6, "addresses": 2, "vanishing": 4, "gradients": 4, "gates": 2, "dilated": 5, "convolutions": 5, "attention": 8, "mechanisms": 5, "dl": 5, "shines": 2, "underperform": 4, "recurrent": 3, "network": 2, "tanh": 3, "forget": 5, "gate": 3, "input": 19, "cell": 4, "tilde": 4, "output": 16, "convolutional": 3, "connections": 2, "sequence": 8, "gradient": 9, "loss": 14, "respect": 4, "activation": 2, "typically": 6, "product": 4, "numbers": 2, "vanishes": 4, "preserving": 2, "sequences": 7, "dilation": 3, "receptive": 3, "field": 3, "layers": 3, "softmax": 3, "qk": 3, "derived": 4, "timesteps": 3, "position": 7, "encoding": 3, "added": 2, "maintain": 5, "training": 28, "teacher": 2, "forcing": 2, "during": 14, "actual": 16, "optimize": 10, "horizons": 22, "torch": 7, "nn": 5, "lstmforecaster": 3, "module": 3, "def": 22, "super": 3, "fc": 3, "forward": 12, "batch": 2, "return": 34, "loop": 4, "64": 4, "optimizer": 4, "optim": 3, "adam": 3, "lr": 4, "001": 2, "mseloss": 3, "epoch": 3, "100": 40, "dataloader": 4, "scenario": 6, "recommendation": 2, "1000": 8, "medium": 4, "related": 10, "transfer": 2, "latency": 2, "parallelizable": 2, "little": 6, "thousands": 4, "wins": 10, "complicated": 4, "architecture": 6, "baselines": 9, "naive": 15, "before": 16, "success": 4, "lookback": 5, "aware": 5, "origin": 15, "split": 21, "clipping": 3, "scheduling": 2, "careful": 4, "initialization": 14, "utils": 2, "tensordataset": 3, "2000": 9, "len": 17, "append": 11, "floattensor": 2, "unsqueeze": 2, "train": 32, "simplelstm": 2, "32": 5, "shuffle": 2, "evaluate": 12, "eval": 2, "rmse": 15, "address": 4, "vanilla": 2, "multiplied": 2, "timestep": 2, "vanish": 2, "preventing": 2, "updated": 4, "additively": 2, "multiplicatively": 2, "controls": 4, "keep": 2, "flows": 2, "unchanged": 2, "hundreds": 2, "acts": 2, "highway": 2, "bypassing": 2, "completely": 4, "solves": 2, "structures": 2, "parallelization": 2, "sequential": 9, "efficiently": 4, "easily": 2, "speed": 4, "length": 12, "truly": 2, "processing": 4, "online": 15, "streaming": 2, "naturally": 4, "lengths": 4, "tracking": 12, "smaller": 7, "research": 7, "finding": 4, "matches": 8, "benchmarks": 2, "defaulting": 4, "comparable": 2, "convolution": 2, "increasing": 4, "gaps": 2, "inputs": 7, "layer": 2, "rf": 4, "255": 2, "511": 2, "actually": 4, "positional": 2, "permutation": 2, "invariant": 4, "treats": 2, "permute": 2, "pairwise": 2, "similarities": 2, "embeddings": 2, "pe": 2, "2i": 2, "10000": 2, "bag": 2, "vectors": 2, "losing": 4, "entirely": 2, "energy": 2, "oscillate": 4, "defaults": 4, "scheduler": 2, "occur": 6, "norms": 2, "scaled": 11, "saturation": 2, "standardize": 4, "targets": 4, "epochs": 2, "learned": 4, "longer": 10, "curve": 2, "mse": 2, "dominated": 2, "diagnosis": 4, "decreasing": 4, "magnitudes": 2, "visualize": 2, "actuals": 6, "failure": 4, "hochreiter": 2, "schmidhuber": 2, "1997": 4, "computation": 5, "1735": 2, "1780": 2, "bai": 2, "kolter": 2, "koltun": 2, "2018": 4, "empirical": 8, "generic": 2, "networks": 2, "arxiv": 6, "1803": 2, "01271": 2, "vaswani": 2, "et": 2, "al": 2, "neurips": 2, "lim": 2, "zohren": 2, "survey": 6, "philosophical": 2, "transactions": 4, "379": 2, "2194": 2, "unifying": 2, "named": 2, "30": 14, "variants": 10, "taxonomy": 2, "none": 6, "transitions": 3, "exp": 4, "ahead": 15, "formulation": 3, "analytical": 2, "simulation": 6, "paths": 4, "percentiles": 5, "valid": 2, "certain": 3, "implementations": 6, "restrict": 2, "admissible": 3, "broader": 2, "proportional": 7, "averaging": 5, "impractical": 2, "coverage": 15, "nominal": 6, "holtwinters": 8, "exponentialsmoothing": 8, "120": 4, "various": 2, "mul": 5, "inf": 4, "sorted": 4, "best": 15, "min": 18, "nbest": 4, "traditional": 2, "foundation": 2, "comparing": 8, "systematically": 4, "hoc": 2, "formulas": 6, "unified": 4, "automatic": 10, "algorithmically": 2, "gave": 2, "lacked": 2, "principled": 2, "scales": 8, "absolute": 15, "percentage": 10, "business": 17, "contexts": 4, "fitted": 12, "natural": 4, "underestimates": 10, "transition": 4, "inadmissible": 2, "identifiability": 2, "specifically": 6, "bounds": 6, "let": 6, "admissibility": 2, "much": 10, "standardized": 7, "justifies": 4, "reasonable": 6, "reasonableness": 2, "damping": 4, "98": 4, "boundaries": 6, "risk": 6, "koehler": 4, "grose": 2, "2002": 4, "ijf": 12, "18": 2, "439": 2, "454": 2, "ord": 4, "2008": 6, "trajectory": 3, "persistent": 3, "variant": 2, "dampens": 2, "intercept": 2, "slope": 4, "connection": 11, "asymptote": 2, "dies": 2, "2h": 5, "initialize": 10, "flatten": 4, "historical": 15, "reversals": 3, "generally": 4, "safer": 2, "production": 17, "extrapolating": 2, "far": 2, "continue": 4, "indefinitely": 3, "frequent": 4, "makes": 11, "sticky": 2, "recognize": 2, "extrapolation": 6, "transforms": 4, "outperforms": 2, "optimized": 8, "19": 5, "ndamped": 2, "responsive": 4, "separating": 2, "couldn": 2, "behaviors": 2, "aspects": 2, "optimizing": 6, "independently": 2, "realistic": 2, "quantities": 4, "populations": 2, "linearly": 11, "toward": 6, "safety": 2, "prevents": 2, "extreme": 9, "competitions": 2, "hedges": 2, "continues": 4, "stops": 2, "closer": 2, "limit": 2, "asymptotes": 2, "compound": 4, "recent": 12, "never": 14, "fixed": 21, "pushes": 2, "quality": 4, "track": 10, "closely": 2, "adapts": 7, "keeps": 2, "throughout": 4, "1957": 2, "averages": 13, "onr": 2, "memorandum": 2, "gardner": 6, "mckenzie": 2, "1985": 4, "management": 4, "science": 4, "31": 2, "1237": 2, "1246": 2, "makridakis": 6, "hibon": 2, "m3": 2, "competition": 6, "16": 4, "451": 2, "476": 2, "version": 3, "ell": 2, "hb": 2, "variation": 8, "lfloor": 2, "rfloor": 2, "amount": 3, "swings": 5, "ratio": 13, "std": 23, "indices": 11, "normalization": 6, "applied": 2, "equivalence": 4, "combine": 12, "flattens": 2, "2nd": 2, "1st": 2, "normalize": 9, "sse": 5, "mae": 11, "typical": 8, "ranges": 2, "degrades": 2, "significantly": 12, "insufficient": 2, "full": 23, "reliable": 4, "years": 7, "hw": 2, "nmultiplicative": 2, "decide": 2, "roughly": 6, "decreases": 4, "proportionally": 2, "visual": 7, "inspection": 9, "subperiods": 2, "normalized": 6, "systematic": 13, "shift": 18, "implementing": 2, "develop": 2, "assume": 10, "simplification": 2, "2m": 8, "case": 6, "chase": 2, "simplified": 6, "accurate": 8, "widening": 4, "narrowing": 2, "retail": 6, "december": 6, "grown": 2, "100k": 2, "200k": 2, "doubled": 2, "grew": 2, "400k": 2, "deviation": 8, "150k": 2, "match": 9, "verification": 2, "percentages": 2, "growing": 2, "looking": 4, "deviations": 2, "question": 2, "scale": 14, "1960": 4, "324": 2, "342": 2, "chatfield": 4, "yar": 2, "1988": 4, "statistician": 2, "129": 2, "140": 2, "2006": 4, "art": 4, "ii": 2, "637": 2, "666": 2, "responsiveness": 3, "closed": 2, "heavy": 13, "light": 2, "trending": 3, "bar": 14, "weight": 7, "practitioners": 2, "expect": 8, "simpleexpsmoothing": 3, "alphas": 2, "fittedvalues": 4, "modeled": 4, "reason": 2, "distant": 6, "adapt": 5, "potential": 4, "balances": 4, "considerations": 6, "found": 4, "smoother": 8, "behind": 4, "geometric": 2, "goes": 2, "almost": 4, "volatility": 4, "pulling": 2, "inflate": 4, "chasing": 4, "confirming": 2, "investigate": 4, "28": 6, "brown": 2, "1959": 2, "inventory": 4, "mcgraw": 2, "hill": 2, "quantify": 4, "giving": 8, "fall": 5, "bootstrap": 10, "parametric": 4, "containing": 2, "probability": 12, "proportion": 2, "falling": 2, "specific": 11, "80": 15, "accounting": 2, "intrinsic": 2, "calculate": 5, "refit": 4, "replacement": 2, "97": 2, "pis": 3, "undercoverage": 3, "misspecified": 2, "underestimated": 3, "overcoverage": 2, "skewed": 2, "tailed": 6, "uncertain": 4, "wider": 5, "accumulate": 8, "know": 2, "exceptions": 2, "reverting": 2, "unconditional": 2, "85": 5, "supposed": 2, "tested": 6, "ignored": 8, "remedies": 2, "heavier": 2, "distributions": 7, "trusting": 2, "stakeholders": 4, "want": 4, "worst": 2, "translate": 2, "upper": 6, "exceeds": 4, "quantiles": 2, "exceeding": 2, "message": 2, "statements": 2, "acceptable": 4, "exceeded": 2, "calculating": 2, "statistics": 20, "121": 2, "135": 4, "thombs": 2, "schucany": 2, "1990": 6, "410": 2, "486": 2, "iterate": 5, "mimo": 10, "accumulates": 5, "accumulation": 6, "ml": 14, "given": 7, "iterated": 3, "repeatedly": 4, "feeding": 3, "outputs": 5, "dirrec": 2, "hybrid": 7, "underlying": 4, "dgp": 2, "iterations": 7, "propagation": 2, "trained": 2, "target": 11, "violate": 2, "rec": 3, "compounds": 6, "dir": 3, "compounding": 5, "ben": 3, "taieb": 3, "correct": 11, "msfe": 2, "outperform": 2, "guidelines": 2, "optimality": 2, "quantification": 6, "elif": 4, "nonparametric": 2, "implementation": 4, "once": 3, "multioutputregressor": 5, "tree": 8, "extrapolate": 3, "exploding": 2, "trust": 4, "inconsistency": 4, "computational": 12, "cost": 16, "expensive": 6, "suboptimal": 6, "sklearn": 10, "ridge": 4, "multioutput": 2, "dataset": 4, "400": 2, "reshape": 2, "predicted": 2, "theoretically": 4, "mainly": 4, "trees": 4, "nns": 2, "mode": 9, "anchors": 2, "efficiency": 4, "boosting": 3, "expectation": 2, "correctly": 2, "place": 2, "rao": 2, "effectively": 2, "xgboost": 4, "back": 6, "erratic": 2, "computationally": 6, "xgbregressor": 2, "custom": 4, "lightgbm": 2, "2014": 4, "kaggle": 2, "load": 2, "382": 2, "394": 2, "chevillon": 2, "2007": 4, "surveys": 6, "746": 2, "785": 2, "marcellino": 2, "multistep": 2, "macroeconomic": 2, "499": 2, "526": 2, "bontempi": 2, "atiya": 2, "sorjamaa": 2, "2012": 8, "review": 8, "nn5": 2, "expert": 2, "39": 2, "7067": 2, "7083": 2, "compiles": 2, "asked": 3, "concise": 2, "topics": 2, "span": 2, "fundamentals": 2, "advanced": 2, "interviews": 2, "unpredictable": 2, "conclusions": 4, "detrending": 5, "changing": 14, "separates": 6, "remainder": 10, "dummy": 5, "arimax": 2, "interpretability": 8, "primary": 2, "goal": 5, "whichever": 2, "implement": 2, "context": 12, "machine": 6, "calibrated": 2, "lots": 2, "q6": 2, "2ln": 2, "overfit": 2, "40": 7, "guideline": 2, "q7": 2, "interpolation": 4, "sporadic": 2, "fill": 4, "imputation": 4, "previous": 2, "em": 2, "indicator": 4, "binary": 8, "delete": 2, "continuity": 2, "globally": 2, "q8": 2, "prevent": 4, "sources": 4, "prevention": 2, "temporally": 2, "scalers": 2, "availability": 6, "q9": 2, "feed": 2, "offs": 2, "q10": 2, "mape": 10, "undefined": 4, "mase": 9, "free": 2, "compares": 4, "q11": 2, "filtering": 7, "arrives": 4, "q12": 2, "limitations": 2, "missed": 6, "q13": 2, "occam": 2, "razor": 2, "popular": 2, "architectures": 2, "cnn": 2, "q14": 2, "chow": 4, "cusum": 9, "cumulative": 9, "pelt": 8, "markov": 2, "post": 2, "q15": 2, "reconciliation": 4, "ensuring": 4, "aggregation": 4, "category": 2, "sums": 6, "aggregate": 2, "distribute": 2, "bottom": 2, "individuals": 2, "optimally": 2, "mint": 2, "budgets": 2, "plots": 2, "skipping": 2, "underfitting": 2, "manifestation": 2, "great": 2, "regularization": 2, "penalize": 2, "relate": 4, "deeply": 2, "connected": 2, "equivalents": 2, "presence": 2, "hegy": 4, "canova": 2, "hansen": 2, "ocsb": 2, "retailer": 2, "understand": 2, "granularity": 2, "holidays": 4, "promotions": 4, "cv": 11, "weeks": 2, "communication": 2, "monitoring": 12, "retraining": 8, "schedule": 4, "jumping": 2, "hard": 2, "beat": 2, "justified": 4, "fold": 6, "windows": 10, "blocked": 3, "hyperparameter": 2, "tuning": 6, "seen": 2, "sliding": 4, "oldest": 3, "origins": 5, "smape": 2, "fails": 5, "randomly": 5, "splits": 9, "sees": 4, "optimistic": 5, "iteration": 4, "store": 4, "blocks": 2, "contiguous": 2, "ideal": 2, "accidentally": 2, "overlapping": 5, "guarantee": 6, "refitting": 2, "representative": 2, "old": 3, "mislead": 4, "nan": 4, "isnan": 4, "assigns": 2, "folds": 2, "breaking": 4, "unrealistic": 4, "implicitly": 2, "learns": 6, "inflating": 2, "apparent": 2, "effective": 4, "independence": 4, "contains": 2, "60": 2, "55": 2, "timeseriessplit": 6, "evolving": 8, "refits": 2, "misleads": 2, "advantages": 6, "division": 2, "asymmetrically": 2, "defined": 2, "intermittent": 2, "asymmetric": 4, "150": 4, "treated": 2, "differently": 4, "dependent": 4, "requirements": 2, "guidance": 2, "5k": 2, "leaves": 4, "1095": 2, "design": 2, "scheme": 2, "103": 2, "sets": 2, "gap": 4, "skip": 2, "simulate": 6, "delay": 5, "723": 4, "bergmeir": 2, "predictor": 4, "sciences": 2, "191": 2, "192": 2, "213": 2, "tashman": 2, "437": 2, "450": 2, "cerqueira": 2, "torgo": 2, "2020": 6, "evaluating": 4, "109": 2, "2028": 2, "2log": 2, "recovery": 2, "tends": 2, "disagree": 2, "minimizes": 3, "kullback": 2, "leibler": 2, "divergence": 2, "kl": 2, "showed": 3, "approximates": 2, "marginal": 4, "08k": 2, "00k": 2, "61k": 2, "91k": 2, "rank": 3, "support": 4, "delta": 7, "1002": 2, "transformations": 7, "adjusting": 2, "increasingly": 2, "ties": 2, "display": 2, "dataframe": 4, "stronger": 2, "6k": 2, "improvement": 4, "underfit": 2, "capturing": 2, "underpenalizes": 2, "overly": 2, "hurvich": 3, "tsai": 3, "1989": 3, "2kn": 2, "corrections": 4, "nested": 4, "offset": 6, "identifies": 8, "goals": 2, "lean": 2, "ensemble": 8, "picking": 2, "dogmatically": 2, "alongside": 2, "1974": 2, "ieee": 2, "716": 2, "estimating": 4, "annals": 2, "461": 2, "464": 2, "burnham": 2, "anderson": 2, "multimodel": 2, "76": 2, "307": 4, "homoskedasticity": 4, "garch": 8, "adjusted": 2, "jarque": 4, "bera": 4, "jb": 2, "skewness": 2, "kurtosis": 2, "arch": 4, "lm": 3, "nr": 3, "runs": 2, "randomness": 2, "consecutive": 2, "checklist": 2, "fanning": 2, "clustering": 2, "histogram": 2, "bell": 2, "shaped": 2, "mostly": 2, "violations": 2, "violation": 2, "outlier": 12, "treatment": 2, "misses": 4, "obsession": 2, "14": 5, "graphics": 4, "tsaplots": 4, "intentionally": 2, "n2": 2, "n3": 2, "stat": 2, "correlate": 2, "n4": 2, "remain": 4, "unexploited": 2, "fixable": 2, "clt": 2, "ignorable": 2, "spending": 2, "effort": 2, "reflexively": 2, "subtract": 7, "account": 4, "computed": 2, "constrain": 2, "oversized": 2, "subtracted": 2, "otherwise": 4, "clusters": 2, "ok": 4, "stabilizing": 2, "adjustment": 4, "calm": 2, "02": 2, "slight": 6, "conclude": 4, "borderline": 4, "fine": 2, "minor": 2, "invalidate": 2, "actions": 4, "visually": 4, "isolated": 6, "ultimate": 2, "accept": 2, "unless": 2, "perfect": 2, "engle": 2, "1982": 2, "heteroscedasticity": 2, "united": 2, "kingdom": 2, "987": 2, "1007": 2, "concerns": 2, "tips": 2, "document": 2, "monitor": 4, "fallback": 2, "performed": 2, "mimicking": 2, "updating": 2, "periodically": 2, "mad": 7, "miscalibration": 2, "pipeline": 11, "config": 2, "float": 2, "score": 11, "retrain": 6, "dashboard": 2, "metric": 3, "warning": 3, "recalibrate": 2, "claim": 2, "static": 2, "regularly": 2, "makers": 2, "tune": 6, "else": 9, "simulated": 2, "alert": 4, "alerts": 6, "consequences": 2, "overoptimistic": 2, "incorrectly": 4, "kfold": 2, "manual": 4, "sudden": 4, "covid": 2, "impact": 4, "customer": 2, "evolution": 2, "recurring": 2, "oscillates": 2, "states": 9, "charts": 2, "flag": 5, "adaptive": 4, "rsfe": 2, "normalizes": 4, "comparability": 2, "distinguishes": 2, "reacting": 2, "fluctuation": 2, "wait": 2, "sustained": 4, "75": 7, "unmodeled": 2, "quantile": 2, "975": 2, "875": 2, "seem": 2, "truthful": 2, "decisions": 4, "worse": 4, "strictly": 5, "historically": 2, "delayed": 2, "unusual": 7, "lucky": 2, "faces": 2, "delays": 2, "evaluated": 2, "convenient": 2, "generalizes": 4, "spanning": 2, "spiliotis": 2, "assimakopoulos": 2, "m4": 2, "000": 2, "61": 2, "54": 4, "74": 4, "gama": 2, "bifet": 2, "pechenizkiy": 2, "bouchachia": 2, "adaptation": 2, "acm": 4, "46": 2, "kolassa": 2, "count": 2, "788": 2, "803": 2, "ready": 2, "date": 4, "predictors": 4, "roll": 2, "extracted": 4, "timestamp": 2, "quarter": 2, "event": 2, "sine": 3, "cosine": 3, "ema": 2, "standardization": 2, "median": 12, "iqr": 6, "scaler": 4, "copy": 3, "datetime": 2, "dayofweek": 3, "dayofyear": 3, "dropna": 4, "leak": 2, "impute": 2, "dimensionality": 2, "inherit": 2, "randomforestregressor": 3, "dates": 2, "2023": 2, "freq": 2, "calendar": 5, "doy": 2, "axis": 2, "importance": 2, "ntop": 2, "ascending": 2, "head": 2, "influence": 6, "influenced": 2, "knows": 2, "world": 2, "manageable": 2, "monday": 2, "excludes": 2, "moves": 2, "represented": 2, "alias": 2, "182": 2, "overkill": 2, "simplest": 4, "plenty": 2, "fillna": 2, "bfill": 2, "earliest": 2, "preserves": 2, "neutral": 2, "isna": 2, "astype": 2, "flexible": 2, "shorter": 2, "warmup": 2, "maximizes": 2, "dropping": 2, "christ": 2, "braun": 2, "neuffer": 2, "kempa": 2, "liehr": 2, "extraction": 7, "scalable": 2, "hypothesis": 6, "neurocomputing": 2, "72": 2, "77": 2, "fulcher": 2, "jones": 2, "hctsa": 2, "phenotyping": 2, "brownlee": 2, "mastery": 2, "propagate": 2, "incorporate": 5, "unifies": 2, "eta": 4, "recursions": 3, "filtered": 6, "gain": 5, "scalar": 2, "steady": 4, "x0": 2, "p0": 3, "inv": 2, "det": 2, "definite": 4, "square": 2, "ud": 2, "factorization": 2, "diffuse": 2, "extensions": 2, "ekf": 2, "ukf": 2, "particle": 2, "mu0": 5, "nsteady": 2, "trusted": 2, "evolves": 2, "prior": 2, "stabilizes": 2, "recognizing": 2, "4qr": 2, "2q": 2, "reach": 2, "smoothed": 7, "retrospective": 4, "refine": 2, "gps": 2, "measurements": 2, "vehicle": 2, "treat": 2, "minus": 2, "grid": 2, "overconfident": 2, "harvey": 2, "durbin": 2, "koopman": 2, "oxford": 2, "basic": 2, "82": 2, "35": 4, "anomalies": 9, "collective": 5, "contextual": 5, "distance": 6, "lof": 4, "isolation": 5, "forest": 5, "autoencoders": 2, "challenge": 2, "defining": 2, "ac": 2, "usage": 2, "winter": 3, "settings": 4, "supervised": 3, "labeled": 2, "semi": 5, "unsupervised": 2, "labels": 2, "chosen": 2, "6745": 3, "isolate": 3, "build": 2, "neighbors": 2, "lrd": 2, "zscore": 4, "events": 2, "masking": 3, "hiding": 2, "swamping": 2, "flagged": 3, "cleaning": 2, "global": 5, "saturday": 2, "weekday": 2, "insert": 2, "convolve": 2, "thresholds": 5, "anomalous": 2, "distinction": 4, "compared": 2, "scores": 2, "masks": 2, "robustness": 7, "4826": 2, "hides": 2, "distances": 2, "rest": 2, "partition": 2, "surrounded": 2, "clustered": 2, "deploy": 2, "triggers": 2, "investigation": 2, "precision": 2, "raise": 2, "detectors": 2, "majority": 2, "agree": 2, "label": 2, "stage": 2, "catch": 2, "rules": 2, "logic": 2, "filters": 2, "recall": 2, "f1": 2, "catching": 2, "fatigue": 2, "chandola": 2, "banerjee": 2, "kumar": 2, "2009": 2, "41": 6, "58": 2, "liu": 2, "ting": 2, "zhou": 2, "icdm": 2, "413": 2, "422": 2, "breunig": 2, "kriegel": 2, "ng": 2, "sander": 2, "identifying": 2, "sigmod": 2, "93": 2, "104": 2, "hochenbaum": 2, "vallis": 2, "kejariwal": 2, "cloud": 2, "1704": 2, "07706": 2, "irregular": 6, "centered": 5, "repeating": 2, "cma": 3, "odd": 5, "center": 2, "calculation": 2, "detrended": 2, "season": 6, "jm": 2, "ratios": 2, "attenuates": 2, "leave": 2, "mod": 2, "replicate": 2, "trading": 3, "easter": 2, "ntrend": 2, "ncompare": 2, "nanstd": 2, "temp": 2, "locally": 7, "cancels": 2, "removed": 2, "lost": 2, "positions": 5, "half": 2, "seasons": 2, "deflate": 2, "decompose": 4, "wheelwright": 2, "1998": 2, "census": 2, "bureau": 2, "13arima": 2, "seats": 2, "reference": 2, "loess": 8, "robustly": 2, "movement": 2, "everything": 4, "scatterplot": 2, "downweight": 2, "tricube": 3, "bandwidth": 3, "outer": 3, "inner": 2, "subseries": 4, "deseasonalize": 3, "deseasonalized": 3, "assign": 3, "bisquare": 3, "downweighted": 2, "extract": 2, "absorbs": 2, "distort": 3, "exponentiate": 2, "nremainder": 2, "fig": 4, "flexibility": 2, "adjustable": 2, "smoothness": 2, "ends": 2, "work": 2, "iterative": 2, "reweighting": 2, "normally": 2, "wondering": 2, "distorts": 2, "differentiable": 2, "curves": 2, "downweighting": 2, "smoothly": 2, "nice": 2, "jagged": 2, "leaked": 2, "januaries": 2, "drifts": 4, "leaving": 2, "remains": 2, "biases": 2, "cleveland": 2, "mcrae": 2, "terpenning": 2, "official": 2, "73": 2, "devlin": 2, "83": 2, "403": 2, "596": 2, "610": 2, "dokumentov": 2, "str": 2, "monash": 2, "working": 2, "papers": 2, "penalized": 2, "offline": 6, "programming": 3, "tau": 3, "away": 3, "allowance": 2, "segmentation": 6, "greedy": 3, "rss": 2, "pruned": 2, "partitioning": 3, "pruning": 2, "eliminate": 2, "segmentations": 3, "achieve": 2, "cp": 2, "alarms": 3, "reset": 2, "penalties": 2, "segment": 3, "segments": 4, "ruptures": 2, "rpt": 4, "concatenate": 2, "algo": 2, "l2": 4, "pen": 2, "binseg": 2, "norm": 2, "hazard": 2, "cpd": 2, "probabilities": 2, "predprob": 2, "pdf": 2, "argmax": 2, "revise": 2, "fraud": 2, "locations": 2, "building": 2, "providing": 2, "capability": 2, "style": 2, "intensive": 2, "elbow": 2, "diminishing": 2, "detections": 2, "sic": 2, "mbic": 2, "datasets": 2, "considering": 2, "searches": 2, "signals": 2, "server": 2, "turns": 2, "functions": 4, "l1": 2, "huber": 2, "removal": 2, "resolutions": 2, "confirmation": 4, "alarm": 2, "dominate": 2, "trigger": 2, "truong": 2, "oudre": 2, "vayatis": 2, "selective": 2, "167": 2, "107299": 2, "killick": 2, "fearnhead": 2, "eckley": 2, "changepoints": 2, "107": 2, "1590": 2, "1598": 2, "adams": 2, "mackay": 2, "0710": 2, "3742": 2, "1954": 2, "schemes": 2, "acvf": 2, "corr": 2, "symmetry": 5, "kk": 3, "k1": 2, "k2": 2, "geometrically": 2, "exceed": 3, "applying": 4, "meaningless": 2, "validating": 2, "armaprocess": 3, "75l": 2, "25l": 2, "nsample": 2, "axes": 2, "subplots": 2, "figsize": 2, "ax": 2, "title": 2, "staying": 2, "strongly": 2, "relatively": 2, "formally": 2, "successive": 2, "highly": 2, "conditioning": 2, "thus": 2, "33": 2, "induction": 2, "variability": 2, "individual": 2, "se": 2, "band": 2, "196": 2, "1946": 2, "jrss": 2, "27": 2, "made": 2, "strict": 4, "weakly": 4, "ergodicity": 2, "ergodic": 2, "population": 2, "realization": 2, "encountered": 2, "cauchy": 2, "dickey": 3, "fuller": 3, "exists": 2, "complementary": 2, "quadratic": 2, "unnecessary": 2, "ar1": 3, "rw": 2, "covariances": 2, "gaussianity": 2, "constrains": 2, "violating": 2, "summation": 2, "differentiation": 2, "recurrence": 2, "08": 2, "03": 2, "contradictory": 2, "inspect": 2, "fractionally": 2, "supports": 2, "serial": 2, "1979": 2, "estimators": 2, "366": 2, "427": 2, "431": 2, "kwiatkowski": 2, "phillips": 2, "schmidt": 2, "shin": 2, "1992": 2, "159": 2, "178": 2}, "inverted_index": {"time": {"docs/en/index.md": 12, "docs/en/index.md#section_0": 3, "docs/en/index.md#section_1": 3, "docs/en/index.md#section_3": 5, "docs/en/spectral/spectral-analysis.md": 8, "docs/en/spectral/spectral-analysis.md#section_0": 1, "docs/en/spectral/spectral-analysis.md#section_1": 2, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/spectral/spectral-analysis.md#section_16": 3, "docs/en/multivariate/granger-causality.md": 4, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/multivariate/granger-causality.md#section_16": 2, "docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_0": 1, "docs/en/multivariate/var.md#section_16": 2, "docs/en/time-domain/ma.md": 5, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/ma.md#section_14": 4, "docs/en/time-domain/arma.md": 5, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/arma.md#section_15": 4, "docs/en/time-domain/ar.md": 7, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/ar.md#section_15": 4, "docs/en/time-domain/arima.md": 5, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/time-domain/arima.md#section_18": 3, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_16": 3, "docs/en/time-domain/sarima.md": 3, "docs/en/time-domain/sarima.md#section_16": 3, "docs/en/deep-learning/deep-learning-ts.md": 8, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/deep-learning/deep-learning-ts.md#section_4": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 3, "docs/en/exponential-smoothing/holt-winters.md#section_13": 3, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_6": 1, "docs/en/forecasting/prediction-intervals.md": 3, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_18": 2, "docs/en/interview/interview-questions.md": 14, "docs/en/interview/interview-questions.md#section_0": 1, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/interview/interview-questions.md#section_8": 2, "docs/en/interview/interview-questions.md#section_11": 2, "docs/en/interview/interview-questions.md#section_13": 1, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/interview/interview-questions.md#section_17": 3, "docs/en/model-selection/cross-validation.md": 11, "docs/en/model-selection/cross-validation.md#section_0": 2, "docs/en/model-selection/cross-validation.md#section_1": 1, "docs/en/model-selection/cross-validation.md#section_5": 2, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/model-selection/cross-validation.md#section_12": 2, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_4": 1, "docs/en/model-selection/information-criteria.md#section_14": 1, "docs/en/model-selection/residual-diagnostics.md": 5, "docs/en/model-selection/residual-diagnostics.md#section_6": 2, "docs/en/model-selection/residual-diagnostics.md#section_18": 3, "docs/en/practical/practical-modeling.md": 12, "docs/en/practical/practical-modeling.md#section_0": 2, "docs/en/practical/practical-modeling.md#section_1": 1, "docs/en/practical/practical-modeling.md#section_13": 7, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/features/feature-engineering.md": 9, "docs/en/features/feature-engineering.md#section_0": 3, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/features/feature-engineering.md#section_17": 3, "docs/en/state-space/kalman-filter.md": 10, "docs/en/state-space/kalman-filter.md#section_0": 2, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/state-space/kalman-filter.md#section_13": 4, "docs/en/state-space/kalman-filter.md#section_14": 3, "docs/en/anomaly-detection/anomaly-detection.md": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_3": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/classical.md#section_13": 1, "docs/en/change-detection/change-point.md": 7, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/change-detection/change-point.md#section_1": 1, "docs/en/change-detection/change-point.md#section_4": 1, "docs/en/change-detection/change-point.md#section_5": 1, "docs/en/change-detection/change-point.md#section_14": 3, "docs/en/foundations/autocorrelation.md": 5, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/autocorrelation.md#section_17": 4, "docs/en/foundations/stationarity.md": 9, "docs/en/foundations/stationarity.md#section_0": 2, "docs/en/foundations/stationarity.md#section_1": 2, "docs/en/foundations/stationarity.md#section_13": 2, "docs/en/foundations/stationarity.md#section_14": 3}, "series": {"docs/en/index.md": 6, "docs/en/index.md#section_0": 3, "docs/en/index.md#section_1": 2, "docs/en/spectral/spectral-analysis.md": 6, "docs/en/spectral/spectral-analysis.md#section_0": 2, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/spectral/spectral-analysis.md#section_16": 3, "docs/en/multivariate/granger-causality.md": 8, "docs/en/multivariate/granger-causality.md#section_6": 2, "docs/en/multivariate/granger-causality.md#section_15": 4, "docs/en/multivariate/granger-causality.md#section_16": 2, "docs/en/multivariate/var.md": 4, "docs/en/multivariate/var.md#section_0": 2, "docs/en/multivariate/var.md#section_16": 2, "docs/en/time-domain/ma.md": 4, "docs/en/time-domain/ma.md#section_14": 4, "docs/en/time-domain/arma.md": 4, "docs/en/time-domain/arma.md#section_15": 4, "docs/en/time-domain/ar.md": 6, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/ar.md#section_15": 4, "docs/en/time-domain/arima.md": 21, "docs/en/time-domain/arima.md#section_0": 3, "docs/en/time-domain/arima.md#section_8": 4, "docs/en/time-domain/arima.md#section_9": 4, "docs/en/time-domain/arima.md#section_12": 1, "docs/en/time-domain/arima.md#section_17": 6, "docs/en/time-domain/arima.md#section_18": 3, "docs/en/time-domain/identification.md": 8, "docs/en/time-domain/identification.md#section_6": 2, "docs/en/time-domain/identification.md#section_10": 1, "docs/en/time-domain/identification.md#section_13": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/identification.md#section_16": 3, "docs/en/time-domain/sarima.md": 6, "docs/en/time-domain/sarima.md#section_7": 2, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/time-domain/sarima.md#section_16": 3, "docs/en/deep-learning/deep-learning-ts.md": 8, "docs/en/deep-learning/deep-learning-ts.md#section_0": 2, "docs/en/deep-learning/deep-learning-ts.md#section_4": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1, "docs/en/exponential-smoothing/holt.md": 3, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 4, "docs/en/exponential-smoothing/holt-winters.md#section_2": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 3, "docs/en/exponential-smoothing/ses.md": 10, "docs/en/exponential-smoothing/ses.md#section_0": 1, "docs/en/exponential-smoothing/ses.md#section_6": 1, "docs/en/exponential-smoothing/ses.md#section_13": 8, "docs/en/forecasting/prediction-intervals.md": 3, "docs/en/forecasting/prediction-intervals.md#section_6": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_18": 2, "docs/en/interview/interview-questions.md": 18, "docs/en/interview/interview-questions.md#section_0": 1, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/interview/interview-questions.md#section_11": 1, "docs/en/interview/interview-questions.md#section_13": 2, "docs/en/interview/interview-questions.md#section_16": 6, "docs/en/interview/interview-questions.md#section_17": 3, "docs/en/model-selection/cross-validation.md": 13, "docs/en/model-selection/cross-validation.md#section_0": 2, "docs/en/model-selection/cross-validation.md#section_1": 1, "docs/en/model-selection/cross-validation.md#section_5": 3, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 3, "docs/en/model-selection/cross-validation.md#section_12": 2, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_4": 1, "docs/en/model-selection/information-criteria.md#section_14": 1, "docs/en/model-selection/residual-diagnostics.md": 6, "docs/en/model-selection/residual-diagnostics.md#section_17": 3, "docs/en/model-selection/residual-diagnostics.md#section_18": 3, "docs/en/practical/practical-modeling.md": 5, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/features/feature-engineering.md": 8, "docs/en/features/feature-engineering.md#section_0": 2, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/features/feature-engineering.md#section_15": 1, "docs/en/features/feature-engineering.md#section_17": 3, "docs/en/state-space/kalman-filter.md": 5, "docs/en/state-space/kalman-filter.md#section_0": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/state-space/kalman-filter.md#section_14": 3, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_3": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1, "docs/en/decomposition/classical.md": 7, "docs/en/decomposition/classical.md#section_0": 1, "docs/en/decomposition/classical.md#section_5": 1, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/classical.md#section_12": 3, "docs/en/decomposition/classical.md#section_13": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_0": 1, "docs/en/decomposition/stl.md#section_3": 1, "docs/en/foundations/autocorrelation.md": 10, "docs/en/foundations/autocorrelation.md#section_0": 1, "docs/en/foundations/autocorrelation.md#section_9": 1, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_16": 3, "docs/en/foundations/autocorrelation.md#section_17": 4, "docs/en/foundations/stationarity.md": 21, "docs/en/foundations/stationarity.md#section_0": 2, "docs/en/foundations/stationarity.md#section_5": 2, "docs/en/foundations/stationarity.md#section_6": 3, "docs/en/foundations/stationarity.md#section_7": 3, "docs/en/foundations/stationarity.md#section_13": 8, "docs/en/foundations/stationarity.md#section_14": 3}, "study": {"docs/en/index.md": 3, "docs/en/index.md#section_0": 2, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1}, "notes": {"docs/en/index.md": 3, "docs/en/index.md#section_0": 2}, "div": {"docs/en/index.md": 2, "docs/en/index.md#section_0": 2, "docs/en/spectral/spectral-analysis.md": 22, "docs/en/spectral/spectral-analysis.md#section_0": 2, "docs/en/spectral/spectral-analysis.md#section_15": 20, "docs/en/multivariate/granger-causality.md": 22, "docs/en/multivariate/granger-causality.md#section_0": 2, "docs/en/multivariate/granger-causality.md#section_15": 20, "docs/en/multivariate/var.md": 22, "docs/en/multivariate/var.md#section_0": 2, "docs/en/multivariate/var.md#section_15": 20, "docs/en/time-domain/ma.md": 22, "docs/en/time-domain/ma.md#section_0": 2, "docs/en/time-domain/ma.md#section_13": 20, "docs/en/time-domain/arma.md": 22, "docs/en/time-domain/arma.md#section_0": 2, "docs/en/time-domain/arma.md#section_14": 20, "docs/en/time-domain/ar.md": 22, "docs/en/time-domain/ar.md#section_0": 2, "docs/en/time-domain/ar.md#section_14": 20, "docs/en/time-domain/arima.md": 22, "docs/en/time-domain/arima.md#section_0": 2, "docs/en/time-domain/arima.md#section_17": 20, "docs/en/time-domain/identification.md": 22, "docs/en/time-domain/identification.md#section_0": 2, "docs/en/time-domain/identification.md#section_15": 20, "docs/en/time-domain/sarima.md": 22, "docs/en/time-domain/sarima.md#section_0": 2, "docs/en/time-domain/sarima.md#section_15": 20, "docs/en/deep-learning/deep-learning-ts.md": 22, "docs/en/deep-learning/deep-learning-ts.md#section_0": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 20, "docs/en/exponential-smoothing/ets.md": 22, "docs/en/exponential-smoothing/ets.md#section_0": 2, "docs/en/exponential-smoothing/ets.md#section_13": 20, "docs/en/exponential-smoothing/holt.md": 22, "docs/en/exponential-smoothing/holt.md#section_0": 2, "docs/en/exponential-smoothing/holt.md#section_14": 20, "docs/en/exponential-smoothing/holt-winters.md": 22, "docs/en/exponential-smoothing/holt-winters.md#section_0": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 20, "docs/en/exponential-smoothing/ses.md": 22, "docs/en/exponential-smoothing/ses.md#section_0": 2, "docs/en/exponential-smoothing/ses.md#section_13": 20, "docs/en/forecasting/prediction-intervals.md": 22, "docs/en/forecasting/prediction-intervals.md#section_0": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 20, "docs/en/forecasting/multi-step.md": 22, "docs/en/forecasting/multi-step.md#section_0": 2, "docs/en/forecasting/multi-step.md#section_17": 20, "docs/en/interview/interview-questions.md": 22, "docs/en/interview/interview-questions.md#section_0": 2, "docs/en/interview/interview-questions.md#section_16": 20, "docs/en/model-selection/cross-validation.md": 22, "docs/en/model-selection/cross-validation.md#section_0": 2, "docs/en/model-selection/cross-validation.md#section_11": 20, "docs/en/model-selection/information-criteria.md": 22, "docs/en/model-selection/information-criteria.md#section_0": 2, "docs/en/model-selection/information-criteria.md#section_13": 20, "docs/en/model-selection/residual-diagnostics.md": 22, "docs/en/model-selection/residual-diagnostics.md#section_0": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 20, "docs/en/practical/practical-modeling.md": 22, "docs/en/practical/practical-modeling.md#section_0": 2, "docs/en/practical/practical-modeling.md#section_13": 20, "docs/en/features/feature-engineering.md": 22, "docs/en/features/feature-engineering.md#section_0": 2, "docs/en/features/feature-engineering.md#section_16": 20, "docs/en/state-space/kalman-filter.md": 22, "docs/en/state-space/kalman-filter.md#section_0": 2, "docs/en/state-space/kalman-filter.md#section_13": 20, "docs/en/anomaly-detection/anomaly-detection.md": 22, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 20, "docs/en/decomposition/classical.md": 22, "docs/en/decomposition/classical.md#section_0": 2, "docs/en/decomposition/classical.md#section_12": 20, "docs/en/decomposition/stl.md": 22, "docs/en/decomposition/stl.md#section_0": 2, "docs/en/decomposition/stl.md#section_13": 20, "docs/en/change-detection/change-point.md": 22, "docs/en/change-detection/change-point.md#section_0": 2, "docs/en/change-detection/change-point.md#section_14": 20, "docs/en/foundations/autocorrelation.md": 22, "docs/en/foundations/autocorrelation.md#section_0": 2, "docs/en/foundations/autocorrelation.md#section_16": 20, "docs/en/foundations/stationarity.md": 22, "docs/en/foundations/stationarity.md#section_0": 2, "docs/en/foundations/stationarity.md#section_13": 20}, "class": {"docs/en/index.md": 1, "docs/en/index.md#section_0": 1, "docs/en/spectral/spectral-analysis.md": 16, "docs/en/spectral/spectral-analysis.md#section_0": 1, "docs/en/spectral/spectral-analysis.md#section_15": 15, "docs/en/multivariate/granger-causality.md": 16, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/multivariate/granger-causality.md#section_15": 15, "docs/en/multivariate/var.md": 16, "docs/en/multivariate/var.md#section_0": 1, "docs/en/multivariate/var.md#section_15": 15, "docs/en/time-domain/ma.md": 16, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/ma.md#section_13": 15, "docs/en/time-domain/arma.md": 16, "docs/en/time-domain/arma.md#section_0": 1, "docs/en/time-domain/arma.md#section_14": 15, "docs/en/time-domain/ar.md": 16, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/ar.md#section_14": 15, "docs/en/time-domain/arima.md": 16, "docs/en/time-domain/arima.md#section_0": 1, "docs/en/time-domain/arima.md#section_17": 15, "docs/en/time-domain/identification.md": 16, "docs/en/time-domain/identification.md#section_0": 1, "docs/en/time-domain/identification.md#section_15": 15, "docs/en/time-domain/sarima.md": 16, "docs/en/time-domain/sarima.md#section_0": 1, "docs/en/time-domain/sarima.md#section_15": 15, "docs/en/deep-learning/deep-learning-ts.md": 18, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/deep-learning/deep-learning-ts.md#section_6": 1, "docs/en/deep-learning/deep-learning-ts.md#section_13": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 15, "docs/en/exponential-smoothing/ets.md": 16, "docs/en/exponential-smoothing/ets.md#section_0": 1, "docs/en/exponential-smoothing/ets.md#section_13": 15, "docs/en/exponential-smoothing/holt.md": 16, "docs/en/exponential-smoothing/holt.md#section_0": 1, "docs/en/exponential-smoothing/holt.md#section_14": 15, "docs/en/exponential-smoothing/holt-winters.md": 16, "docs/en/exponential-smoothing/holt-winters.md#section_0": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 15, "docs/en/exponential-smoothing/ses.md": 16, "docs/en/exponential-smoothing/ses.md#section_0": 1, "docs/en/exponential-smoothing/ses.md#section_13": 15, "docs/en/forecasting/prediction-intervals.md": 16, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 15, "docs/en/forecasting/multi-step.md": 16, "docs/en/forecasting/multi-step.md#section_0": 1, "docs/en/forecasting/multi-step.md#section_17": 15, "docs/en/interview/interview-questions.md": 16, "docs/en/interview/interview-questions.md#section_0": 1, "docs/en/interview/interview-questions.md#section_16": 15, "docs/en/model-selection/cross-validation.md": 16, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/cross-validation.md#section_11": 15, "docs/en/model-selection/information-criteria.md": 16, "docs/en/model-selection/information-criteria.md#section_0": 1, "docs/en/model-selection/information-criteria.md#section_13": 15, "docs/en/model-selection/residual-diagnostics.md": 16, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 15, "docs/en/practical/practical-modeling.md": 16, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/practical/practical-modeling.md#section_13": 15, "docs/en/features/feature-engineering.md": 16, "docs/en/features/feature-engineering.md#section_0": 1, "docs/en/features/feature-engineering.md#section_16": 15, "docs/en/state-space/kalman-filter.md": 16, "docs/en/state-space/kalman-filter.md#section_0": 1, "docs/en/state-space/kalman-filter.md#section_13": 15, "docs/en/anomaly-detection/anomaly-detection.md": 16, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 15, "docs/en/decomposition/classical.md": 16, "docs/en/decomposition/classical.md#section_0": 1, "docs/en/decomposition/classical.md#section_12": 15, "docs/en/decomposition/stl.md": 16, "docs/en/decomposition/stl.md#section_0": 1, "docs/en/decomposition/stl.md#section_13": 15, "docs/en/change-detection/change-point.md": 16, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/change-detection/change-point.md#section_14": 15, "docs/en/foundations/autocorrelation.md": 16, "docs/en/foundations/autocorrelation.md#section_0": 1, "docs/en/foundations/autocorrelation.md#section_16": 15, "docs/en/foundations/stationarity.md": 16, "docs/en/foundations/stationarity.md#section_0": 1, "docs/en/foundations/stationarity.md#section_13": 15}, "interview": {"docs/en/index.md": 3, "docs/en/index.md#section_0": 2, "docs/en/index.md#section_2": 1, "docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_0": 2, "docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_0": 2, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_0": 2, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_0": 2, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_0": 2, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_0": 2, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_0": 2, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_0": 2, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_0": 2, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_0": 2, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_0": 2, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_0": 2, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_0": 2, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_0": 2, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_0": 2, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_0": 2, "docs/en/interview/interview-questions.md": 5, "docs/en/interview/interview-questions.md#section_0": 4, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_0": 2, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_0": 2, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_0": 2, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_0": 2, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_0": 2, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_0": 2, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 2, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_0": 2, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_0": 2, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_0": 2, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_0": 2, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_0": 2}, "summary": {"docs/en/index.md": 2, "docs/en/index.md#section_0": 1, "docs/en/index.md#section_2": 1, "docs/en/spectral/spectral-analysis.md": 12, "docs/en/spectral/spectral-analysis.md#section_0": 2, "docs/en/spectral/spectral-analysis.md#section_15": 10, "docs/en/multivariate/granger-causality.md": 12, "docs/en/multivariate/granger-causality.md#section_0": 2, "docs/en/multivariate/granger-causality.md#section_15": 10, "docs/en/multivariate/var.md": 13, "docs/en/multivariate/var.md#section_0": 2, "docs/en/multivariate/var.md#section_11": 1, "docs/en/multivariate/var.md#section_15": 10, "docs/en/time-domain/ma.md": 12, "docs/en/time-domain/ma.md#section_0": 2, "docs/en/time-domain/ma.md#section_13": 10, "docs/en/time-domain/arma.md": 12, "docs/en/time-domain/arma.md#section_0": 2, "docs/en/time-domain/arma.md#section_14": 10, "docs/en/time-domain/ar.md": 12, "docs/en/time-domain/ar.md#section_0": 2, "docs/en/time-domain/ar.md#section_14": 10, "docs/en/time-domain/arima.md": 12, "docs/en/time-domain/arima.md#section_0": 2, "docs/en/time-domain/arima.md#section_17": 10, "docs/en/time-domain/identification.md": 12, "docs/en/time-domain/identification.md#section_0": 2, "docs/en/time-domain/identification.md#section_15": 10, "docs/en/time-domain/sarima.md": 13, "docs/en/time-domain/sarima.md#section_0": 2, "docs/en/time-domain/sarima.md#section_12": 1, "docs/en/time-domain/sarima.md#section_15": 10, "docs/en/deep-learning/deep-learning-ts.md": 12, "docs/en/deep-learning/deep-learning-ts.md#section_0": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 10, "docs/en/exponential-smoothing/ets.md": 12, "docs/en/exponential-smoothing/ets.md#section_0": 2, "docs/en/exponential-smoothing/ets.md#section_13": 10, "docs/en/exponential-smoothing/holt.md": 12, "docs/en/exponential-smoothing/holt.md#section_0": 2, "docs/en/exponential-smoothing/holt.md#section_14": 10, "docs/en/exponential-smoothing/holt-winters.md": 12, "docs/en/exponential-smoothing/holt-winters.md#section_0": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 10, "docs/en/exponential-smoothing/ses.md": 12, "docs/en/exponential-smoothing/ses.md#section_0": 2, "docs/en/exponential-smoothing/ses.md#section_13": 10, "docs/en/forecasting/prediction-intervals.md": 13, "docs/en/forecasting/prediction-intervals.md#section_0": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 11, "docs/en/forecasting/multi-step.md": 12, "docs/en/forecasting/multi-step.md#section_0": 2, "docs/en/forecasting/multi-step.md#section_17": 10, "docs/en/interview/interview-questions.md": 12, "docs/en/interview/interview-questions.md#section_0": 2, "docs/en/interview/interview-questions.md#section_16": 10, "docs/en/model-selection/cross-validation.md": 12, "docs/en/model-selection/cross-validation.md#section_0": 2, "docs/en/model-selection/cross-validation.md#section_11": 10, "docs/en/model-selection/information-criteria.md": 12, "docs/en/model-selection/information-criteria.md#section_0": 2, "docs/en/model-selection/information-criteria.md#section_13": 10, "docs/en/model-selection/residual-diagnostics.md": 12, "docs/en/model-selection/residual-diagnostics.md#section_0": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 10, "docs/en/practical/practical-modeling.md": 12, "docs/en/practical/practical-modeling.md#section_0": 2, "docs/en/practical/practical-modeling.md#section_13": 10, "docs/en/features/feature-engineering.md": 12, "docs/en/features/feature-engineering.md#section_0": 2, "docs/en/features/feature-engineering.md#section_16": 10, "docs/en/state-space/kalman-filter.md": 12, "docs/en/state-space/kalman-filter.md#section_0": 2, "docs/en/state-space/kalman-filter.md#section_13": 10, "docs/en/anomaly-detection/anomaly-detection.md": 12, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 10, "docs/en/decomposition/classical.md": 12, "docs/en/decomposition/classical.md#section_0": 2, "docs/en/decomposition/classical.md#section_12": 10, "docs/en/decomposition/stl.md": 12, "docs/en/decomposition/stl.md#section_0": 2, "docs/en/decomposition/stl.md#section_13": 10, "docs/en/change-detection/change-point.md": 12, "docs/en/change-detection/change-point.md#section_0": 2, "docs/en/change-detection/change-point.md#section_14": 10, "docs/en/foundations/autocorrelation.md": 12, "docs/en/foundations/autocorrelation.md#section_0": 2, "docs/en/foundations/autocorrelation.md#section_16": 10, "docs/en/foundations/stationarity.md": 12, "docs/en/foundations/stationarity.md#section_0": 2, "docs/en/foundations/stationarity.md#section_13": 10}, "strong": {"docs/en/index.md": 2, "docs/en/index.md#section_0": 2, "docs/en/spectral/spectral-analysis.md": 33, "docs/en/spectral/spectral-analysis.md#section_0": 2, "docs/en/spectral/spectral-analysis.md#section_15": 31, "docs/en/multivariate/granger-causality.md": 32, "docs/en/multivariate/granger-causality.md#section_0": 2, "docs/en/multivariate/granger-causality.md#section_15": 30, "docs/en/multivariate/var.md": 32, "docs/en/multivariate/var.md#section_0": 2, "docs/en/multivariate/var.md#section_15": 30, "docs/en/time-domain/ma.md": 42, "docs/en/time-domain/ma.md#section_0": 2, "docs/en/time-domain/ma.md#section_13": 40, "docs/en/time-domain/arma.md": 43, "docs/en/time-domain/arma.md#section_0": 2, "docs/en/time-domain/arma.md#section_14": 41, "docs/en/time-domain/ar.md": 42, "docs/en/time-domain/ar.md#section_0": 2, "docs/en/time-domain/ar.md#section_14": 40, "docs/en/time-domain/arima.md": 40, "docs/en/time-domain/arima.md#section_0": 2, "docs/en/time-domain/arima.md#section_17": 38, "docs/en/time-domain/identification.md": 38, "docs/en/time-domain/identification.md#section_0": 2, "docs/en/time-domain/identification.md#section_15": 36, "docs/en/time-domain/sarima.md": 36, "docs/en/time-domain/sarima.md#section_0": 2, "docs/en/time-domain/sarima.md#section_15": 34, "docs/en/deep-learning/deep-learning-ts.md": 32, "docs/en/deep-learning/deep-learning-ts.md#section_0": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 30, "docs/en/exponential-smoothing/ets.md": 32, "docs/en/exponential-smoothing/ets.md#section_0": 2, "docs/en/exponential-smoothing/ets.md#section_13": 30, "docs/en/exponential-smoothing/holt.md": 36, "docs/en/exponential-smoothing/holt.md#section_0": 2, "docs/en/exponential-smoothing/holt.md#section_14": 34, "docs/en/exponential-smoothing/holt-winters.md": 32, "docs/en/exponential-smoothing/holt-winters.md#section_0": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 30, "docs/en/exponential-smoothing/ses.md": 38, "docs/en/exponential-smoothing/ses.md#section_0": 2, "docs/en/exponential-smoothing/ses.md#section_13": 36, "docs/en/forecasting/prediction-intervals.md": 32, "docs/en/forecasting/prediction-intervals.md#section_0": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 30, "docs/en/forecasting/multi-step.md": 32, "docs/en/forecasting/multi-step.md#section_0": 2, "docs/en/forecasting/multi-step.md#section_17": 30, "docs/en/interview/interview-questions.md": 32, "docs/en/interview/interview-questions.md#section_0": 2, "docs/en/interview/interview-questions.md#section_16": 30, "docs/en/model-selection/cross-validation.md": 32, "docs/en/model-selection/cross-validation.md#section_0": 2, "docs/en/model-selection/cross-validation.md#section_11": 30, "docs/en/model-selection/information-criteria.md": 33, "docs/en/model-selection/information-criteria.md#section_0": 2, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/information-criteria.md#section_13": 30, "docs/en/model-selection/residual-diagnostics.md": 32, "docs/en/model-selection/residual-diagnostics.md#section_0": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 30, "docs/en/practical/practical-modeling.md": 32, "docs/en/practical/practical-modeling.md#section_0": 2, "docs/en/practical/practical-modeling.md#section_13": 30, "docs/en/features/feature-engineering.md": 32, "docs/en/features/feature-engineering.md#section_0": 2, "docs/en/features/feature-engineering.md#section_16": 30, "docs/en/state-space/kalman-filter.md": 32, "docs/en/state-space/kalman-filter.md#section_0": 2, "docs/en/state-space/kalman-filter.md#section_13": 30, "docs/en/anomaly-detection/anomaly-detection.md": 32, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 30, "docs/en/decomposition/classical.md": 33, "docs/en/decomposition/classical.md#section_0": 2, "docs/en/decomposition/classical.md#section_12": 31, "docs/en/decomposition/stl.md": 33, "docs/en/decomposition/stl.md#section_0": 2, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/decomposition/stl.md#section_13": 30, "docs/en/change-detection/change-point.md": 32, "docs/en/change-detection/change-point.md#section_0": 2, "docs/en/change-detection/change-point.md#section_14": 30, "docs/en/foundations/autocorrelation.md": 42, "docs/en/foundations/autocorrelation.md#section_0": 2, "docs/en/foundations/autocorrelation.md#section_16": 40, "docs/en/foundations/stationarity.md": 43, "docs/en/foundations/stationarity.md#section_0": 2, "docs/en/foundations/stationarity.md#section_1": 1, "docs/en/foundations/stationarity.md#section_13": 40}, "welcome": {"docs/en/index.md": 1, "docs/en/index.md#section_0": 1}, "comprehensive": {"docs/en/index.md": 1, "docs/en/index.md#section_0": 1}, "bilingual": {"docs/en/index.md": 1, "docs/en/index.md#section_0": 1}, "english": {"docs/en/index.md": 2, "docs/en/index.md#section_0": 1, "docs/en/index.md#section_5": 1}, "resource": {"docs/en/index.md": 1, "docs/en/index.md#section_0": 1}, "learning": {"docs/en/index.md": 3, "docs/en/index.md#section_0": 1, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_3": 1, "docs/en/deep-learning/deep-learning-ts.md": 10, "docs/en/deep-learning/deep-learning-ts.md#section_0": 2, "docs/en/deep-learning/deep-learning-ts.md#section_7": 3, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_0": 1, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/interview/interview-questions.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_17": 2, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "analysis": {"docs/en/index.md": 2, "docs/en/index.md#section_0": 1, "docs/en/index.md#section_1": 1, "docs/en/spectral/spectral-analysis.md": 8, "docs/en/spectral/spectral-analysis.md#section_0": 2, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_16": 3, "docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/granger-causality.md#section_16": 2, "docs/en/multivariate/var.md": 5, "docs/en/multivariate/var.md#section_6": 1, "docs/en/multivariate/var.md#section_15": 2, "docs/en/multivariate/var.md#section_16": 2, "docs/en/time-domain/ma.md": 3, "docs/en/time-domain/ma.md#section_14": 3, "docs/en/time-domain/arma.md": 4, "docs/en/time-domain/arma.md#section_15": 4, "docs/en/time-domain/ar.md": 4, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/ar.md#section_15": 3, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_18": 2, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_16": 2, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_16": 2, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_4": 1, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_17": 3, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 2, "docs/en/state-space/kalman-filter.md": 5, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/state-space/kalman-filter.md#section_13": 2, "docs/en/state-space/kalman-filter.md#section_14": 2, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_14": 3, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_17": 2, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "forecasting": {"docs/en/index.md": 2, "docs/en/index.md#section_0": 1, "docs/en/index.md#section_1": 1, "docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_16": 1, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_0": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 3, "docs/en/time-domain/ma.md#section_4": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/ma.md#section_14": 1, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_15": 1, "docs/en/time-domain/arima.md": 5, "docs/en/time-domain/arima.md#section_7": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_18": 3, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/time-domain/identification.md#section_16": 1, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_16": 2, "docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_6": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1, "docs/en/exponential-smoothing/ets.md": 4, "docs/en/exponential-smoothing/ets.md#section_14": 4, "docs/en/exponential-smoothing/holt.md": 4, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt.md#section_15": 3, "docs/en/exponential-smoothing/holt-winters.md": 5, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/exponential-smoothing/holt-winters.md#section_14": 3, "docs/en/exponential-smoothing/ses.md": 4, "docs/en/exponential-smoothing/ses.md#section_14": 4, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/forecasting/multi-step.md": 10, "docs/en/forecasting/multi-step.md#section_0": 2, "docs/en/forecasting/multi-step.md#section_10": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/forecasting/multi-step.md#section_18": 5, "docs/en/interview/interview-questions.md": 6, "docs/en/interview/interview-questions.md#section_0": 1, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/interview/interview-questions.md#section_6": 1, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/interview/interview-questions.md#section_17": 1, "docs/en/model-selection/cross-validation.md": 5, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/model-selection/cross-validation.md#section_12": 3, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_13": 2, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 8, "docs/en/practical/practical-modeling.md#section_6": 2, "docs/en/practical/practical-modeling.md#section_13": 3, "docs/en/practical/practical-modeling.md#section_14": 3, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_17": 2, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/state-space/kalman-filter.md#section_14": 1, "docs/en/decomposition/classical.md": 5, "docs/en/decomposition/classical.md#section_12": 2, "docs/en/decomposition/classical.md#section_13": 3, "docs/en/decomposition/stl.md": 4, "docs/en/decomposition/stl.md#section_0": 1, "docs/en/decomposition/stl.md#section_13": 2, "docs/en/decomposition/stl.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_17": 2, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_13": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "designed": {"docs/en/index.md": 1, "docs/en/index.md#section_0": 1}, "preparation": {"docs/en/index.md": 1, "docs/en/index.md#section_0": 1}, "practical": {"docs/en/index.md": 2, "docs/en/index.md#section_0": 1, "docs/en/index.md#section_1": 1, "docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_15": 2, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 4, "docs/en/exponential-smoothing/holt-winters.md#section_13": 3, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 4, "docs/en/interview/interview-questions.md#section_0": 1, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 4, "docs/en/practical/practical-modeling.md#section_0": 2, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_13": 2, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "application": {"docs/en/index.md": 1, "docs/en/index.md#section_0": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_6": 1}, "included": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "knowledge": {"docs/en/index.md": 2, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_5": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "base": {"docs/en/index.md": 2, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_5": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "covers": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1}, "foundations": {"docs/en/index.md": 4, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_2": 1, "docs/en/index.md#section_3": 2}, "stationarity": {"docs/en/index.md": 3, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_3": 2, "docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_7": 2, "docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 5, "docs/en/multivariate/var.md#section_1": 1, "docs/en/multivariate/var.md#section_3": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/multivariate/var.md#section_7": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 4, "docs/en/time-domain/ma.md#section_13": 4, "docs/en/time-domain/arma.md": 8, "docs/en/time-domain/arma.md#section_0": 1, "docs/en/time-domain/arma.md#section_1": 1, "docs/en/time-domain/arma.md#section_2": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_14": 3, "docs/en/time-domain/ar.md": 12, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/ar.md#section_1": 1, "docs/en/time-domain/ar.md#section_2": 1, "docs/en/time-domain/ar.md#section_3": 1, "docs/en/time-domain/ar.md#section_7": 2, "docs/en/time-domain/ar.md#section_14": 6, "docs/en/time-domain/arima.md": 4, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_14": 1, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_11": 2, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/interview/interview-questions.md": 7, "docs/en/interview/interview-questions.md#section_0": 1, "docs/en/interview/interview-questions.md#section_1": 3, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/foundations/autocorrelation.md": 4, "docs/en/foundations/autocorrelation.md#section_9": 1, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_16": 2, "docs/en/foundations/stationarity.md": 34, "docs/en/foundations/stationarity.md#section_0": 3, "docs/en/foundations/stationarity.md#section_1": 4, "docs/en/foundations/stationarity.md#section_6": 2, "docs/en/foundations/stationarity.md#section_7": 4, "docs/en/foundations/stationarity.md#section_13": 19, "docs/en/foundations/stationarity.md#section_14": 1}, "autocorrelation": {"docs/en/index.md": 4, "docs/en/index.md#section_1": 2, "docs/en/index.md#section_3": 2, "docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_6": 1, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ar.md": 4, "docs/en/time-domain/ar.md#section_7": 2, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_5": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md": 14, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_5": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 2, "docs/en/model-selection/residual-diagnostics.md#section_13": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 7, "docs/en/decomposition/classical.md": 5, "docs/en/decomposition/classical.md#section_12": 5, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_13": 2, "docs/en/foundations/autocorrelation.md": 6, "docs/en/foundations/autocorrelation.md#section_0": 2, "docs/en/foundations/autocorrelation.md#section_1": 2, "docs/en/foundations/stationarity.md": 3, "docs/en/foundations/stationarity.md#section_3": 1, "docs/en/foundations/stationarity.md#section_13": 2}, "partial": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md": 8, "docs/en/deep-learning/deep-learning-ts.md#section_2": 8, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_5": 2, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_0": 1, "docs/en/foundations/autocorrelation.md#section_1": 1}, "domain": {"docs/en/index.md": 7, "docs/en/index.md#section_1": 2, "docs/en/index.md#section_3": 5, "docs/en/spectral/spectral-analysis.md": 3, "docs/en/spectral/spectral-analysis.md#section_15": 3, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "models": {"docs/en/index.md": 6, "docs/en/index.md#section_1": 3, "docs/en/index.md#section_3": 3, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_0": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 6, "docs/en/time-domain/ma.md#section_0": 2, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 8, "docs/en/time-domain/arma.md#section_0": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/arma.md#section_14": 5, "docs/en/time-domain/ar.md": 5, "docs/en/time-domain/ar.md#section_0": 2, "docs/en/time-domain/ar.md#section_7": 2, "docs/en/time-domain/arima.md": 6, "docs/en/time-domain/arima.md#section_0": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/time-domain/identification.md": 9, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/time-domain/identification.md#section_6": 2, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/identification.md#section_14": 3, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/identification.md#section_16": 1, "docs/en/time-domain/sarima.md": 5, "docs/en/time-domain/sarima.md#section_0": 1, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/exponential-smoothing/ets.md": 11, "docs/en/exponential-smoothing/ets.md#section_6": 1, "docs/en/exponential-smoothing/ets.md#section_7": 3, "docs/en/exponential-smoothing/ets.md#section_8": 2, "docs/en/exponential-smoothing/ets.md#section_11": 3, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/holt.md": 3, "docs/en/exponential-smoothing/holt.md#section_14": 3, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 3, "docs/en/forecasting/prediction-intervals.md#section_3": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 14, "docs/en/forecasting/multi-step.md#section_0": 1, "docs/en/forecasting/multi-step.md#section_3": 2, "docs/en/forecasting/multi-step.md#section_9": 4, "docs/en/forecasting/multi-step.md#section_17": 7, "docs/en/interview/interview-questions.md": 10, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/interview/interview-questions.md#section_11": 2, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/interview/interview-questions.md#section_16": 4, "docs/en/model-selection/cross-validation.md": 4, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_10": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/model-selection/information-criteria.md": 10, "docs/en/model-selection/information-criteria.md#section_0": 1, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/information-criteria.md#section_7": 3, "docs/en/model-selection/information-criteria.md#section_10": 1, "docs/en/model-selection/information-criteria.md#section_13": 4, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/state-space/kalman-filter.md": 6, "docs/en/state-space/kalman-filter.md#section_0": 3, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/state-space/kalman-filter.md#section_14": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_9": 1, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 5, "docs/en/foundations/stationarity.md#section_0": 1, "docs/en/foundations/stationarity.md#section_7": 2, "docs/en/foundations/stationarity.md#section_13": 2}, "ar": {"docs/en/index.md": 3, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_3": 2, "docs/en/spectral/spectral-analysis.md": 6, "docs/en/spectral/spectral-analysis.md#section_0": 1, "docs/en/spectral/spectral-analysis.md#section_3": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/spectral/spectral-analysis.md#section_15": 3, "docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_2": 1, "docs/en/multivariate/granger-causality.md#section_10": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 8, "docs/en/time-domain/ma.md#section_4": 2, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/ma.md#section_7": 2, "docs/en/time-domain/ma.md#section_13": 3, "docs/en/time-domain/arma.md": 28, "docs/en/time-domain/arma.md#section_0": 3, "docs/en/time-domain/arma.md#section_1": 1, "docs/en/time-domain/arma.md#section_2": 2, "docs/en/time-domain/arma.md#section_3": 2, "docs/en/time-domain/arma.md#section_4": 1, "docs/en/time-domain/arma.md#section_5": 1, "docs/en/time-domain/arma.md#section_6": 3, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_13": 2, "docs/en/time-domain/arma.md#section_14": 12, "docs/en/time-domain/ar.md": 37, "docs/en/time-domain/ar.md#section_0": 3, "docs/en/time-domain/ar.md#section_1": 2, "docs/en/time-domain/ar.md#section_2": 1, "docs/en/time-domain/ar.md#section_3": 2, "docs/en/time-domain/ar.md#section_4": 1, "docs/en/time-domain/ar.md#section_5": 3, "docs/en/time-domain/ar.md#section_6": 4, "docs/en/time-domain/ar.md#section_7": 4, "docs/en/time-domain/ar.md#section_9": 1, "docs/en/time-domain/ar.md#section_11": 1, "docs/en/time-domain/ar.md#section_12": 1, "docs/en/time-domain/ar.md#section_14": 13, "docs/en/time-domain/arima.md": 5, "docs/en/time-domain/arima.md#section_1": 1, "docs/en/time-domain/arima.md#section_4": 2, "docs/en/time-domain/arima.md#section_6": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 15, "docs/en/time-domain/identification.md#section_0": 1, "docs/en/time-domain/identification.md#section_1": 2, "docs/en/time-domain/identification.md#section_3": 6, "docs/en/time-domain/identification.md#section_6": 2, "docs/en/time-domain/identification.md#section_15": 4, "docs/en/time-domain/sarima.md": 11, "docs/en/time-domain/sarima.md#section_0": 1, "docs/en/time-domain/sarima.md#section_1": 4, "docs/en/time-domain/sarima.md#section_2": 1, "docs/en/time-domain/sarima.md#section_5": 1, "docs/en/time-domain/sarima.md#section_6": 1, "docs/en/time-domain/sarima.md#section_15": 3, "docs/en/forecasting/prediction-intervals.md": 7, "docs/en/forecasting/prediction-intervals.md#section_3": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_9": 1, "docs/en/forecasting/prediction-intervals.md#section_12": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 3, "docs/en/forecasting/multi-step.md": 3, "docs/en/forecasting/multi-step.md#section_11": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/forecasting/multi-step.md#section_18": 1, "docs/en/interview/interview-questions.md": 4, "docs/en/interview/interview-questions.md#section_2": 1, "docs/en/interview/interview-questions.md#section_3": 1, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_10": 2, "docs/en/model-selection/residual-diagnostics.md": 6, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 2, "docs/en/model-selection/residual-diagnostics.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_14": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 4, "docs/en/decomposition/stl.md#section_13": 4, "docs/en/foundations/autocorrelation.md": 22, "docs/en/foundations/autocorrelation.md#section_0": 2, "docs/en/foundations/autocorrelation.md#section_3": 1, "docs/en/foundations/autocorrelation.md#section_4": 1, "docs/en/foundations/autocorrelation.md#section_7": 1, "docs/en/foundations/autocorrelation.md#section_9": 3, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_12": 1, "docs/en/foundations/autocorrelation.md#section_13": 2, "docs/en/foundations/autocorrelation.md#section_16": 9, "docs/en/foundations/stationarity.md": 3, "docs/en/foundations/stationarity.md#section_10": 1, "docs/en/foundations/stationarity.md#section_11": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "ma": {"docs/en/index.md": 3, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_3": 2, "docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_0": 1, "docs/en/spectral/spectral-analysis.md#section_4": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_4": 1, "docs/en/time-domain/ma.md": 33, "docs/en/time-domain/ma.md#section_0": 3, "docs/en/time-domain/ma.md#section_1": 1, "docs/en/time-domain/ma.md#section_2": 1, "docs/en/time-domain/ma.md#section_3": 1, "docs/en/time-domain/ma.md#section_4": 1, "docs/en/time-domain/ma.md#section_5": 2, "docs/en/time-domain/ma.md#section_6": 4, "docs/en/time-domain/ma.md#section_7": 5, "docs/en/time-domain/ma.md#section_9": 1, "docs/en/time-domain/ma.md#section_11": 1, "docs/en/time-domain/ma.md#section_13": 12, "docs/en/time-domain/arma.md": 21, "docs/en/time-domain/arma.md#section_0": 3, "docs/en/time-domain/arma.md#section_1": 1, "docs/en/time-domain/arma.md#section_3": 1, "docs/en/time-domain/arma.md#section_4": 1, "docs/en/time-domain/arma.md#section_5": 1, "docs/en/time-domain/arma.md#section_6": 2, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_13": 2, "docs/en/time-domain/arma.md#section_14": 9, "docs/en/time-domain/ar.md": 5, "docs/en/time-domain/ar.md#section_5": 2, "docs/en/time-domain/ar.md#section_14": 3, "docs/en/time-domain/arima.md": 7, "docs/en/time-domain/arima.md#section_1": 1, "docs/en/time-domain/arima.md#section_6": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/arima.md#section_17": 4, "docs/en/time-domain/identification.md": 13, "docs/en/time-domain/identification.md#section_0": 1, "docs/en/time-domain/identification.md#section_1": 2, "docs/en/time-domain/identification.md#section_3": 2, "docs/en/time-domain/identification.md#section_6": 2, "docs/en/time-domain/identification.md#section_15": 6, "docs/en/time-domain/sarima.md": 13, "docs/en/time-domain/sarima.md#section_0": 1, "docs/en/time-domain/sarima.md#section_1": 4, "docs/en/time-domain/sarima.md#section_3": 1, "docs/en/time-domain/sarima.md#section_4": 2, "docs/en/time-domain/sarima.md#section_5": 1, "docs/en/time-domain/sarima.md#section_15": 4, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 3, "docs/en/forecasting/prediction-intervals.md#section_2": 1, "docs/en/forecasting/prediction-intervals.md#section_3": 1, "docs/en/forecasting/prediction-intervals.md#section_6": 1, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_2": 1, "docs/en/interview/interview-questions.md#section_3": 1, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_10": 2, "docs/en/model-selection/residual-diagnostics.md": 8, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 2, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 3, "docs/en/decomposition/classical.md": 10, "docs/en/decomposition/classical.md#section_2": 3, "docs/en/decomposition/classical.md#section_4": 1, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/classical.md#section_12": 5, "docs/en/foundations/autocorrelation.md": 14, "docs/en/foundations/autocorrelation.md#section_0": 2, "docs/en/foundations/autocorrelation.md#section_5": 1, "docs/en/foundations/autocorrelation.md#section_6": 1, "docs/en/foundations/autocorrelation.md#section_8": 1, "docs/en/foundations/autocorrelation.md#section_9": 3, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_16": 5, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_7": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "arma": {"docs/en/index.md": 3, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_3": 2, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/arma.md": 34, "docs/en/time-domain/arma.md#section_0": 2, "docs/en/time-domain/arma.md#section_1": 1, "docs/en/time-domain/arma.md#section_2": 1, "docs/en/time-domain/arma.md#section_3": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/arma.md#section_7": 5, "docs/en/time-domain/arma.md#section_9": 1, "docs/en/time-domain/arma.md#section_11": 1, "docs/en/time-domain/arma.md#section_13": 1, "docs/en/time-domain/arma.md#section_14": 19, "docs/en/time-domain/ar.md": 4, "docs/en/time-domain/ar.md#section_14": 4, "docs/en/time-domain/arima.md": 4, "docs/en/time-domain/arima.md#section_0": 2, "docs/en/time-domain/arima.md#section_1": 1, "docs/en/time-domain/arima.md#section_12": 1, "docs/en/time-domain/identification.md": 7, "docs/en/time-domain/identification.md#section_0": 1, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/time-domain/identification.md#section_6": 2, "docs/en/time-domain/identification.md#section_7": 2, "docs/en/time-domain/identification.md#section_10": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_2": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_2": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/information-criteria.md": 4, "docs/en/model-selection/information-criteria.md#section_9": 1, "docs/en/model-selection/information-criteria.md#section_10": 2, "docs/en/model-selection/information-criteria.md#section_12": 1, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_10": 1, "docs/en/model-selection/residual-diagnostics.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/foundations/autocorrelation.md": 4, "docs/en/foundations/autocorrelation.md#section_9": 3, "docs/en/foundations/autocorrelation.md#section_16": 1}, "arima": {"docs/en/index.md": 4, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_3": 2, "docs/en/index.md#section_4": 1, "docs/en/time-domain/ma.md": 3, "docs/en/time-domain/ma.md#section_8": 2, "docs/en/time-domain/ma.md#section_11": 1, "docs/en/time-domain/arma.md": 5, "docs/en/time-domain/arma.md#section_8": 2, "docs/en/time-domain/arma.md#section_11": 1, "docs/en/time-domain/arma.md#section_13": 2, "docs/en/time-domain/arima.md": 27, "docs/en/time-domain/arima.md#section_0": 2, "docs/en/time-domain/arima.md#section_1": 1, "docs/en/time-domain/arima.md#section_2": 1, "docs/en/time-domain/arima.md#section_3": 1, "docs/en/time-domain/arima.md#section_4": 1, "docs/en/time-domain/arima.md#section_5": 1, "docs/en/time-domain/arima.md#section_6": 1, "docs/en/time-domain/arima.md#section_7": 2, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_9": 2, "docs/en/time-domain/arima.md#section_10": 2, "docs/en/time-domain/arima.md#section_11": 1, "docs/en/time-domain/arima.md#section_15": 2, "docs/en/time-domain/arima.md#section_17": 8, "docs/en/time-domain/identification.md": 15, "docs/en/time-domain/identification.md#section_2": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_7": 2, "docs/en/time-domain/identification.md#section_8": 2, "docs/en/time-domain/identification.md#section_9": 1, "docs/en/time-domain/identification.md#section_14": 5, "docs/en/time-domain/identification.md#section_15": 3, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_0": 1, "docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 2, "docs/en/exponential-smoothing/holt.md": 3, "docs/en/exponential-smoothing/holt.md#section_0": 1, "docs/en/exponential-smoothing/holt.md#section_3": 2, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_4": 1, "docs/en/exponential-smoothing/ses.md": 8, "docs/en/exponential-smoothing/ses.md#section_0": 1, "docs/en/exponential-smoothing/ses.md#section_3": 2, "docs/en/exponential-smoothing/ses.md#section_4": 1, "docs/en/exponential-smoothing/ses.md#section_13": 4, "docs/en/forecasting/prediction-intervals.md": 6, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/forecasting/prediction-intervals.md#section_3": 1, "docs/en/forecasting/prediction-intervals.md#section_8": 2, "docs/en/forecasting/prediction-intervals.md#section_10": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 4, "docs/en/forecasting/multi-step.md#section_0": 1, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 14, "docs/en/interview/interview-questions.md#section_0": 1, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/interview/interview-questions.md#section_4": 2, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/interview/interview-questions.md#section_11": 1, "docs/en/interview/interview-questions.md#section_13": 1, "docs/en/interview/interview-questions.md#section_16": 6, "docs/en/model-selection/cross-validation.md": 10, "docs/en/model-selection/cross-validation.md#section_7": 2, "docs/en/model-selection/cross-validation.md#section_9": 7, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 7, "docs/en/model-selection/information-criteria.md#section_8": 2, "docs/en/model-selection/information-criteria.md#section_10": 1, "docs/en/model-selection/information-criteria.md#section_13": 4, "docs/en/model-selection/residual-diagnostics.md": 6, "docs/en/model-selection/residual-diagnostics.md#section_8": 2, "docs/en/model-selection/residual-diagnostics.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_0": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_12": 2, "docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_13": 3, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_7": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "sarima": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/arima.md": 4, "docs/en/time-domain/arima.md#section_17": 4, "docs/en/time-domain/sarima.md": 21, "docs/en/time-domain/sarima.md#section_0": 3, "docs/en/time-domain/sarima.md#section_1": 1, "docs/en/time-domain/sarima.md#section_2": 1, "docs/en/time-domain/sarima.md#section_3": 1, "docs/en/time-domain/sarima.md#section_4": 1, "docs/en/time-domain/sarima.md#section_5": 2, "docs/en/time-domain/sarima.md#section_8": 2, "docs/en/time-domain/sarima.md#section_10": 1, "docs/en/time-domain/sarima.md#section_12": 1, "docs/en/time-domain/sarima.md#section_15": 7, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_4": 2, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_3": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "identification": {"docs/en/index.md": 3, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_3": 2, "docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_7": 1, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 3, "docs/en/time-domain/arma.md#section_6": 2, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/identification.md": 6, "docs/en/time-domain/identification.md#section_0": 2, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_9": 1, "docs/en/time-domain/sarima.md": 3, "docs/en/time-domain/sarima.md#section_0": 1, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/interview/interview-questions.md": 4, "docs/en/interview/interview-questions.md#section_2": 1, "docs/en/interview/interview-questions.md#section_16": 3, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_9": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "estimation": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/time-domain/ma.md": 5, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/ma.md#section_6": 3, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/arma.md": 4, "docs/en/time-domain/arma.md#section_0": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 3, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_0": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/forecasting/multi-step.md#section_18": 1, "docs/en/interview/interview-questions.md": 4, "docs/en/interview/interview-questions.md#section_11": 2, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/model-selection/cross-validation.md": 3, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "exponential": {"docs/en/index.md": 3, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_3": 2, "docs/en/time-domain/arima.md": 4, "docs/en/time-domain/arima.md#section_5": 1, "docs/en/time-domain/arima.md#section_17": 3, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_1": 2, "docs/en/time-domain/sarima.md": 6, "docs/en/time-domain/sarima.md#section_2": 1, "docs/en/time-domain/sarima.md#section_5": 2, "docs/en/time-domain/sarima.md#section_15": 2, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_3": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 8, "docs/en/exponential-smoothing/ets.md#section_0": 1, "docs/en/exponential-smoothing/ets.md#section_1": 1, "docs/en/exponential-smoothing/ets.md#section_13": 3, "docs/en/exponential-smoothing/ets.md#section_14": 3, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 6, "docs/en/exponential-smoothing/ses.md#section_0": 1, "docs/en/exponential-smoothing/ses.md#section_1": 1, "docs/en/exponential-smoothing/ses.md#section_2": 1, "docs/en/exponential-smoothing/ses.md#section_14": 2, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_3": 1, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_0": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_9": 2}, "smoothing": {"docs/en/index.md": 3, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_3": 2, "docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/time-domain/arima.md": 4, "docs/en/time-domain/arima.md#section_5": 1, "docs/en/time-domain/arima.md#section_17": 3, "docs/en/time-domain/sarima.md": 3, "docs/en/time-domain/sarima.md#section_15": 2, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/exponential-smoothing/ets.md": 8, "docs/en/exponential-smoothing/ets.md#section_0": 1, "docs/en/exponential-smoothing/ets.md#section_1": 1, "docs/en/exponential-smoothing/ets.md#section_13": 3, "docs/en/exponential-smoothing/ets.md#section_14": 3, "docs/en/exponential-smoothing/holt.md": 8, "docs/en/exponential-smoothing/holt.md#section_0": 2, "docs/en/exponential-smoothing/holt.md#section_1": 2, "docs/en/exponential-smoothing/holt.md#section_7": 2, "docs/en/exponential-smoothing/holt.md#section_14": 2, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 12, "docs/en/exponential-smoothing/ses.md#section_0": 1, "docs/en/exponential-smoothing/ses.md#section_1": 2, "docs/en/exponential-smoothing/ses.md#section_5": 1, "docs/en/exponential-smoothing/ses.md#section_6": 3, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/exponential-smoothing/ses.md#section_14": 2, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 3, "docs/en/state-space/kalman-filter.md#section_0": 1, "docs/en/state-space/kalman-filter.md#section_13": 2, "docs/en/decomposition/stl.md": 8, "docs/en/decomposition/stl.md#section_1": 2, "docs/en/decomposition/stl.md#section_3": 1, "docs/en/decomposition/stl.md#section_6": 2, "docs/en/decomposition/stl.md#section_13": 3}, "ses": {"docs/en/index.md": 2, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_3": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 7, "docs/en/exponential-smoothing/holt.md#section_0": 1, "docs/en/exponential-smoothing/holt.md#section_5": 1, "docs/en/exponential-smoothing/holt.md#section_14": 5, "docs/en/exponential-smoothing/ses.md": 23, "docs/en/exponential-smoothing/ses.md#section_0": 2, "docs/en/exponential-smoothing/ses.md#section_3": 1, "docs/en/exponential-smoothing/ses.md#section_6": 1, "docs/en/exponential-smoothing/ses.md#section_7": 5, "docs/en/exponential-smoothing/ses.md#section_9": 1, "docs/en/exponential-smoothing/ses.md#section_10": 1, "docs/en/exponential-smoothing/ses.md#section_13": 11, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 8, "docs/en/state-space/kalman-filter.md#section_3": 1, "docs/en/state-space/kalman-filter.md#section_12": 1, "docs/en/state-space/kalman-filter.md#section_13": 6}, "holt": {"docs/en/index.md": 2, "docs/en/index.md#section_1": 2, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 5, "docs/en/exponential-smoothing/ets.md#section_0": 1, "docs/en/exponential-smoothing/ets.md#section_1": 2, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 19, "docs/en/exponential-smoothing/holt.md#section_0": 2, "docs/en/exponential-smoothing/holt.md#section_1": 1, "docs/en/exponential-smoothing/holt.md#section_2": 1, "docs/en/exponential-smoothing/holt.md#section_5": 1, "docs/en/exponential-smoothing/holt.md#section_6": 1, "docs/en/exponential-smoothing/holt.md#section_7": 2, "docs/en/exponential-smoothing/holt.md#section_8": 1, "docs/en/exponential-smoothing/holt.md#section_10": 2, "docs/en/exponential-smoothing/holt.md#section_12": 1, "docs/en/exponential-smoothing/holt.md#section_14": 5, "docs/en/exponential-smoothing/holt.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 15, "docs/en/exponential-smoothing/holt-winters.md#section_0": 3, "docs/en/exponential-smoothing/holt-winters.md#section_1": 2, "docs/en/exponential-smoothing/holt-winters.md#section_4": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md#section_10": 1, "docs/en/exponential-smoothing/holt-winters.md#section_11": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 4, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 5, "docs/en/exponential-smoothing/ses.md#section_7": 2, "docs/en/exponential-smoothing/ses.md#section_13": 3}, "winters": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 4, "docs/en/exponential-smoothing/ets.md#section_0": 1, "docs/en/exponential-smoothing/ets.md#section_1": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt-winters.md": 15, "docs/en/exponential-smoothing/holt-winters.md#section_0": 2, "docs/en/exponential-smoothing/holt-winters.md#section_1": 2, "docs/en/exponential-smoothing/holt-winters.md#section_4": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md#section_10": 1, "docs/en/exponential-smoothing/holt-winters.md#section_11": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 4, "docs/en/exponential-smoothing/holt-winters.md#section_14": 2, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1}, "ets": {"docs/en/index.md": 2, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_4": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/exponential-smoothing/ets.md": 38, "docs/en/exponential-smoothing/ets.md#section_0": 3, "docs/en/exponential-smoothing/ets.md#section_1": 7, "docs/en/exponential-smoothing/ets.md#section_3": 1, "docs/en/exponential-smoothing/ets.md#section_4": 1, "docs/en/exponential-smoothing/ets.md#section_7": 3, "docs/en/exponential-smoothing/ets.md#section_8": 4, "docs/en/exponential-smoothing/ets.md#section_11": 5, "docs/en/exponential-smoothing/ets.md#section_13": 13, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 8, "docs/en/interview/interview-questions.md#section_0": 1, "docs/en/interview/interview-questions.md#section_4": 2, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/interview/interview-questions.md#section_11": 1, "docs/en/interview/interview-questions.md#section_16": 3, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "framework": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_3": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/exponential-smoothing/ets.md": 7, "docs/en/exponential-smoothing/ets.md#section_0": 2, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/ets.md#section_14": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/interview/interview-questions.md#section_11": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1}, "decomposition": {"docs/en/index.md": 4, "docs/en/index.md#section_1": 2, "docs/en/index.md#section_3": 2, "docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_5": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_3": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/decomposition/classical.md": 10, "docs/en/decomposition/classical.md#section_0": 2, "docs/en/decomposition/classical.md#section_5": 1, "docs/en/decomposition/classical.md#section_6": 2, "docs/en/decomposition/classical.md#section_9": 1, "docs/en/decomposition/classical.md#section_12": 3, "docs/en/decomposition/stl.md": 13, "docs/en/decomposition/stl.md#section_0": 3, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_5": 1, "docs/en/decomposition/stl.md#section_10": 1, "docs/en/decomposition/stl.md#section_13": 4, "docs/en/decomposition/stl.md#section_14": 2}, "stl": {"docs/en/index.md": 3, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_3": 1, "docs/en/index.md#section_4": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_3": 1, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_12": 3, "docs/en/decomposition/stl.md": 27, "docs/en/decomposition/stl.md#section_0": 3, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_3": 1, "docs/en/decomposition/stl.md#section_5": 1, "docs/en/decomposition/stl.md#section_6": 2, "docs/en/decomposition/stl.md#section_7": 1, "docs/en/decomposition/stl.md#section_10": 4, "docs/en/decomposition/stl.md#section_13": 12, "docs/en/decomposition/stl.md#section_14": 1}, "classical": {"docs/en/index.md": 2, "docs/en/index.md#section_1": 2, "docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 2, "docs/en/interview/interview-questions.md": 5, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/interview/interview-questions.md#section_5": 4, "docs/en/decomposition/classical.md": 7, "docs/en/decomposition/classical.md#section_0": 2, "docs/en/decomposition/classical.md#section_5": 1, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/classical.md#section_9": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 4, "docs/en/decomposition/stl.md#section_0": 1, "docs/en/decomposition/stl.md#section_13": 3, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_0": 1}, "handling": {"docs/en/index.md": 2, "docs/en/index.md#section_1": 2, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_11": 1, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_0": 1}, "seasonality": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/sarima.md": 4, "docs/en/time-domain/sarima.md#section_8": 2, "docs/en/time-domain/sarima.md#section_15": 2, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_10": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_0": 1, "docs/en/exponential-smoothing/holt-winters.md": 7, "docs/en/exponential-smoothing/holt-winters.md#section_0": 1, "docs/en/exponential-smoothing/holt-winters.md#section_2": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md#section_9": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 3, "docs/en/exponential-smoothing/ses.md": 4, "docs/en/exponential-smoothing/ses.md#section_0": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/interview/interview-questions.md": 6, "docs/en/interview/interview-questions.md#section_0": 1, "docs/en/interview/interview-questions.md#section_3": 1, "docs/en/interview/interview-questions.md#section_16": 4, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/features/feature-engineering.md": 9, "docs/en/features/feature-engineering.md#section_0": 1, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/features/feature-engineering.md#section_16": 6, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_3": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/decomposition/classical.md": 7, "docs/en/decomposition/classical.md#section_4": 1, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/classical.md#section_12": 5, "docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/decomposition/stl.md#section_8": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_6": 2}, "prediction": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_1": 1, "docs/en/time-domain/arima.md": 3, "docs/en/time-domain/arima.md#section_7": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 8, "docs/en/exponential-smoothing/ets.md#section_0": 1, "docs/en/exponential-smoothing/ets.md#section_6": 2, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/ets.md#section_13": 3, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_5": 1, "docs/en/exponential-smoothing/holt-winters.md": 3, "docs/en/exponential-smoothing/holt-winters.md#section_13": 3, "docs/en/exponential-smoothing/ses.md": 4, "docs/en/exponential-smoothing/ses.md#section_4": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/forecasting/prediction-intervals.md": 13, "docs/en/forecasting/prediction-intervals.md#section_0": 2, "docs/en/forecasting/prediction-intervals.md#section_1": 1, "docs/en/forecasting/prediction-intervals.md#section_4": 1, "docs/en/forecasting/prediction-intervals.md#section_6": 2, "docs/en/forecasting/prediction-intervals.md#section_11": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 4, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_6": 1, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 5, "docs/en/model-selection/information-criteria.md#section_0": 2, "docs/en/model-selection/information-criteria.md#section_2": 1, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 5, "docs/en/model-selection/residual-diagnostics.md#section_1": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 4, "docs/en/practical/practical-modeling.md": 7, "docs/en/practical/practical-modeling.md#section_3": 2, "docs/en/practical/practical-modeling.md#section_5": 1, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/practical/practical-modeling.md#section_12": 1, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/state-space/kalman-filter.md": 11, "docs/en/state-space/kalman-filter.md#section_0": 1, "docs/en/state-space/kalman-filter.md#section_2": 2, "docs/en/state-space/kalman-filter.md#section_5": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/state-space/kalman-filter.md#section_7": 1, "docs/en/state-space/kalman-filter.md#section_13": 4, "docs/en/state-space/kalman-filter.md#section_14": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "intervals": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/time-domain/arima.md": 4, "docs/en/time-domain/arima.md#section_7": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 8, "docs/en/exponential-smoothing/ets.md#section_0": 1, "docs/en/exponential-smoothing/ets.md#section_6": 2, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/ets.md#section_13": 3, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_5": 1, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_7": 2, "docs/en/forecasting/prediction-intervals.md": 14, "docs/en/forecasting/prediction-intervals.md#section_0": 3, "docs/en/forecasting/prediction-intervals.md#section_4": 1, "docs/en/forecasting/prediction-intervals.md#section_6": 2, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_11": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 4, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/model-selection/residual-diagnostics.md": 8, "docs/en/model-selection/residual-diagnostics.md#section_17": 8, "docs/en/practical/practical-modeling.md": 11, "docs/en/practical/practical-modeling.md#section_6": 4, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/practical/practical-modeling.md#section_11": 1, "docs/en/practical/practical-modeling.md#section_13": 5}, "multi": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_5": 1, "docs/en/forecasting/multi-step.md": 9, "docs/en/forecasting/multi-step.md#section_0": 2, "docs/en/forecasting/multi-step.md#section_1": 1, "docs/en/forecasting/multi-step.md#section_17": 3, "docs/en/forecasting/multi-step.md#section_18": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "step": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_5": 1, "docs/en/multivariate/var.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_13": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_7": 1, "docs/en/time-domain/arima.md#section_16": 1, "docs/en/time-domain/identification.md": 12, "docs/en/time-domain/identification.md#section_6": 6, "docs/en/time-domain/identification.md#section_11": 2, "docs/en/time-domain/identification.md#section_12": 1, "docs/en/time-domain/identification.md#section_13": 1, "docs/en/time-domain/identification.md#section_14": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_5": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md#section_14": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_5": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 4, "docs/en/exponential-smoothing/ses.md#section_5": 1, "docs/en/exponential-smoothing/ses.md#section_6": 1, "docs/en/exponential-smoothing/ses.md#section_12": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 6, "docs/en/forecasting/prediction-intervals.md#section_2": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 5, "docs/en/forecasting/multi-step.md": 15, "docs/en/forecasting/multi-step.md#section_0": 3, "docs/en/forecasting/multi-step.md#section_1": 2, "docs/en/forecasting/multi-step.md#section_2": 1, "docs/en/forecasting/multi-step.md#section_17": 6, "docs/en/forecasting/multi-step.md#section_18": 2, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_9": 2, "docs/en/model-selection/cross-validation.md": 12, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/cross-validation.md#section_5": 3, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_9": 2, "docs/en/model-selection/cross-validation.md#section_11": 5, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_1": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 4, "docs/en/state-space/kalman-filter.md#section_0": 2, "docs/en/state-space/kalman-filter.md#section_2": 2}, "strategies": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_5": 1, "docs/en/forecasting/multi-step.md": 4, "docs/en/forecasting/multi-step.md#section_0": 1, "docs/en/forecasting/multi-step.md#section_1": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/forecasting/multi-step.md#section_18": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "rolling": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/interview/interview-questions.md": 4, "docs/en/interview/interview-questions.md#section_8": 2, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 11, "docs/en/model-selection/cross-validation.md#section_0": 2, "docs/en/model-selection/cross-validation.md#section_1": 2, "docs/en/model-selection/cross-validation.md#section_2": 1, "docs/en/model-selection/cross-validation.md#section_5": 1, "docs/en/model-selection/cross-validation.md#section_6": 2, "docs/en/model-selection/cross-validation.md#section_9": 1, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/practical/practical-modeling.md#section_2": 1, "docs/en/features/feature-engineering.md": 22, "docs/en/features/feature-engineering.md#section_0": 1, "docs/en/features/feature-engineering.md#section_1": 1, "docs/en/features/feature-engineering.md#section_3": 3, "docs/en/features/feature-engineering.md#section_5": 5, "docs/en/features/feature-engineering.md#section_8": 5, "docs/en/features/feature-engineering.md#section_11": 3, "docs/en/features/feature-engineering.md#section_16": 4, "docs/en/anomaly-detection/anomaly-detection.md": 7, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 4, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_12": 2}, "evaluation": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1, "docs/en/interview/interview-questions.md": 4, "docs/en/interview/interview-questions.md#section_10": 2, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/model-selection/cross-validation.md": 5, "docs/en/model-selection/cross-validation.md#section_1": 1, "docs/en/model-selection/cross-validation.md#section_11": 3, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "model": {"docs/en/index.md": 4, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_2": 1, "docs/en/index.md#section_3": 2, "docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/multivariate/granger-causality.md": 8, "docs/en/multivariate/granger-causality.md#section_2": 2, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/granger-causality.md#section_14": 2, "docs/en/multivariate/granger-causality.md#section_15": 3, "docs/en/multivariate/var.md": 6, "docs/en/multivariate/var.md#section_1": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/multivariate/var.md#section_10": 1, "docs/en/multivariate/var.md#section_11": 1, "docs/en/multivariate/var.md#section_12": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 16, "docs/en/time-domain/ma.md#section_1": 1, "docs/en/time-domain/ma.md#section_2": 1, "docs/en/time-domain/ma.md#section_4": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/ma.md#section_7": 2, "docs/en/time-domain/ma.md#section_8": 1, "docs/en/time-domain/ma.md#section_11": 4, "docs/en/time-domain/ma.md#section_13": 5, "docs/en/time-domain/arma.md": 15, "docs/en/time-domain/arma.md#section_1": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/arma.md#section_8": 1, "docs/en/time-domain/arma.md#section_11": 3, "docs/en/time-domain/arma.md#section_12": 2, "docs/en/time-domain/arma.md#section_13": 1, "docs/en/time-domain/arma.md#section_14": 6, "docs/en/time-domain/ar.md": 14, "docs/en/time-domain/ar.md#section_1": 1, "docs/en/time-domain/ar.md#section_2": 1, "docs/en/time-domain/ar.md#section_3": 1, "docs/en/time-domain/ar.md#section_6": 2, "docs/en/time-domain/ar.md#section_11": 1, "docs/en/time-domain/ar.md#section_12": 3, "docs/en/time-domain/ar.md#section_13": 1, "docs/en/time-domain/ar.md#section_14": 4, "docs/en/time-domain/arima.md": 10, "docs/en/time-domain/arima.md#section_1": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_10": 1, "docs/en/time-domain/arima.md#section_15": 3, "docs/en/time-domain/arima.md#section_16": 2, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/time-domain/identification.md": 21, "docs/en/time-domain/identification.md#section_0": 2, "docs/en/time-domain/identification.md#section_1": 2, "docs/en/time-domain/identification.md#section_6": 2, "docs/en/time-domain/identification.md#section_7": 2, "docs/en/time-domain/identification.md#section_8": 1, "docs/en/time-domain/identification.md#section_14": 5, "docs/en/time-domain/identification.md#section_15": 6, "docs/en/time-domain/sarima.md": 19, "docs/en/time-domain/sarima.md#section_0": 1, "docs/en/time-domain/sarima.md#section_1": 1, "docs/en/time-domain/sarima.md#section_4": 2, "docs/en/time-domain/sarima.md#section_6": 1, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_12": 2, "docs/en/time-domain/sarima.md#section_15": 10, "docs/en/deep-learning/deep-learning-ts.md": 15, "docs/en/deep-learning/deep-learning-ts.md#section_6": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 3, "docs/en/deep-learning/deep-learning-ts.md#section_13": 3, "docs/en/deep-learning/deep-learning-ts.md#section_14": 1, "docs/en/deep-learning/deep-learning-ts.md#section_15": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 5, "docs/en/exponential-smoothing/ets.md": 18, "docs/en/exponential-smoothing/ets.md#section_0": 1, "docs/en/exponential-smoothing/ets.md#section_7": 4, "docs/en/exponential-smoothing/ets.md#section_8": 2, "docs/en/exponential-smoothing/ets.md#section_11": 4, "docs/en/exponential-smoothing/ets.md#section_12": 2, "docs/en/exponential-smoothing/ets.md#section_13": 5, "docs/en/exponential-smoothing/holt.md": 7, "docs/en/exponential-smoothing/holt.md#section_6": 1, "docs/en/exponential-smoothing/holt.md#section_10": 2, "docs/en/exponential-smoothing/holt.md#section_14": 4, "docs/en/exponential-smoothing/holt-winters.md": 3, "docs/en/exponential-smoothing/holt-winters.md#section_6": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/exponential-smoothing/ses.md": 5, "docs/en/exponential-smoothing/ses.md#section_6": 1, "docs/en/exponential-smoothing/ses.md#section_10": 2, "docs/en/exponential-smoothing/ses.md#section_11": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 17, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/forecasting/prediction-intervals.md#section_6": 5, "docs/en/forecasting/prediction-intervals.md#section_7": 2, "docs/en/forecasting/prediction-intervals.md#section_8": 1, "docs/en/forecasting/prediction-intervals.md#section_10": 2, "docs/en/forecasting/prediction-intervals.md#section_11": 1, "docs/en/forecasting/prediction-intervals.md#section_12": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 3, "docs/en/forecasting/multi-step.md": 35, "docs/en/forecasting/multi-step.md#section_0": 2, "docs/en/forecasting/multi-step.md#section_1": 3, "docs/en/forecasting/multi-step.md#section_2": 2, "docs/en/forecasting/multi-step.md#section_3": 2, "docs/en/forecasting/multi-step.md#section_4": 3, "docs/en/forecasting/multi-step.md#section_5": 1, "docs/en/forecasting/multi-step.md#section_6": 3, "docs/en/forecasting/multi-step.md#section_7": 2, "docs/en/forecasting/multi-step.md#section_8": 1, "docs/en/forecasting/multi-step.md#section_9": 2, "docs/en/forecasting/multi-step.md#section_17": 13, "docs/en/interview/interview-questions.md": 18, "docs/en/interview/interview-questions.md#section_0": 1, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/interview/interview-questions.md#section_6": 2, "docs/en/interview/interview-questions.md#section_7": 2, "docs/en/interview/interview-questions.md#section_9": 4, "docs/en/interview/interview-questions.md#section_16": 7, "docs/en/model-selection/cross-validation.md": 13, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/cross-validation.md#section_1": 1, "docs/en/model-selection/cross-validation.md#section_4": 1, "docs/en/model-selection/cross-validation.md#section_5": 2, "docs/en/model-selection/cross-validation.md#section_7": 1, "docs/en/model-selection/cross-validation.md#section_9": 2, "docs/en/model-selection/cross-validation.md#section_11": 5, "docs/en/model-selection/information-criteria.md": 30, "docs/en/model-selection/information-criteria.md#section_0": 2, "docs/en/model-selection/information-criteria.md#section_2": 1, "docs/en/model-selection/information-criteria.md#section_3": 1, "docs/en/model-selection/information-criteria.md#section_6": 8, "docs/en/model-selection/information-criteria.md#section_7": 2, "docs/en/model-selection/information-criteria.md#section_8": 1, "docs/en/model-selection/information-criteria.md#section_10": 4, "docs/en/model-selection/information-criteria.md#section_12": 3, "docs/en/model-selection/information-criteria.md#section_13": 4, "docs/en/model-selection/information-criteria.md#section_14": 4, "docs/en/model-selection/residual-diagnostics.md": 13, "docs/en/model-selection/residual-diagnostics.md#section_0": 2, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_8": 1, "docs/en/model-selection/residual-diagnostics.md#section_9": 1, "docs/en/model-selection/residual-diagnostics.md#section_16": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 6, "docs/en/practical/practical-modeling.md": 20, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/practical/practical-modeling.md#section_1": 3, "docs/en/practical/practical-modeling.md#section_4": 1, "docs/en/practical/practical-modeling.md#section_6": 5, "docs/en/practical/practical-modeling.md#section_7": 2, "docs/en/practical/practical-modeling.md#section_13": 8, "docs/en/features/feature-engineering.md": 9, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/features/feature-engineering.md#section_13": 3, "docs/en/features/feature-engineering.md#section_14": 1, "docs/en/features/feature-engineering.md#section_15": 1, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/state-space/kalman-filter.md": 11, "docs/en/state-space/kalman-filter.md#section_1": 1, "docs/en/state-space/kalman-filter.md#section_3": 1, "docs/en/state-space/kalman-filter.md#section_4": 1, "docs/en/state-space/kalman-filter.md#section_5": 1, "docs/en/state-space/kalman-filter.md#section_6": 2, "docs/en/state-space/kalman-filter.md#section_7": 1, "docs/en/state-space/kalman-filter.md#section_13": 4, "docs/en/anomaly-detection/anomaly-detection.md": 18, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_3": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 6, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_14": 4, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 3, "docs/en/decomposition/classical.md": 10, "docs/en/decomposition/classical.md#section_1": 2, "docs/en/decomposition/classical.md#section_5": 1, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/classical.md#section_9": 1, "docs/en/decomposition/classical.md#section_10": 2, "docs/en/decomposition/classical.md#section_12": 3, "docs/en/decomposition/stl.md": 4, "docs/en/decomposition/stl.md#section_5": 1, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/decomposition/stl.md#section_13": 2, "docs/en/change-detection/change-point.md": 6, "docs/en/change-detection/change-point.md#section_6": 1, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_11": 1, "docs/en/change-detection/change-point.md#section_12": 1, "docs/en/change-detection/change-point.md#section_14": 2, "docs/en/foundations/autocorrelation.md": 5, "docs/en/foundations/autocorrelation.md#section_9": 4, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_5": 1, "docs/en/foundations/stationarity.md#section_6": 1}, "selection": {"docs/en/index.md": 2, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_3": 1, "docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_11": 1, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/exponential-smoothing/ets.md": 4, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/exponential-smoothing/ets.md#section_13": 3, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_6": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/interview/interview-questions.md": 6, "docs/en/interview/interview-questions.md#section_0": 1, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/interview/interview-questions.md#section_6": 1, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/information-criteria.md#section_14": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_7": 1}, "aic": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_0": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/arma.md": 17, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/arma.md#section_13": 3, "docs/en/time-domain/arma.md#section_14": 13, "docs/en/time-domain/ar.md": 5, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/ar.md#section_11": 2, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 24, "docs/en/time-domain/identification.md#section_0": 1, "docs/en/time-domain/identification.md#section_2": 6, "docs/en/time-domain/identification.md#section_6": 4, "docs/en/time-domain/identification.md#section_7": 2, "docs/en/time-domain/identification.md#section_14": 2, "docs/en/time-domain/identification.md#section_15": 9, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/exponential-smoothing/ets.md": 12, "docs/en/exponential-smoothing/ets.md#section_7": 2, "docs/en/exponential-smoothing/ets.md#section_11": 6, "docs/en/exponential-smoothing/ets.md#section_12": 2, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 5, "docs/en/exponential-smoothing/holt-winters.md#section_11": 4, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/interview/interview-questions.md": 5, "docs/en/interview/interview-questions.md#section_6": 3, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/model-selection/information-criteria.md": 51, "docs/en/model-selection/information-criteria.md#section_0": 3, "docs/en/model-selection/information-criteria.md#section_1": 4, "docs/en/model-selection/information-criteria.md#section_2": 3, "docs/en/model-selection/information-criteria.md#section_4": 1, "docs/en/model-selection/information-criteria.md#section_5": 2, "docs/en/model-selection/information-criteria.md#section_6": 4, "docs/en/model-selection/information-criteria.md#section_7": 5, "docs/en/model-selection/information-criteria.md#section_10": 2, "docs/en/model-selection/information-criteria.md#section_11": 4, "docs/en/model-selection/information-criteria.md#section_12": 1, "docs/en/model-selection/information-criteria.md#section_13": 22, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "bic": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_0": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/identification.md": 15, "docs/en/time-domain/identification.md#section_0": 1, "docs/en/time-domain/identification.md#section_2": 3, "docs/en/time-domain/identification.md#section_6": 3, "docs/en/time-domain/identification.md#section_14": 2, "docs/en/time-domain/identification.md#section_15": 6, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/exponential-smoothing/ets.md": 8, "docs/en/exponential-smoothing/ets.md#section_7": 2, "docs/en/exponential-smoothing/ets.md#section_11": 5, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/interview/interview-questions.md": 4, "docs/en/interview/interview-questions.md#section_6": 3, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/information-criteria.md": 39, "docs/en/model-selection/information-criteria.md#section_0": 3, "docs/en/model-selection/information-criteria.md#section_1": 2, "docs/en/model-selection/information-criteria.md#section_3": 4, "docs/en/model-selection/information-criteria.md#section_4": 1, "docs/en/model-selection/information-criteria.md#section_5": 2, "docs/en/model-selection/information-criteria.md#section_6": 2, "docs/en/model-selection/information-criteria.md#section_7": 3, "docs/en/model-selection/information-criteria.md#section_10": 2, "docs/en/model-selection/information-criteria.md#section_11": 2, "docs/en/model-selection/information-criteria.md#section_12": 2, "docs/en/model-selection/information-criteria.md#section_13": 16, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_14": 2}, "cross": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_2": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_4": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/model-selection/cross-validation.md": 6, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/cross-validation.md#section_1": 1, "docs/en/model-selection/cross-validation.md#section_10": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "validation": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_8": 2, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/model-selection/cross-validation.md": 6, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/cross-validation.md#section_1": 1, "docs/en/model-selection/cross-validation.md#section_10": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 4, "docs/en/practical/practical-modeling.md#section_1": 2, "docs/en/practical/practical-modeling.md#section_6": 2, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "residual": {"docs/en/index.md": 3, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_3": 2, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 4, "docs/en/time-domain/ar.md#section_14": 4, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_8": 2, "docs/en/time-domain/identification.md": 9, "docs/en/time-domain/identification.md#section_0": 1, "docs/en/time-domain/identification.md#section_5": 1, "docs/en/time-domain/identification.md#section_6": 3, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/identification.md#section_15": 3, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/model-selection/information-criteria.md": 4, "docs/en/model-selection/information-criteria.md#section_4": 1, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/information-criteria.md#section_13": 2, "docs/en/model-selection/residual-diagnostics.md": 7, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 4, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_1": 1, "docs/en/decomposition/classical.md#section_10": 1, "docs/en/decomposition/classical.md#section_11": 1, "docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_13": 2, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_9": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "diagnostics": {"docs/en/index.md": 3, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_3": 2, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_0": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 5, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 2}, "spectral": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/spectral/spectral-analysis.md": 22, "docs/en/spectral/spectral-analysis.md#section_0": 3, "docs/en/spectral/spectral-analysis.md#section_1": 3, "docs/en/spectral/spectral-analysis.md#section_2": 1, "docs/en/spectral/spectral-analysis.md#section_3": 1, "docs/en/spectral/spectral-analysis.md#section_4": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/spectral/spectral-analysis.md#section_7": 3, "docs/en/spectral/spectral-analysis.md#section_15": 6, "docs/en/spectral/spectral-analysis.md#section_16": 2, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1}, "periodogram": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/spectral/spectral-analysis.md": 14, "docs/en/spectral/spectral-analysis.md#section_0": 1, "docs/en/spectral/spectral-analysis.md#section_1": 1, "docs/en/spectral/spectral-analysis.md#section_5": 1, "docs/en/spectral/spectral-analysis.md#section_6": 2, "docs/en/spectral/spectral-analysis.md#section_7": 2, "docs/en/spectral/spectral-analysis.md#section_12": 2, "docs/en/spectral/spectral-analysis.md#section_15": 5}, "frequency": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/spectral/spectral-analysis.md": 27, "docs/en/spectral/spectral-analysis.md#section_0": 2, "docs/en/spectral/spectral-analysis.md#section_1": 3, "docs/en/spectral/spectral-analysis.md#section_3": 2, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/spectral/spectral-analysis.md#section_7": 2, "docs/en/spectral/spectral-analysis.md#section_13": 1, "docs/en/spectral/spectral-analysis.md#section_15": 16, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_4": 2, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_3": 1, "docs/en/decomposition/stl.md#section_13": 1}, "basics": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1}, "state": {"docs/en/index.md": 3, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_3": 2, "docs/en/deep-learning/deep-learning-ts.md": 10, "docs/en/deep-learning/deep-learning-ts.md#section_1": 2, "docs/en/deep-learning/deep-learning-ts.md#section_2": 2, "docs/en/deep-learning/deep-learning-ts.md#section_6": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 5, "docs/en/exponential-smoothing/ets.md": 13, "docs/en/exponential-smoothing/ets.md#section_0": 1, "docs/en/exponential-smoothing/ets.md#section_2": 2, "docs/en/exponential-smoothing/ets.md#section_3": 1, "docs/en/exponential-smoothing/ets.md#section_4": 1, "docs/en/exponential-smoothing/ets.md#section_6": 2, "docs/en/exponential-smoothing/ets.md#section_13": 4, "docs/en/exponential-smoothing/ets.md#section_14": 2, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_14": 1, "docs/en/interview/interview-questions.md": 5, "docs/en/interview/interview-questions.md#section_0": 1, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/interview/interview-questions.md#section_11": 3, "docs/en/state-space/kalman-filter.md": 33, "docs/en/state-space/kalman-filter.md#section_0": 3, "docs/en/state-space/kalman-filter.md#section_1": 5, "docs/en/state-space/kalman-filter.md#section_2": 2, "docs/en/state-space/kalman-filter.md#section_3": 2, "docs/en/state-space/kalman-filter.md#section_4": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/state-space/kalman-filter.md#section_11": 3, "docs/en/state-space/kalman-filter.md#section_12": 2, "docs/en/state-space/kalman-filter.md#section_13": 12, "docs/en/state-space/kalman-filter.md#section_14": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "space": {"docs/en/index.md": 3, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_3": 2, "docs/en/exponential-smoothing/ets.md": 8, "docs/en/exponential-smoothing/ets.md#section_0": 1, "docs/en/exponential-smoothing/ets.md#section_2": 1, "docs/en/exponential-smoothing/ets.md#section_6": 1, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/ets.md#section_14": 2, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/interview/interview-questions.md": 4, "docs/en/interview/interview-questions.md#section_0": 1, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/interview/interview-questions.md#section_11": 2, "docs/en/state-space/kalman-filter.md": 7, "docs/en/state-space/kalman-filter.md#section_0": 2, "docs/en/state-space/kalman-filter.md#section_1": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/state-space/kalman-filter.md#section_14": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "kalman": {"docs/en/index.md": 3, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_3": 1, "docs/en/index.md#section_4": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/interview/interview-questions.md#section_11": 2, "docs/en/state-space/kalman-filter.md": 25, "docs/en/state-space/kalman-filter.md#section_0": 2, "docs/en/state-space/kalman-filter.md#section_2": 2, "docs/en/state-space/kalman-filter.md#section_3": 1, "docs/en/state-space/kalman-filter.md#section_5": 2, "docs/en/state-space/kalman-filter.md#section_6": 2, "docs/en/state-space/kalman-filter.md#section_7": 1, "docs/en/state-space/kalman-filter.md#section_11": 2, "docs/en/state-space/kalman-filter.md#section_12": 2, "docs/en/state-space/kalman-filter.md#section_13": 8, "docs/en/state-space/kalman-filter.md#section_14": 2}, "filter": {"docs/en/index.md": 2, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_3": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_3": 1, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/interview/interview-questions.md#section_11": 2, "docs/en/state-space/kalman-filter.md": 21, "docs/en/state-space/kalman-filter.md#section_0": 2, "docs/en/state-space/kalman-filter.md#section_2": 1, "docs/en/state-space/kalman-filter.md#section_3": 1, "docs/en/state-space/kalman-filter.md#section_5": 1, "docs/en/state-space/kalman-filter.md#section_6": 4, "docs/en/state-space/kalman-filter.md#section_7": 1, "docs/en/state-space/kalman-filter.md#section_11": 2, "docs/en/state-space/kalman-filter.md#section_13": 7, "docs/en/state-space/kalman-filter.md#section_14": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_4": 1, "docs/en/decomposition/stl.md": 5, "docs/en/decomposition/stl.md#section_3": 1, "docs/en/decomposition/stl.md#section_5": 1, "docs/en/decomposition/stl.md#section_13": 3, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "local": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 10, "docs/en/state-space/kalman-filter.md#section_1": 1, "docs/en/state-space/kalman-filter.md#section_3": 1, "docs/en/state-space/kalman-filter.md#section_4": 1, "docs/en/state-space/kalman-filter.md#section_7": 1, "docs/en/state-space/kalman-filter.md#section_8": 1, "docs/en/state-space/kalman-filter.md#section_13": 5, "docs/en/anomaly-detection/anomaly-detection.md": 4, "docs/en/anomaly-detection/anomaly-detection.md#section_5": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1, "docs/en/decomposition/stl.md": 4, "docs/en/decomposition/stl.md#section_0": 1, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/decomposition/stl.md#section_14": 1}, "level": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_1": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_10": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 8, "docs/en/exponential-smoothing/ets.md#section_2": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/ets.md#section_10": 2, "docs/en/exponential-smoothing/ets.md#section_13": 4, "docs/en/exponential-smoothing/holt.md": 16, "docs/en/exponential-smoothing/holt.md#section_0": 2, "docs/en/exponential-smoothing/holt.md#section_1": 3, "docs/en/exponential-smoothing/holt.md#section_2": 1, "docs/en/exponential-smoothing/holt.md#section_6": 1, "docs/en/exponential-smoothing/holt.md#section_14": 9, "docs/en/exponential-smoothing/holt-winters.md": 16, "docs/en/exponential-smoothing/holt-winters.md#section_0": 2, "docs/en/exponential-smoothing/holt-winters.md#section_1": 2, "docs/en/exponential-smoothing/holt-winters.md#section_2": 3, "docs/en/exponential-smoothing/holt-winters.md#section_6": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 8, "docs/en/exponential-smoothing/ses.md": 13, "docs/en/exponential-smoothing/ses.md#section_1": 1, "docs/en/exponential-smoothing/ses.md#section_9": 3, "docs/en/exponential-smoothing/ses.md#section_10": 1, "docs/en/exponential-smoothing/ses.md#section_13": 8, "docs/en/forecasting/prediction-intervals.md": 3, "docs/en/forecasting/prediction-intervals.md#section_1": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_11": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/state-space/kalman-filter.md": 10, "docs/en/state-space/kalman-filter.md#section_1": 1, "docs/en/state-space/kalman-filter.md#section_3": 1, "docs/en/state-space/kalman-filter.md#section_4": 1, "docs/en/state-space/kalman-filter.md#section_7": 1, "docs/en/state-space/kalman-filter.md#section_8": 1, "docs/en/state-space/kalman-filter.md#section_13": 5, "docs/en/decomposition/classical.md": 8, "docs/en/decomposition/classical.md#section_1": 3, "docs/en/decomposition/classical.md#section_12": 5, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_1": 1}, "trend": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/spectral/spectral-analysis.md": 4, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_10": 2, "docs/en/time-domain/arima.md": 10, "docs/en/time-domain/arima.md#section_3": 1, "docs/en/time-domain/arima.md#section_7": 1, "docs/en/time-domain/arima.md#section_8": 2, "docs/en/time-domain/arima.md#section_9": 4, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/sarima.md": 8, "docs/en/time-domain/sarima.md#section_4": 1, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/time-domain/sarima.md#section_8": 2, "docs/en/time-domain/sarima.md#section_11": 3, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 14, "docs/en/exponential-smoothing/ets.md#section_0": 2, "docs/en/exponential-smoothing/ets.md#section_1": 3, "docs/en/exponential-smoothing/ets.md#section_2": 1, "docs/en/exponential-smoothing/ets.md#section_3": 1, "docs/en/exponential-smoothing/ets.md#section_10": 1, "docs/en/exponential-smoothing/ets.md#section_11": 4, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/holt.md": 56, "docs/en/exponential-smoothing/holt.md#section_0": 3, "docs/en/exponential-smoothing/holt.md#section_1": 5, "docs/en/exponential-smoothing/holt.md#section_2": 1, "docs/en/exponential-smoothing/holt.md#section_4": 2, "docs/en/exponential-smoothing/holt.md#section_5": 1, "docs/en/exponential-smoothing/holt.md#section_6": 4, "docs/en/exponential-smoothing/holt.md#section_7": 12, "docs/en/exponential-smoothing/holt.md#section_9": 3, "docs/en/exponential-smoothing/holt.md#section_12": 1, "docs/en/exponential-smoothing/holt.md#section_14": 24, "docs/en/exponential-smoothing/holt-winters.md": 15, "docs/en/exponential-smoothing/holt-winters.md#section_0": 1, "docs/en/exponential-smoothing/holt-winters.md#section_1": 2, "docs/en/exponential-smoothing/holt-winters.md#section_4": 1, "docs/en/exponential-smoothing/holt-winters.md#section_5": 2, "docs/en/exponential-smoothing/holt-winters.md#section_6": 1, "docs/en/exponential-smoothing/holt-winters.md#section_9": 3, "docs/en/exponential-smoothing/holt-winters.md#section_10": 1, "docs/en/exponential-smoothing/holt-winters.md#section_11": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 3, "docs/en/exponential-smoothing/ses.md": 7, "docs/en/exponential-smoothing/ses.md#section_0": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/exponential-smoothing/ses.md#section_13": 5, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/interview/interview-questions.md": 5, "docs/en/interview/interview-questions.md#section_1": 2, "docs/en/interview/interview-questions.md#section_3": 1, "docs/en/interview/interview-questions.md#section_11": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_10": 2, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_4": 2, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_3": 1, "docs/en/decomposition/classical.md": 20, "docs/en/decomposition/classical.md#section_0": 2, "docs/en/decomposition/classical.md#section_1": 1, "docs/en/decomposition/classical.md#section_2": 1, "docs/en/decomposition/classical.md#section_5": 2, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/classical.md#section_8": 2, "docs/en/decomposition/classical.md#section_9": 4, "docs/en/decomposition/classical.md#section_12": 7, "docs/en/decomposition/stl.md": 29, "docs/en/decomposition/stl.md#section_0": 3, "docs/en/decomposition/stl.md#section_1": 3, "docs/en/decomposition/stl.md#section_3": 1, "docs/en/decomposition/stl.md#section_5": 3, "docs/en/decomposition/stl.md#section_6": 5, "docs/en/decomposition/stl.md#section_8": 3, "docs/en/decomposition/stl.md#section_10": 3, "docs/en/decomposition/stl.md#section_13": 6, "docs/en/decomposition/stl.md#section_14": 2, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/change-detection/change-point.md#section_1": 1, "docs/en/foundations/stationarity.md": 16, "docs/en/foundations/stationarity.md#section_1": 3, "docs/en/foundations/stationarity.md#section_6": 4, "docs/en/foundations/stationarity.md#section_13": 9}, "multivariate": {"docs/en/index.md": 3, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_3": 2, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_5": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_6": 1}, "ts": {"docs/en/index.md": 2, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_3": 1, "docs/en/practical/practical-modeling.md": 9, "docs/en/practical/practical-modeling.md#section_4": 1, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/practical/practical-modeling.md#section_13": 7}, "var": {"docs/en/index.md": 3, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_3": 1, "docs/en/index.md#section_4": 1, "docs/en/spectral/spectral-analysis.md": 3, "docs/en/spectral/spectral-analysis.md#section_2": 1, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/multivariate/granger-causality.md": 15, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/multivariate/granger-causality.md#section_1": 1, "docs/en/multivariate/granger-causality.md#section_2": 1, "docs/en/multivariate/granger-causality.md#section_3": 2, "docs/en/multivariate/granger-causality.md#section_6": 3, "docs/en/multivariate/granger-causality.md#section_8": 1, "docs/en/multivariate/granger-causality.md#section_14": 3, "docs/en/multivariate/granger-causality.md#section_15": 3, "docs/en/multivariate/var.md": 42, "docs/en/multivariate/var.md#section_0": 3, "docs/en/multivariate/var.md#section_1": 1, "docs/en/multivariate/var.md#section_2": 1, "docs/en/multivariate/var.md#section_3": 2, "docs/en/multivariate/var.md#section_4": 1, "docs/en/multivariate/var.md#section_6": 4, "docs/en/multivariate/var.md#section_7": 5, "docs/en/multivariate/var.md#section_8": 1, "docs/en/multivariate/var.md#section_9": 1, "docs/en/multivariate/var.md#section_10": 2, "docs/en/multivariate/var.md#section_12": 1, "docs/en/multivariate/var.md#section_15": 19, "docs/en/time-domain/ma.md": 7, "docs/en/time-domain/ma.md#section_2": 1, "docs/en/time-domain/ma.md#section_13": 6, "docs/en/time-domain/ar.md": 8, "docs/en/time-domain/ar.md#section_2": 1, "docs/en/time-domain/ar.md#section_14": 7, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_6": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_5": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_4": 1, "docs/en/forecasting/prediction-intervals.md": 7, "docs/en/forecasting/prediction-intervals.md#section_1": 1, "docs/en/forecasting/prediction-intervals.md#section_3": 1, "docs/en/forecasting/prediction-intervals.md#section_5": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 3, "docs/en/forecasting/multi-step.md": 5, "docs/en/forecasting/multi-step.md#section_17": 5, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_1": 1, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_13": 2, "docs/en/foundations/autocorrelation.md": 4, "docs/en/foundations/autocorrelation.md#section_8": 2, "docs/en/foundations/autocorrelation.md#section_16": 2, "docs/en/foundations/stationarity.md": 8, "docs/en/foundations/stationarity.md#section_1": 1, "docs/en/foundations/stationarity.md#section_2": 1, "docs/en/foundations/stationarity.md#section_3": 1, "docs/en/foundations/stationarity.md#section_4": 1, "docs/en/foundations/stationarity.md#section_13": 4}, "varma": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1}, "granger": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/multivariate/granger-causality.md": 31, "docs/en/multivariate/granger-causality.md#section_0": 3, "docs/en/multivariate/granger-causality.md#section_1": 4, "docs/en/multivariate/granger-causality.md#section_2": 1, "docs/en/multivariate/granger-causality.md#section_5": 1, "docs/en/multivariate/granger-causality.md#section_6": 4, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_9": 1, "docs/en/multivariate/granger-causality.md#section_13": 3, "docs/en/multivariate/granger-causality.md#section_14": 1, "docs/en/multivariate/granger-causality.md#section_15": 10, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_12": 2}, "causality": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/multivariate/granger-causality.md": 24, "docs/en/multivariate/granger-causality.md#section_0": 3, "docs/en/multivariate/granger-causality.md#section_1": 2, "docs/en/multivariate/granger-causality.md#section_2": 1, "docs/en/multivariate/granger-causality.md#section_4": 1, "docs/en/multivariate/granger-causality.md#section_6": 4, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_13": 1, "docs/en/multivariate/granger-causality.md#section_14": 1, "docs/en/multivariate/granger-causality.md#section_15": 9, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_12": 1}, "change": {"docs/en/index.md": 2, "docs/en/index.md#section_1": 2, "docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_14": 2, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/model-selection/cross-validation.md": 3, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/change-detection/change-point.md": 36, "docs/en/change-detection/change-point.md#section_0": 2, "docs/en/change-detection/change-point.md#section_1": 5, "docs/en/change-detection/change-point.md#section_2": 2, "docs/en/change-detection/change-point.md#section_3": 2, "docs/en/change-detection/change-point.md#section_4": 1, "docs/en/change-detection/change-point.md#section_5": 3, "docs/en/change-detection/change-point.md#section_6": 1, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_9": 1, "docs/en/change-detection/change-point.md#section_11": 2, "docs/en/change-detection/change-point.md#section_12": 1, "docs/en/change-detection/change-point.md#section_13": 1, "docs/en/change-detection/change-point.md#section_14": 12, "docs/en/change-detection/change-point.md#section_15": 1}, "detection": {"docs/en/index.md": 2, "docs/en/index.md#section_1": 2, "docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_5": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 11, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_3": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 2, "docs/en/change-detection/change-point.md": 23, "docs/en/change-detection/change-point.md#section_0": 5, "docs/en/change-detection/change-point.md#section_2": 1, "docs/en/change-detection/change-point.md#section_5": 1, "docs/en/change-detection/change-point.md#section_6": 2, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_11": 1, "docs/en/change-detection/change-point.md#section_13": 1, "docs/en/change-detection/change-point.md#section_14": 7, "docs/en/change-detection/change-point.md#section_15": 3}, "point": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_0": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 3, "docs/en/forecasting/prediction-intervals.md#section_1": 1, "docs/en/forecasting/prediction-intervals.md#section_6": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md": 9, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_4": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 5, "docs/en/decomposition/classical.md": 4, "docs/en/decomposition/classical.md#section_4": 1, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/classical.md#section_12": 2, "docs/en/decomposition/stl.md": 4, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_2": 1, "docs/en/decomposition/stl.md#section_13": 2, "docs/en/change-detection/change-point.md": 11, "docs/en/change-detection/change-point.md#section_0": 2, "docs/en/change-detection/change-point.md#section_1": 1, "docs/en/change-detection/change-point.md#section_3": 1, "docs/en/change-detection/change-point.md#section_4": 1, "docs/en/change-detection/change-point.md#section_5": 1, "docs/en/change-detection/change-point.md#section_14": 3, "docs/en/change-detection/change-point.md#section_15": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "anomaly": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/anomaly-detection/anomaly-detection.md": 29, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 5, "docs/en/anomaly-detection/anomaly-detection.md#section_2": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_3": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_4": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_5": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 9, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 2}, "methods": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_8": 2, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_0": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 3, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/forecasting/multi-step.md#section_18": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_5": 2, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_4": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_14": 1, "docs/en/anomaly-detection/anomaly-detection.md": 8, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_2": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 3, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/classical.md#section_13": 1, "docs/en/change-detection/change-point.md": 11, "docs/en/change-detection/change-point.md#section_0": 2, "docs/en/change-detection/change-point.md#section_7": 2, "docs/en/change-detection/change-point.md#section_14": 6, "docs/en/change-detection/change-point.md#section_15": 1}, "feature": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/practical/practical-modeling.md": 4, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/practical/practical-modeling.md#section_13": 3, "docs/en/features/feature-engineering.md": 7, "docs/en/features/feature-engineering.md#section_0": 2, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/features/feature-engineering.md#section_15": 1, "docs/en/features/feature-engineering.md#section_17": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "engineering": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 4, "docs/en/features/feature-engineering.md#section_0": 2, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_14": 1}, "pipelines": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "scaling": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/features/feature-engineering.md": 5, "docs/en/features/feature-engineering.md#section_0": 1, "docs/en/features/feature-engineering.md#section_4": 3, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "missing": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/interview/interview-questions.md": 7, "docs/en/interview/interview-questions.md#section_0": 1, "docs/en/interview/interview-questions.md#section_7": 3, "docs/en/interview/interview-questions.md#section_11": 1, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md": 7, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 3, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/features/feature-engineering.md": 5, "docs/en/features/feature-engineering.md#section_0": 1, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/features/feature-engineering.md#section_16": 3, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_5": 1, "docs/en/decomposition/classical.md#section_12": 2}, "data": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/spectral/spectral-analysis.md": 4, "docs/en/spectral/spectral-analysis.md#section_5": 1, "docs/en/spectral/spectral-analysis.md#section_15": 3, "docs/en/multivariate/granger-causality.md": 8, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_9": 1, "docs/en/multivariate/granger-causality.md#section_12": 2, "docs/en/multivariate/granger-causality.md#section_13": 2, "docs/en/multivariate/granger-causality.md#section_14": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 5, "docs/en/multivariate/var.md#section_9": 1, "docs/en/multivariate/var.md#section_15": 4, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_9": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 4, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_17": 3, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/identification.md#section_9": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 9, "docs/en/time-domain/sarima.md#section_0": 1, "docs/en/time-domain/sarima.md#section_6": 1, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_10": 1, "docs/en/time-domain/sarima.md#section_15": 4, "docs/en/deep-learning/deep-learning-ts.md": 19, "docs/en/deep-learning/deep-learning-ts.md#section_0": 2, "docs/en/deep-learning/deep-learning-ts.md#section_7": 3, "docs/en/deep-learning/deep-learning-ts.md#section_8": 2, "docs/en/deep-learning/deep-learning-ts.md#section_9": 1, "docs/en/deep-learning/deep-learning-ts.md#section_10": 1, "docs/en/deep-learning/deep-learning-ts.md#section_11": 4, "docs/en/deep-learning/deep-learning-ts.md#section_17": 6, "docs/en/exponential-smoothing/ets.md": 9, "docs/en/exponential-smoothing/ets.md#section_7": 2, "docs/en/exponential-smoothing/ets.md#section_8": 2, "docs/en/exponential-smoothing/ets.md#section_10": 1, "docs/en/exponential-smoothing/ets.md#section_13": 4, "docs/en/exponential-smoothing/holt.md": 5, "docs/en/exponential-smoothing/holt.md#section_0": 1, "docs/en/exponential-smoothing/holt.md#section_9": 1, "docs/en/exponential-smoothing/holt.md#section_14": 3, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_9": 2, "docs/en/exponential-smoothing/ses.md": 10, "docs/en/exponential-smoothing/ses.md#section_7": 3, "docs/en/exponential-smoothing/ses.md#section_9": 1, "docs/en/exponential-smoothing/ses.md#section_13": 6, "docs/en/forecasting/prediction-intervals.md": 5, "docs/en/forecasting/prediction-intervals.md#section_6": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_9": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/forecasting/multi-step.md": 7, "docs/en/forecasting/multi-step.md#section_9": 3, "docs/en/forecasting/multi-step.md#section_11": 1, "docs/en/forecasting/multi-step.md#section_17": 3, "docs/en/interview/interview-questions.md": 13, "docs/en/interview/interview-questions.md#section_0": 1, "docs/en/interview/interview-questions.md#section_5": 2, "docs/en/interview/interview-questions.md#section_8": 3, "docs/en/interview/interview-questions.md#section_11": 2, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/interview/interview-questions.md#section_16": 3, "docs/en/model-selection/cross-validation.md": 26, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/cross-validation.md#section_1": 4, "docs/en/model-selection/cross-validation.md#section_4": 2, "docs/en/model-selection/cross-validation.md#section_6": 2, "docs/en/model-selection/cross-validation.md#section_8": 1, "docs/en/model-selection/cross-validation.md#section_11": 16, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_9": 1, "docs/en/model-selection/residual-diagnostics.md": 5, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_9": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 3, "docs/en/practical/practical-modeling.md": 19, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/practical/practical-modeling.md#section_1": 1, "docs/en/practical/practical-modeling.md#section_6": 5, "docs/en/practical/practical-modeling.md#section_7": 3, "docs/en/practical/practical-modeling.md#section_13": 8, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/features/feature-engineering.md": 22, "docs/en/features/feature-engineering.md#section_0": 2, "docs/en/features/feature-engineering.md#section_4": 1, "docs/en/features/feature-engineering.md#section_6": 1, "docs/en/features/feature-engineering.md#section_8": 3, "docs/en/features/feature-engineering.md#section_10": 1, "docs/en/features/feature-engineering.md#section_16": 14, "docs/en/state-space/kalman-filter.md": 7, "docs/en/state-space/kalman-filter.md#section_1": 1, "docs/en/state-space/kalman-filter.md#section_2": 1, "docs/en/state-space/kalman-filter.md#section_5": 1, "docs/en/state-space/kalman-filter.md#section_6": 2, "docs/en/state-space/kalman-filter.md#section_8": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 10, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_9": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 5, "docs/en/decomposition/classical.md": 7, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/classical.md#section_8": 1, "docs/en/decomposition/classical.md#section_10": 1, "docs/en/decomposition/classical.md#section_12": 4, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_8": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 9, "docs/en/change-detection/change-point.md#section_1": 1, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_9": 1, "docs/en/change-detection/change-point.md#section_14": 6, "docs/en/foundations/autocorrelation.md": 5, "docs/en/foundations/autocorrelation.md#section_1": 1, "docs/en/foundations/autocorrelation.md#section_10": 3, "docs/en/foundations/autocorrelation.md#section_16": 1}, "deep": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md": 7, "docs/en/deep-learning/deep-learning-ts.md#section_0": 2, "docs/en/deep-learning/deep-learning-ts.md#section_7": 3, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_0": 1, "docs/en/interview/interview-questions.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1}, "rnn": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md": 4, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md#section_2": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "lstm": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 26, "docs/en/deep-learning/deep-learning-ts.md#section_0": 2, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md#section_2": 1, "docs/en/deep-learning/deep-learning-ts.md#section_6": 4, "docs/en/deep-learning/deep-learning-ts.md#section_8": 2, "docs/en/deep-learning/deep-learning-ts.md#section_13": 4, "docs/en/deep-learning/deep-learning-ts.md#section_17": 12, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/interview/interview-questions.md#section_13": 1}, "tcn": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md": 11, "docs/en/deep-learning/deep-learning-ts.md#section_0": 2, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md#section_3": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 6, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_13": 1}, "transformers": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_13": 1}, "modeling": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/practical/practical-modeling.md": 3, "docs/en/practical/practical-modeling.md#section_0": 2, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "backtesting": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/practical/practical-modeling.md#section_1": 1}, "deployment": {"docs/en/index.md": 1, "docs/en/index.md#section_1": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/practical/practical-modeling.md#section_6": 1}, "common": {"docs/en/index.md": 2, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_2": 1, "docs/en/spectral/spectral-analysis.md": 7, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_15": 5, "docs/en/multivariate/granger-causality.md": 10, "docs/en/multivariate/granger-causality.md#section_7": 2, "docs/en/multivariate/granger-causality.md#section_15": 8, "docs/en/multivariate/var.md": 7, "docs/en/multivariate/var.md#section_7": 1, "docs/en/multivariate/var.md#section_15": 6, "docs/en/time-domain/ma.md": 6, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ma.md#section_13": 5, "docs/en/time-domain/arma.md": 11, "docs/en/time-domain/arma.md#section_5": 2, "docs/en/time-domain/arma.md#section_7": 2, "docs/en/time-domain/arma.md#section_14": 7, "docs/en/time-domain/ar.md": 6, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/ar.md#section_14": 5, "docs/en/time-domain/arima.md": 6, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/arima.md#section_17": 5, "docs/en/time-domain/identification.md": 8, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/identification.md#section_15": 7, "docs/en/time-domain/sarima.md": 7, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_15": 5, "docs/en/deep-learning/deep-learning-ts.md": 7, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 6, "docs/en/exponential-smoothing/ets.md": 6, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/ets.md#section_13": 5, "docs/en/exponential-smoothing/holt.md": 6, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/exponential-smoothing/holt.md#section_14": 5, "docs/en/exponential-smoothing/holt-winters.md": 6, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 5, "docs/en/exponential-smoothing/ses.md": 7, "docs/en/exponential-smoothing/ses.md#section_7": 2, "docs/en/exponential-smoothing/ses.md#section_13": 5, "docs/en/forecasting/prediction-intervals.md": 6, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 5, "docs/en/forecasting/multi-step.md": 6, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/forecasting/multi-step.md#section_17": 5, "docs/en/interview/interview-questions.md": 9, "docs/en/interview/interview-questions.md#section_0": 1, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/interview/interview-questions.md#section_16": 5, "docs/en/model-selection/cross-validation.md": 7, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 6, "docs/en/model-selection/information-criteria.md": 6, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/information-criteria.md#section_13": 5, "docs/en/model-selection/residual-diagnostics.md": 7, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 6, "docs/en/practical/practical-modeling.md": 7, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/practical/practical-modeling.md#section_13": 6, "docs/en/features/feature-engineering.md": 6, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/features/feature-engineering.md#section_16": 5, "docs/en/state-space/kalman-filter.md": 6, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/state-space/kalman-filter.md#section_13": 5, "docs/en/anomaly-detection/anomaly-detection.md": 6, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 5, "docs/en/decomposition/classical.md": 6, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/classical.md#section_12": 5, "docs/en/decomposition/stl.md": 7, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/decomposition/stl.md#section_13": 6, "docs/en/change-detection/change-point.md": 6, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_14": 5, "docs/en/foundations/autocorrelation.md": 6, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_16": 5, "docs/en/foundations/stationarity.md": 6, "docs/en/foundations/stationarity.md#section_7": 1, "docs/en/foundations/stationarity.md#section_13": 5}, "pitfalls": {"docs/en/index.md": 2, "docs/en/index.md#section_1": 1, "docs/en/index.md#section_2": 1, "docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_7": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "page": {"docs/en/index.md": 3, "docs/en/index.md#section_2": 2, "docs/en/index.md#section_3": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_2": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "structure": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_3": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 4, "docs/en/time-domain/sarima.md#section_6": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_15": 2, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_4": 2, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_7": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "topic": {"docs/en/index.md": 2, "docs/en/index.md#section_2": 1, "docs/en/index.md#section_3": 1}, "follows": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "consistent": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/forecasting/multi-step.md": 4, "docs/en/forecasting/multi-step.md#section_2": 1, "docs/en/forecasting/multi-step.md#section_5": 1, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_6": 1, "docs/en/interview/interview-questions.md#section_15": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_3": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_1": 1}, "section": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_0": 1}, "format": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1}, "key": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/spectral/spectral-analysis.md": 3, "docs/en/spectral/spectral-analysis.md#section_0": 1, "docs/en/spectral/spectral-analysis.md#section_1": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_7": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_4": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_11": 1, "docs/en/exponential-smoothing/ets.md#section_12": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_0": 1, "docs/en/state-space/kalman-filter.md": 3, "docs/en/state-space/kalman-filter.md#section_2": 1, "docs/en/state-space/kalman-filter.md#section_13": 2, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_0": 1, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_16": 2, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_13": 2}, "points": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_6": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_6": 2, "docs/en/anomaly-detection/anomaly-detection.md": 4, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 3, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_5": 1, "docs/en/decomposition/classical.md#section_12": 2, "docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_13": 3, "docs/en/change-detection/change-point.md": 13, "docs/en/change-detection/change-point.md#section_1": 2, "docs/en/change-detection/change-point.md#section_3": 1, "docs/en/change-detection/change-point.md#section_6": 1, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_9": 1, "docs/en/change-detection/change-point.md#section_11": 2, "docs/en/change-detection/change-point.md#section_12": 1, "docs/en/change-detection/change-point.md#section_14": 4, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_1": 1}, "lines": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "core": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_1": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_1": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_1": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_1": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_1": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_1": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_1": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_1": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_1": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_1": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_1": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_1": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_1": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_1": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_1": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_1": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_1": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_1": 1, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_0": 1, "docs/en/state-space/kalman-filter.md#section_1": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_1": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_1": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_1": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_1": 1}, "definitions": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_1": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_1": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_1": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_1": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_1": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_1": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_1": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_1": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_1": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_1": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_1": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_1": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_1": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_1": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_1": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_1": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_1": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_1": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_1": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_1": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_1": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_1": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_1": 1}, "essential": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_7": 1}, "terminology": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1}, "concepts": {"docs/en/index.md": 2, "docs/en/index.md#section_2": 1, "docs/en/index.md#section_3": 1}, "math": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/spectral/spectral-analysis.md": 3, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ma.md": 3, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/arma.md": 3, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/time-domain/ar.md": 3, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/arima.md": 3, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/time-domain/sarima.md": 3, "docs/en/time-domain/sarima.md#section_15": 2, "docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2, "docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/holt.md": 3, "docs/en/exponential-smoothing/holt.md#section_14": 2, "docs/en/exponential-smoothing/holt-winters.md": 3, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/exponential-smoothing/ses.md": 3, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/forecasting/prediction-intervals.md": 3, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/forecasting/multi-step.md": 3, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/model-selection/cross-validation.md": 3, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_13": 2, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/practical/practical-modeling.md": 3, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/state-space/kalman-filter.md": 3, "docs/en/state-space/kalman-filter.md#section_13": 2, "docs/en/anomaly-detection/anomaly-detection.md": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_12": 2, "docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_13": 2, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_14": 2, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_16": 2, "docs/en/foundations/stationarity.md": 3, "docs/en/foundations/stationarity.md#section_13": 2}, "derivations": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/spectral/spectral-analysis.md": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/var.md": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/stl.md": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/stationarity.md": 1}, "rigorous": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1}, "mathematical": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "algorithm": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_6": 2, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_6": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_6": 2, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_6": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_6": 2, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_6": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/interview/interview-questions.md#section_11": 1, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_5": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_5": 2, "docs/en/anomaly-detection/anomaly-detection.md": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_4": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_5": 2, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_3": 1, "docs/en/decomposition/stl.md#section_5": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_3": 1, "docs/en/change-detection/change-point.md#section_6": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_9": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_6": 1}, "sketch": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_6": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_6": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_6": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_6": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_6": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_5": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_5": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_5": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_5": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_6": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_9": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_6": 1}, "method": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_3": 1, "docs/en/exponential-smoothing/holt.md": 11, "docs/en/exponential-smoothing/holt.md#section_0": 2, "docs/en/exponential-smoothing/holt.md#section_1": 1, "docs/en/exponential-smoothing/holt.md#section_5": 1, "docs/en/exponential-smoothing/holt.md#section_6": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/exponential-smoothing/holt.md#section_10": 1, "docs/en/exponential-smoothing/holt.md#section_14": 3, "docs/en/exponential-smoothing/holt-winters.md": 5, "docs/en/exponential-smoothing/holt-winters.md#section_0": 2, "docs/en/exponential-smoothing/holt-winters.md#section_1": 2, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/anomaly-detection/anomaly-detection.md": 10, "docs/en/anomaly-detection/anomaly-detection.md#section_2": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 4, "docs/en/anomaly-detection/anomaly-detection.md#section_11": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_12": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_14": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "works": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/decomposition/stl.md#section_13": 1}, "mistakes": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1}, "avoid": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_0": 1}, "mini": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_8": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_8": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_8": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_8": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_8": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_8": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_10": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_8": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_9": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_9": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_9": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_8": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_8": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_8": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_8": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_10": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_7": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_8": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_8": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_8": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_9": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_8": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_7": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_7": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_8": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_11": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_8": 1}, "example": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_8": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_8": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 5, "docs/en/multivariate/var.md#section_2": 1, "docs/en/multivariate/var.md#section_8": 1, "docs/en/multivariate/var.md#section_15": 3, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_8": 1, "docs/en/time-domain/arma.md": 4, "docs/en/time-domain/arma.md#section_5": 1, "docs/en/time-domain/arma.md#section_8": 1, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_8": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_10": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_8": 1, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_9": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_9": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_9": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_8": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_8": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_8": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_8": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_10": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_15": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_7": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_8": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_8": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_8": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_9": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_8": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_7": 1, "docs/en/decomposition/classical.md#section_12": 2, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_7": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_8": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_11": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_4": 1, "docs/en/foundations/stationarity.md#section_8": 1}, "quick": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_0": 1}, "illustration": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1}, "quiz": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/spectral/spectral-analysis.md": 6, "docs/en/spectral/spectral-analysis.md#section_15": 6, "docs/en/multivariate/granger-causality.md": 6, "docs/en/multivariate/granger-causality.md#section_15": 6, "docs/en/multivariate/var.md": 6, "docs/en/multivariate/var.md#section_15": 6, "docs/en/time-domain/ma.md": 6, "docs/en/time-domain/ma.md#section_13": 6, "docs/en/time-domain/arma.md": 6, "docs/en/time-domain/arma.md#section_14": 6, "docs/en/time-domain/ar.md": 6, "docs/en/time-domain/ar.md#section_14": 6, "docs/en/time-domain/arima.md": 6, "docs/en/time-domain/arima.md#section_17": 6, "docs/en/time-domain/identification.md": 6, "docs/en/time-domain/identification.md#section_15": 6, "docs/en/time-domain/sarima.md": 6, "docs/en/time-domain/sarima.md#section_15": 6, "docs/en/deep-learning/deep-learning-ts.md": 6, "docs/en/deep-learning/deep-learning-ts.md#section_17": 6, "docs/en/exponential-smoothing/ets.md": 6, "docs/en/exponential-smoothing/ets.md#section_13": 6, "docs/en/exponential-smoothing/holt.md": 6, "docs/en/exponential-smoothing/holt.md#section_14": 6, "docs/en/exponential-smoothing/holt-winters.md": 6, "docs/en/exponential-smoothing/holt-winters.md#section_13": 6, "docs/en/exponential-smoothing/ses.md": 6, "docs/en/exponential-smoothing/ses.md#section_13": 6, "docs/en/forecasting/prediction-intervals.md": 6, "docs/en/forecasting/prediction-intervals.md#section_13": 6, "docs/en/forecasting/multi-step.md": 6, "docs/en/forecasting/multi-step.md#section_17": 6, "docs/en/interview/interview-questions.md": 6, "docs/en/interview/interview-questions.md#section_16": 6, "docs/en/model-selection/cross-validation.md": 6, "docs/en/model-selection/cross-validation.md#section_11": 6, "docs/en/model-selection/information-criteria.md": 6, "docs/en/model-selection/information-criteria.md#section_13": 6, "docs/en/model-selection/residual-diagnostics.md": 6, "docs/en/model-selection/residual-diagnostics.md#section_17": 6, "docs/en/practical/practical-modeling.md": 6, "docs/en/practical/practical-modeling.md#section_13": 6, "docs/en/features/feature-engineering.md": 6, "docs/en/features/feature-engineering.md#section_16": 6, "docs/en/state-space/kalman-filter.md": 6, "docs/en/state-space/kalman-filter.md#section_13": 6, "docs/en/anomaly-detection/anomaly-detection.md": 6, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 6, "docs/en/decomposition/classical.md": 6, "docs/en/decomposition/classical.md#section_12": 6, "docs/en/decomposition/stl.md": 6, "docs/en/decomposition/stl.md#section_13": 6, "docs/en/change-detection/change-point.md": 6, "docs/en/change-detection/change-point.md#section_14": 6, "docs/en/foundations/autocorrelation.md": 6, "docs/en/foundations/autocorrelation.md#section_16": 6, "docs/en/foundations/stationarity.md": 6, "docs/en/foundations/stationarity.md#section_13": 6}, "questions": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_0": 2}, "hidden": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_0": 1, "docs/en/deep-learning/deep-learning-ts.md": 6, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md#section_2": 1, "docs/en/deep-learning/deep-learning-ts.md#section_6": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 3, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_0": 1}, "answers": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_0": 1}, "click": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1}, "reveal": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1}, "references": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_16": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_16": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_14": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_15": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_18": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_16": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_14": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_17": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_14": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_13": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_17": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "further": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "reading": {"docs/en/index.md": 1, "docs/en/index.md#section_2": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "getting": {"docs/en/index.md": 1, "docs/en/index.md#section_3": 1}, "started": {"docs/en/index.md": 1, "docs/en/index.md#section_3": 1}, "choose": {"docs/en/index.md": 1, "docs/en/index.md#section_3": 1, "docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_0": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_5": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "sidebar": {"docs/en/index.md": 1, "docs/en/index.md#section_3": 1}, "begin": {"docs/en/index.md": 1, "docs/en/index.md#section_3": 1, "docs/en/multivariate/var.md": 6, "docs/en/multivariate/var.md#section_2": 5, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_3": 1, "docs/en/time-domain/ar.md": 3, "docs/en/time-domain/ar.md#section_4": 3, "docs/en/exponential-smoothing/ets.md": 4, "docs/en/exponential-smoothing/ets.md#section_3": 4, "docs/en/state-space/kalman-filter.md": 5, "docs/en/state-space/kalman-filter.md#section_4": 5, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_2": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_1": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_3": 3}, "self": {"docs/en/index.md": 1, "docs/en/index.md#section_3": 1, "docs/en/deep-learning/deep-learning-ts.md": 15, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md#section_4": 1, "docs/en/deep-learning/deep-learning-ts.md#section_6": 6, "docs/en/deep-learning/deep-learning-ts.md#section_13": 6, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "contained": {"docs/en/index.md": 1, "docs/en/index.md#section_3": 1}, "builds": {"docs/en/index.md": 1, "docs/en/index.md#section_3": 1}, "foundational": {"docs/en/index.md": 1, "docs/en/index.md#section_3": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_0": 1}, "recommended": {"docs/en/index.md": 1, "docs/en/index.md#section_3": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "path": {"docs/en/index.md": 1, "docs/en/index.md#section_3": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_2": 1, "docs/en/anomaly-detection/anomaly-detection.md": 7, "docs/en/anomaly-detection/anomaly-detection.md#section_4": 4, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 3}, "beginners": {"docs/en/index.md": 1, "docs/en/index.md#section_3": 1}, "start": {"docs/en/index.md": 1, "docs/en/index.md#section_3": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_6": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "md": {"docs/en/index.md": 12, "docs/en/index.md#section_3": 12, "docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_0": 1, "docs/en/exponential-smoothing/ets.md#section_1": 1, "docs/en/exponential-smoothing/ets.md#section_7": 1}, "move": {"docs/en/index.md": 1, "docs/en/index.md#section_3": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_5": 1}, "learn": {"docs/en/index.md": 1, "docs/en/index.md#section_3": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "explore": {"docs/en/index.md": 1, "docs/en/index.md#section_3": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "advance": {"docs/en/index.md": 1, "docs/en/index.md#section_3": 1}, "code": {"docs/en/index.md": 1, "docs/en/index.md#section_4": 1}, "examples": {"docs/en/index.md": 1, "docs/en/index.md#section_4": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_1": 1}, "runnable": {"docs/en/index.md": 1, "docs/en/index.md#section_4": 1}, "python": {"docs/en/index.md": 2, "docs/en/index.md#section_4": 2, "docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_8": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_8": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_8": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_8": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_8": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_8": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_10": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_8": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_9": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_6": 1, "docs/en/deep-learning/deep-learning-ts.md#section_9": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_9": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_8": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_8": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_8": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_8": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/forecasting/multi-step.md#section_10": 1, "docs/en/model-selection/cross-validation.md": 3, "docs/en/model-selection/cross-validation.md#section_5": 1, "docs/en/model-selection/cross-validation.md#section_7": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_8": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_8": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/practical/practical-modeling.md#section_8": 1, "docs/en/features/feature-engineering.md": 11, "docs/en/features/feature-engineering.md#section_5": 2, "docs/en/features/feature-engineering.md#section_9": 1, "docs/en/features/feature-engineering.md#section_16": 8, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_8": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_7": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_7": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_6": 2, "docs/en/change-detection/change-point.md#section_8": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_11": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_8": 1}, "demos": {"docs/en/index.md": 2, "docs/en/index.md#section_4": 2}, "available": {"docs/en/index.md": 2, "docs/en/index.md#section_4": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/practical/practical-modeling.md": 3, "docs/en/practical/practical-modeling.md#section_13": 3, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_16": 3, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_9": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_1": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "directory": {"docs/en/index.md": 1, "docs/en/index.md#section_4": 1}, "run": {"docs/en/index.md": 2, "docs/en/index.md#section_4": 2, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_14": 2, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_11": 1, "docs/en/change-detection/change-point.md": 5, "docs/en/change-detection/change-point.md#section_5": 1, "docs/en/change-detection/change-point.md#section_13": 3, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "bash": {"docs/en/index.md": 1, "docs/en/index.md#section_4": 1}, "demo": {"docs/en/index.md": 1, "docs/en/index.md#section_4": 1}, "changepoint": {"docs/en/index.md": 1, "docs/en/index.md#section_4": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "backtest": {"docs/en/index.md": 1, "docs/en/index.md#section_4": 1, "docs/en/practical/practical-modeling.md": 3, "docs/en/practical/practical-modeling.md#section_2": 1, "docs/en/practical/practical-modeling.md#section_13": 2}, "metrics": {"docs/en/index.md": 1, "docs/en/index.md#section_4": 1, "docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_11": 3, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_3": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/practical/practical-modeling.md": 5, "docs/en/practical/practical-modeling.md#section_6": 2, "docs/en/practical/practical-modeling.md#section_8": 1, "docs/en/practical/practical-modeling.md#section_10": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_9": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "language": {"docs/en/index.md": 2, "docs/en/index.md#section_5": 2}, "toggle": {"docs/en/index.md": 1, "docs/en/index.md#section_5": 1}, "use": {"docs/en/index.md": 1, "docs/en/index.md#section_5": 1, "docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 5, "docs/en/multivariate/granger-causality.md#section_6": 3, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 5, "docs/en/multivariate/var.md#section_7": 2, "docs/en/multivariate/var.md#section_15": 3, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/ar.md": 4, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/identification.md": 6, "docs/en/time-domain/identification.md#section_0": 2, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/time-domain/sarima.md": 5, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_15": 4, "docs/en/deep-learning/deep-learning-ts.md": 7, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/deep-learning/deep-learning-ts.md#section_5": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 3, "docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/holt.md": 5, "docs/en/exponential-smoothing/holt.md#section_0": 1, "docs/en/exponential-smoothing/holt.md#section_6": 2, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 6, "docs/en/exponential-smoothing/holt-winters.md#section_7": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 4, "docs/en/exponential-smoothing/ses.md": 5, "docs/en/exponential-smoothing/ses.md#section_5": 1, "docs/en/exponential-smoothing/ses.md#section_7": 3, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 7, "docs/en/forecasting/prediction-intervals.md#section_6": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 5, "docs/en/forecasting/multi-step.md": 8, "docs/en/forecasting/multi-step.md#section_1": 1, "docs/en/forecasting/multi-step.md#section_6": 3, "docs/en/forecasting/multi-step.md#section_17": 4, "docs/en/interview/interview-questions.md": 16, "docs/en/interview/interview-questions.md#section_0": 1, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/interview/interview-questions.md#section_2": 1, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/interview/interview-questions.md#section_6": 3, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/interview/interview-questions.md#section_11": 2, "docs/en/interview/interview-questions.md#section_13": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 7, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/cross-validation.md#section_6": 2, "docs/en/model-selection/cross-validation.md#section_11": 3, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/model-selection/information-criteria.md": 5, "docs/en/model-selection/information-criteria.md#section_0": 1, "docs/en/model-selection/information-criteria.md#section_13": 4, "docs/en/practical/practical-modeling.md": 7, "docs/en/practical/practical-modeling.md#section_7": 2, "docs/en/practical/practical-modeling.md#section_13": 5, "docs/en/features/feature-engineering.md": 12, "docs/en/features/feature-engineering.md#section_0": 1, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_8": 2, "docs/en/features/feature-engineering.md#section_16": 8, "docs/en/state-space/kalman-filter.md": 4, "docs/en/state-space/kalman-filter.md#section_6": 3, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 5, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2, "docs/en/decomposition/classical.md": 5, "docs/en/decomposition/classical.md#section_5": 1, "docs/en/decomposition/classical.md#section_12": 4, "docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_2": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 10, "docs/en/change-detection/change-point.md#section_0": 2, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_14": 7, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_9": 1, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 7, "docs/en/foundations/stationarity.md#section_5": 1, "docs/en/foundations/stationarity.md#section_7": 2, "docs/en/foundations/stationarity.md#section_13": 4}, "selector": {"docs/en/index.md": 1, "docs/en/index.md#section_5": 1}, "header": {"docs/en/index.md": 1, "docs/en/index.md#section_5": 1}, "switch": {"docs/en/index.md": 1, "docs/en/index.md#section_5": 1}, "between": {"docs/en/index.md": 1, "docs/en/index.md#section_5": 1, "docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 4, "docs/en/interview/interview-questions.md#section_2": 3, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/model-selection/cross-validation.md": 3, "docs/en/model-selection/cross-validation.md#section_11": 3, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_2": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 3, "docs/en/practical/practical-modeling.md#section_1": 1, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_14": 2, "docs/en/foundations/autocorrelation.md": 4, "docs/en/foundations/autocorrelation.md#section_0": 1, "docs/en/foundations/autocorrelation.md#section_1": 1, "docs/en/foundations/autocorrelation.md#section_16": 2, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_13": 2}, "site": {"docs/en/index.md": 1, "docs/en/index.md#section_5": 1}, "maintains": {"docs/en/index.md": 1, "docs/en/index.md#section_5": 1}, "parallel": {"docs/en/index.md": 1, "docs/en/index.md#section_5": 1}, "content": {"docs/en/index.md": 2, "docs/en/index.md#section_5": 2, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "languages": {"docs/en/index.md": 1, "docs/en/index.md#section_5": 1}, "open": {"docs/en/index.md": 1, "docs/en/index.md#section_5": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1}, "extensible": {"docs/en/index.md": 1, "docs/en/index.md#section_5": 1}, "see": {"docs/en/index.md": 1, "docs/en/index.md#section_5": 1, "docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "repository": {"docs/en/index.md": 1, "docs/en/index.md#section_5": 1}, "readme": {"docs/en/index.md": 1, "docs/en/index.md#section_5": 1}, "instructions": {"docs/en/index.md": 1, "docs/en/index.md#section_5": 1}, "adding": {"docs/en/index.md": 1, "docs/en/index.md#section_5": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "new": {"docs/en/index.md": 1, "docs/en/index.md#section_5": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_16": 1, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_15": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_2": 1, "docs/en/model-selection/information-criteria.md#section_14": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_1": 1, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/state-space/kalman-filter.md": 3, "docs/en/state-space/kalman-filter.md#section_0": 1, "docs/en/state-space/kalman-filter.md#section_2": 1, "docs/en/state-space/kalman-filter.md#section_14": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_12": 2}, "decomposes": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_0": 1, "docs/en/spectral/spectral-analysis.md#section_2": 1}, "into": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_0": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_0": 1, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_0": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_0": 1, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/decomposition/stl.md#section_13": 1}, "components": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_0": 1, "docs/en/spectral/spectral-analysis.md#section_10": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_0": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_1": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_1": 1, "docs/en/exponential-smoothing/ets.md": 4, "docs/en/exponential-smoothing/ets.md#section_0": 1, "docs/en/exponential-smoothing/ets.md#section_2": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_11": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_1": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_1": 1, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_0": 1, "docs/en/decomposition/classical.md#section_1": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_0": 1, "docs/en/decomposition/stl.md#section_13": 1}, "estimates": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_0": 1, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_7": 2, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 4, "docs/en/time-domain/ar.md#section_6": 2, "docs/en/time-domain/ar.md#section_12": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 5, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/cross-validation.md#section_4": 1, "docs/en/model-selection/cross-validation.md#section_11": 3, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1, "docs/en/state-space/kalman-filter.md": 5, "docs/en/state-space/kalman-filter.md#section_0": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/state-space/kalman-filter.md#section_13": 3, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_10": 1}, "power": {"docs/en/spectral/spectral-analysis.md": 4, "docs/en/spectral/spectral-analysis.md#section_0": 1, "docs/en/spectral/spectral-analysis.md#section_1": 1, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "peaks": {"docs/en/spectral/spectral-analysis.md": 14, "docs/en/spectral/spectral-analysis.md#section_0": 2, "docs/en/spectral/spectral-analysis.md#section_6": 2, "docs/en/spectral/spectral-analysis.md#section_7": 3, "docs/en/spectral/spectral-analysis.md#section_13": 3, "docs/en/spectral/spectral-analysis.md#section_14": 1, "docs/en/spectral/spectral-analysis.md#section_15": 3, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "indicate": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_0": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_14": 2, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_6": 1}, "dominant": {"docs/en/spectral/spectral-analysis.md": 3, "docs/en/spectral/spectral-analysis.md#section_0": 1, "docs/en/spectral/spectral-analysis.md#section_5": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1}, "cycles": {"docs/en/spectral/spectral-analysis.md": 8, "docs/en/spectral/spectral-analysis.md#section_0": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_15": 6, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "stationary": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_0": 1, "docs/en/spectral/spectral-analysis.md#section_1": 1, "docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_4": 1, "docs/en/multivariate/var.md#section_7": 1, "docs/en/time-domain/ma.md": 4, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/ma.md#section_13": 3, "docs/en/time-domain/arma.md": 4, "docs/en/time-domain/arma.md#section_4": 1, "docs/en/time-domain/arma.md#section_14": 3, "docs/en/time-domain/ar.md": 10, "docs/en/time-domain/ar.md#section_1": 1, "docs/en/time-domain/ar.md#section_5": 1, "docs/en/time-domain/ar.md#section_6": 2, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/ar.md#section_14": 5, "docs/en/time-domain/arima.md": 17, "docs/en/time-domain/arima.md#section_0": 2, "docs/en/time-domain/arima.md#section_1": 1, "docs/en/time-domain/arima.md#section_2": 1, "docs/en/time-domain/arima.md#section_6": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/arima.md#section_14": 2, "docs/en/time-domain/arima.md#section_17": 8, "docs/en/time-domain/identification.md": 4, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/time-domain/identification.md#section_4": 2, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/forecasting/prediction-intervals.md": 5, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 4, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 6, "docs/en/interview/interview-questions.md#section_1": 4, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 3, "docs/en/model-selection/cross-validation.md#section_11": 3, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_0": 1, "docs/en/foundations/autocorrelation.md": 9, "docs/en/foundations/autocorrelation.md#section_1": 1, "docs/en/foundations/autocorrelation.md#section_2": 1, "docs/en/foundations/autocorrelation.md#section_9": 3, "docs/en/foundations/autocorrelation.md#section_10": 2, "docs/en/foundations/autocorrelation.md#section_16": 2, "docs/en/foundations/stationarity.md": 42, "docs/en/foundations/stationarity.md#section_0": 3, "docs/en/foundations/stationarity.md#section_1": 7, "docs/en/foundations/stationarity.md#section_2": 1, "docs/en/foundations/stationarity.md#section_4": 3, "docs/en/foundations/stationarity.md#section_5": 3, "docs/en/foundations/stationarity.md#section_6": 5, "docs/en/foundations/stationarity.md#section_7": 2, "docs/en/foundations/stationarity.md#section_9": 1, "docs/en/foundations/stationarity.md#section_10": 1, "docs/en/foundations/stationarity.md#section_11": 2, "docs/en/foundations/stationarity.md#section_12": 1, "docs/en/foundations/stationarity.md#section_13": 13}, "density": {"docs/en/spectral/spectral-analysis.md": 14, "docs/en/spectral/spectral-analysis.md#section_0": 1, "docs/en/spectral/spectral-analysis.md#section_1": 3, "docs/en/spectral/spectral-analysis.md#section_2": 1, "docs/en/spectral/spectral-analysis.md#section_3": 1, "docs/en/spectral/spectral-analysis.md#section_4": 1, "docs/en/spectral/spectral-analysis.md#section_7": 2, "docs/en/spectral/spectral-analysis.md#section_15": 5, "docs/en/anomaly-detection/anomaly-detection.md": 4, "docs/en/anomaly-detection/anomaly-detection.md#section_5": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "fourier": {"docs/en/spectral/spectral-analysis.md": 7, "docs/en/spectral/spectral-analysis.md#section_0": 1, "docs/en/spectral/spectral-analysis.md#section_1": 1, "docs/en/spectral/spectral-analysis.md#section_2": 2, "docs/en/spectral/spectral-analysis.md#section_15": 3, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/sarima.md": 3, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_15": 2, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_3": 1, "docs/en/features/feature-engineering.md": 14, "docs/en/features/feature-engineering.md#section_0": 1, "docs/en/features/feature-engineering.md#section_1": 1, "docs/en/features/feature-engineering.md#section_2": 2, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/features/feature-engineering.md#section_11": 1, "docs/en/features/feature-engineering.md#section_16": 8}, "transform": {"docs/en/spectral/spectral-analysis.md": 4, "docs/en/spectral/spectral-analysis.md#section_0": 1, "docs/en/spectral/spectral-analysis.md#section_2": 2, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_16": 3, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_6": 1}, "autocovariance": {"docs/en/spectral/spectral-analysis.md": 3, "docs/en/spectral/spectral-analysis.md#section_0": 1, "docs/en/spectral/spectral-analysis.md#section_2": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/ma.md": 4, "docs/en/time-domain/ma.md#section_2": 2, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_2": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/foundations/autocorrelation.md": 4, "docs/en/foundations/autocorrelation.md#section_1": 2, "docs/en/foundations/autocorrelation.md#section_16": 2, "docs/en/foundations/stationarity.md": 4, "docs/en/foundations/stationarity.md#section_0": 1, "docs/en/foundations/stationarity.md#section_1": 1, "docs/en/foundations/stationarity.md#section_2": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "insight": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_0": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "produces": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_0": 2, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "smooth": {"docs/en/spectral/spectral-analysis.md": 7, "docs/en/spectral/spectral-analysis.md#section_0": 1, "docs/en/spectral/spectral-analysis.md#section_6": 2, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_15": 3, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/features/feature-engineering.md": 5, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_16": 4, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_1": 1, "docs/en/decomposition/stl.md": 8, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_3": 3, "docs/en/decomposition/stl.md#section_5": 2, "docs/en/decomposition/stl.md#section_13": 2}, "spectrum": {"docs/en/spectral/spectral-analysis.md": 3, "docs/en/spectral/spectral-analysis.md#section_0": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "useful": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_0": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_0": 1, "docs/en/time-domain/sarima.md": 3, "docs/en/time-domain/sarima.md#section_15": 3, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_5": 1}, "detecting": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_0": 1}, "periodicities": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_0": 1}, "understanding": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_0": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1}, "cyclical": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_0": 1}, "behavior": {"docs/en/spectral/spectral-analysis.md": 4, "docs/en/spectral/spectral-analysis.md#section_0": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "process": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_1": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_9": 1, "docs/en/time-domain/arma.md": 3, "docs/en/time-domain/arma.md#section_9": 1, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/time-domain/ar.md": 3, "docs/en/time-domain/ar.md#section_14": 3, "docs/en/time-domain/arima.md": 7, "docs/en/time-domain/arima.md#section_1": 2, "docs/en/time-domain/arima.md#section_5": 1, "docs/en/time-domain/arima.md#section_11": 1, "docs/en/time-domain/arima.md#section_17": 3, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 8, "docs/en/foundations/autocorrelation.md#section_1": 1, "docs/en/foundations/autocorrelation.md#section_2": 1, "docs/en/foundations/autocorrelation.md#section_8": 2, "docs/en/foundations/autocorrelation.md#section_12": 1, "docs/en/foundations/autocorrelation.md#section_16": 3, "docs/en/foundations/stationarity.md": 10, "docs/en/foundations/stationarity.md#section_0": 1, "docs/en/foundations/stationarity.md#section_1": 3, "docs/en/foundations/stationarity.md#section_2": 1, "docs/en/foundations/stationarity.md#section_4": 1, "docs/en/foundations/stationarity.md#section_13": 4}, "omega": {"docs/en/spectral/spectral-analysis.md": 43, "docs/en/spectral/spectral-analysis.md#section_1": 7, "docs/en/spectral/spectral-analysis.md#section_2": 7, "docs/en/spectral/spectral-analysis.md#section_3": 5, "docs/en/spectral/spectral-analysis.md#section_4": 3, "docs/en/spectral/spectral-analysis.md#section_5": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_15": 18}, "represents": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_1": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1}, "frac": {"docs/en/spectral/spectral-analysis.md": 15, "docs/en/spectral/spectral-analysis.md#section_1": 2, "docs/en/spectral/spectral-analysis.md#section_2": 1, "docs/en/spectral/spectral-analysis.md#section_3": 2, "docs/en/spectral/spectral-analysis.md#section_4": 2, "docs/en/spectral/spectral-analysis.md#section_5": 2, "docs/en/spectral/spectral-analysis.md#section_6": 2, "docs/en/spectral/spectral-analysis.md#section_15": 4, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_2": 1, "docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_15": 3, "docs/en/time-domain/ma.md": 11, "docs/en/time-domain/ma.md#section_2": 1, "docs/en/time-domain/ma.md#section_3": 1, "docs/en/time-domain/ma.md#section_5": 1, "docs/en/time-domain/ma.md#section_13": 8, "docs/en/time-domain/arma.md": 4, "docs/en/time-domain/arma.md#section_1": 1, "docs/en/time-domain/arma.md#section_2": 3, "docs/en/time-domain/ar.md": 7, "docs/en/time-domain/ar.md#section_1": 1, "docs/en/time-domain/ar.md#section_2": 3, "docs/en/time-domain/ar.md#section_3": 1, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_2": 1, "docs/en/time-domain/identification.md#section_5": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 4, "docs/en/time-domain/sarima.md#section_15": 4, "docs/en/deep-learning/deep-learning-ts.md": 6, "docs/en/deep-learning/deep-learning-ts.md#section_2": 4, "docs/en/deep-learning/deep-learning-ts.md#section_4": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 5, "docs/en/exponential-smoothing/ets.md#section_4": 1, "docs/en/exponential-smoothing/ets.md#section_5": 4, "docs/en/exponential-smoothing/holt.md": 8, "docs/en/exponential-smoothing/holt.md#section_3": 1, "docs/en/exponential-smoothing/holt.md#section_4": 2, "docs/en/exponential-smoothing/holt.md#section_5": 1, "docs/en/exponential-smoothing/holt.md#section_14": 4, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_1": 2, "docs/en/exponential-smoothing/ses.md": 12, "docs/en/exponential-smoothing/ses.md#section_2": 1, "docs/en/exponential-smoothing/ses.md#section_3": 2, "docs/en/exponential-smoothing/ses.md#section_13": 9, "docs/en/forecasting/prediction-intervals.md": 5, "docs/en/forecasting/prediction-intervals.md#section_3": 1, "docs/en/forecasting/prediction-intervals.md#section_5": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/model-selection/cross-validation.md": 10, "docs/en/model-selection/cross-validation.md#section_2": 1, "docs/en/model-selection/cross-validation.md#section_3": 8, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 11, "docs/en/model-selection/information-criteria.md#section_1": 1, "docs/en/model-selection/information-criteria.md#section_3": 1, "docs/en/model-selection/information-criteria.md#section_6": 3, "docs/en/model-selection/information-criteria.md#section_13": 6, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_2": 1, "docs/en/model-selection/residual-diagnostics.md#section_3": 2, "docs/en/practical/practical-modeling.md": 7, "docs/en/practical/practical-modeling.md#section_2": 1, "docs/en/practical/practical-modeling.md#section_3": 1, "docs/en/practical/practical-modeling.md#section_4": 1, "docs/en/practical/practical-modeling.md#section_5": 1, "docs/en/practical/practical-modeling.md#section_13": 3, "docs/en/features/feature-engineering.md": 14, "docs/en/features/feature-engineering.md#section_1": 3, "docs/en/features/feature-engineering.md#section_2": 2, "docs/en/features/feature-engineering.md#section_3": 2, "docs/en/features/feature-engineering.md#section_4": 3, "docs/en/features/feature-engineering.md#section_16": 4, "docs/en/state-space/kalman-filter.md": 9, "docs/en/state-space/kalman-filter.md#section_3": 1, "docs/en/state-space/kalman-filter.md#section_13": 8, "docs/en/anomaly-detection/anomaly-detection.md": 9, "docs/en/anomaly-detection/anomaly-detection.md#section_2": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_5": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 5, "docs/en/decomposition/classical.md": 10, "docs/en/decomposition/classical.md#section_2": 2, "docs/en/decomposition/classical.md#section_3": 5, "docs/en/decomposition/classical.md#section_12": 3, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 10, "docs/en/foundations/autocorrelation.md#section_1": 3, "docs/en/foundations/autocorrelation.md#section_5": 1, "docs/en/foundations/autocorrelation.md#section_8": 2, "docs/en/foundations/autocorrelation.md#section_16": 4, "docs/en/foundations/stationarity.md": 3, "docs/en/foundations/stationarity.md#section_3": 2, "docs/en/foundations/stationarity.md#section_13": 1}, "pi": {"docs/en/spectral/spectral-analysis.md": 34, "docs/en/spectral/spectral-analysis.md#section_1": 4, "docs/en/spectral/spectral-analysis.md#section_2": 5, "docs/en/spectral/spectral-analysis.md#section_3": 3, "docs/en/spectral/spectral-analysis.md#section_4": 2, "docs/en/spectral/spectral-analysis.md#section_5": 2, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_10": 2, "docs/en/spectral/spectral-analysis.md#section_15": 14, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_4": 2, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_11": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_10": 2, "docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_5": 2, "docs/en/exponential-smoothing/ets.md#section_10": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_9": 1, "docs/en/forecasting/prediction-intervals.md": 22, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/forecasting/prediction-intervals.md#section_1": 1, "docs/en/forecasting/prediction-intervals.md#section_3": 1, "docs/en/forecasting/prediction-intervals.md#section_4": 2, "docs/en/forecasting/prediction-intervals.md#section_6": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 2, "docs/en/forecasting/prediction-intervals.md#section_11": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 12, "docs/en/practical/practical-modeling.md": 4, "docs/en/practical/practical-modeling.md#section_5": 1, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/practical/practical-modeling.md#section_11": 2, "docs/en/features/feature-engineering.md": 11, "docs/en/features/feature-engineering.md#section_1": 2, "docs/en/features/feature-engineering.md#section_2": 2, "docs/en/features/feature-engineering.md#section_5": 2, "docs/en/features/feature-engineering.md#section_10": 1, "docs/en/features/feature-engineering.md#section_11": 2, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_9": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_8": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_8": 1}, "infty": {"docs/en/spectral/spectral-analysis.md": 7, "docs/en/spectral/spectral-analysis.md#section_1": 2, "docs/en/spectral/spectral-analysis.md#section_2": 2, "docs/en/spectral/spectral-analysis.md#section_15": 3, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_4": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_4": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_4": 1, "docs/en/time-domain/ar.md": 3, "docs/en/time-domain/ar.md#section_5": 1, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/exponential-smoothing/holt.md": 3, "docs/en/exponential-smoothing/holt.md#section_4": 1, "docs/en/exponential-smoothing/holt.md#section_14": 2, "docs/en/exponential-smoothing/ses.md": 5, "docs/en/exponential-smoothing/ses.md#section_2": 1, "docs/en/exponential-smoothing/ses.md#section_13": 4, "docs/en/forecasting/prediction-intervals.md": 4, "docs/en/forecasting/prediction-intervals.md#section_3": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 3, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_13": 2, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_3": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_1": 1, "docs/en/foundations/stationarity.md#section_4": 1}, "gamma": {"docs/en/spectral/spectral-analysis.md": 7, "docs/en/spectral/spectral-analysis.md#section_1": 1, "docs/en/spectral/spectral-analysis.md#section_2": 3, "docs/en/spectral/spectral-analysis.md#section_15": 3, "docs/en/time-domain/ma.md": 9, "docs/en/time-domain/ma.md#section_2": 3, "docs/en/time-domain/ma.md#section_3": 1, "docs/en/time-domain/ma.md#section_13": 5, "docs/en/time-domain/arma.md": 17, "docs/en/time-domain/arma.md#section_2": 5, "docs/en/time-domain/arma.md#section_3": 6, "docs/en/time-domain/arma.md#section_14": 6, "docs/en/time-domain/ar.md": 28, "docs/en/time-domain/ar.md#section_2": 3, "docs/en/time-domain/ar.md#section_4": 16, "docs/en/time-domain/ar.md#section_6": 2, "docs/en/time-domain/ar.md#section_14": 7, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 5, "docs/en/time-domain/sarima.md#section_15": 5, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_4": 1, "docs/en/exponential-smoothing/holt-winters.md": 6, "docs/en/exponential-smoothing/holt-winters.md#section_1": 4, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/foundations/autocorrelation.md": 10, "docs/en/foundations/autocorrelation.md#section_1": 5, "docs/en/foundations/autocorrelation.md#section_16": 5, "docs/en/foundations/stationarity.md": 18, "docs/en/foundations/stationarity.md#section_1": 1, "docs/en/foundations/stationarity.md#section_2": 6, "docs/en/foundations/stationarity.md#section_3": 2, "docs/en/foundations/stationarity.md#section_5": 3, "docs/en/foundations/stationarity.md#section_13": 6}, "sample": {"docs/en/spectral/spectral-analysis.md": 11, "docs/en/spectral/spectral-analysis.md#section_1": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_15": 9, "docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/identification.md": 7, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/identification.md#section_15": 6, "docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_6": 2, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_6": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/model-selection/information-criteria.md": 9, "docs/en/model-selection/information-criteria.md#section_1": 1, "docs/en/model-selection/information-criteria.md#section_2": 1, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/information-criteria.md#section_13": 5, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_10": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 7, "docs/en/foundations/autocorrelation.md#section_0": 1, "docs/en/foundations/autocorrelation.md#section_1": 1, "docs/en/foundations/autocorrelation.md#section_8": 1, "docs/en/foundations/autocorrelation.md#section_9": 1, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_16": 2}, "estimate": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_1": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_0": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 3, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_1": 1, "docs/en/forecasting/prediction-intervals.md#section_6": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/state-space/kalman-filter.md": 6, "docs/en/state-space/kalman-filter.md#section_2": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/state-space/kalman-filter.md#section_11": 1, "docs/en/state-space/kalman-filter.md#section_13": 3, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "left": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_1": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_3": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_4": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_5": 2, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_5": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_3": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_3": 1, "docs/en/features/feature-engineering.md": 8, "docs/en/features/feature-engineering.md#section_1": 2, "docs/en/features/feature-engineering.md#section_2": 3, "docs/en/features/feature-engineering.md#section_16": 3, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_2": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_8": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "right": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_4": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_5": 2, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_5": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_3": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_3": 1, "docs/en/features/feature-engineering.md": 9, "docs/en/features/feature-engineering.md#section_1": 2, "docs/en/features/feature-engineering.md#section_2": 3, "docs/en/features/feature-engineering.md#section_7": 1, "docs/en/features/feature-engineering.md#section_16": 3, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_2": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_8": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "frequencies": {"docs/en/spectral/spectral-analysis.md": 10, "docs/en/spectral/spectral-analysis.md#section_1": 2, "docs/en/spectral/spectral-analysis.md#section_2": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_9": 1, "docs/en/spectral/spectral-analysis.md#section_15": 5, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_3": 1, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_4": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "ldots": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_1": 1, "docs/en/multivariate/granger-causality.md": 4, "docs/en/multivariate/granger-causality.md#section_1": 3, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_3": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_3": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_7": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_6": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_2": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_1": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 9, "docs/en/forecasting/multi-step.md#section_1": 2, "docs/en/forecasting/multi-step.md#section_2": 4, "docs/en/forecasting/multi-step.md#section_3": 1, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_2": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_2": 1, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_13": 2, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_3": 2, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_1": 1, "docs/en/foundations/autocorrelation.md#section_2": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 3, "docs/en/foundations/stationarity.md#section_1": 3}, "mean": {"docs/en/spectral/spectral-analysis.md": 6, "docs/en/spectral/spectral-analysis.md#section_1": 1, "docs/en/spectral/spectral-analysis.md#section_6": 2, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/time-domain/ma.md": 3, "docs/en/time-domain/ma.md#section_2": 1, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_1": 1, "docs/en/time-domain/arma.md#section_2": 1, "docs/en/time-domain/ar.md": 4, "docs/en/time-domain/ar.md#section_1": 1, "docs/en/time-domain/ar.md#section_2": 1, "docs/en/time-domain/ar.md#section_7": 2, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_14": 1, "docs/en/deep-learning/deep-learning-ts.md": 5, "docs/en/deep-learning/deep-learning-ts.md#section_16": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 4, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_14": 2, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_2": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 4, "docs/en/forecasting/prediction-intervals.md#section_13": 4, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/model-selection/cross-validation.md": 9, "docs/en/model-selection/cross-validation.md#section_3": 4, "docs/en/model-selection/cross-validation.md#section_7": 3, "docs/en/model-selection/cross-validation.md#section_10": 2, "docs/en/model-selection/residual-diagnostics.md": 5, "docs/en/model-selection/residual-diagnostics.md#section_6": 2, "docs/en/model-selection/residual-diagnostics.md#section_12": 3, "docs/en/practical/practical-modeling.md": 6, "docs/en/practical/practical-modeling.md#section_8": 2, "docs/en/practical/practical-modeling.md#section_10": 1, "docs/en/practical/practical-modeling.md#section_11": 1, "docs/en/practical/practical-modeling.md#section_12": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 10, "docs/en/features/feature-engineering.md#section_3": 1, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/features/feature-engineering.md#section_11": 1, "docs/en/features/feature-engineering.md#section_16": 6, "docs/en/anomaly-detection/anomaly-detection.md": 4, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_5": 2, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 14, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/change-detection/change-point.md#section_1": 1, "docs/en/change-detection/change-point.md#section_2": 2, "docs/en/change-detection/change-point.md#section_10": 3, "docs/en/change-detection/change-point.md#section_14": 7, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_1": 1, "docs/en/foundations/stationarity.md": 5, "docs/en/foundations/stationarity.md#section_0": 1, "docs/en/foundations/stationarity.md#section_1": 1, "docs/en/foundations/stationarity.md#section_7": 1, "docs/en/foundations/stationarity.md#section_13": 2}, "zero": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_1": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 5, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_1": 2, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_3": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_3": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_2": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_12": 2, "docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_13": 3, "docs/en/foundations/autocorrelation.md": 8, "docs/en/foundations/autocorrelation.md#section_9": 2, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_15": 1, "docs/en/foundations/autocorrelation.md#section_16": 4, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_6": 1}, "period": {"docs/en/spectral/spectral-analysis.md": 25, "docs/en/spectral/spectral-analysis.md#section_1": 1, "docs/en/spectral/spectral-analysis.md#section_5": 2, "docs/en/spectral/spectral-analysis.md#section_6": 2, "docs/en/spectral/spectral-analysis.md#section_7": 3, "docs/en/spectral/spectral-analysis.md#section_10": 4, "docs/en/spectral/spectral-analysis.md#section_13": 3, "docs/en/spectral/spectral-analysis.md#section_14": 2, "docs/en/spectral/spectral-analysis.md#section_15": 8, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/sarima.md": 5, "docs/en/time-domain/sarima.md#section_1": 1, "docs/en/time-domain/sarima.md#section_7": 2, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/holt-winters.md": 3, "docs/en/exponential-smoothing/holt-winters.md#section_1": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 2, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/features/feature-engineering.md": 4, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_16": 3, "docs/en/decomposition/classical.md": 10, "docs/en/decomposition/classical.md#section_0": 1, "docs/en/decomposition/classical.md#section_1": 2, "docs/en/decomposition/classical.md#section_4": 1, "docs/en/decomposition/classical.md#section_5": 1, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/classical.md#section_9": 1, "docs/en/decomposition/classical.md#section_10": 1, "docs/en/decomposition/classical.md#section_12": 2, "docs/en/decomposition/stl.md": 7, "docs/en/decomposition/stl.md#section_0": 1, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_5": 1, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/decomposition/stl.md#section_10": 1, "docs/en/decomposition/stl.md#section_13": 2, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_10": 1}, "units": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_1": 2}, "nyquist": {"docs/en/spectral/spectral-analysis.md": 6, "docs/en/spectral/spectral-analysis.md#section_1": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_15": 4, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "fastest": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_1": 1}, "observable": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_1": 1}, "cycle": {"docs/en/spectral/spectral-analysis.md": 6, "docs/en/spectral/spectral-analysis.md#section_1": 1, "docs/en/spectral/spectral-analysis.md#section_5": 1, "docs/en/spectral/spectral-analysis.md#section_15": 4, "docs/en/exponential-smoothing/holt-winters.md": 6, "docs/en/exponential-smoothing/holt-winters.md#section_3": 1, "docs/en/exponential-smoothing/holt-winters.md#section_6": 3, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/decomposition/classical.md": 4, "docs/en/decomposition/classical.md#section_1": 1, "docs/en/decomposition/classical.md#section_4": 1, "docs/en/decomposition/classical.md#section_12": 2, "docs/en/decomposition/stl.md": 4, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_3": 1, "docs/en/decomposition/stl.md#section_13": 2}, "relationship": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_2": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_15": 3, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_3": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_1": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 3, "docs/en/state-space/kalman-filter.md#section_13": 3, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "pairs": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_2": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "parseval": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_2": 1}, "relation": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_2": 1}, "text": {"docs/en/spectral/spectral-analysis.md": 5, "docs/en/spectral/spectral-analysis.md#section_2": 1, "docs/en/spectral/spectral-analysis.md#section_6": 2, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_4": 1, "docs/en/time-domain/ma.md": 9, "docs/en/time-domain/ma.md#section_2": 2, "docs/en/time-domain/ma.md#section_13": 7, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_2": 1, "docs/en/time-domain/ar.md": 10, "docs/en/time-domain/ar.md#section_2": 1, "docs/en/time-domain/ar.md#section_4": 1, "docs/en/time-domain/ar.md#section_14": 8, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 8, "docs/en/time-domain/identification.md#section_2": 5, "docs/en/time-domain/identification.md#section_15": 3, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 4, "docs/en/deep-learning/deep-learning-ts.md#section_4": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_6": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_5": 1, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_4": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 3, "docs/en/exponential-smoothing/ses.md#section_4": 1, "docs/en/exponential-smoothing/ses.md#section_5": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 8, "docs/en/forecasting/prediction-intervals.md#section_1": 1, "docs/en/forecasting/prediction-intervals.md#section_3": 2, "docs/en/forecasting/prediction-intervals.md#section_5": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 3, "docs/en/forecasting/multi-step.md": 12, "docs/en/forecasting/multi-step.md#section_4": 2, "docs/en/forecasting/multi-step.md#section_17": 10, "docs/en/model-selection/cross-validation.md": 12, "docs/en/model-selection/cross-validation.md#section_2": 1, "docs/en/model-selection/cross-validation.md#section_3": 6, "docs/en/model-selection/cross-validation.md#section_11": 5, "docs/en/model-selection/information-criteria.md": 20, "docs/en/model-selection/information-criteria.md#section_1": 4, "docs/en/model-selection/information-criteria.md#section_2": 1, "docs/en/model-selection/information-criteria.md#section_4": 2, "docs/en/model-selection/information-criteria.md#section_6": 2, "docs/en/model-selection/information-criteria.md#section_13": 11, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_1": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 6, "docs/en/practical/practical-modeling.md#section_2": 1, "docs/en/practical/practical-modeling.md#section_3": 1, "docs/en/practical/practical-modeling.md#section_4": 1, "docs/en/practical/practical-modeling.md#section_5": 1, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/features/feature-engineering.md": 4, "docs/en/features/feature-engineering.md#section_3": 2, "docs/en/features/feature-engineering.md#section_4": 2, "docs/en/state-space/kalman-filter.md": 5, "docs/en/state-space/kalman-filter.md#section_13": 5, "docs/en/anomaly-detection/anomaly-detection.md": 14, "docs/en/anomaly-detection/anomaly-detection.md#section_2": 4, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 10, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_12": 2, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_4": 1, "docs/en/foundations/autocorrelation.md": 16, "docs/en/foundations/autocorrelation.md#section_1": 3, "docs/en/foundations/autocorrelation.md#section_5": 1, "docs/en/foundations/autocorrelation.md#section_6": 1, "docs/en/foundations/autocorrelation.md#section_7": 1, "docs/en/foundations/autocorrelation.md#section_8": 3, "docs/en/foundations/autocorrelation.md#section_16": 7, "docs/en/foundations/stationarity.md": 11, "docs/en/foundations/stationarity.md#section_1": 2, "docs/en/foundations/stationarity.md#section_2": 2, "docs/en/foundations/stationarity.md#section_3": 2, "docs/en/foundations/stationarity.md#section_4": 1, "docs/en/foundations/stationarity.md#section_13": 4}, "total": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_2": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_0": 1, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_2": 1, "docs/en/interview/interview-questions.md#section_15": 1, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_12": 3, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "variance": {"docs/en/spectral/spectral-analysis.md": 6, "docs/en/spectral/spectral-analysis.md#section_2": 1, "docs/en/spectral/spectral-analysis.md#section_15": 5, "docs/en/multivariate/var.md": 5, "docs/en/multivariate/var.md#section_5": 3, "docs/en/multivariate/var.md#section_6": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 5, "docs/en/time-domain/ma.md#section_2": 1, "docs/en/time-domain/ma.md#section_13": 4, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_2": 1, "docs/en/time-domain/ar.md": 9, "docs/en/time-domain/ar.md#section_2": 1, "docs/en/time-domain/ar.md#section_14": 8, "docs/en/time-domain/arima.md": 3, "docs/en/time-domain/arima.md#section_17": 3, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 12, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/ets.md#section_13": 11, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_5": 1, "docs/en/exponential-smoothing/holt-winters.md": 3, "docs/en/exponential-smoothing/holt-winters.md#section_13": 3, "docs/en/exponential-smoothing/ses.md": 3, "docs/en/exponential-smoothing/ses.md#section_4": 1, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/forecasting/prediction-intervals.md": 12, "docs/en/forecasting/prediction-intervals.md#section_1": 1, "docs/en/forecasting/prediction-intervals.md#section_2": 2, "docs/en/forecasting/prediction-intervals.md#section_3": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 7, "docs/en/forecasting/multi-step.md": 6, "docs/en/forecasting/multi-step.md#section_17": 6, "docs/en/interview/interview-questions.md": 7, "docs/en/interview/interview-questions.md#section_1": 2, "docs/en/interview/interview-questions.md#section_16": 5, "docs/en/model-selection/cross-validation.md": 4, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 3, "docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_4": 1, "docs/en/model-selection/information-criteria.md#section_10": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 9, "docs/en/model-selection/residual-diagnostics.md#section_0": 2, "docs/en/model-selection/residual-diagnostics.md#section_6": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 4, "docs/en/model-selection/residual-diagnostics.md#section_18": 1, "docs/en/practical/practical-modeling.md": 3, "docs/en/practical/practical-modeling.md#section_13": 3, "docs/en/state-space/kalman-filter.md": 6, "docs/en/state-space/kalman-filter.md#section_1": 1, "docs/en/state-space/kalman-filter.md#section_13": 5, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/change-detection/change-point.md#section_1": 1, "docs/en/foundations/autocorrelation.md": 4, "docs/en/foundations/autocorrelation.md#section_8": 1, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_16": 2, "docs/en/foundations/stationarity.md": 11, "docs/en/foundations/stationarity.md#section_0": 1, "docs/en/foundations/stationarity.md#section_1": 1, "docs/en/foundations/stationarity.md#section_6": 2, "docs/en/foundations/stationarity.md#section_7": 2, "docs/en/foundations/stationarity.md#section_13": 5}, "across": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_2": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_7": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_3": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_5": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "phi": {"docs/en/spectral/spectral-analysis.md": 14, "docs/en/spectral/spectral-analysis.md#section_3": 6, "docs/en/spectral/spectral-analysis.md#section_15": 8, "docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_4": 3, "docs/en/time-domain/arma.md": 41, "docs/en/time-domain/arma.md#section_0": 1, "docs/en/time-domain/arma.md#section_1": 3, "docs/en/time-domain/arma.md#section_2": 12, "docs/en/time-domain/arma.md#section_4": 2, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_9": 2, "docs/en/time-domain/arma.md#section_11": 3, "docs/en/time-domain/arma.md#section_14": 17, "docs/en/time-domain/ar.md": 47, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/ar.md#section_1": 4, "docs/en/time-domain/ar.md#section_2": 9, "docs/en/time-domain/ar.md#section_4": 1, "docs/en/time-domain/ar.md#section_5": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/ar.md#section_7": 2, "docs/en/time-domain/ar.md#section_14": 28, "docs/en/time-domain/arima.md": 12, "docs/en/time-domain/arima.md#section_1": 1, "docs/en/time-domain/arima.md#section_4": 4, "docs/en/time-domain/arima.md#section_6": 2, "docs/en/time-domain/arima.md#section_11": 1, "docs/en/time-domain/arima.md#section_12": 1, "docs/en/time-domain/arima.md#section_15": 3, "docs/en/time-domain/sarima.md": 14, "docs/en/time-domain/sarima.md#section_1": 2, "docs/en/time-domain/sarima.md#section_6": 1, "docs/en/time-domain/sarima.md#section_8": 2, "docs/en/time-domain/sarima.md#section_15": 9, "docs/en/exponential-smoothing/holt.md": 24, "docs/en/exponential-smoothing/holt.md#section_1": 4, "docs/en/exponential-smoothing/holt.md#section_4": 6, "docs/en/exponential-smoothing/holt.md#section_12": 1, "docs/en/exponential-smoothing/holt.md#section_14": 13, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_5": 1, "docs/en/forecasting/prediction-intervals.md": 13, "docs/en/forecasting/prediction-intervals.md#section_3": 5, "docs/en/forecasting/prediction-intervals.md#section_9": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 6, "docs/en/forecasting/multi-step.md": 9, "docs/en/forecasting/multi-step.md#section_17": 9, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_9": 2, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_10": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/foundations/autocorrelation.md": 5, "docs/en/foundations/autocorrelation.md#section_4": 3, "docs/en/foundations/autocorrelation.md#section_16": 2, "docs/en/foundations/stationarity.md": 10, "docs/en/foundations/stationarity.md#section_13": 10}, "sigma": {"docs/en/spectral/spectral-analysis.md": 4, "docs/en/spectral/spectral-analysis.md#section_3": 2, "docs/en/spectral/spectral-analysis.md#section_4": 2, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_3": 1, "docs/en/multivariate/var.md": 5, "docs/en/multivariate/var.md#section_1": 1, "docs/en/multivariate/var.md#section_7": 1, "docs/en/multivariate/var.md#section_15": 3, "docs/en/time-domain/ma.md": 14, "docs/en/time-domain/ma.md#section_1": 1, "docs/en/time-domain/ma.md#section_2": 2, "docs/en/time-domain/ma.md#section_3": 1, "docs/en/time-domain/ma.md#section_13": 10, "docs/en/time-domain/arma.md": 3, "docs/en/time-domain/arma.md#section_2": 3, "docs/en/time-domain/ar.md": 9, "docs/en/time-domain/ar.md#section_1": 1, "docs/en/time-domain/ar.md#section_2": 2, "docs/en/time-domain/ar.md#section_14": 6, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_2": 1, "docs/en/time-domain/sarima.md": 4, "docs/en/time-domain/sarima.md#section_15": 4, "docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_1": 3, "docs/en/exponential-smoothing/ets.md": 6, "docs/en/exponential-smoothing/ets.md#section_5": 4, "docs/en/exponential-smoothing/ets.md#section_6": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_5": 1, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_4": 2, "docs/en/forecasting/prediction-intervals.md": 3, "docs/en/forecasting/prediction-intervals.md#section_9": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_4": 3, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_1": 2, "docs/en/state-space/kalman-filter.md": 6, "docs/en/state-space/kalman-filter.md#section_1": 2, "docs/en/state-space/kalman-filter.md#section_3": 4, "docs/en/anomaly-detection/anomaly-detection.md": 7, "docs/en/anomaly-detection/anomaly-detection.md#section_3": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 4, "docs/en/change-detection/change-point.md": 4, "docs/en/change-detection/change-point.md#section_13": 3, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 6, "docs/en/foundations/autocorrelation.md#section_16": 6, "docs/en/foundations/stationarity.md": 6, "docs/en/foundations/stationarity.md#section_1": 1, "docs/en/foundations/stationarity.md#section_2": 1, "docs/en/foundations/stationarity.md#section_4": 1, "docs/en/foundations/stationarity.md#section_13": 3}, "cos": {"docs/en/spectral/spectral-analysis.md": 4, "docs/en/spectral/spectral-analysis.md#section_3": 1, "docs/en/spectral/spectral-analysis.md#section_4": 1, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_3": 1, "docs/en/features/feature-engineering.md": 5, "docs/en/features/feature-engineering.md#section_1": 1, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/features/feature-engineering.md#section_11": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "properties": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_3": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_4": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_2": 1, "docs/en/forecasting/multi-step.md#section_3": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_1": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_4": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_2": 1, "docs/en/foundations/autocorrelation.md#section_17": 1, "docs/en/foundations/stationarity.md": 3, "docs/en/foundations/stationarity.md#section_2": 1, "docs/en/foundations/stationarity.md#section_3": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "peak": {"docs/en/spectral/spectral-analysis.md": 8, "docs/en/spectral/spectral-analysis.md#section_3": 2, "docs/en/spectral/spectral-analysis.md#section_5": 2, "docs/en/spectral/spectral-analysis.md#section_15": 4, "docs/en/exponential-smoothing/holt-winters.md": 5, "docs/en/exponential-smoothing/holt-winters.md#section_13": 5}, "low": {"docs/en/spectral/spectral-analysis.md": 4, "docs/en/spectral/spectral-analysis.md#section_3": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_14": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/exponential-smoothing/holt.md": 5, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/exponential-smoothing/holt.md#section_14": 4, "docs/en/exponential-smoothing/ses.md": 3, "docs/en/exponential-smoothing/ses.md#section_13": 3, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_5": 2, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_12": 1, "docs/en/state-space/kalman-filter.md": 4, "docs/en/state-space/kalman-filter.md#section_13": 4, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_4": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 7, "docs/en/decomposition/stl.md#section_3": 2, "docs/en/decomposition/stl.md#section_5": 1, "docs/en/decomposition/stl.md#section_13": 4, "docs/en/foundations/stationarity.md": 6, "docs/en/foundations/stationarity.md#section_7": 1, "docs/en/foundations/stationarity.md#section_11": 1, "docs/en/foundations/stationarity.md#section_12": 1, "docs/en/foundations/stationarity.md#section_13": 3}, "dominance": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_3": 2}, "high": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_3": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_7": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/arima.md#section_14": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 4, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/exponential-smoothing/holt.md#section_14": 3, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/ses.md": 7, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/exponential-smoothing/ses.md#section_13": 6, "docs/en/interview/interview-questions.md": 6, "docs/en/interview/interview-questions.md#section_5": 2, "docs/en/interview/interview-questions.md#section_16": 4, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 5, "docs/en/state-space/kalman-filter.md#section_13": 5, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_4": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/foundations/autocorrelation.md": 5, "docs/en/foundations/autocorrelation.md#section_10": 3, "docs/en/foundations/autocorrelation.md#section_16": 2, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_1": 1, "docs/en/foundations/stationarity.md#section_11": 1}, "theta": {"docs/en/spectral/spectral-analysis.md": 4, "docs/en/spectral/spectral-analysis.md#section_4": 4, "docs/en/time-domain/ma.md": 61, "docs/en/time-domain/ma.md#section_1": 4, "docs/en/time-domain/ma.md#section_2": 8, "docs/en/time-domain/ma.md#section_4": 7, "docs/en/time-domain/ma.md#section_5": 4, "docs/en/time-domain/ma.md#section_7": 6, "docs/en/time-domain/ma.md#section_13": 32, "docs/en/time-domain/arma.md": 35, "docs/en/time-domain/arma.md#section_0": 1, "docs/en/time-domain/arma.md#section_1": 3, "docs/en/time-domain/arma.md#section_2": 11, "docs/en/time-domain/arma.md#section_4": 2, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_9": 3, "docs/en/time-domain/arma.md#section_11": 3, "docs/en/time-domain/arma.md#section_12": 1, "docs/en/time-domain/arma.md#section_14": 10, "docs/en/time-domain/arima.md": 26, "docs/en/time-domain/arima.md#section_1": 1, "docs/en/time-domain/arima.md#section_5": 1, "docs/en/time-domain/arima.md#section_6": 2, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/arima.md#section_11": 1, "docs/en/time-domain/arima.md#section_12": 2, "docs/en/time-domain/arima.md#section_15": 3, "docs/en/time-domain/arima.md#section_17": 15, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_9": 1, "docs/en/time-domain/identification.md#section_10": 1, "docs/en/time-domain/sarima.md": 27, "docs/en/time-domain/sarima.md#section_1": 2, "docs/en/time-domain/sarima.md#section_4": 8, "docs/en/time-domain/sarima.md#section_15": 17, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_5": 2, "docs/en/exponential-smoothing/ses.md": 19, "docs/en/exponential-smoothing/ses.md#section_3": 4, "docs/en/exponential-smoothing/ses.md#section_13": 15, "docs/en/forecasting/prediction-intervals.md": 4, "docs/en/forecasting/prediction-intervals.md#section_3": 2, "docs/en/forecasting/prediction-intervals.md#section_5": 2, "docs/en/interview/interview-questions.md": 4, "docs/en/interview/interview-questions.md#section_16": 4, "docs/en/model-selection/information-criteria.md": 8, "docs/en/model-selection/information-criteria.md#section_2": 4, "docs/en/model-selection/information-criteria.md#section_3": 1, "docs/en/model-selection/information-criteria.md#section_9": 2, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_10": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/foundations/autocorrelation.md": 20, "docs/en/foundations/autocorrelation.md#section_5": 3, "docs/en/foundations/autocorrelation.md#section_16": 17}, "monthly": {"docs/en/spectral/spectral-analysis.md": 3, "docs/en/spectral/spectral-analysis.md#section_5": 1, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/sarima.md": 5, "docs/en/time-domain/sarima.md#section_0": 1, "docs/en/time-domain/sarima.md#section_1": 1, "docs/en/time-domain/sarima.md#section_6": 1, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_9": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_8": 1, "docs/en/decomposition/classical.md#section_12": 2, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "annual": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_5": 1, "docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_15": 3, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/sarima.md": 4, "docs/en/time-domain/sarima.md#section_4": 2, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_9": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/features/feature-engineering.md": 5, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/features/feature-engineering.md#section_11": 1, "docs/en/features/feature-engineering.md#section_16": 3, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_10": 1}, "12": {"docs/en/spectral/spectral-analysis.md": 9, "docs/en/spectral/spectral-analysis.md#section_5": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_10": 3, "docs/en/spectral/spectral-analysis.md#section_14": 1, "docs/en/spectral/spectral-analysis.md#section_15": 3, "docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_2": 3, "docs/en/time-domain/arima.md": 7, "docs/en/time-domain/arima.md#section_17": 7, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/sarima.md": 47, "docs/en/time-domain/sarima.md#section_0": 2, "docs/en/time-domain/sarima.md#section_1": 1, "docs/en/time-domain/sarima.md#section_2": 3, "docs/en/time-domain/sarima.md#section_3": 3, "docs/en/time-domain/sarima.md#section_4": 5, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_10": 2, "docs/en/time-domain/sarima.md#section_12": 1, "docs/en/time-domain/sarima.md#section_13": 1, "docs/en/time-domain/sarima.md#section_14": 1, "docs/en/time-domain/sarima.md#section_15": 26, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_10": 1, "docs/en/exponential-smoothing/ets.md#section_11": 1, "docs/en/exponential-smoothing/holt-winters.md": 5, "docs/en/exponential-smoothing/holt-winters.md#section_9": 1, "docs/en/exponential-smoothing/holt-winters.md#section_10": 1, "docs/en/exponential-smoothing/holt-winters.md#section_11": 1, "docs/en/exponential-smoothing/holt-winters.md#section_12": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/practical/practical-modeling.md": 3, "docs/en/practical/practical-modeling.md#section_9": 2, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/decomposition/classical.md": 12, "docs/en/decomposition/classical.md#section_2": 1, "docs/en/decomposition/classical.md#section_8": 1, "docs/en/decomposition/classical.md#section_9": 2, "docs/en/decomposition/classical.md#section_10": 1, "docs/en/decomposition/classical.md#section_12": 7, "docs/en/decomposition/stl.md": 5, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_8": 1, "docs/en/decomposition/stl.md#section_10": 1, "docs/en/decomposition/stl.md#section_13": 2, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_13": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "approx": {"docs/en/spectral/spectral-analysis.md": 3, "docs/en/spectral/spectral-analysis.md#section_5": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_5": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_5": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_5": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_2": 1, "docs/en/model-selection/information-criteria.md#section_3": 1, "docs/en/anomaly-detection/anomaly-detection.md": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 3, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/foundations/autocorrelation.md": 4, "docs/en/foundations/autocorrelation.md#section_8": 2, "docs/en/foundations/autocorrelation.md#section_16": 2}, "524": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_5": 1}, "procedure": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_14": 2, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_9": 1}, "remove": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_3": 1, "docs/en/decomposition/stl.md#section_5": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_1": 2}, "necessary": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1}, "apply": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/time-domain/arima.md": 3, "docs/en/time-domain/arima.md#section_1": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/sarima.md": 3, "docs/en/time-domain/sarima.md#section_7": 2, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "window": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/model-selection/cross-validation.md": 10, "docs/en/model-selection/cross-validation.md#section_1": 3, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 6, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_1": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/anomaly-detection/anomaly-detection.md": 8, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 4, "docs/en/anomaly-detection/anomaly-detection.md#section_12": 4, "docs/en/decomposition/stl.md": 6, "docs/en/decomposition/stl.md#section_0": 2, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_6": 2, "docs/en/decomposition/stl.md#section_13": 1}, "optional": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_6": 2}, "reduces": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "leakage": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/model-selection/cross-validation.md": 4, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/cross-validation.md#section_4": 1, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/practical/practical-modeling.md": 6, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/practical/practical-modeling.md#section_13": 4, "docs/en/features/feature-engineering.md": 7, "docs/en/features/feature-engineering.md#section_0": 1, "docs/en/features/feature-engineering.md#section_6": 1, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/features/feature-engineering.md#section_16": 4, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_5": 1, "docs/en/decomposition/stl.md#section_6": 1}, "hanning": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1}, "hamming": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1}, "blackman": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1}, "compute": {"docs/en/spectral/spectral-analysis.md": 3, "docs/en/spectral/spectral-analysis.md#section_6": 2, "docs/en/spectral/spectral-analysis.md#section_12": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_3": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_6": 1, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_6": 2, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/decomposition/classical.md": 4, "docs/en/decomposition/classical.md#section_5": 1, "docs/en/decomposition/classical.md#section_12": 3, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_4": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_9": 1, "docs/en/foundations/autocorrelation.md#section_16": 2}, "fft": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_6": 2}, "daniell": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1}, "kernel": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1}, "log": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_0": 1, "docs/en/model-selection/information-criteria.md#section_3": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/state-space/kalman-filter.md": 3, "docs/en/state-space/kalman-filter.md#section_5": 1, "docs/en/state-space/kalman-filter.md#section_13": 2, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_6": 1}, "welch": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1}, "identify": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_3": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_0": 1, "docs/en/foundations/autocorrelation.md#section_9": 1}, "compare": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/arma.md": 3, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_13": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_0": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_14": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md#section_16": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/holt.md": 3, "docs/en/exponential-smoothing/holt.md#section_12": 1, "docs/en/exponential-smoothing/holt.md#section_14": 2, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_11": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_16": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_10": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_10": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_10": 1}, "red": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1}, "white": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_6": 2, "docs/en/time-domain/ma.md": 3, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_5": 1, "docs/en/time-domain/ar.md": 4, "docs/en/time-domain/ar.md#section_1": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_2": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 11, "docs/en/time-domain/identification.md#section_1": 2, "docs/en/time-domain/identification.md#section_5": 2, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_15": 6, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_1": 1, "docs/en/model-selection/residual-diagnostics.md#section_2": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 4, "docs/en/foundations/autocorrelation.md#section_0": 1, "docs/en/foundations/autocorrelation.md#section_8": 1, "docs/en/foundations/autocorrelation.md#section_9": 2}, "noise": {"docs/en/spectral/spectral-analysis.md": 6, "docs/en/spectral/spectral-analysis.md#section_6": 2, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_10": 2, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/time-domain/ma.md": 4, "docs/en/time-domain/ma.md#section_0": 2, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_5": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 5, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/ar.md#section_1": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_2": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 11, "docs/en/time-domain/identification.md#section_1": 2, "docs/en/time-domain/identification.md#section_5": 2, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_15": 6, "docs/en/time-domain/sarima.md": 7, "docs/en/time-domain/sarima.md#section_4": 2, "docs/en/time-domain/sarima.md#section_11": 3, "docs/en/time-domain/sarima.md#section_15": 2, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_9": 1, "docs/en/exponential-smoothing/holt-winters.md": 3, "docs/en/exponential-smoothing/holt-winters.md#section_9": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 3, "docs/en/exponential-smoothing/ses.md#section_9": 1, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md": 4, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_1": 1, "docs/en/model-selection/residual-diagnostics.md#section_2": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_10": 2, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 7, "docs/en/state-space/kalman-filter.md#section_1": 2, "docs/en/state-space/kalman-filter.md#section_13": 5, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_1": 1, "docs/en/decomposition/classical.md#section_4": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 4, "docs/en/decomposition/stl.md#section_6": 2, "docs/en/decomposition/stl.md#section_8": 2, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 6, "docs/en/foundations/autocorrelation.md#section_0": 1, "docs/en/foundations/autocorrelation.md#section_8": 1, "docs/en/foundations/autocorrelation.md#section_9": 2, "docs/en/foundations/autocorrelation.md#section_16": 2}, "baseline": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_7": 1}, "test": {"docs/en/spectral/spectral-analysis.md": 4, "docs/en/spectral/spectral-analysis.md#section_6": 2, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 21, "docs/en/multivariate/granger-causality.md#section_0": 2, "docs/en/multivariate/granger-causality.md#section_1": 1, "docs/en/multivariate/granger-causality.md#section_2": 1, "docs/en/multivariate/granger-causality.md#section_3": 2, "docs/en/multivariate/granger-causality.md#section_5": 2, "docs/en/multivariate/granger-causality.md#section_6": 4, "docs/en/multivariate/granger-causality.md#section_15": 9, "docs/en/multivariate/var.md": 4, "docs/en/multivariate/var.md#section_6": 2, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 5, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/arima.md#section_14": 1, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/time-domain/identification.md": 10, "docs/en/time-domain/identification.md#section_5": 1, "docs/en/time-domain/identification.md#section_6": 4, "docs/en/time-domain/identification.md#section_11": 1, "docs/en/time-domain/identification.md#section_12": 1, "docs/en/time-domain/identification.md#section_15": 3, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_13": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_12": 1, "docs/en/deep-learning/deep-learning-ts.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_2": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_12": 1, "docs/en/forecasting/multi-step.md#section_16": 1, "docs/en/interview/interview-questions.md": 11, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/interview/interview-questions.md#section_8": 2, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/interview/interview-questions.md#section_16": 7, "docs/en/model-selection/cross-validation.md": 19, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/cross-validation.md#section_1": 3, "docs/en/model-selection/cross-validation.md#section_4": 1, "docs/en/model-selection/cross-validation.md#section_5": 5, "docs/en/model-selection/cross-validation.md#section_9": 2, "docs/en/model-selection/cross-validation.md#section_11": 7, "docs/en/model-selection/residual-diagnostics.md": 18, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_2": 1, "docs/en/model-selection/residual-diagnostics.md#section_3": 1, "docs/en/model-selection/residual-diagnostics.md#section_4": 3, "docs/en/model-selection/residual-diagnostics.md#section_5": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 9, "docs/en/practical/practical-modeling.md": 13, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/practical/practical-modeling.md#section_13": 10, "docs/en/features/feature-engineering.md": 11, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/features/feature-engineering.md#section_12": 4, "docs/en/features/feature-engineering.md#section_14": 1, "docs/en/features/feature-engineering.md#section_16": 4, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_3": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 12, "docs/en/foundations/stationarity.md#section_0": 1, "docs/en/foundations/stationarity.md#section_5": 2, "docs/en/foundations/stationarity.md#section_6": 2, "docs/en/foundations/stationarity.md#section_7": 1, "docs/en/foundations/stationarity.md#section_11": 1, "docs/en/foundations/stationarity.md#section_13": 5}, "significance": {"docs/en/spectral/spectral-analysis.md": 3, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/time-domain/identification.md#section_6": 1}, "against": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_0": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "continuum": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1}, "interpret": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "periods": {"docs/en/spectral/spectral-analysis.md": 5, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/spectral/spectral-analysis.md#section_13": 1, "docs/en/spectral/spectral-analysis.md#section_15": 3, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/time-domain/sarima.md": 3, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_3": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_5": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/features/feature-engineering.md": 5, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_10": 1, "docs/en/features/feature-engineering.md#section_16": 3, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_1": 1, "docs/en/decomposition/classical.md#section_4": 1, "docs/en/decomposition/classical.md#section_12": 1}, "decay": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/arma.md": 5, "docs/en/time-domain/arma.md#section_0": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_14": 3, "docs/en/time-domain/ar.md": 3, "docs/en/time-domain/ar.md#section_3": 1, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/identification.md": 4, "docs/en/time-domain/identification.md#section_1": 3, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 3, "docs/en/time-domain/sarima.md#section_2": 1, "docs/en/time-domain/sarima.md#section_5": 2, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/foundations/autocorrelation.md": 9, "docs/en/foundations/autocorrelation.md#section_9": 4, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_16": 3}, "like": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/time-domain/arma.md": 3, "docs/en/time-domain/arma.md#section_2": 1, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_6": 1}, "flat": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 5, "docs/en/exponential-smoothing/ses.md#section_0": 1, "docs/en/exponential-smoothing/ses.md#section_6": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/forecasting/multi-step.md#section_17": 1}, "conversion": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1}, "index": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_6": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_11": 1, "docs/en/features/feature-engineering.md": 11, "docs/en/features/feature-engineering.md#section_5": 6, "docs/en/features/feature-engineering.md#section_10": 1, "docs/en/features/feature-engineering.md#section_11": 3, "docs/en/features/feature-engineering.md#section_15": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_3": 1}, "sharp": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "true": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 6, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/multivariate/granger-causality.md#section_7": 2, "docs/en/multivariate/granger-causality.md#section_13": 2, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_11": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_12": 1, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_14": 1, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_13": 1, "docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_6": 1, "docs/en/deep-learning/deep-learning-ts.md#section_13": 1, "docs/en/deep-learning/deep-learning-ts.md#section_14": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_11": 2, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_10": 1, "docs/en/exponential-smoothing/holt.md#section_12": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_10": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_6": 1, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/model-selection/information-criteria.md": 8, "docs/en/model-selection/information-criteria.md#section_0": 1, "docs/en/model-selection/information-criteria.md#section_2": 1, "docs/en/model-selection/information-criteria.md#section_3": 1, "docs/en/model-selection/information-criteria.md#section_12": 1, "docs/en/model-selection/information-criteria.md#section_13": 4, "docs/en/model-selection/residual-diagnostics.md": 4, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md#section_10": 1, "docs/en/model-selection/residual-diagnostics.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md#section_16": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 3, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/state-space/kalman-filter.md#section_9": 1, "docs/en/state-space/kalman-filter.md#section_11": 1, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 4, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/decomposition/stl.md#section_10": 1, "docs/en/decomposition/stl.md#section_13": 2, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_11": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_8": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "appear": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_7": 2, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "spread": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1}, "out": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_2": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 4, "docs/en/deep-learning/deep-learning-ts.md#section_6": 2, "docs/en/deep-learning/deep-learning-ts.md#section_13": 2, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_4": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_2": 1, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_14": 2}, "due": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_7": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "finite": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/time-domain/ma.md": 4, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/ma.md#section_13": 3, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 4, "docs/en/foundations/stationarity.md#section_1": 2, "docs/en/foundations/stationarity.md#section_13": 2}, "windowing": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1}, "reduce": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "confusing": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "inconsistent": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_15": 1}, "doesn": {"docs/en/spectral/spectral-analysis.md": 3, "docs/en/spectral/spectral-analysis.md#section_7": 2, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_1": 1, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_7": 2, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_8": 2, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_6": 2, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "converge": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_14": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_1": 1}, "ignoring": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_7": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_7": 2, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_7": 2, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_9": 2, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_7": 2, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "aliasing": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "faster": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_5": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "wrong": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_7": 2, "docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_7": 2, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_9": 2, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md": 5, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 2, "docs/en/model-selection/residual-diagnostics.md#section_9": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_6": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/classical.md#section_10": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_6": 2, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "ensure": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_5": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "adequate": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "sampling": {"docs/en/spectral/spectral-analysis.md": 3, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/autocorrelation.md#section_17": 1}, "non": {"docs/en/spectral/spectral-analysis.md": 4, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_15": 3, "docs/en/multivariate/granger-causality.md": 4, "docs/en/multivariate/granger-causality.md#section_1": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_7": 1, "docs/en/time-domain/ma.md": 5, "docs/en/time-domain/ma.md#section_7": 3, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 4, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/arima.md": 7, "docs/en/time-domain/arima.md#section_0": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_14": 1, "docs/en/time-domain/arima.md#section_17": 4, "docs/en/time-domain/identification.md": 4, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/time-domain/identification.md#section_4": 1, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/time-domain/sarima.md": 9, "docs/en/time-domain/sarima.md#section_1": 3, "docs/en/time-domain/sarima.md#section_4": 1, "docs/en/time-domain/sarima.md#section_5": 1, "docs/en/time-domain/sarima.md#section_8": 2, "docs/en/time-domain/sarima.md#section_15": 2, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_7": 2, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 4, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/forecasting/prediction-intervals.md#section_6": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 5, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 4, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 5, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_8": 2, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_6": 2, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/foundations/autocorrelation.md": 6, "docs/en/foundations/autocorrelation.md#section_9": 2, "docs/en/foundations/autocorrelation.md#section_10": 2, "docs/en/foundations/autocorrelation.md#section_16": 2, "docs/en/foundations/stationarity.md": 10, "docs/en/foundations/stationarity.md#section_0": 1, "docs/en/foundations/stationarity.md#section_4": 1, "docs/en/foundations/stationarity.md#section_5": 1, "docs/en/foundations/stationarity.md#section_6": 1, "docs/en/foundations/stationarity.md#section_7": 1, "docs/en/foundations/stationarity.md#section_9": 1, "docs/en/foundations/stationarity.md#section_11": 1, "docs/en/foundations/stationarity.md#section_13": 3}, "assumes": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_0": 1, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "causes": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/multivariate/granger-causality.md": 12, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/multivariate/granger-causality.md#section_1": 1, "docs/en/multivariate/granger-causality.md#section_5": 1, "docs/en/multivariate/granger-causality.md#section_6": 2, "docs/en/multivariate/granger-causality.md#section_7": 2, "docs/en/multivariate/granger-causality.md#section_9": 1, "docs/en/multivariate/granger-causality.md#section_15": 4, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/sarima.md": 3, "docs/en/time-domain/sarima.md#section_8": 2, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_12": 3, "docs/en/model-selection/cross-validation.md": 4, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/cross-validation.md#section_4": 1, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 3, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_16": 2}, "blow": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1}, "up": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_3": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_15": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/state-space/kalman-filter.md": 3, "docs/en/state-space/kalman-filter.md#section_2": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "detrend": {"docs/en/spectral/spectral-analysis.md": 3, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_11": 2, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_3": 2, "docs/en/decomposition/classical.md#section_5": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_3": 1, "docs/en/decomposition/stl.md#section_5": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_6": 1}, "first": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/multivariate/granger-causality.md": 4, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/granger-causality.md#section_15": 3, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_10": 2, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/arima.md": 7, "docs/en/time-domain/arima.md#section_0": 1, "docs/en/time-domain/arima.md#section_2": 1, "docs/en/time-domain/arima.md#section_12": 1, "docs/en/time-domain/arima.md#section_17": 4, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_4": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_6": 1, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_6": 2, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_6": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_2": 1, "docs/en/decomposition/classical.md#section_9": 2, "docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_6": 2, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_14": 3, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_9": 1, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 6, "docs/en/foundations/stationarity.md#section_6": 1, "docs/en/foundations/stationarity.md#section_12": 1, "docs/en/foundations/stationarity.md#section_13": 4}, "over": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_7": 1, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 3, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/arima.md": 5, "docs/en/time-domain/arima.md#section_9": 2, "docs/en/time-domain/arima.md#section_17": 3, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md": 6, "docs/en/deep-learning/deep-learning-ts.md#section_2": 1, "docs/en/deep-learning/deep-learning-ts.md#section_5": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 3, "docs/en/exponential-smoothing/ets.md": 4, "docs/en/exponential-smoothing/ets.md#section_13": 4, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md": 6, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 5, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/model-selection/cross-validation.md": 3, "docs/en/model-selection/cross-validation.md#section_2": 1, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md": 6, "docs/en/model-selection/residual-diagnostics.md#section_5": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 2, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/practical/practical-modeling.md": 8, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/practical/practical-modeling.md#section_1": 1, "docs/en/practical/practical-modeling.md#section_3": 1, "docs/en/practical/practical-modeling.md#section_13": 5, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_1": 1, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_0": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 4, "docs/en/decomposition/classical.md#section_1": 2, "docs/en/decomposition/classical.md#section_4": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 5, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/decomposition/stl.md#section_13": 3, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_5": 1, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "interpreting": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_7": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_10": 1}, "random": {"docs/en/spectral/spectral-analysis.md": 3, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_9": 1, "docs/en/spectral/spectral-analysis.md#section_10": 1, "docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_9": 1, "docs/en/multivariate/granger-causality.md#section_10": 1, "docs/en/multivariate/granger-causality.md#section_11": 1, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_9": 2, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_9": 2, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_9": 2, "docs/en/time-domain/ar.md": 5, "docs/en/time-domain/ar.md#section_9": 2, "docs/en/time-domain/ar.md#section_14": 3, "docs/en/time-domain/arima.md": 7, "docs/en/time-domain/arima.md#section_2": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/arima.md#section_11": 2, "docs/en/time-domain/arima.md#section_17": 3, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_9": 2, "docs/en/time-domain/sarima.md": 3, "docs/en/time-domain/sarima.md#section_10": 1, "docs/en/time-domain/sarima.md#section_11": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md#section_10": 2, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_10": 2, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_9": 2, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_9": 2, "docs/en/exponential-smoothing/ses.md": 5, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/exponential-smoothing/ses.md#section_9": 2, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/forecasting/prediction-intervals.md": 7, "docs/en/forecasting/prediction-intervals.md#section_3": 1, "docs/en/forecasting/prediction-intervals.md#section_9": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 4, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_11": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_8": 2, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_9": 2, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_9": 1, "docs/en/model-selection/residual-diagnostics.md#section_10": 1, "docs/en/practical/practical-modeling.md": 8, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/practical/practical-modeling.md#section_9": 3, "docs/en/practical/practical-modeling.md#section_13": 3, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_6": 1, "docs/en/features/feature-engineering.md#section_10": 2, "docs/en/state-space/kalman-filter.md": 3, "docs/en/state-space/kalman-filter.md#section_8": 1, "docs/en/state-space/kalman-filter.md#section_9": 1, "docs/en/state-space/kalman-filter.md#section_10": 1, "docs/en/anomaly-detection/anomaly-detection.md": 5, "docs/en/anomaly-detection/anomaly-detection.md#section_4": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_9": 2, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_1": 1, "docs/en/decomposition/classical.md#section_8": 2, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_8": 2, "docs/en/change-detection/change-point.md": 5, "docs/en/change-detection/change-point.md#section_9": 1, "docs/en/change-detection/change-point.md#section_10": 3, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_12": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 8, "docs/en/foundations/stationarity.md#section_4": 1, "docs/en/foundations/stationarity.md#section_9": 3, "docs/en/foundations/stationarity.md#section_10": 1, "docs/en/foundations/stationarity.md#section_11": 1, "docs/en/foundations/stationarity.md#section_12": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "fluctuations": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_12": 2}, "create": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_3": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_11": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_11": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_10": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_11": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "spurious": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_16": 2}, "interpretation": {"docs/en/spectral/spectral-analysis.md": 3, "docs/en/spectral/spectral-analysis.md#section_7": 1, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_7": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_4": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_6": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_6": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_9": 1, "docs/en/foundations/autocorrelation.md#section_16": 2}, "import": {"docs/en/spectral/spectral-analysis.md": 3, "docs/en/spectral/spectral-analysis.md#section_8": 3, "docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_8": 3, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_8": 2, "docs/en/time-domain/ma.md": 3, "docs/en/time-domain/ma.md#section_8": 3, "docs/en/time-domain/arma.md": 3, "docs/en/time-domain/arma.md#section_8": 3, "docs/en/time-domain/ar.md": 4, "docs/en/time-domain/ar.md#section_8": 3, "docs/en/time-domain/ar.md#section_11": 1, "docs/en/time-domain/arima.md": 3, "docs/en/time-domain/arima.md#section_10": 3, "docs/en/time-domain/identification.md": 5, "docs/en/time-domain/identification.md#section_8": 5, "docs/en/time-domain/sarima.md": 5, "docs/en/time-domain/sarima.md#section_9": 4, "docs/en/time-domain/sarima.md#section_13": 1, "docs/en/deep-learning/deep-learning-ts.md": 6, "docs/en/deep-learning/deep-learning-ts.md#section_6": 2, "docs/en/deep-learning/deep-learning-ts.md#section_9": 4, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_9": 2, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_8": 2, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_8": 2, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_8": 2, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_8": 2, "docs/en/forecasting/multi-step.md": 4, "docs/en/forecasting/multi-step.md#section_10": 3, "docs/en/forecasting/multi-step.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_7": 2, "docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_8": 2, "docs/en/model-selection/information-criteria.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md": 5, "docs/en/model-selection/residual-diagnostics.md#section_8": 5, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_8": 2, "docs/en/features/feature-engineering.md": 4, "docs/en/features/feature-engineering.md#section_9": 4, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_8": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_14": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_7": 2, "docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_7": 3, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_8": 2, "docs/en/change-detection/change-point.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 5, "docs/en/foundations/autocorrelation.md#section_11": 4, "docs/en/foundations/autocorrelation.md#section_14": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_8": 2}, "numpy": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_8": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_8": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_8": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_8": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_8": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_8": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_10": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_8": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_9": 1, "docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_9": 1, "docs/en/deep-learning/deep-learning-ts.md#section_16": 2, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_9": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_8": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_8": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_8": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_8": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_10": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_7": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_8": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_8": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_8": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_9": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_8": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_7": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_7": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_8": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_11": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_8": 1}, "np": {"docs/en/spectral/spectral-analysis.md": 9, "docs/en/spectral/spectral-analysis.md#section_8": 1, "docs/en/spectral/spectral-analysis.md#section_9": 2, "docs/en/spectral/spectral-analysis.md#section_10": 5, "docs/en/spectral/spectral-analysis.md#section_13": 1, "docs/en/multivariate/granger-causality.md": 7, "docs/en/multivariate/granger-causality.md#section_8": 1, "docs/en/multivariate/granger-causality.md#section_9": 1, "docs/en/multivariate/granger-causality.md#section_10": 2, "docs/en/multivariate/granger-causality.md#section_11": 2, "docs/en/multivariate/granger-causality.md#section_12": 1, "docs/en/multivariate/var.md": 6, "docs/en/multivariate/var.md#section_8": 1, "docs/en/multivariate/var.md#section_9": 5, "docs/en/time-domain/ma.md": 5, "docs/en/time-domain/ma.md#section_8": 1, "docs/en/time-domain/ma.md#section_9": 3, "docs/en/time-domain/ma.md#section_10": 1, "docs/en/time-domain/arma.md": 6, "docs/en/time-domain/arma.md#section_8": 1, "docs/en/time-domain/arma.md#section_9": 3, "docs/en/time-domain/arma.md#section_10": 2, "docs/en/time-domain/ar.md": 5, "docs/en/time-domain/ar.md#section_8": 1, "docs/en/time-domain/ar.md#section_9": 3, "docs/en/time-domain/ar.md#section_10": 1, "docs/en/time-domain/arima.md": 6, "docs/en/time-domain/arima.md#section_10": 1, "docs/en/time-domain/arima.md#section_11": 2, "docs/en/time-domain/arima.md#section_12": 1, "docs/en/time-domain/arima.md#section_13": 1, "docs/en/time-domain/arima.md#section_14": 1, "docs/en/time-domain/identification.md": 8, "docs/en/time-domain/identification.md#section_8": 1, "docs/en/time-domain/identification.md#section_9": 2, "docs/en/time-domain/identification.md#section_10": 2, "docs/en/time-domain/identification.md#section_12": 1, "docs/en/time-domain/identification.md#section_13": 2, "docs/en/time-domain/sarima.md": 7, "docs/en/time-domain/sarima.md#section_9": 1, "docs/en/time-domain/sarima.md#section_10": 1, "docs/en/time-domain/sarima.md#section_11": 5, "docs/en/deep-learning/deep-learning-ts.md": 12, "docs/en/deep-learning/deep-learning-ts.md#section_9": 1, "docs/en/deep-learning/deep-learning-ts.md#section_10": 7, "docs/en/deep-learning/deep-learning-ts.md#section_11": 2, "docs/en/deep-learning/deep-learning-ts.md#section_16": 2, "docs/en/exponential-smoothing/ets.md": 8, "docs/en/exponential-smoothing/ets.md#section_9": 1, "docs/en/exponential-smoothing/ets.md#section_10": 5, "docs/en/exponential-smoothing/ets.md#section_11": 2, "docs/en/exponential-smoothing/holt.md": 4, "docs/en/exponential-smoothing/holt.md#section_8": 1, "docs/en/exponential-smoothing/holt.md#section_9": 3, "docs/en/exponential-smoothing/holt-winters.md": 6, "docs/en/exponential-smoothing/holt-winters.md#section_8": 1, "docs/en/exponential-smoothing/holt-winters.md#section_9": 5, "docs/en/exponential-smoothing/ses.md": 4, "docs/en/exponential-smoothing/ses.md#section_8": 1, "docs/en/exponential-smoothing/ses.md#section_9": 2, "docs/en/exponential-smoothing/ses.md#section_11": 1, "docs/en/forecasting/prediction-intervals.md": 6, "docs/en/forecasting/prediction-intervals.md#section_8": 1, "docs/en/forecasting/prediction-intervals.md#section_9": 3, "docs/en/forecasting/prediction-intervals.md#section_12": 2, "docs/en/forecasting/multi-step.md": 11, "docs/en/forecasting/multi-step.md#section_10": 3, "docs/en/forecasting/multi-step.md#section_11": 3, "docs/en/forecasting/multi-step.md#section_14": 1, "docs/en/forecasting/multi-step.md#section_16": 4, "docs/en/model-selection/cross-validation.md": 15, "docs/en/model-selection/cross-validation.md#section_5": 1, "docs/en/model-selection/cross-validation.md#section_7": 6, "docs/en/model-selection/cross-validation.md#section_8": 4, "docs/en/model-selection/cross-validation.md#section_9": 1, "docs/en/model-selection/cross-validation.md#section_10": 3, "docs/en/model-selection/information-criteria.md": 4, "docs/en/model-selection/information-criteria.md#section_8": 1, "docs/en/model-selection/information-criteria.md#section_9": 3, "docs/en/model-selection/residual-diagnostics.md": 7, "docs/en/model-selection/residual-diagnostics.md#section_8": 1, "docs/en/model-selection/residual-diagnostics.md#section_9": 1, "docs/en/model-selection/residual-diagnostics.md#section_10": 2, "docs/en/model-selection/residual-diagnostics.md#section_12": 1, "docs/en/model-selection/residual-diagnostics.md#section_15": 2, "docs/en/practical/practical-modeling.md": 15, "docs/en/practical/practical-modeling.md#section_8": 6, "docs/en/practical/practical-modeling.md#section_9": 3, "docs/en/practical/practical-modeling.md#section_10": 2, "docs/en/practical/practical-modeling.md#section_11": 2, "docs/en/practical/practical-modeling.md#section_12": 2, "docs/en/features/feature-engineering.md": 16, "docs/en/features/feature-engineering.md#section_5": 4, "docs/en/features/feature-engineering.md#section_9": 1, "docs/en/features/feature-engineering.md#section_10": 6, "docs/en/features/feature-engineering.md#section_11": 4, "docs/en/features/feature-engineering.md#section_14": 1, "docs/en/state-space/kalman-filter.md": 9, "docs/en/state-space/kalman-filter.md#section_7": 3, "docs/en/state-space/kalman-filter.md#section_8": 1, "docs/en/state-space/kalman-filter.md#section_9": 2, "docs/en/state-space/kalman-filter.md#section_10": 1, "docs/en/state-space/kalman-filter.md#section_11": 1, "docs/en/state-space/kalman-filter.md#section_12": 1, "docs/en/anomaly-detection/anomaly-detection.md": 24, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 6, "docs/en/anomaly-detection/anomaly-detection.md#section_8": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_9": 5, "docs/en/anomaly-detection/anomaly-detection.md#section_11": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_12": 6, "docs/en/anomaly-detection/anomaly-detection.md#section_13": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_14": 2, "docs/en/decomposition/classical.md": 11, "docs/en/decomposition/classical.md#section_7": 1, "docs/en/decomposition/classical.md#section_8": 5, "docs/en/decomposition/classical.md#section_9": 3, "docs/en/decomposition/classical.md#section_10": 2, "docs/en/decomposition/stl.md": 6, "docs/en/decomposition/stl.md#section_7": 1, "docs/en/decomposition/stl.md#section_8": 5, "docs/en/change-detection/change-point.md": 9, "docs/en/change-detection/change-point.md#section_8": 1, "docs/en/change-detection/change-point.md#section_9": 1, "docs/en/change-detection/change-point.md#section_10": 4, "docs/en/change-detection/change-point.md#section_13": 3, "docs/en/foundations/autocorrelation.md": 4, "docs/en/foundations/autocorrelation.md#section_11": 1, "docs/en/foundations/autocorrelation.md#section_12": 3, "docs/en/foundations/stationarity.md": 7, "docs/en/foundations/stationarity.md#section_8": 1, "docs/en/foundations/stationarity.md#section_9": 3, "docs/en/foundations/stationarity.md#section_10": 2, "docs/en/foundations/stationarity.md#section_12": 1}, "scipy": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_8": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_8": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_8": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_13": 1}, "signal": {"docs/en/spectral/spectral-analysis.md": 7, "docs/en/spectral/spectral-analysis.md#section_8": 1, "docs/en/spectral/spectral-analysis.md#section_9": 1, "docs/en/spectral/spectral-analysis.md#section_11": 1, "docs/en/spectral/spectral-analysis.md#section_12": 1, "docs/en/spectral/spectral-analysis.md#section_13": 1, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/practical/practical-modeling.md": 7, "docs/en/practical/practical-modeling.md#section_4": 1, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/practical/practical-modeling.md#section_10": 1, "docs/en/practical/practical-modeling.md#section_12": 1, "docs/en/practical/practical-modeling.md#section_13": 3, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_2": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/change-detection/change-point.md#section_15": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "matplotlib": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_8": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_7": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_11": 1}, "pyplot": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_8": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_7": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_11": 1}, "plt": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_8": 1, "docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_7": 1, "docs/en/decomposition/stl.md#section_12": 2, "docs/en/foundations/autocorrelation.md": 4, "docs/en/foundations/autocorrelation.md#section_11": 1, "docs/en/foundations/autocorrelation.md#section_13": 3}, "generate": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_9": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_9": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_9": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_9": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_9": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_9": 1, "docs/en/time-domain/arima.md": 3, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_11": 1, "docs/en/time-domain/arima.md#section_12": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_9": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_10": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_10": 1, "docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_6": 1, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/exponential-smoothing/ets.md#section_10": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_9": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_9": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_9": 1, "docs/en/forecasting/prediction-intervals.md": 5, "docs/en/forecasting/prediction-intervals.md#section_6": 4, "docs/en/forecasting/prediction-intervals.md#section_9": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_11": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_8": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_9": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_9": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_10": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_8": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_9": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_8": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_8": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_9": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_9": 1, "docs/en/foundations/stationarity.md#section_10": 1}, "known": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_9": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_5": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_2": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "seed": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_9": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_9": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_9": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_9": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_9": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_9": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_11": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_9": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_10": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_10": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_10": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_9": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_9": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_9": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_9": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_11": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_8": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_9": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_9": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_9": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_10": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_8": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_9": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_8": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_8": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_9": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_12": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_9": 1}, "42": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_9": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_9": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_9": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_9": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_9": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_9": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_11": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_9": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_10": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_10": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_10": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_9": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_9": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_9": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_9": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_11": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_8": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_9": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_9": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_9": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_10": 1, "docs/en/features/feature-engineering.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_8": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_9": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_8": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_8": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_9": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_12": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_9": 1}, "500": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_9": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_9": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_11": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "arange": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_9": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_11": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_10": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_10": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_9": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_9": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_8": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_10": 2, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_9": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_8": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_8": 1}, "50": {"docs/en/spectral/spectral-analysis.md": 4, "docs/en/spectral/spectral-analysis.md#section_10": 3, "docs/en/spectral/spectral-analysis.md#section_14": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/identification.md#section_10": 1, "docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_10": 1, "docs/en/deep-learning/deep-learning-ts.md#section_11": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/holt-winters.md": 3, "docs/en/exponential-smoothing/holt-winters.md#section_9": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_9": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 5, "docs/en/model-selection/cross-validation.md#section_11": 5, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_13": 2, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1, "docs/en/anomaly-detection/anomaly-detection.md": 5, "docs/en/anomaly-detection/anomaly-detection.md#section_9": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_10": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 3, "docs/en/decomposition/stl.md": 4, "docs/en/decomposition/stl.md#section_8": 1, "docs/en/decomposition/stl.md#section_9": 1, "docs/en/decomposition/stl.md#section_11": 2}, "01": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_10": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 3, "docs/en/exponential-smoothing/holt.md#section_14": 3, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_10": 2, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_13": 2}, "sin": {"docs/en/spectral/spectral-analysis.md": 3, "docs/en/spectral/spectral-analysis.md#section_10": 2, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_11": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_10": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_10": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_9": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_3": 1, "docs/en/features/feature-engineering.md": 6, "docs/en/features/feature-engineering.md#section_1": 1, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/features/feature-engineering.md#section_10": 1, "docs/en/features/feature-engineering.md#section_11": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_9": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_8": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_8": 1}, "randn": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_10": 1, "docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_10": 1, "docs/en/multivariate/granger-causality.md#section_11": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_9": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_9": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_9": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_9": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_11": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_9": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_11": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_10": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_10": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_9": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_9": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_9": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_9": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_11": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_8": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_9": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_10": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_9": 2, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_10": 1, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_9": 1, "docs/en/state-space/kalman-filter.md#section_10": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_9": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_8": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_8": 1, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_10": 3, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_9": 1, "docs/en/foundations/stationarity.md#section_10": 1}, "freqs": {"docs/en/spectral/spectral-analysis.md": 4, "docs/en/spectral/spectral-analysis.md#section_12": 1, "docs/en/spectral/spectral-analysis.md#section_13": 3}, "psd": {"docs/en/spectral/spectral-analysis.md": 3, "docs/en/spectral/spectral-analysis.md#section_12": 1, "docs/en/spectral/spectral-analysis.md#section_13": 2}, "fs": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_12": 1}, "find": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_13": 1, "docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_1": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "height": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_13": 1}, "percentile": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_13": 1}, "90": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_13": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_6": 2}, "print": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_13": 2, "docs/en/multivariate/granger-causality.md": 5, "docs/en/multivariate/granger-causality.md#section_13": 2, "docs/en/multivariate/granger-causality.md#section_14": 3, "docs/en/multivariate/var.md": 7, "docs/en/multivariate/var.md#section_11": 2, "docs/en/multivariate/var.md#section_12": 3, "docs/en/multivariate/var.md#section_13": 1, "docs/en/multivariate/var.md#section_14": 1, "docs/en/time-domain/ma.md": 4, "docs/en/time-domain/ma.md#section_10": 1, "docs/en/time-domain/ma.md#section_11": 2, "docs/en/time-domain/ma.md#section_12": 1, "docs/en/time-domain/arma.md": 6, "docs/en/time-domain/arma.md#section_10": 2, "docs/en/time-domain/arma.md#section_11": 2, "docs/en/time-domain/arma.md#section_12": 1, "docs/en/time-domain/arma.md#section_13": 1, "docs/en/time-domain/ar.md": 5, "docs/en/time-domain/ar.md#section_10": 1, "docs/en/time-domain/ar.md#section_11": 1, "docs/en/time-domain/ar.md#section_12": 2, "docs/en/time-domain/ar.md#section_13": 1, "docs/en/time-domain/arima.md": 6, "docs/en/time-domain/arima.md#section_14": 2, "docs/en/time-domain/arima.md#section_15": 2, "docs/en/time-domain/arima.md#section_16": 2, "docs/en/time-domain/identification.md": 11, "docs/en/time-domain/identification.md#section_11": 3, "docs/en/time-domain/identification.md#section_12": 2, "docs/en/time-domain/identification.md#section_13": 3, "docs/en/time-domain/identification.md#section_14": 3, "docs/en/time-domain/sarima.md": 4, "docs/en/time-domain/sarima.md#section_12": 1, "docs/en/time-domain/sarima.md#section_13": 2, "docs/en/time-domain/sarima.md#section_14": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md#section_16": 1, "docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_11": 2, "docs/en/exponential-smoothing/ets.md#section_12": 1, "docs/en/exponential-smoothing/holt.md": 6, "docs/en/exponential-smoothing/holt.md#section_10": 2, "docs/en/exponential-smoothing/holt.md#section_11": 2, "docs/en/exponential-smoothing/holt.md#section_12": 2, "docs/en/exponential-smoothing/holt-winters.md": 8, "docs/en/exponential-smoothing/holt-winters.md#section_11": 7, "docs/en/exponential-smoothing/holt-winters.md#section_12": 1, "docs/en/exponential-smoothing/ses.md": 4, "docs/en/exponential-smoothing/ses.md#section_10": 2, "docs/en/exponential-smoothing/ses.md#section_11": 1, "docs/en/exponential-smoothing/ses.md#section_12": 1, "docs/en/forecasting/prediction-intervals.md": 3, "docs/en/forecasting/prediction-intervals.md#section_11": 2, "docs/en/forecasting/prediction-intervals.md#section_12": 1, "docs/en/forecasting/multi-step.md": 5, "docs/en/forecasting/multi-step.md#section_16": 5, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_10": 2, "docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_11": 1, "docs/en/model-selection/information-criteria.md#section_12": 2, "docs/en/model-selection/residual-diagnostics.md": 8, "docs/en/model-selection/residual-diagnostics.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md#section_12": 1, "docs/en/model-selection/residual-diagnostics.md#section_13": 2, "docs/en/model-selection/residual-diagnostics.md#section_14": 1, "docs/en/model-selection/residual-diagnostics.md#section_15": 1, "docs/en/model-selection/residual-diagnostics.md#section_16": 2, "docs/en/practical/practical-modeling.md": 11, "docs/en/practical/practical-modeling.md#section_10": 6, "docs/en/practical/practical-modeling.md#section_11": 1, "docs/en/practical/practical-modeling.md#section_12": 4, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_14": 1, "docs/en/features/feature-engineering.md#section_15": 2, "docs/en/state-space/kalman-filter.md": 6, "docs/en/state-space/kalman-filter.md#section_11": 4, "docs/en/state-space/kalman-filter.md#section_12": 2, "docs/en/anomaly-detection/anomaly-detection.md": 5, "docs/en/anomaly-detection/anomaly-detection.md#section_11": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_12": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_14": 2, "docs/en/decomposition/classical.md": 8, "docs/en/decomposition/classical.md#section_9": 5, "docs/en/decomposition/classical.md#section_10": 3, "docs/en/decomposition/stl.md": 7, "docs/en/decomposition/stl.md#section_10": 4, "docs/en/decomposition/stl.md#section_11": 3, "docs/en/change-detection/change-point.md": 4, "docs/en/change-detection/change-point.md#section_11": 2, "docs/en/change-detection/change-point.md#section_12": 1, "docs/en/change-detection/change-point.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_14": 1, "docs/en/foundations/stationarity.md": 3, "docs/en/foundations/stationarity.md#section_11": 2, "docs/en/foundations/stationarity.md#section_12": 1}, "detected": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_13": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_12": 1, "docs/en/anomaly-detection/anomaly-detection.md": 6, "docs/en/anomaly-detection/anomaly-detection.md#section_11": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_12": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_14": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_11": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "4f": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_13": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_14": 2, "docs/en/time-domain/identification.md": 4, "docs/en/time-domain/identification.md#section_11": 2, "docs/en/time-domain/identification.md#section_12": 2, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_12": 1, "docs/en/model-selection/residual-diagnostics.md#section_14": 1, "docs/en/foundations/stationarity.md": 3, "docs/en/foundations/stationarity.md#section_11": 2, "docs/en/foundations/stationarity.md#section_12": 1}, "1f": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_13": 1, "docs/en/time-domain/arma.md": 3, "docs/en/time-domain/arma.md#section_13": 3, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_14": 2, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_11": 2, "docs/en/exponential-smoothing/holt-winters.md": 4, "docs/en/exponential-smoothing/holt-winters.md#section_11": 2, "docs/en/exponential-smoothing/holt-winters.md#section_12": 2, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_11": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_11": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_9": 2, "docs/en/decomposition/stl.md": 6, "docs/en/decomposition/stl.md#section_10": 4, "docs/en/decomposition/stl.md#section_11": 2}, "expected": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_14": 1, "docs/en/time-domain/ma.md": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_6": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_2": 1, "docs/en/model-selection/information-criteria.md#section_13": 2, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_14": 2, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "near": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_14": 1, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/arma.md": 3, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_1": 2, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 5, "docs/en/foundations/autocorrelation.md#section_9": 2, "docs/en/foundations/autocorrelation.md#section_15": 1, "docs/en/foundations/autocorrelation.md#section_16": 2, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_13": 2}, "details": {"docs/en/spectral/spectral-analysis.md": 11, "docs/en/spectral/spectral-analysis.md#section_15": 11, "docs/en/multivariate/granger-causality.md": 11, "docs/en/multivariate/granger-causality.md#section_15": 11, "docs/en/multivariate/var.md": 10, "docs/en/multivariate/var.md#section_15": 10, "docs/en/time-domain/ma.md": 10, "docs/en/time-domain/ma.md#section_13": 10, "docs/en/time-domain/arma.md": 10, "docs/en/time-domain/arma.md#section_14": 10, "docs/en/time-domain/ar.md": 10, "docs/en/time-domain/ar.md#section_14": 10, "docs/en/time-domain/arima.md": 10, "docs/en/time-domain/arima.md#section_17": 10, "docs/en/time-domain/identification.md": 10, "docs/en/time-domain/identification.md#section_15": 10, "docs/en/time-domain/sarima.md": 10, "docs/en/time-domain/sarima.md#section_15": 10, "docs/en/deep-learning/deep-learning-ts.md": 10, "docs/en/deep-learning/deep-learning-ts.md#section_17": 10, "docs/en/exponential-smoothing/ets.md": 10, "docs/en/exponential-smoothing/ets.md#section_13": 10, "docs/en/exponential-smoothing/holt.md": 10, "docs/en/exponential-smoothing/holt.md#section_14": 10, "docs/en/exponential-smoothing/holt-winters.md": 10, "docs/en/exponential-smoothing/holt-winters.md#section_13": 10, "docs/en/exponential-smoothing/ses.md": 10, "docs/en/exponential-smoothing/ses.md#section_13": 10, "docs/en/forecasting/prediction-intervals.md": 10, "docs/en/forecasting/prediction-intervals.md#section_13": 10, "docs/en/forecasting/multi-step.md": 10, "docs/en/forecasting/multi-step.md#section_17": 10, "docs/en/interview/interview-questions.md": 10, "docs/en/interview/interview-questions.md#section_16": 10, "docs/en/model-selection/cross-validation.md": 10, "docs/en/model-selection/cross-validation.md#section_11": 10, "docs/en/model-selection/information-criteria.md": 10, "docs/en/model-selection/information-criteria.md#section_13": 10, "docs/en/model-selection/residual-diagnostics.md": 10, "docs/en/model-selection/residual-diagnostics.md#section_17": 10, "docs/en/practical/practical-modeling.md": 10, "docs/en/practical/practical-modeling.md#section_13": 10, "docs/en/features/feature-engineering.md": 10, "docs/en/features/feature-engineering.md#section_16": 10, "docs/en/state-space/kalman-filter.md": 10, "docs/en/state-space/kalman-filter.md#section_13": 10, "docs/en/anomaly-detection/anomaly-detection.md": 10, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 10, "docs/en/decomposition/classical.md": 10, "docs/en/decomposition/classical.md#section_12": 10, "docs/en/decomposition/stl.md": 10, "docs/en/decomposition/stl.md#section_13": 10, "docs/en/change-detection/change-point.md": 10, "docs/en/change-detection/change-point.md#section_14": 10, "docs/en/foundations/autocorrelation.md": 10, "docs/en/foundations/autocorrelation.md#section_16": 10, "docs/en/foundations/stationarity.md": 10, "docs/en/foundations/stationarity.md#section_13": 10}, "q1": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 4, "docs/en/anomaly-detection/anomaly-detection.md#section_13": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "conceptual": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_15": 2, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_14": 2, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_13": 2, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_13": 2, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_12": 2, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_13": 2, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_14": 2, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_16": 2, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_13": 2}, "function": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_5": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_5": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/decomposition/stl.md": 5, "docs/en/decomposition/stl.md#section_2": 1, "docs/en/decomposition/stl.md#section_4": 1, "docs/en/decomposition/stl.md#section_13": 3, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_3": 1, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_1": 3, "docs/en/foundations/stationarity.md": 3, "docs/en/foundations/stationarity.md#section_2": 1, "docs/en/foundations/stationarity.md#section_3": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "answer": {"docs/en/spectral/spectral-analysis.md": 10, "docs/en/spectral/spectral-analysis.md#section_15": 10, "docs/en/multivariate/granger-causality.md": 10, "docs/en/multivariate/granger-causality.md#section_15": 10, "docs/en/multivariate/var.md": 10, "docs/en/multivariate/var.md#section_15": 10, "docs/en/time-domain/ma.md": 10, "docs/en/time-domain/ma.md#section_13": 10, "docs/en/time-domain/arma.md": 10, "docs/en/time-domain/arma.md#section_14": 10, "docs/en/time-domain/ar.md": 10, "docs/en/time-domain/ar.md#section_14": 10, "docs/en/time-domain/arima.md": 10, "docs/en/time-domain/arima.md#section_17": 10, "docs/en/time-domain/identification.md": 10, "docs/en/time-domain/identification.md#section_15": 10, "docs/en/time-domain/sarima.md": 10, "docs/en/time-domain/sarima.md#section_15": 10, "docs/en/deep-learning/deep-learning-ts.md": 10, "docs/en/deep-learning/deep-learning-ts.md#section_17": 10, "docs/en/exponential-smoothing/ets.md": 10, "docs/en/exponential-smoothing/ets.md#section_13": 10, "docs/en/exponential-smoothing/holt.md": 10, "docs/en/exponential-smoothing/holt.md#section_14": 10, "docs/en/exponential-smoothing/holt-winters.md": 10, "docs/en/exponential-smoothing/holt-winters.md#section_13": 10, "docs/en/exponential-smoothing/ses.md": 10, "docs/en/exponential-smoothing/ses.md#section_13": 10, "docs/en/forecasting/prediction-intervals.md": 10, "docs/en/forecasting/prediction-intervals.md#section_13": 10, "docs/en/forecasting/multi-step.md": 10, "docs/en/forecasting/multi-step.md#section_17": 10, "docs/en/interview/interview-questions.md": 25, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/interview/interview-questions.md#section_2": 1, "docs/en/interview/interview-questions.md#section_3": 1, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/interview/interview-questions.md#section_6": 1, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/interview/interview-questions.md#section_11": 1, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/interview/interview-questions.md#section_13": 1, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/interview/interview-questions.md#section_15": 1, "docs/en/interview/interview-questions.md#section_16": 10, "docs/en/model-selection/cross-validation.md": 10, "docs/en/model-selection/cross-validation.md#section_11": 10, "docs/en/model-selection/information-criteria.md": 10, "docs/en/model-selection/information-criteria.md#section_13": 10, "docs/en/model-selection/residual-diagnostics.md": 10, "docs/en/model-selection/residual-diagnostics.md#section_17": 10, "docs/en/practical/practical-modeling.md": 10, "docs/en/practical/practical-modeling.md#section_13": 10, "docs/en/features/feature-engineering.md": 10, "docs/en/features/feature-engineering.md#section_16": 10, "docs/en/state-space/kalman-filter.md": 10, "docs/en/state-space/kalman-filter.md#section_13": 10, "docs/en/anomaly-detection/anomaly-detection.md": 10, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 10, "docs/en/decomposition/classical.md": 10, "docs/en/decomposition/classical.md#section_12": 10, "docs/en/decomposition/stl.md": 10, "docs/en/decomposition/stl.md#section_13": 10, "docs/en/change-detection/change-point.md": 10, "docs/en/change-detection/change-point.md#section_14": 10, "docs/en/foundations/autocorrelation.md": 10, "docs/en/foundations/autocorrelation.md#section_16": 10, "docs/en/foundations/stationarity.md": 10, "docs/en/foundations/stationarity.md#section_13": 10}, "acf": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/ma.md": 16, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/ma.md#section_2": 1, "docs/en/time-domain/ma.md#section_3": 1, "docs/en/time-domain/ma.md#section_6": 3, "docs/en/time-domain/ma.md#section_7": 4, "docs/en/time-domain/ma.md#section_8": 1, "docs/en/time-domain/ma.md#section_10": 3, "docs/en/time-domain/ma.md#section_12": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 17, "docs/en/time-domain/arma.md#section_0": 1, "docs/en/time-domain/arma.md#section_2": 2, "docs/en/time-domain/arma.md#section_3": 2, "docs/en/time-domain/arma.md#section_6": 4, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_8": 1, "docs/en/time-domain/arma.md#section_10": 3, "docs/en/time-domain/arma.md#section_14": 3, "docs/en/time-domain/ar.md": 8, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/ar.md#section_2": 1, "docs/en/time-domain/ar.md#section_3": 2, "docs/en/time-domain/ar.md#section_14": 4, "docs/en/time-domain/arima.md": 10, "docs/en/time-domain/arima.md#section_8": 3, "docs/en/time-domain/arima.md#section_9": 2, "docs/en/time-domain/arima.md#section_17": 5, "docs/en/time-domain/identification.md": 26, "docs/en/time-domain/identification.md#section_0": 2, "docs/en/time-domain/identification.md#section_1": 2, "docs/en/time-domain/identification.md#section_3": 2, "docs/en/time-domain/identification.md#section_6": 3, "docs/en/time-domain/identification.md#section_7": 3, "docs/en/time-domain/identification.md#section_8": 1, "docs/en/time-domain/identification.md#section_13": 4, "docs/en/time-domain/identification.md#section_15": 9, "docs/en/time-domain/sarima.md": 10, "docs/en/time-domain/sarima.md#section_0": 1, "docs/en/time-domain/sarima.md#section_2": 1, "docs/en/time-domain/sarima.md#section_3": 1, "docs/en/time-domain/sarima.md#section_5": 3, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/time-domain/sarima.md#section_15": 3, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/interview/interview-questions.md": 7, "docs/en/interview/interview-questions.md#section_0": 1, "docs/en/interview/interview-questions.md#section_2": 3, "docs/en/interview/interview-questions.md#section_16": 3, "docs/en/model-selection/residual-diagnostics.md": 12, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 4, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md#section_15": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 4, "docs/en/foundations/autocorrelation.md": 36, "docs/en/foundations/autocorrelation.md#section_0": 4, "docs/en/foundations/autocorrelation.md#section_1": 2, "docs/en/foundations/autocorrelation.md#section_2": 1, "docs/en/foundations/autocorrelation.md#section_4": 2, "docs/en/foundations/autocorrelation.md#section_5": 2, "docs/en/foundations/autocorrelation.md#section_6": 1, "docs/en/foundations/autocorrelation.md#section_8": 1, "docs/en/foundations/autocorrelation.md#section_9": 6, "docs/en/foundations/autocorrelation.md#section_10": 2, "docs/en/foundations/autocorrelation.md#section_13": 2, "docs/en/foundations/autocorrelation.md#section_16": 12, "docs/en/foundations/stationarity.md": 5, "docs/en/foundations/stationarity.md#section_3": 1, "docs/en/foundations/stationarity.md#section_6": 2, "docs/en/foundations/stationarity.md#section_7": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "describes": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_15": 2}, "correlation": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_4": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_7": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 3, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_2": 2, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/foundations/autocorrelation.md": 5, "docs/en/foundations/autocorrelation.md#section_0": 2, "docs/en/foundations/autocorrelation.md#section_1": 1, "docs/en/foundations/autocorrelation.md#section_16": 2, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "information": {"docs/en/spectral/spectral-analysis.md": 3, "docs/en/spectral/spectral-analysis.md#section_15": 3, "docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_1": 1, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/identification.md": 5, "docs/en/time-domain/identification.md#section_0": 1, "docs/en/time-domain/identification.md#section_2": 3, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_6": 1, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_4": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/model-selection/information-criteria.md": 5, "docs/en/model-selection/information-criteria.md#section_0": 2, "docs/en/model-selection/information-criteria.md#section_1": 2, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_0": 1, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_9": 1, "docs/en/foundations/autocorrelation.md#section_16": 2}, "different": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/multivariate/granger-causality.md": 4, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_15": 3, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_14": 2, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_11": 1, "docs/en/forecasting/multi-step.md": 8, "docs/en/forecasting/multi-step.md#section_3": 1, "docs/en/forecasting/multi-step.md#section_9": 3, "docs/en/forecasting/multi-step.md#section_17": 4, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/interview/interview-questions.md#section_15": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/information-criteria.md#section_13": 2, "docs/en/practical/practical-modeling.md": 3, "docs/en/practical/practical-modeling.md#section_1": 1, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/anomaly-detection/anomaly-detection.md": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 3, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_10": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "representation": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_4": 1, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_4": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_5": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_1": 1}, "int": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_12": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_7": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "integral": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "pitfall": {"docs/en/spectral/spectral-analysis.md": 10, "docs/en/spectral/spectral-analysis.md#section_15": 10, "docs/en/multivariate/granger-causality.md": 10, "docs/en/multivariate/granger-causality.md#section_15": 10, "docs/en/multivariate/var.md": 10, "docs/en/multivariate/var.md#section_15": 10, "docs/en/time-domain/ma.md": 10, "docs/en/time-domain/ma.md#section_13": 10, "docs/en/time-domain/arma.md": 10, "docs/en/time-domain/arma.md#section_14": 10, "docs/en/time-domain/ar.md": 10, "docs/en/time-domain/ar.md#section_14": 10, "docs/en/time-domain/arima.md": 10, "docs/en/time-domain/arima.md#section_17": 10, "docs/en/time-domain/identification.md": 10, "docs/en/time-domain/identification.md#section_15": 10, "docs/en/time-domain/sarima.md": 10, "docs/en/time-domain/sarima.md#section_15": 10, "docs/en/deep-learning/deep-learning-ts.md": 10, "docs/en/deep-learning/deep-learning-ts.md#section_17": 10, "docs/en/exponential-smoothing/ets.md": 10, "docs/en/exponential-smoothing/ets.md#section_13": 10, "docs/en/exponential-smoothing/holt.md": 10, "docs/en/exponential-smoothing/holt.md#section_14": 10, "docs/en/exponential-smoothing/holt-winters.md": 10, "docs/en/exponential-smoothing/holt-winters.md#section_13": 10, "docs/en/exponential-smoothing/ses.md": 10, "docs/en/exponential-smoothing/ses.md#section_13": 10, "docs/en/forecasting/prediction-intervals.md": 10, "docs/en/forecasting/prediction-intervals.md#section_13": 10, "docs/en/forecasting/multi-step.md": 10, "docs/en/forecasting/multi-step.md#section_17": 10, "docs/en/interview/interview-questions.md": 10, "docs/en/interview/interview-questions.md#section_16": 10, "docs/en/model-selection/cross-validation.md": 10, "docs/en/model-selection/cross-validation.md#section_11": 10, "docs/en/model-selection/information-criteria.md": 10, "docs/en/model-selection/information-criteria.md#section_13": 10, "docs/en/model-selection/residual-diagnostics.md": 10, "docs/en/model-selection/residual-diagnostics.md#section_17": 10, "docs/en/practical/practical-modeling.md": 10, "docs/en/practical/practical-modeling.md#section_13": 10, "docs/en/features/feature-engineering.md": 10, "docs/en/features/feature-engineering.md#section_16": 10, "docs/en/state-space/kalman-filter.md": 10, "docs/en/state-space/kalman-filter.md#section_13": 10, "docs/en/anomaly-detection/anomaly-detection.md": 10, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 10, "docs/en/decomposition/classical.md": 10, "docs/en/decomposition/classical.md#section_12": 10, "docs/en/decomposition/stl.md": 10, "docs/en/decomposition/stl.md#section_13": 10, "docs/en/change-detection/change-point.md": 10, "docs/en/change-detection/change-point.md#section_14": 10, "docs/en/foundations/autocorrelation.md": 10, "docs/en/foundations/autocorrelation.md#section_16": 10, "docs/en/foundations/stationarity.md": 10, "docs/en/foundations/stationarity.md#section_13": 10}, "thinking": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "analyses": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "give": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_4": 1}, "re": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_12": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "equivalent": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_0": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_0": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_12": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "representations": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_4": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_13": 1}, "based": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_14": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_0": 1, "docs/en/exponential-smoothing/ets.md#section_6": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/forecasting/multi-step.md": 3, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/forecasting/multi-step.md#section_18": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 12, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_3": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_14": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1, "docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_5": 1, "docs/en/decomposition/stl.md#section_14": 2, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_14": 2}, "easier": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_4": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "problem": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/deep-learning/deep-learning-ts.md": 5, "docs/en/deep-learning/deep-learning-ts.md#section_2": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 4, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "q2": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_2": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "positive": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/time-domain/ar.md": 3, "docs/en/time-domain/ar.md#section_7": 2, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/exponential-smoothing/ets.md": 7, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/exponential-smoothing/ets.md#section_8": 2, "docs/en/exponential-smoothing/ets.md#section_13": 4, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_3": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_10": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_2": 1}, "coefficient": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_5": 1, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_1": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_6": 1, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_3": 1, "docs/en/foundations/autocorrelation.md#section_16": 2}, "creates": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_3": 1, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_6": 1, "docs/en/time-domain/sarima.md#section_15": 1}, "persistence": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1}, "values": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/time-domain/ma.md": 6, "docs/en/time-domain/ma.md#section_6": 2, "docs/en/time-domain/ma.md#section_7": 2, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/arma.md": 3, "docs/en/time-domain/arma.md#section_3": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/ar.md": 5, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/ar.md#section_14": 3, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_14": 1, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_14": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_5": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 4, "docs/en/exponential-smoothing/ets.md#section_13": 4, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_14": 2, "docs/en/exponential-smoothing/ses.md": 4, "docs/en/exponential-smoothing/ses.md#section_6": 1, "docs/en/exponential-smoothing/ses.md#section_11": 1, "docs/en/exponential-smoothing/ses.md#section_12": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 5, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/forecasting/prediction-intervals.md#section_1": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 3, "docs/en/forecasting/multi-step.md": 3, "docs/en/forecasting/multi-step.md#section_0": 1, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_7": 2, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 4, "docs/en/features/feature-engineering.md#section_1": 1, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 5, "docs/en/decomposition/classical.md#section_1": 2, "docs/en/decomposition/classical.md#section_3": 1, "docs/en/decomposition/classical.md#section_12": 2, "docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_3": 1, "docs/en/decomposition/stl.md#section_5": 2, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 6, "docs/en/foundations/autocorrelation.md#section_0": 1, "docs/en/foundations/autocorrelation.md#section_1": 1, "docs/en/foundations/autocorrelation.md#section_14": 2, "docs/en/foundations/autocorrelation.md#section_16": 2, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_7": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "tend": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "stay": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1}, "above": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/exponential-smoothing/holt-winters.md": 5, "docs/en/exponential-smoothing/holt-winters.md#section_13": 5, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "below": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1}, "extended": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_6": 2, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_3": 1}, "translates": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "slow": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_6": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_9": 1, "docs/en/foundations/autocorrelation.md#section_16": 2}, "oscillations": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_15": 2}, "explanation": {"docs/en/spectral/spectral-analysis.md": 3, "docs/en/spectral/spectral-analysis.md#section_15": 3, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_16": 2, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_13": 2}, "propto": {"docs/en/spectral/spectral-analysis.md": 3, "docs/en/spectral/spectral-analysis.md#section_15": 3, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_5": 1}, "maximum": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/ma.md": 3, "docs/en/time-domain/ma.md#section_2": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_2": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_1": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "minimum": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_5": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_7": 2, "docs/en/change-detection/change-point.md#section_14": 1}, "intuition": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_2": 1, "docs/en/model-selection/information-criteria.md#section_3": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "today": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1}, "value": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_14": 2, "docs/en/time-domain/identification.md": 5, "docs/en/time-domain/identification.md#section_11": 2, "docs/en/time-domain/identification.md#section_12": 2, "docs/en/time-domain/identification.md#section_14": 1, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_15": 2, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_14": 2, "docs/en/forecasting/prediction-intervals.md": 4, "docs/en/forecasting/prediction-intervals.md#section_1": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_2": 1, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/anomaly-detection/anomaly-detection.md": 7, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 4, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/stationarity.md": 8, "docs/en/foundations/stationarity.md#section_7": 1, "docs/en/foundations/stationarity.md#section_11": 2, "docs/en/foundations/stationarity.md#section_12": 1, "docs/en/foundations/stationarity.md#section_13": 4}, "predicts": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_0": 1, "docs/en/forecasting/multi-step.md#section_3": 1}, "tomorrow": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_15": 2}, "opposite": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "choppy": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "expecting": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "processes": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/time-domain/ma.md": 3, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 3, "docs/en/time-domain/ar.md#section_5": 1, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/forecasting/prediction-intervals.md": 5, "docs/en/forecasting/prediction-intervals.md#section_7": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 3, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_1": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "look": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_6": 1}, "similar": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_4": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_12": 2, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "sign": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_5": 1}, "magnitude": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "drastically": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "shape": {"docs/en/spectral/spectral-analysis.md": 4, "docs/en/spectral/spectral-analysis.md#section_15": 4, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_16": 2}, "q3": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_3": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 4, "docs/en/anomaly-detection/anomaly-detection.md#section_13": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "derive": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "explain": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/interview/interview-questions.md": 5, "docs/en/interview/interview-questions.md#section_2": 1, "docs/en/interview/interview-questions.md#section_6": 1, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "cannot": {"docs/en/spectral/spectral-analysis.md": 3, "docs/en/spectral/spectral-analysis.md#section_15": 3, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "rad": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "derivation": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_2": 1, "docs/en/model-selection/information-criteria.md#section_3": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_13": 2}, "observe": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "need": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arima.md": 3, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md": 4, "docs/en/deep-learning/deep-learning-ts.md#section_17": 3, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1, "docs/en/exponential-smoothing/holt.md": 4, "docs/en/exponential-smoothing/holt.md#section_14": 4, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/interview/interview-questions.md": 4, "docs/en/interview/interview-questions.md#section_4": 2, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/classical.md#section_12": 2, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_14": 2, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_16": 2}, "least": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "samples": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ar.md": 3, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/identification.md": 4, "docs/en/time-domain/identification.md#section_2": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/forecasting/prediction-intervals.md": 3, "docs/en/forecasting/prediction-intervals.md#section_5": 1, "docs/en/forecasting/prediction-intervals.md#section_6": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_6": 2, "docs/en/model-selection/information-criteria.md": 5, "docs/en/model-selection/information-criteria.md#section_0": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/information-criteria.md#section_13": 2, "docs/en/model-selection/information-criteria.md#section_14": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "per": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_15": 2, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_0": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_4": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "trough": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "detectable": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "angular": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "times": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_0": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt-winters.md": 4, "docs/en/exponential-smoothing/holt-winters.md#section_0": 1, "docs/en/exponential-smoothing/holt-winters.md#section_1": 1, "docs/en/exponential-smoothing/holt-winters.md#section_2": 2, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md": 5, "docs/en/anomaly-detection/anomaly-detection.md#section_2": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_3": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2, "docs/en/decomposition/classical.md": 7, "docs/en/decomposition/classical.md#section_0": 2, "docs/en/decomposition/classical.md#section_1": 2, "docs/en/decomposition/classical.md#section_2": 1, "docs/en/decomposition/classical.md#section_3": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/change-detection/change-point.md#section_14": 2}, "appears": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "reflected": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "consequence": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "without": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 3, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_7": 2, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "higher": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_3": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "rate": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_14": 2}, "distinguish": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "trying": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "detect": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/change-detection/change-point.md": 5, "docs/en/change-detection/change-point.md#section_1": 1, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_14": 3, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "daily": {"docs/en/spectral/spectral-analysis.md": 3, "docs/en/spectral/spectral-analysis.md#section_15": 3, "docs/en/time-domain/sarima.md": 4, "docs/en/time-domain/sarima.md#section_7": 2, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_16": 2}, "months": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_9": 1}, "anything": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_5": 1}, "bimonthly": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "q4": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "raw": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_0": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 5, "docs/en/decomposition/classical.md#section_12": 5, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_5": 1}, "estimator": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "decrease": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_1": 1}, "size": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_3": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/model-selection/cross-validation.md": 4, "docs/en/model-selection/cross-validation.md#section_1": 1, "docs/en/model-selection/cross-validation.md#section_5": 1, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/model-selection/information-criteria.md": 4, "docs/en/model-selection/information-criteria.md#section_1": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/information-criteria.md#section_13": 2, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_7": 1}, "increases": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "technical": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1}, "neq": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_1": 1, "docs/en/multivariate/granger-causality.md#section_4": 1, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_15": 3, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_1": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_1": 2}, "equals": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "squared": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_5": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_3": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_4": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_14": 2}, "relative": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_5": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "error": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_7": 2, "docs/en/multivariate/var.md": 4, "docs/en/multivariate/var.md#section_1": 1, "docs/en/multivariate/var.md#section_5": 2, "docs/en/multivariate/var.md#section_6": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 24, "docs/en/exponential-smoothing/ets.md#section_0": 2, "docs/en/exponential-smoothing/ets.md#section_1": 2, "docs/en/exponential-smoothing/ets.md#section_3": 1, "docs/en/exponential-smoothing/ets.md#section_4": 1, "docs/en/exponential-smoothing/ets.md#section_7": 2, "docs/en/exponential-smoothing/ets.md#section_8": 3, "docs/en/exponential-smoothing/ets.md#section_13": 13, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_4": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 4, "docs/en/forecasting/prediction-intervals.md#section_1": 1, "docs/en/forecasting/prediction-intervals.md#section_2": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/forecasting/multi-step.md": 14, "docs/en/forecasting/multi-step.md#section_0": 1, "docs/en/forecasting/multi-step.md#section_2": 1, "docs/en/forecasting/multi-step.md#section_3": 1, "docs/en/forecasting/multi-step.md#section_4": 6, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/forecasting/multi-step.md#section_17": 3, "docs/en/interview/interview-questions.md": 5, "docs/en/interview/interview-questions.md#section_6": 1, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/interview/interview-questions.md#section_16": 3, "docs/en/model-selection/cross-validation.md": 12, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/cross-validation.md#section_2": 1, "docs/en/model-selection/cross-validation.md#section_3": 4, "docs/en/model-selection/cross-validation.md#section_4": 1, "docs/en/model-selection/cross-validation.md#section_7": 1, "docs/en/model-selection/cross-validation.md#section_9": 2, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_2": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_2": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_14": 2, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_0": 1}, "stays": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_5": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "constant": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_15": 3, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_1": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 4, "docs/en/time-domain/ma.md#section_13": 4, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_7": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_13": 3, "docs/en/exponential-smoothing/holt.md": 3, "docs/en/exponential-smoothing/holt.md#section_4": 1, "docs/en/exponential-smoothing/holt.md#section_14": 2, "docs/en/exponential-smoothing/holt-winters.md": 9, "docs/en/exponential-smoothing/holt-winters.md#section_0": 1, "docs/en/exponential-smoothing/holt-winters.md#section_2": 3, "docs/en/exponential-smoothing/holt-winters.md#section_13": 5, "docs/en/exponential-smoothing/ses.md": 3, "docs/en/exponential-smoothing/ses.md#section_0": 1, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_1": 2, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_5": 1, "docs/en/model-selection/residual-diagnostics.md": 4, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_1": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/decomposition/classical.md": 7, "docs/en/decomposition/classical.md#section_0": 1, "docs/en/decomposition/classical.md#section_1": 1, "docs/en/decomposition/classical.md#section_12": 5, "docs/en/foundations/stationarity.md": 7, "docs/en/foundations/stationarity.md#section_0": 2, "docs/en/foundations/stationarity.md#section_1": 2, "docs/en/foundations/stationarity.md#section_7": 1, "docs/en/foundations/stationarity.md#section_13": 2}, "happens": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1}, "uses": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_0": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/forecasting/multi-step.md": 4, "docs/en/forecasting/multi-step.md#section_0": 1, "docs/en/forecasting/multi-step.md#section_2": 1, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/model-selection/cross-validation.md": 5, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/cross-validation.md#section_1": 1, "docs/en/model-selection/cross-validation.md#section_11": 3, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_16": 3, "docs/en/state-space/kalman-filter.md": 4, "docs/en/state-space/kalman-filter.md#section_5": 1, "docs/en/state-space/kalman-filter.md#section_13": 3, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "entire": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "essentially": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_6": 1}, "one": {"docs/en/spectral/spectral-analysis.md": 3, "docs/en/spectral/spectral-analysis.md#section_15": 3, "docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_5": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 4, "docs/en/exponential-smoothing/holt.md#section_0": 2, "docs/en/exponential-smoothing/holt.md#section_14": 2, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_5": 1, "docs/en/exponential-smoothing/ses.md#section_6": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_1": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/decomposition/classical.md": 7, "docs/en/decomposition/classical.md#section_1": 2, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/classical.md#section_12": 4, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_13": 2, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_3": 1}, "observation": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_6": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 12, "docs/en/state-space/kalman-filter.md#section_0": 1, "docs/en/state-space/kalman-filter.md#section_1": 4, "docs/en/state-space/kalman-filter.md#section_2": 1, "docs/en/state-space/kalman-filter.md#section_13": 6}, "still": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_14": 2, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "solution": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_2": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_14": 2, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_6": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "average": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_4": 1, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_1": 1, "docs/en/time-domain/arima.md#section_5": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 11, "docs/en/exponential-smoothing/holt-winters.md#section_6": 3, "docs/en/exponential-smoothing/holt-winters.md#section_13": 8, "docs/en/exponential-smoothing/ses.md": 3, "docs/en/exponential-smoothing/ses.md#section_0": 1, "docs/en/exponential-smoothing/ses.md#section_1": 1, "docs/en/exponential-smoothing/ses.md#section_6": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_2": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_3": 2, "docs/en/anomaly-detection/anomaly-detection.md": 4, "docs/en/anomaly-detection/anomaly-detection.md#section_4": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2, "docs/en/decomposition/classical.md": 17, "docs/en/decomposition/classical.md#section_0": 1, "docs/en/decomposition/classical.md#section_1": 1, "docs/en/decomposition/classical.md#section_2": 2, "docs/en/decomposition/classical.md#section_3": 2, "docs/en/decomposition/classical.md#section_4": 2, "docs/en/decomposition/classical.md#section_5": 2, "docs/en/decomposition/classical.md#section_12": 7, "docs/en/decomposition/stl.md": 4, "docs/en/decomposition/stl.md#section_5": 1, "docs/en/decomposition/stl.md#section_13": 3}, "nearby": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "multitaper": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "trade": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "bias": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md": 3, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/forecasting/multi-step.md": 4, "docs/en/forecasting/multi-step.md#section_17": 4, "docs/en/interview/interview-questions.md": 5, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/interview/interview-questions.md#section_16": 4, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_13": 2, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 14, "docs/en/practical/practical-modeling.md#section_3": 4, "docs/en/practical/practical-modeling.md#section_6": 3, "docs/en/practical/practical-modeling.md#section_9": 1, "docs/en/practical/practical-modeling.md#section_10": 1, "docs/en/practical/practical-modeling.md#section_12": 1, "docs/en/practical/practical-modeling.md#section_13": 4, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_12": 3}, "treating": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "definitive": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "always": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/ma.md": 5, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ma.md#section_13": 3, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 5, "docs/en/time-domain/identification.md#section_0": 1, "docs/en/time-domain/identification.md#section_7": 3, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 3, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 3, "docs/en/forecasting/multi-step.md#section_17": 3, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 3, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/information-criteria.md#section_13": 2, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/practical/practical-modeling.md": 3, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_4": 1, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/decomposition/stl.md#section_13": 2, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 3, "docs/en/foundations/stationarity.md#section_13": 3}, "q5": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "analyze": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "hourly": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/time-domain/sarima.md#section_15": 1}, "temperature": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "24": {"docs/en/spectral/spectral-analysis.md": 9, "docs/en/spectral/spectral-analysis.md#section_15": 9, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/sarima.md": 7, "docs/en/time-domain/sarima.md#section_2": 1, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/time-domain/sarima.md#section_13": 1, "docs/en/time-domain/sarima.md#section_15": 4, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_12": 2}, "hours": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_15": 2}, "unexpected": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "happening": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "harmonics": {"docs/en/spectral/spectral-analysis.md": 5, "docs/en/spectral/spectral-analysis.md#section_15": 5, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "fundamental": {"docs/en/spectral/spectral-analysis.md": 3, "docs/en/spectral/spectral-analysis.md#section_15": 3, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "pure": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 7, "docs/en/time-domain/arma.md#section_0": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/arma.md#section_13": 2, "docs/en/time-domain/arma.md#section_14": 3, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_5": 2, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "hour": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_1": 1, "docs/en/features/feature-engineering.md#section_5": 2}, "show": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_4": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_12": 1, "docs/en/foundations/autocorrelation.md": 4, "docs/en/foundations/autocorrelation.md#section_13": 1, "docs/en/foundations/autocorrelation.md#section_16": 3, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "real": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_3": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 3, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/state-space/kalman-filter.md#section_13": 2, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_14": 2, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_16": 3}, "patterns": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/arma.md": 3, "docs/en/time-domain/arma.md#section_0": 1, "docs/en/time-domain/arma.md#section_7": 2, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 4, "docs/en/time-domain/identification.md#section_0": 1, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/time-domain/identification.md#section_7": 2, "docs/en/time-domain/sarima.md": 8, "docs/en/time-domain/sarima.md#section_5": 4, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_15": 2, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/deep-learning/deep-learning-ts.md": 7, "docs/en/deep-learning/deep-learning-ts.md#section_0": 3, "docs/en/deep-learning/deep-learning-ts.md#section_7": 2, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 7, "docs/en/interview/interview-questions.md#section_5": 2, "docs/en/interview/interview-questions.md#section_13": 2, "docs/en/interview/interview-questions.md#section_16": 3, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md": 5, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 3, "docs/en/practical/practical-modeling.md#section_13": 3, "docs/en/features/feature-engineering.md": 5, "docs/en/features/feature-engineering.md#section_1": 1, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_16": 3, "docs/en/anomaly-detection/anomaly-detection.md": 5, "docs/en/anomaly-detection/anomaly-detection.md#section_3": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 3, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_12": 2, "docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/decomposition/stl.md#section_13": 2, "docs/en/foundations/autocorrelation.md": 5, "docs/en/foundations/autocorrelation.md#section_0": 1, "docs/en/foundations/autocorrelation.md#section_9": 1, "docs/en/foundations/autocorrelation.md#section_16": 3}, "aren": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "sinusoids": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "morning": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_15": 2}, "rise": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "gradual": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_7": 2, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_10": 1}, "afternoon": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "decline": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "sinusoidal": {"docs/en/spectral/spectral-analysis.md": 3, "docs/en/spectral/spectral-analysis.md#section_15": 3, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_3": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_1": 2, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_9": 2}, "shapes": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "require": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_4": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_14": 2}, "multiple": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/granger-causality.md": 4, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_0": 1, "docs/en/multivariate/var.md#section_16": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_15": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/sarima.md": 5, "docs/en/time-domain/sarima.md#section_8": 2, "docs/en/time-domain/sarima.md#section_15": 3, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/deep-learning/deep-learning-ts.md#section_5": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_7": 2, "docs/en/forecasting/multi-step.md": 4, "docs/en/forecasting/multi-step.md#section_0": 3, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/interview/interview-questions.md": 4, "docs/en/interview/interview-questions.md#section_3": 1, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/interview/interview-questions.md#section_13": 1, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_5": 1, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 4, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/practical/practical-modeling.md#section_13": 3, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2, "docs/en/change-detection/change-point.md": 5, "docs/en/change-detection/change-point.md#section_1": 1, "docs/en/change-detection/change-point.md#section_3": 1, "docs/en/change-detection/change-point.md#section_7": 2, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "represent": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_0": 1, "docs/en/state-space/kalman-filter.md#section_6": 1}, "theorem": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_5": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "any": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_0": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_2": 2, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "periodic": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_1": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_1": 1}, "sum": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/ma.md": 3, "docs/en/time-domain/ma.md#section_6": 2, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 4, "docs/en/exponential-smoothing/holt-winters.md#section_3": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/exponential-smoothing/ses.md": 5, "docs/en/exponential-smoothing/ses.md#section_2": 1, "docs/en/exponential-smoothing/ses.md#section_5": 1, "docs/en/exponential-smoothing/ses.md#section_11": 1, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/interview/interview-questions.md#section_15": 2, "docs/en/model-selection/cross-validation.md": 4, "docs/en/model-selection/cross-validation.md#section_3": 4, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_10": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/decomposition/classical.md": 4, "docs/en/decomposition/classical.md#section_1": 1, "docs/en/decomposition/classical.md#section_5": 1, "docs/en/decomposition/classical.md#section_12": 2, "docs/en/change-detection/change-point.md": 4, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/change-detection/change-point.md#section_2": 1, "docs/en/change-detection/change-point.md#section_13": 2}, "kt": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_15": 2, "docs/en/features/feature-engineering.md": 4, "docs/en/features/feature-engineering.md#section_2": 2, "docs/en/features/feature-engineering.md#section_16": 2}, "etc": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_6": 1}, "asymmetry": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "evening": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "action": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "normal": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 14, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 9, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_7": 1}, "focus": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_16": 2}, "separate": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_15": 2, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/forecasting/multi-step.md": 5, "docs/en/forecasting/multi-step.md#section_0": 1, "docs/en/forecasting/multi-step.md#section_1": 1, "docs/en/forecasting/multi-step.md#section_3": 1, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "physical": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_15": 1, "docs/en/spectral/spectral-analysis.md#section_16": 1}, "phenomena": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "artifacts": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_15": 1}, "shumway": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_16": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_14": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_17": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_17": 1}, "stoffer": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_16": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_14": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_17": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_17": 1}, "2017": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_16": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_14": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_17": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_17": 1}, "applications": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_16": 2, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_14": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_17": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_13": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_17": 1}, "springer": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_16": 2, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_16": 1, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_14": 2, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_15": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_15": 2, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_18": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_14": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_17": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_17": 2, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "chapter": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_16": 2, "docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_16": 2, "docs/en/time-domain/ma.md": 4, "docs/en/time-domain/ma.md#section_14": 4, "docs/en/time-domain/arma.md": 4, "docs/en/time-domain/arma.md#section_15": 4, "docs/en/time-domain/ar.md": 4, "docs/en/time-domain/ar.md#section_15": 4, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_18": 2, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_16": 2, "docs/en/time-domain/sarima.md": 3, "docs/en/time-domain/sarima.md#section_16": 3, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_14": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_14": 2, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_18": 2, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_14": 1, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_13": 3, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_17": 2, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "brockwell": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_16": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_15": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_18": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_17": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "davis": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_16": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_15": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_18": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_17": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "2016": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_16": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_15": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_18": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_17": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "introduction": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_16": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_16": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_14": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_15": 2, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_18": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_17": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "priestley": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_16": 1}, "1981": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_16": 1}, "academic": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_16": 1}, "press": {"docs/en/spectral/spectral-analysis.md": 2, "docs/en/spectral/spectral-analysis.md#section_16": 2, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_16": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_14": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_15": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_18": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_17": 1, "docs/en/state-space/kalman-filter.md": 3, "docs/en/state-space/kalman-filter.md#section_14": 3, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "percival": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_16": 1}, "walden": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_16": 1}, "1993": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_16": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1}, "cambridge": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_14": 1}, "university": {"docs/en/spectral/spectral-analysis.md": 1, "docs/en/spectral/spectral-analysis.md#section_16": 1, "docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_16": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_14": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_15": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_18": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_17": 1, "docs/en/state-space/kalman-filter.md": 3, "docs/en/state-space/kalman-filter.md#section_14": 3, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "tests": {"docs/en/multivariate/granger-causality.md": 8, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/multivariate/granger-causality.md#section_4": 2, "docs/en/multivariate/granger-causality.md#section_6": 2, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_13": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_6": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_0": 1, "docs/en/time-domain/identification.md#section_11": 1, "docs/en/interview/interview-questions.md": 8, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/interview/interview-questions.md#section_12": 2, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/interview/interview-questions.md#section_16": 4, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_2": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1, "docs/en/foundations/stationarity.md": 3, "docs/en/foundations/stationarity.md#section_0": 1, "docs/en/foundations/stationarity.md#section_5": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "whether": {"docs/en/multivariate/granger-causality.md": 5, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/multivariate/granger-causality.md#section_4": 1, "docs/en/multivariate/granger-causality.md#section_5": 1, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_2": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "past": {"docs/en/multivariate/granger-causality.md": 5, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/multivariate/granger-causality.md#section_1": 3, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/ar.md": 3, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_4": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_0": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_12": 2, "docs/en/model-selection/cross-validation.md": 4, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/cross-validation.md#section_1": 2, "docs/en/model-selection/cross-validation.md#section_4": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_4": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_1": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "help": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_13": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "predict": {"docs/en/multivariate/granger-causality.md": 4, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/multivariate/granger-causality.md#section_4": 1, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/multi-step.md": 6, "docs/en/forecasting/multi-step.md#section_1": 2, "docs/en/forecasting/multi-step.md#section_8": 1, "docs/en/forecasting/multi-step.md#section_14": 1, "docs/en/forecasting/multi-step.md#section_15": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_14": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_11": 1, "docs/en/change-detection/change-point.md#section_12": 1}, "beyond": {"docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/multivariate/granger-causality.md#section_1": 1, "docs/en/multivariate/granger-causality.md#section_4": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_3": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "own": {"docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/multivariate/granger-causality.md#section_1": 1, "docs/en/multivariate/granger-causality.md#section_11": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_0": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_12": 1}, "history": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "about": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/multivariate/granger-causality.md#section_1": 1, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "predictive": {"docs/en/multivariate/granger-causality.md": 7, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/multivariate/granger-causality.md#section_1": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_15": 4, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "precedence": {"docs/en/multivariate/granger-causality.md": 4, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/multivariate/granger-causality.md#section_4": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "causation": {"docs/en/multivariate/granger-causality.md": 5, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_15": 3, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_7": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_12": 1}, "via": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_0": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_0": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/sarima.md": 3, "docs/en/time-domain/sarima.md#section_15": 3, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_0": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_0": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1, "docs/en/decomposition/classical.md": 4, "docs/en/decomposition/classical.md#section_0": 2, "docs/en/decomposition/classical.md#section_2": 1, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_4": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_3": 1}, "coefficients": {"docs/en/multivariate/granger-causality.md": 7, "docs/en/multivariate/granger-causality.md#section_0": 2, "docs/en/multivariate/granger-causality.md#section_3": 1, "docs/en/multivariate/granger-causality.md#section_6": 3, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 4, "docs/en/multivariate/var.md#section_2": 1, "docs/en/multivariate/var.md#section_15": 3, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_4": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_2": 1, "docs/en/forecasting/prediction-intervals.md#section_6": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "lagged": {"docs/en/multivariate/granger-causality.md": 6, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/multivariate/granger-causality.md#section_3": 1, "docs/en/multivariate/granger-causality.md#section_4": 1, "docs/en/multivariate/granger-causality.md#section_6": 2, "docs/en/multivariate/granger-causality.md#section_11": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_0": 1}, "equation": {"docs/en/multivariate/granger-causality.md": 4, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/multivariate/granger-causality.md#section_3": 1, "docs/en/multivariate/granger-causality.md#section_6": 2, "docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_0": 2, "docs/en/multivariate/var.md#section_6": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 5, "docs/en/exponential-smoothing/holt.md#section_1": 3, "docs/en/exponential-smoothing/holt.md#section_14": 2, "docs/en/exponential-smoothing/holt-winters.md": 3, "docs/en/exponential-smoothing/holt-winters.md#section_13": 3, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_1": 2, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "jointly": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_2": 1}, "significant": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ar.md": 4, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/ar.md#section_14": 3, "docs/en/time-domain/arima.md": 4, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/arima.md#section_17": 3, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_3": 1, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_2": 1, "docs/en/time-domain/sarima.md#section_3": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 5, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md#section_15": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_12": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_3": 1, "docs/en/foundations/autocorrelation.md": 8, "docs/en/foundations/autocorrelation.md#section_9": 1, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_14": 1, "docs/en/foundations/autocorrelation.md#section_15": 1, "docs/en/foundations/autocorrelation.md#section_16": 4}, "bidirectional": {"docs/en/multivariate/granger-causality.md": 5, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/granger-causality.md#section_15": 3}, "possible": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_4": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "sensitive": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/anomaly-detection/anomaly-detection.md": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_3": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_0": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "omitted": {"docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_12": 1}, "variables": {"docs/en/multivariate/granger-causality.md": 4, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/multivariate/granger-causality.md#section_5": 1, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/multivariate/var.md": 15, "docs/en/multivariate/var.md#section_1": 1, "docs/en/multivariate/var.md#section_2": 1, "docs/en/multivariate/var.md#section_7": 4, "docs/en/multivariate/var.md#section_15": 9, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_3": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "lag": {"docs/en/multivariate/granger-causality.md": 11, "docs/en/multivariate/granger-causality.md#section_0": 1, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/granger-causality.md#section_7": 2, "docs/en/multivariate/granger-causality.md#section_11": 1, "docs/en/multivariate/granger-causality.md#section_15": 6, "docs/en/multivariate/var.md": 10, "docs/en/multivariate/var.md#section_4": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/multivariate/var.md#section_11": 2, "docs/en/multivariate/var.md#section_13": 1, "docs/en/multivariate/var.md#section_15": 5, "docs/en/time-domain/ma.md": 9, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/ma.md#section_1": 1, "docs/en/time-domain/ma.md#section_2": 2, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ma.md#section_10": 1, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/arma.md": 3, "docs/en/time-domain/arma.md#section_1": 1, "docs/en/time-domain/arma.md#section_2": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 11, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/ar.md#section_1": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/ar.md#section_10": 1, "docs/en/time-domain/ar.md#section_14": 7, "docs/en/time-domain/arima.md": 7, "docs/en/time-domain/arima.md#section_6": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/arima.md#section_17": 5, "docs/en/time-domain/identification.md": 11, "docs/en/time-domain/identification.md#section_1": 3, "docs/en/time-domain/identification.md#section_6": 2, "docs/en/time-domain/identification.md#section_15": 6, "docs/en/time-domain/sarima.md": 11, "docs/en/time-domain/sarima.md#section_0": 1, "docs/en/time-domain/sarima.md#section_3": 1, "docs/en/time-domain/sarima.md#section_4": 1, "docs/en/time-domain/sarima.md#section_5": 2, "docs/en/time-domain/sarima.md#section_6": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_15": 4, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/interview/interview-questions.md": 5, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/interview/interview-questions.md#section_2": 2, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md": 10, "docs/en/model-selection/residual-diagnostics.md#section_6": 3, "docs/en/model-selection/residual-diagnostics.md#section_15": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 6, "docs/en/features/feature-engineering.md": 12, "docs/en/features/feature-engineering.md#section_1": 1, "docs/en/features/feature-engineering.md#section_5": 4, "docs/en/features/feature-engineering.md#section_8": 2, "docs/en/features/feature-engineering.md#section_11": 3, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_14": 1, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_4": 1, "docs/en/decomposition/classical.md#section_12": 2, "docs/en/foundations/autocorrelation.md": 31, "docs/en/foundations/autocorrelation.md#section_0": 3, "docs/en/foundations/autocorrelation.md#section_3": 1, "docs/en/foundations/autocorrelation.md#section_5": 1, "docs/en/foundations/autocorrelation.md#section_9": 5, "docs/en/foundations/autocorrelation.md#section_10": 4, "docs/en/foundations/autocorrelation.md#section_16": 16, "docs/en/foundations/stationarity.md": 4, "docs/en/foundations/stationarity.md#section_0": 1, "docs/en/foundations/stationarity.md#section_1": 1, "docs/en/foundations/stationarity.md#section_4": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "provides": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_1": 1, "docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_0": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_11": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "cause": {"docs/en/multivariate/granger-causality.md": 6, "docs/en/multivariate/granger-causality.md#section_1": 2, "docs/en/multivariate/granger-causality.md#section_13": 2, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_13": 2}, "knowing": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_1": 1}, "improve": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_1": 1, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "bivariate": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_1": 1, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_2": 1, "docs/en/multivariate/var.md#section_9": 1, "docs/en/multivariate/var.md#section_15": 1}, "cdots": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_1": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 4, "docs/en/multivariate/var.md#section_0": 1, "docs/en/multivariate/var.md#section_1": 2, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 6, "docs/en/time-domain/ma.md#section_1": 3, "docs/en/time-domain/ma.md#section_4": 1, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/arma.md": 6, "docs/en/time-domain/arma.md#section_1": 5, "docs/en/time-domain/arma.md#section_3": 1, "docs/en/time-domain/ar.md": 8, "docs/en/time-domain/ar.md#section_1": 4, "docs/en/time-domain/ar.md#section_4": 4, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_6": 2, "docs/en/time-domain/sarima.md": 4, "docs/en/time-domain/sarima.md#section_1": 4, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_1": 1, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_4": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_2": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_4": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_1": 1, "docs/en/foundations/autocorrelation.md": 4, "docs/en/foundations/autocorrelation.md#section_3": 4}, "restricted": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_2": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_7": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "unrestricted": {"docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_2": 1, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_7": 1}, "including": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_2": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_0": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_2": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "statistic": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_2": 1, "docs/en/multivariate/granger-causality.md#section_3": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_4": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 2}, "2p": {"docs/en/multivariate/granger-causality.md": 6, "docs/en/multivariate/granger-causality.md#section_2": 2, "docs/en/multivariate/granger-causality.md#section_15": 4}, "under": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_2": 1, "docs/en/multivariate/granger-causality.md#section_3": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/time-domain/identification.md#section_5": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_4": 1, "docs/en/forecasting/multi-step.md": 5, "docs/en/forecasting/multi-step.md#section_5": 2, "docs/en/forecasting/multi-step.md#section_17": 3, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md": 5, "docs/en/model-selection/residual-diagnostics.md#section_2": 1, "docs/en/model-selection/residual-diagnostics.md#section_3": 1, "docs/en/model-selection/residual-diagnostics.md#section_4": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_3": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/change-detection/change-point.md": 6, "docs/en/change-detection/change-point.md#section_2": 2, "docs/en/change-detection/change-point.md#section_13": 1, "docs/en/change-detection/change-point.md#section_14": 3, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_0": 1, "docs/en/foundations/autocorrelation.md#section_8": 1}, "sim": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_2": 1, "docs/en/multivariate/granger-causality.md#section_3": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_1": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_1": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_1": 1, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/time-domain/identification.md#section_5": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_4": 1, "docs/en/model-selection/residual-diagnostics.md": 5, "docs/en/model-selection/residual-diagnostics.md#section_2": 1, "docs/en/model-selection/residual-diagnostics.md#section_3": 1, "docs/en/model-selection/residual-diagnostics.md#section_4": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/state-space/kalman-filter.md": 4, "docs/en/state-space/kalman-filter.md#section_1": 4, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_1": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "wald": {"docs/en/multivariate/granger-causality.md": 4, "docs/en/multivariate/granger-causality.md#section_3": 2, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "mathbf": {"docs/en/multivariate/granger-causality.md": 9, "docs/en/multivariate/granger-causality.md#section_3": 9, "docs/en/multivariate/var.md": 50, "docs/en/multivariate/var.md#section_0": 6, "docs/en/multivariate/var.md#section_1": 19, "docs/en/multivariate/var.md#section_3": 7, "docs/en/multivariate/var.md#section_4": 1, "docs/en/multivariate/var.md#section_15": 17, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_4": 1, "docs/en/exponential-smoothing/ets.md": 8, "docs/en/exponential-smoothing/ets.md#section_2": 6, "docs/en/exponential-smoothing/ets.md#section_5": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_5": 1, "docs/en/state-space/kalman-filter.md": 54, "docs/en/state-space/kalman-filter.md#section_1": 12, "docs/en/state-space/kalman-filter.md#section_2": 29, "docs/en/state-space/kalman-filter.md#section_3": 1, "docs/en/state-space/kalman-filter.md#section_4": 1, "docs/en/state-space/kalman-filter.md#section_5": 9, "docs/en/state-space/kalman-filter.md#section_6": 2}, "boldsymbol": {"docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_3": 3, "docs/en/multivariate/var.md": 19, "docs/en/multivariate/var.md#section_0": 1, "docs/en/multivariate/var.md#section_1": 4, "docs/en/multivariate/var.md#section_3": 3, "docs/en/multivariate/var.md#section_4": 5, "docs/en/multivariate/var.md#section_7": 1, "docs/en/multivariate/var.md#section_15": 5, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_4": 2, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_5": 2}, "beta": {"docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_3": 3, "docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_3": 2, "docs/en/exponential-smoothing/ets.md#section_4": 1, "docs/en/exponential-smoothing/holt.md": 7, "docs/en/exponential-smoothing/holt.md#section_1": 3, "docs/en/exponential-smoothing/holt.md#section_3": 1, "docs/en/exponential-smoothing/holt.md#section_5": 2, "docs/en/exponential-smoothing/holt.md#section_10": 1, "docs/en/exponential-smoothing/holt-winters.md": 4, "docs/en/exponential-smoothing/holt-winters.md#section_1": 4, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_2": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_4": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_5": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "selects": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_3": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_6": 1, "docs/en/model-selection/information-criteria.md": 5, "docs/en/model-selection/information-criteria.md#section_3": 1, "docs/en/model-selection/information-criteria.md#section_13": 4}, "hat": {"docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_3": 3, "docs/en/time-domain/ma.md": 6, "docs/en/time-domain/ma.md#section_13": 6, "docs/en/time-domain/ar.md": 5, "docs/en/time-domain/ar.md#section_6": 2, "docs/en/time-domain/ar.md#section_14": 3, "docs/en/time-domain/arima.md": 14, "docs/en/time-domain/arima.md#section_7": 1, "docs/en/time-domain/arima.md#section_17": 13, "docs/en/time-domain/identification.md": 13, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/time-domain/identification.md#section_2": 4, "docs/en/time-domain/identification.md#section_5": 1, "docs/en/time-domain/identification.md#section_15": 7, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md#section_5": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/holt.md": 8, "docs/en/exponential-smoothing/holt.md#section_1": 2, "docs/en/exponential-smoothing/holt.md#section_2": 1, "docs/en/exponential-smoothing/holt.md#section_4": 2, "docs/en/exponential-smoothing/holt.md#section_5": 1, "docs/en/exponential-smoothing/holt.md#section_14": 2, "docs/en/exponential-smoothing/holt-winters.md": 9, "docs/en/exponential-smoothing/holt-winters.md#section_0": 2, "docs/en/exponential-smoothing/holt-winters.md#section_1": 2, "docs/en/exponential-smoothing/holt-winters.md#section_5": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 4, "docs/en/exponential-smoothing/ses.md": 24, "docs/en/exponential-smoothing/ses.md#section_0": 2, "docs/en/exponential-smoothing/ses.md#section_1": 4, "docs/en/exponential-smoothing/ses.md#section_2": 1, "docs/en/exponential-smoothing/ses.md#section_3": 3, "docs/en/exponential-smoothing/ses.md#section_4": 2, "docs/en/exponential-smoothing/ses.md#section_5": 1, "docs/en/exponential-smoothing/ses.md#section_13": 11, "docs/en/forecasting/prediction-intervals.md": 11, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/forecasting/prediction-intervals.md#section_1": 4, "docs/en/forecasting/prediction-intervals.md#section_4": 3, "docs/en/forecasting/prediction-intervals.md#section_5": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/forecasting/multi-step.md": 21, "docs/en/forecasting/multi-step.md#section_2": 7, "docs/en/forecasting/multi-step.md#section_3": 1, "docs/en/forecasting/multi-step.md#section_9": 2, "docs/en/forecasting/multi-step.md#section_17": 11, "docs/en/interview/interview-questions.md": 5, "docs/en/interview/interview-questions.md#section_16": 5, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_2": 1, "docs/en/model-selection/cross-validation.md#section_3": 1, "docs/en/model-selection/information-criteria.md": 12, "docs/en/model-selection/information-criteria.md#section_1": 3, "docs/en/model-selection/information-criteria.md#section_2": 4, "docs/en/model-selection/information-criteria.md#section_3": 1, "docs/en/model-selection/information-criteria.md#section_4": 3, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 6, "docs/en/model-selection/residual-diagnostics.md#section_1": 2, "docs/en/model-selection/residual-diagnostics.md#section_2": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 3, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_2": 1, "docs/en/practical/practical-modeling.md#section_3": 1, "docs/en/state-space/kalman-filter.md": 24, "docs/en/state-space/kalman-filter.md#section_2": 7, "docs/en/state-space/kalman-filter.md#section_3": 5, "docs/en/state-space/kalman-filter.md#section_5": 4, "docs/en/state-space/kalman-filter.md#section_13": 8, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_3": 2, "docs/en/decomposition/stl.md": 4, "docs/en/decomposition/stl.md#section_13": 4, "docs/en/foundations/autocorrelation.md": 11, "docs/en/foundations/autocorrelation.md#section_1": 7, "docs/en/foundations/autocorrelation.md#section_8": 2, "docs/en/foundations/autocorrelation.md#section_16": 2}, "chi": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_3": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_5": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/model-selection/residual-diagnostics.md": 6, "docs/en/model-selection/residual-diagnostics.md#section_2": 1, "docs/en/model-selection/residual-diagnostics.md#section_3": 1, "docs/en/model-selection/residual-diagnostics.md#section_4": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 3}, "instantaneous": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_4": 1}, "current": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_4": 2, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_0": 2, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_2": 2, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "helps": {"docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_4": 1, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1}, "effects": {"docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_4": 1, "docs/en/multivariate/granger-causality.md#section_7": 2, "docs/en/multivariate/var.md": 4, "docs/en/multivariate/var.md#section_15": 4, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/sarima.md": 3, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/interview/interview-questions.md": 4, "docs/en/interview/interview-questions.md#section_2": 2, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/interview/interview-questions.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_0": 1, "docs/en/foundations/autocorrelation.md#section_16": 2, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "cov": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_4": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_1": 1, "docs/en/foundations/autocorrelation.md": 5, "docs/en/foundations/autocorrelation.md#section_1": 1, "docs/en/foundations/autocorrelation.md#section_16": 4, "docs/en/foundations/stationarity.md": 3, "docs/en/foundations/stationarity.md#section_1": 1, "docs/en/foundations/stationarity.md#section_2": 1, "docs/en/foundations/stationarity.md#section_3": 1}, "yt": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_4": 1}, "xt": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_4": 1}, "contemporaneous": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_4": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_7": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_12": 1}, "temporal": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_4": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/deep-learning/deep-learning-ts.md": 4, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md#section_4": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/interview/interview-questions.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 4, "docs/en/model-selection/cross-validation.md#section_0": 2, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/practical/practical-modeling.md": 6, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/practical/practical-modeling.md#section_13": 3, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_7": 1, "docs/en/features/feature-engineering.md#section_12": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "block": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_5": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_5": 2}, "exogeneity": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_5": 1}, "system": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_5": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "group": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_5": 2, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_5": 2}, "another": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_5": 1}, "joint": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_5": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_1": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "relevant": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_5": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_4": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "matrices": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_5": 1, "docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_1": 1, "docs/en/multivariate/var.md#section_4": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_3": 1, "docs/en/state-space/kalman-filter.md#section_6": 1}, "determine": {"docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_6": 2, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_7": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "difference": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/arma.md": 5, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/arma.md#section_14": 4, "docs/en/time-domain/arima.md": 10, "docs/en/time-domain/arima.md#section_0": 2, "docs/en/time-domain/arima.md#section_1": 2, "docs/en/time-domain/arima.md#section_2": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/arima.md#section_17": 3, "docs/en/time-domain/identification.md": 4, "docs/en/time-domain/identification.md#section_6": 2, "docs/en/time-domain/identification.md#section_12": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 3, "docs/en/time-domain/sarima.md#section_1": 1, "docs/en/time-domain/sarima.md#section_4": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_2": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_13": 2, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/foundations/autocorrelation.md": 4, "docs/en/foundations/autocorrelation.md#section_9": 2, "docs/en/foundations/autocorrelation.md#section_16": 2, "docs/en/foundations/stationarity.md": 15, "docs/en/foundations/stationarity.md#section_1": 2, "docs/en/foundations/stationarity.md#section_6": 4, "docs/en/foundations/stationarity.md#section_7": 1, "docs/en/foundations/stationarity.md#section_12": 1, "docs/en/foundations/stationarity.md#section_13": 7}, "toda": {"docs/en/multivariate/granger-causality.md": 5, "docs/en/multivariate/granger-causality.md#section_6": 2, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/multivariate/granger-causality.md#section_16": 1}, "yamamoto": {"docs/en/multivariate/granger-causality.md": 5, "docs/en/multivariate/granger-causality.md#section_6": 2, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/multivariate/granger-causality.md#section_16": 1}, "approach": {"docs/en/multivariate/granger-causality.md": 5, "docs/en/multivariate/granger-causality.md#section_6": 2, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_4": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/multi-step.md": 3, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/forecasting/multi-step.md#section_18": 1, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_15": 1, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 4, "docs/en/state-space/kalman-filter.md#section_13": 3, "docs/en/state-space/kalman-filter.md#section_14": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1, "docs/en/change-detection/change-point.md": 1}, "standard": {"docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/arma.md": 3, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/time-domain/ar.md": 4, "docs/en/time-domain/ar.md#section_7": 4, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/sarima.md": 3, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_15": 2, "docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_17": 3, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_2": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_0": 1, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 6, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/cross-validation.md#section_4": 2, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_17": 3, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_3": 1, "docs/en/anomaly-detection/anomaly-detection.md": 5, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 5, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_0": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "gc": {"docs/en/multivariate/granger-causality.md": 10, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/granger-causality.md#section_7": 2, "docs/en/multivariate/granger-causality.md#section_15": 7, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_12": 1}, "select": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_0": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/multivariate/var.md#section_11": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/ar.md#section_11": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_0": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_14": 2}, "optimal": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/holt.md": 3, "docs/en/exponential-smoothing/holt.md#section_10": 2, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 7, "docs/en/exponential-smoothing/ses.md#section_3": 1, "docs/en/exponential-smoothing/ses.md#section_5": 1, "docs/en/exponential-smoothing/ses.md#section_10": 1, "docs/en/exponential-smoothing/ses.md#section_13": 4, "docs/en/forecasting/multi-step.md": 6, "docs/en/forecasting/multi-step.md#section_5": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/forecasting/multi-step.md#section_17": 4, "docs/en/interview/interview-questions.md": 4, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/interview/interview-questions.md#section_11": 1, "docs/en/interview/interview-questions.md#section_15": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/state-space/kalman-filter.md": 3, "docs/en/state-space/kalman-filter.md#section_0": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/change-detection/change-point.md": 8, "docs/en/change-detection/change-point.md#section_4": 1, "docs/en/change-detection/change-point.md#section_6": 1, "docs/en/change-detection/change-point.md#section_14": 5, "docs/en/change-detection/change-point.md#section_15": 1}, "order": {"docs/en/multivariate/granger-causality.md": 4, "docs/en/multivariate/granger-causality.md#section_6": 2, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/multivariate/var.md": 5, "docs/en/multivariate/var.md#section_0": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/multivariate/var.md#section_11": 1, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ma.md": 4, "docs/en/time-domain/ma.md#section_6": 2, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ma.md#section_11": 1, "docs/en/time-domain/arma.md": 5, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/arma.md#section_11": 1, "docs/en/time-domain/arma.md#section_13": 2, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 4, "docs/en/time-domain/ar.md#section_6": 2, "docs/en/time-domain/ar.md#section_11": 2, "docs/en/time-domain/arima.md": 8, "docs/en/time-domain/arima.md#section_1": 4, "docs/en/time-domain/arima.md#section_15": 1, "docs/en/time-domain/arima.md#section_17": 3, "docs/en/time-domain/identification.md": 10, "docs/en/time-domain/identification.md#section_1": 3, "docs/en/time-domain/identification.md#section_3": 3, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_14": 3, "docs/en/time-domain/sarima.md": 5, "docs/en/time-domain/sarima.md#section_1": 4, "docs/en/time-domain/sarima.md#section_12": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_4": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_10": 1, "docs/en/model-selection/cross-validation.md": 7, "docs/en/model-selection/cross-validation.md#section_0": 2, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_9": 3, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 4, "docs/en/model-selection/information-criteria.md#section_10": 4, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md#section_16": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_16": 2, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_1": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "consistency": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_3": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_13": 2}, "perform": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1}, "h0": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_6": 2, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_6": 2}, "reject": {"docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_6": 3, "docs/en/time-domain/identification.md": 9, "docs/en/time-domain/identification.md#section_4": 8, "docs/en/time-domain/identification.md#section_5": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_2": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/foundations/stationarity.md": 3, "docs/en/foundations/stationarity.md#section_6": 2, "docs/en/foundations/stationarity.md#section_13": 1}, "reverse": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "direction": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1}, "rejects": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_6": 2, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/foundations/stationarity.md": 4, "docs/en/foundations/stationarity.md#section_7": 1, "docs/en/foundations/stationarity.md#section_13": 3}, "unidirectional": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_6": 1}, "neither": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_6": 1}, "integration": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/arima.md": 3, "docs/en/time-domain/arima.md#section_1": 1, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_4": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "fit": {"docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/granger-causality.md#section_14": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 6, "docs/en/multivariate/var.md#section_6": 1, "docs/en/multivariate/var.md#section_10": 1, "docs/en/multivariate/var.md#section_12": 2, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ma.md": 5, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/ma.md#section_11": 2, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/arma.md": 8, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/arma.md#section_11": 2, "docs/en/time-domain/arma.md#section_13": 2, "docs/en/time-domain/arma.md#section_14": 3, "docs/en/time-domain/ar.md": 5, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/ar.md#section_11": 1, "docs/en/time-domain/ar.md#section_12": 2, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 5, "docs/en/time-domain/arima.md#section_0": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_15": 2, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 4, "docs/en/time-domain/identification.md#section_3": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_14": 1, "docs/en/time-domain/identification.md#section_16": 1, "docs/en/time-domain/sarima.md": 5, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/time-domain/sarima.md#section_12": 2, "docs/en/time-domain/sarima.md#section_15": 2, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_11": 2, "docs/en/exponential-smoothing/holt.md": 8, "docs/en/exponential-smoothing/holt.md#section_10": 5, "docs/en/exponential-smoothing/holt.md#section_11": 1, "docs/en/exponential-smoothing/holt.md#section_12": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 5, "docs/en/exponential-smoothing/holt-winters.md#section_10": 2, "docs/en/exponential-smoothing/holt-winters.md#section_11": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 7, "docs/en/exponential-smoothing/ses.md#section_10": 5, "docs/en/exponential-smoothing/ses.md#section_11": 1, "docs/en/exponential-smoothing/ses.md#section_12": 1, "docs/en/forecasting/prediction-intervals.md": 4, "docs/en/forecasting/prediction-intervals.md#section_6": 1, "docs/en/forecasting/prediction-intervals.md#section_10": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 4, "docs/en/forecasting/multi-step.md#section_7": 1, "docs/en/forecasting/multi-step.md#section_13": 1, "docs/en/forecasting/multi-step.md#section_14": 1, "docs/en/forecasting/multi-step.md#section_15": 1, "docs/en/interview/interview-questions.md": 4, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/interview/interview-questions.md#section_16": 3, "docs/en/model-selection/cross-validation.md": 3, "docs/en/model-selection/cross-validation.md#section_5": 1, "docs/en/model-selection/cross-validation.md#section_9": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 5, "docs/en/model-selection/information-criteria.md#section_0": 1, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/information-criteria.md#section_10": 2, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 6, "docs/en/model-selection/residual-diagnostics.md#section_9": 1, "docs/en/model-selection/residual-diagnostics.md#section_11": 2, "docs/en/model-selection/residual-diagnostics.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_6": 2, "docs/en/features/feature-engineering.md": 9, "docs/en/features/feature-engineering.md#section_0": 1, "docs/en/features/feature-engineering.md#section_4": 1, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/features/feature-engineering.md#section_13": 2, "docs/en/features/feature-engineering.md#section_16": 4, "docs/en/anomaly-detection/anomaly-detection.md": 4, "docs/en/anomaly-detection/anomaly-detection.md#section_3": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_14": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 8, "docs/en/decomposition/stl.md#section_2": 1, "docs/en/decomposition/stl.md#section_10": 1, "docs/en/decomposition/stl.md#section_13": 6, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_11": 1, "docs/en/change-detection/change-point.md#section_12": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_9": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "lags": {"docs/en/multivariate/granger-causality.md": 18, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/granger-causality.md#section_7": 2, "docs/en/multivariate/granger-causality.md#section_15": 15, "docs/en/multivariate/var.md": 4, "docs/en/multivariate/var.md#section_0": 2, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_3": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 4, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/ar.md#section_12": 1, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/arima.md": 4, "docs/en/time-domain/arima.md#section_1": 2, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 7, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/identification.md#section_14": 1, "docs/en/time-domain/identification.md#section_15": 5, "docs/en/time-domain/sarima.md": 17, "docs/en/time-domain/sarima.md#section_0": 1, "docs/en/time-domain/sarima.md#section_2": 1, "docs/en/time-domain/sarima.md#section_5": 5, "docs/en/time-domain/sarima.md#section_7": 3, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_13": 3, "docs/en/time-domain/sarima.md#section_15": 3, "docs/en/forecasting/multi-step.md": 6, "docs/en/forecasting/multi-step.md#section_7": 1, "docs/en/forecasting/multi-step.md#section_10": 3, "docs/en/forecasting/multi-step.md#section_13": 1, "docs/en/forecasting/multi-step.md#section_14": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_2": 1, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/model-selection/residual-diagnostics.md": 9, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 4, "docs/en/model-selection/residual-diagnostics.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/features/feature-engineering.md": 8, "docs/en/features/feature-engineering.md#section_0": 1, "docs/en/features/feature-engineering.md#section_5": 2, "docs/en/features/feature-engineering.md#section_8": 2, "docs/en/features/feature-engineering.md#section_11": 1, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 17, "docs/en/foundations/autocorrelation.md#section_0": 1, "docs/en/foundations/autocorrelation.md#section_10": 3, "docs/en/foundations/autocorrelation.md#section_13": 2, "docs/en/foundations/autocorrelation.md#section_15": 1, "docs/en/foundations/autocorrelation.md#section_16": 10}, "avoids": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/forecasting/multi-step.md": 3, "docs/en/forecasting/multi-step.md#section_0": 1, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_9": 1}, "issues": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "pretesting": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "unit": {"docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_1": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 4, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/ma.md#section_1": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 5, "docs/en/time-domain/arma.md#section_1": 2, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/time-domain/ar.md": 6, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/ar.md#section_1": 1, "docs/en/time-domain/ar.md#section_14": 4, "docs/en/time-domain/arima.md": 4, "docs/en/time-domain/arima.md#section_4": 1, "docs/en/time-domain/arima.md#section_6": 3, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_0": 1, "docs/en/time-domain/identification.md#section_4": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_0": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/interview/interview-questions.md": 10, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/interview/interview-questions.md#section_3": 1, "docs/en/interview/interview-questions.md#section_16": 8, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_16": 2, "docs/en/foundations/stationarity.md": 18, "docs/en/foundations/stationarity.md#section_1": 1, "docs/en/foundations/stationarity.md#section_4": 3, "docs/en/foundations/stationarity.md#section_5": 4, "docs/en/foundations/stationarity.md#section_6": 1, "docs/en/foundations/stationarity.md#section_7": 4, "docs/en/foundations/stationarity.md#section_13": 4, "docs/en/foundations/stationarity.md#section_14": 1}, "roots": {"docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_6": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/ma.md": 3, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/ma.md#section_1": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 6, "docs/en/time-domain/arma.md#section_1": 2, "docs/en/time-domain/arma.md#section_5": 1, "docs/en/time-domain/arma.md#section_7": 2, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 7, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/ar.md#section_1": 1, "docs/en/time-domain/ar.md#section_3": 3, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/arima.md": 3, "docs/en/time-domain/arima.md#section_6": 3, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_0": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_7": 2}, "causal": {"docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_7": 1, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_4": 2, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1}, "mechanism": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "arise": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_7": 1}, "variable": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/var.md": 10, "docs/en/multivariate/var.md#section_0": 1, "docs/en/multivariate/var.md#section_4": 2, "docs/en/multivariate/var.md#section_5": 2, "docs/en/multivariate/var.md#section_15": 5, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_12": 1}, "miss": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/anomaly-detection/anomaly-detection.md": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_7": 1}, "many": {"docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_7": 2, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_7": 1, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 4, "docs/en/deep-learning/deep-learning-ts.md#section_2": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_5": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_8": 2, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_12": 2, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "lose": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_6": 1}, "introduce": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_7": 1}, "distribution": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_6": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_6": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_1": 1, "docs/en/change-detection/change-point.md#section_5": 1, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/foundations/stationarity.md": 4, "docs/en/foundations/stationarity.md#section_1": 1, "docs/en/foundations/stationarity.md#section_13": 2, "docs/en/foundations/stationarity.md#section_14": 1}, "augmented": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_5": 1}, "correction": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_15": 3, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_13": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "testing": {"docs/en/multivariate/granger-causality.md": 6, "docs/en/multivariate/granger-causality.md#section_7": 2, "docs/en/multivariate/granger-causality.md#section_15": 4, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_4": 1, "docs/en/model-selection/cross-validation.md": 3, "docs/en/model-selection/cross-validation.md#section_1": 1, "docs/en/model-selection/cross-validation.md#section_6": 2, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 4, "docs/en/practical/practical-modeling.md#section_13": 4, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_7": 2, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_6": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "inflates": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_7": 1}, "type": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_5": 1, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "adjust": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_14": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "together": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "within": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_1": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "won": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_7": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "statsmodels": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_8": 2, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_8": 1, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_8": 2, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_8": 2, "docs/en/time-domain/ar.md": 3, "docs/en/time-domain/ar.md#section_8": 2, "docs/en/time-domain/ar.md#section_11": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_10": 2, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_8": 3, "docs/en/time-domain/sarima.md": 3, "docs/en/time-domain/sarima.md#section_9": 2, "docs/en/time-domain/sarima.md#section_13": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_9": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_8": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_8": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_8": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_8": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_7": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_8": 1, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_8": 3, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_14": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_7": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_7": 1, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_11": 2, "docs/en/foundations/autocorrelation.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_8": 1}, "tsa": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_8": 2, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_8": 1, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_8": 2, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_8": 2, "docs/en/time-domain/ar.md": 3, "docs/en/time-domain/ar.md#section_8": 2, "docs/en/time-domain/ar.md#section_11": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_10": 2, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_8": 2, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_9": 2, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_9": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_8": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_8": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_8": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_8": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_7": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_8": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_8": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_14": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_7": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_7": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_11": 1, "docs/en/foundations/autocorrelation.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_8": 1}, "stattools": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_8": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_8": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_8": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_8": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_10": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_8": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_8": 1}, "grangercausalitytests": {"docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_8": 1, "docs/en/multivariate/granger-causality.md#section_13": 2}, "api": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_8": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_8": 1}, "vice": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_9": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_6": 1}, "versa": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_9": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_6": 1}, "200": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_9": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_9": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_10": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_9": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_3": 1, "docs/en/model-selection/cross-validation.md#section_8": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_9": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_9": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_9": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_11": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/stationarity.md": 3, "docs/en/foundations/stationarity.md#section_9": 1, "docs/en/foundations/stationarity.md#section_10": 2}, "independent": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_10": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "zeros": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_10": 1, "docs/en/multivariate/granger-causality.md#section_11": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_9": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_9": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_9": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_9": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_12": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_10": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_9": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_11": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_9": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_10": 1, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_7": 2, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_10": 1}, "range": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_10": 1, "docs/en/multivariate/granger-causality.md#section_11": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_9": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_9": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_9": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_9": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_12": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_10": 1, "docs/en/deep-learning/deep-learning-ts.md": 8, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md#section_11": 1, "docs/en/deep-learning/deep-learning-ts.md#section_14": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 4, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_12": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_6": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 7, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/forecasting/prediction-intervals.md#section_1": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_9": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 3, "docs/en/forecasting/multi-step.md": 4, "docs/en/forecasting/multi-step.md#section_10": 1, "docs/en/forecasting/multi-step.md#section_11": 1, "docs/en/forecasting/multi-step.md#section_14": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 3, "docs/en/model-selection/cross-validation.md#section_5": 1, "docs/en/model-selection/cross-validation.md#section_9": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_9": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md#section_10": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_12": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_10": 2, "docs/en/change-detection/change-point.md": 4, "docs/en/change-detection/change-point.md#section_6": 3, "docs/en/change-detection/change-point.md#section_13": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_10": 1}, "depends": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_11": 1, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_0": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/arma.md": 3, "docs/en/time-domain/arma.md#section_0": 2, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_4": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_0": 1, "docs/en/foundations/stationarity.md#section_1": 1}, "stack": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_12": 1}, "maxlag": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_13": 2, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_11": 1}, "verbose": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_13": 2}, "using": {"docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_14": 1, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/ar.md#section_11": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_0": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 5, "docs/en/exponential-smoothing/ets.md#section_6": 1, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/ets.md#section_14": 2, "docs/en/exponential-smoothing/holt.md": 4, "docs/en/exponential-smoothing/holt.md#section_0": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/exponential-smoothing/holt.md#section_14": 2, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 5, "docs/en/exponential-smoothing/ses.md#section_0": 1, "docs/en/exponential-smoothing/ses.md#section_7": 3, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_6": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 5, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/forecasting/multi-step.md#section_9": 2, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/interview/interview-questions.md": 4, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/interview/interview-questions.md#section_16": 3, "docs/en/model-selection/cross-validation.md": 7, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 6, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 5, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/features/feature-engineering.md#section_16": 4, "docs/en/state-space/kalman-filter.md": 3, "docs/en/state-space/kalman-filter.md#section_2": 1, "docs/en/state-space/kalman-filter.md#section_13": 2, "docs/en/anomaly-detection/anomaly-detection.md": 6, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 5, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_0": 1, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 7, "docs/en/decomposition/stl.md#section_0": 2, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_3": 1, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/decomposition/stl.md#section_13": 2, "docs/en/change-detection/change-point.md": 4, "docs/en/change-detection/change-point.md#section_13": 1, "docs/en/change-detection/change-point.md#section_14": 3, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_9": 1, "docs/en/foundations/autocorrelation.md#section_16": 2, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "results": {"docs/en/multivariate/granger-causality.md": 9, "docs/en/multivariate/granger-causality.md#section_14": 3, "docs/en/multivariate/granger-causality.md#section_15": 6, "docs/en/multivariate/var.md": 5, "docs/en/multivariate/var.md#section_12": 2, "docs/en/multivariate/var.md#section_13": 1, "docs/en/multivariate/var.md#section_14": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/sarima.md": 4, "docs/en/time-domain/sarima.md#section_12": 2, "docs/en/time-domain/sarima.md#section_13": 1, "docs/en/time-domain/sarima.md#section_14": 1, "docs/en/exponential-smoothing/ets.md": 5, "docs/en/exponential-smoothing/ets.md#section_11": 4, "docs/en/exponential-smoothing/ets.md#section_12": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/model-selection/cross-validation.md": 6, "docs/en/model-selection/cross-validation.md#section_9": 3, "docs/en/model-selection/cross-validation.md#section_10": 2, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_10": 2, "docs/en/model-selection/information-criteria.md#section_11": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_11": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "y1": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_14": 2, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_13": 1}, "y2": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_14": 2, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_13": 1}, "kind": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_14": 2}, "measures": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 4, "docs/en/foundations/autocorrelation.md#section_0": 2, "docs/en/foundations/autocorrelation.md#section_16": 2}, "establish": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "differ": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "creating": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "respond": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1}, "unobserved": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_11": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_1": 1}, "factor": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/arma.md": 4, "docs/en/time-domain/arma.md#section_5": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_3": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/anomaly-detection/anomaly-detection.md": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_5": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2}, "even": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_2": 1, "docs/en/decomposition/classical.md#section_12": 1}, "chance": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/forecasting/prediction-intervals.md": 4, "docs/en/forecasting/prediction-intervals.md#section_13": 4, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "measurement": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_3": 1, "docs/en/exponential-smoothing/ets.md#section_4": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1}, "timing": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "measured": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "reflects": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "ice": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "cream": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "sales": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 5, "docs/en/exponential-smoothing/holt-winters.md#section_13": 4, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_12": 3}, "drownings": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "caused": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "summer": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "heat": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "claiming": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1}, "say": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "means": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_6": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_4": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_3": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_10": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_10": 2}, "economics": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_0": 1}, "gdp": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 7, "docs/en/multivariate/var.md#section_15": 7}, "employment": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "economic": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_16": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "activity": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "labor": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "market": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "interact": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "prices": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1}, "wages": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "wage": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "price": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "spiral": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "interest": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "rates": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_15": 2}, "exchange": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "monetary": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "policy": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1}, "currency": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "markets": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "feedback": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "contain": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_4": 2, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "unique": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ma.md#section_13": 1}, "interdependent": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "caution": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "simultaneous": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "mutual": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "way": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "complex": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_3": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_0": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/deep-learning/deep-learning-ts.md": 4, "docs/en/deep-learning/deep-learning-ts.md#section_0": 2, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1, "docs/en/interview/interview-questions.md": 5, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/interview/interview-questions.md#section_13": 1, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/model-selection/information-criteria.md": 4, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/information-criteria.md#section_13": 3, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "systems": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_0": 1, "docs/en/state-space/kalman-filter.md#section_6": 1}, "rule": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_2": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1}, "rather": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1}, "exception": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "degrees": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/model-selection/residual-diagnostics.md": 4, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 3}, "freedom": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/model-selection/residual-diagnostics.md": 4, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 3}, "observations": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_6": 1, "docs/en/exponential-smoothing/ses.md": 3, "docs/en/exponential-smoothing/ses.md#section_0": 1, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 8, "docs/en/model-selection/cross-validation.md#section_4": 2, "docs/en/model-selection/cross-validation.md#section_11": 6, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/state-space/kalman-filter.md": 5, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/state-space/kalman-filter.md#section_10": 1, "docs/en/state-space/kalman-filter.md#section_13": 3, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1, "docs/en/decomposition/classical.md": 5, "docs/en/decomposition/classical.md#section_4": 1, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/classical.md#section_12": 3, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_13": 2, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_14": 2, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "numerator": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_15": 2}, "df": {"docs/en/multivariate/granger-causality.md": 4, "docs/en/multivariate/granger-causality.md#section_15": 4, "docs/en/model-selection/information-criteria.md": 10, "docs/en/model-selection/information-criteria.md#section_11": 8, "docs/en/model-selection/information-criteria.md#section_12": 2, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_7": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/features/feature-engineering.md": 6, "docs/en/features/feature-engineering.md#section_5": 2, "docs/en/features/feature-engineering.md#section_10": 1, "docs/en/features/feature-engineering.md#section_11": 3}, "denominator": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1}, "parameters": {"docs/en/multivariate/granger-causality.md": 4, "docs/en/multivariate/granger-causality.md#section_15": 4, "docs/en/multivariate/var.md": 7, "docs/en/multivariate/var.md#section_7": 2, "docs/en/multivariate/var.md#section_15": 5, "docs/en/time-domain/arma.md": 7, "docs/en/time-domain/arma.md#section_3": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_14": 5, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_2": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 5, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md#section_13": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 3, "docs/en/exponential-smoothing/ets.md": 5, "docs/en/exponential-smoothing/ets.md#section_7": 2, "docs/en/exponential-smoothing/ets.md#section_13": 3, "docs/en/exponential-smoothing/holt.md": 5, "docs/en/exponential-smoothing/holt.md#section_0": 1, "docs/en/exponential-smoothing/holt.md#section_1": 1, "docs/en/exponential-smoothing/holt.md#section_14": 3, "docs/en/exponential-smoothing/holt-winters.md": 4, "docs/en/exponential-smoothing/holt-winters.md#section_0": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 3, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_1": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_5": 1, "docs/en/forecasting/prediction-intervals.md#section_6": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_11": 1, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_5": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 6, "docs/en/model-selection/information-criteria.md#section_1": 1, "docs/en/model-selection/information-criteria.md#section_13": 5, "docs/en/model-selection/residual-diagnostics.md": 5, "docs/en/model-selection/residual-diagnostics.md#section_2": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 4, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/decomposition/stl.md": 4, "docs/en/decomposition/stl.md#section_0": 1, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_5": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 4, "docs/en/change-detection/change-point.md#section_1": 1, "docs/en/change-detection/change-point.md#section_2": 2, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_1": 1}, "restriction": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "set": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/model-selection/cross-validation.md": 5, "docs/en/model-selection/cross-validation.md#section_1": 2, "docs/en/model-selection/cross-validation.md#section_11": 3, "docs/en/practical/practical-modeling.md": 3, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/practical/practical-modeling.md#section_13": 2}, "used": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_1": 2, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "number": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_2": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_1": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_1": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_14": 3}, "restrictions": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "3p": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "formulations": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_13": 2}, "slightly": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1}, "depending": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "counted": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "software": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1}, "handled": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "automatically": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "short": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 4, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_3": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_4": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/classical.md#section_12": 2, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_13": 2, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_7": 1}, "reducing": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "balance": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_0": 1}, "handle": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_0": 1, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_3": 1, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_0": 1, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "1995": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/granger-causality.md#section_16": 1}, "max": {"docs/en/multivariate/granger-causality.md": 5, "docs/en/multivariate/granger-causality.md#section_15": 5, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_3": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_12": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_0": 1, "docs/en/features/feature-engineering.md#section_4": 1, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_12": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_10": 2, "docs/en/change-detection/change-point.md": 6, "docs/en/change-detection/change-point.md#section_2": 2, "docs/en/change-detection/change-point.md#section_6": 3, "docs/en/change-detection/change-point.md#section_13": 1}, "usually": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_13": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_0": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "levels": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/time-domain/arima.md": 4, "docs/en/time-domain/arima.md#section_3": 1, "docs/en/time-domain/arima.md#section_4": 1, "docs/en/time-domain/arima.md#section_14": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_11": 2, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_15": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_1": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "don": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 3, "docs/en/forecasting/multi-step.md#section_9": 2, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_17": 3, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_13": 3, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_14": 2, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "extra": {"docs/en/multivariate/granger-causality.md": 4, "docs/en/multivariate/granger-causality.md#section_15": 4, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_13": 3, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "absorb": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "asymptotic": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "pretest": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "cointegration": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_7": 1}, "robust": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_4": 1, "docs/en/anomaly-detection/anomaly-detection.md": 4, "docs/en/anomaly-detection/anomaly-detection.md#section_2": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/stl.md": 10, "docs/en/decomposition/stl.md#section_0": 1, "docs/en/decomposition/stl.md#section_1": 2, "docs/en/decomposition/stl.md#section_6": 2, "docs/en/decomposition/stl.md#section_10": 2, "docs/en/decomposition/stl.md#section_13": 3, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_5": 1}, "ignore": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "nuisance": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "oil": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "stock": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_16": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1}, "returns": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_8": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_5": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "vary": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "others": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_0": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "pattern": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/arima.md": 4, "docs/en/time-domain/arima.md#section_8": 2, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/time-domain/sarima.md": 6, "docs/en/time-domain/sarima.md#section_4": 1, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/time-domain/sarima.md#section_15": 4, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt-winters.md": 5, "docs/en/exponential-smoothing/holt-winters.md#section_5": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 3, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/decomposition/classical.md": 5, "docs/en/decomposition/classical.md#section_0": 1, "docs/en/decomposition/classical.md#section_1": 1, "docs/en/decomposition/classical.md#section_6": 2, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 7, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_13": 6, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_9": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "suggests": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/arima.md": 4, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/arima.md#section_17": 3, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_3": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 5, "docs/en/model-selection/residual-diagnostics.md#section_0": 2, "docs/en/model-selection/residual-diagnostics.md#section_5": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_7": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "matters": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_4": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/interview/interview-questions.md#section_15": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "specification": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_5": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_17": 1}, "weak": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/foundations/stationarity.md": 10, "docs/en/foundations/stationarity.md#section_0": 1, "docs/en/foundations/stationarity.md#section_1": 1, "docs/en/foundations/stationarity.md#section_7": 2, "docs/en/foundations/stationarity.md#section_13": 6}, "specifications": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "false": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_12": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_11": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_15": 1, "docs/en/anomaly-detection/anomaly-detection.md": 7, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 6, "docs/en/change-detection/change-point.md": 5, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_14": 3, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "positives": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/anomaly-detection/anomaly-detection.md": 6, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 5, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "criteria": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_0": 1, "docs/en/time-domain/identification.md#section_2": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_6": 1, "docs/en/model-selection/information-criteria.md": 5, "docs/en/model-selection/information-criteria.md#section_0": 3, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_9": 1}, "report": {"docs/en/multivariate/granger-causality.md": 3, "docs/en/multivariate/granger-causality.md#section_15": 3, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_10": 1}, "single": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_1": 1, "docs/en/forecasting/multi-step.md": 4, "docs/en/forecasting/multi-step.md#section_0": 1, "docs/en/forecasting/multi-step.md#section_1": 1, "docs/en/forecasting/multi-step.md#section_2": 1, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/change-detection/change-point.md": 4, "docs/en/change-detection/change-point.md#section_14": 4, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_1": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "sensitivity": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_0": 1}, "needed": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 3, "docs/en/time-domain/arima.md#section_9": 2, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_9": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_6": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "acknowledge": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "instability": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1}, "consider": {"docs/en/multivariate/granger-causality.md": 2, "docs/en/multivariate/granger-causality.md#section_15": 2, "docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 3, "docs/en/time-domain/sarima.md#section_8": 2, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/ses.md": 3, "docs/en/exponential-smoothing/ses.md#section_13": 3, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_6": 2, "docs/en/model-selection/information-criteria.md": 5, "docs/en/model-selection/information-criteria.md#section_0": 1, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/information-criteria.md#section_13": 2, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_12": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_12": 2, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_14": 2, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_16": 2, "docs/en/foundations/stationarity.md": 4, "docs/en/foundations/stationarity.md#section_7": 2, "docs/en/foundations/stationarity.md#section_13": 2}, "bonferroni": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "validate": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_0": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_13": 2}, "evidence": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "nonlinear": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1}, "varying": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_11": 1, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "threshold": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_12": 1, "docs/en/anomaly-detection/anomaly-detection.md": 12, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_2": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 5, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 3, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_2": 1}, "regime": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "switching": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_14": 2, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "selecting": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_6": 1}, "gives": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_4": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 4, "docs/en/time-domain/ar.md#section_7": 3, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 3, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/ses.md": 3, "docs/en/exponential-smoothing/ses.md#section_13": 3, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/model-selection/cross-validation.md": 3, "docs/en/model-selection/cross-validation.md#section_11": 3, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_6": 1}, "desired": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "result": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_4": 2, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/decomposition/classical.md": 7, "docs/en/decomposition/classical.md#section_9": 6, "docs/en/decomposition/classical.md#section_10": 1, "docs/en/decomposition/stl.md": 9, "docs/en/decomposition/stl.md#section_10": 6, "docs/en/decomposition/stl.md#section_11": 2, "docs/en/decomposition/stl.md#section_12": 1}, "shopping": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "hacking": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1}, "pre": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_14": 2}, "specified": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_15": 1, "docs/en/forecasting/multi-step.md": 5, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/forecasting/multi-step.md#section_17": 4}, "1969": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1}, "investigating": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1}, "relations": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1}, "econometric": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1}, "econometrica": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1}, "37": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1}, "424": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1}, "438": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1}, "statistical": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_14": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 4, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_2": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_0": 1}, "inference": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/interview/interview-questions.md#section_6": 1, "docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_0": 1, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/information-criteria.md#section_14": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "vector": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/multivariate/var.md": 7, "docs/en/multivariate/var.md#section_0": 1, "docs/en/multivariate/var.md#section_1": 3, "docs/en/multivariate/var.md#section_15": 1, "docs/en/multivariate/var.md#section_16": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_2": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_1": 1}, "autoregressions": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_16": 1}, "possibly": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "integrated": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/time-domain/arima.md": 8, "docs/en/time-domain/arima.md#section_0": 1, "docs/en/time-domain/arima.md#section_1": 2, "docs/en/time-domain/arima.md#section_17": 5, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "journal": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_16": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_14": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_18": 2, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_14": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "econometrics": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "66": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1}, "225": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1}, "250": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_9": 1}, "hamilton": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_16": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_14": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_15": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_18": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_17": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "1994": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_16": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_14": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_15": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_18": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_17": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "princeton": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_16": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_14": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_15": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_18": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_17": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "11": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_2": 2, "docs/en/multivariate/var.md#section_16": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_2": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "2005": {"docs/en/multivariate/granger-causality.md": 1, "docs/en/multivariate/granger-causality.md#section_16": 1, "docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_16": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_15": 1}, "autoregression": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_0": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1}, "epsilon": {"docs/en/multivariate/var.md": 7, "docs/en/multivariate/var.md#section_0": 1, "docs/en/multivariate/var.md#section_1": 3, "docs/en/multivariate/var.md#section_4": 1, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/forecasting/prediction-intervals.md": 12, "docs/en/forecasting/prediction-intervals.md#section_2": 1, "docs/en/forecasting/prediction-intervals.md#section_3": 5, "docs/en/forecasting/prediction-intervals.md#section_13": 6, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 7, "docs/en/state-space/kalman-filter.md#section_1": 1, "docs/en/state-space/kalman-filter.md#section_3": 2, "docs/en/state-space/kalman-filter.md#section_13": 4, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 4, "docs/en/foundations/stationarity.md#section_4": 1, "docs/en/foundations/stationarity.md#section_13": 3}, "interrelated": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_0": 1}, "analyzing": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_0": 1}, "dynamic": {"docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_0": 1, "docs/en/multivariate/var.md#section_2": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/change-detection/change-point.md#section_4": 1}, "relationships": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_0": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_12": 1}, "ols": {"docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_0": 1, "docs/en/multivariate/var.md#section_6": 2, "docs/en/time-domain/ar.md": 4, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/ar.md#section_7": 2}, "check": {"docs/en/multivariate/var.md": 4, "docs/en/multivariate/var.md#section_0": 1, "docs/en/multivariate/var.md#section_6": 2, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 4, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ma.md#section_10": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 5, "docs/en/time-domain/arma.md#section_6": 2, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_12": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 5, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/ar.md#section_10": 1, "docs/en/time-domain/ar.md#section_12": 1, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/arima.md": 7, "docs/en/time-domain/arima.md#section_8": 3, "docs/en/time-domain/arima.md#section_9": 2, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/time-domain/identification.md": 5, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_7": 2, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/time-domain/sarima.md": 6, "docs/en/time-domain/sarima.md#section_7": 3, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_13": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 4, "docs/en/deep-learning/deep-learning-ts.md#section_17": 4, "docs/en/exponential-smoothing/ets.md": 4, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_14": 2, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 7, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 2, "docs/en/model-selection/residual-diagnostics.md#section_15": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/practical/practical-modeling.md": 4, "docs/en/practical/practical-modeling.md#section_11": 1, "docs/en/practical/practical-modeling.md#section_12": 1, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_11": 1, "docs/en/foundations/autocorrelation.md": 7, "docs/en/foundations/autocorrelation.md#section_9": 3, "docs/en/foundations/autocorrelation.md#section_10": 2, "docs/en/foundations/autocorrelation.md#section_14": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 3, "docs/en/foundations/stationarity.md#section_6": 1, "docs/en/foundations/stationarity.md#section_7": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "stability": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_0": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1}, "eigenvalues": {"docs/en/multivariate/var.md": 7, "docs/en/multivariate/var.md#section_0": 1, "docs/en/multivariate/var.md#section_1": 1, "docs/en/multivariate/var.md#section_3": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/multivariate/var.md#section_15": 3}, "compact": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_1": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "form": {"docs/en/multivariate/var.md": 4, "docs/en/multivariate/var.md#section_1": 1, "docs/en/multivariate/var.md#section_3": 1, "docs/en/multivariate/var.md#section_4": 1, "docs/en/multivariate/var.md#section_7": 1, "docs/en/time-domain/ma.md": 3, "docs/en/time-domain/ma.md#section_1": 1, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/arma.md": 3, "docs/en/time-domain/arma.md#section_1": 1, "docs/en/time-domain/arma.md#section_4": 2, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_1": 1, "docs/en/time-domain/ar.md#section_4": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_6": 1, "docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_2": 2, "docs/en/exponential-smoothing/ets.md#section_3": 1, "docs/en/exponential-smoothing/ses.md": 3, "docs/en/exponential-smoothing/ses.md#section_1": 2, "docs/en/exponential-smoothing/ses.md#section_5": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_6": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_3": 1}, "condition": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_1": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_1": 1, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 4, "docs/en/time-domain/ar.md#section_1": 1, "docs/en/time-domain/ar.md#section_2": 1, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "companion": {"docs/en/multivariate/var.md": 4, "docs/en/multivariate/var.md#section_1": 1, "docs/en/multivariate/var.md#section_3": 2, "docs/en/multivariate/var.md#section_6": 1}, "matrix": {"docs/en/multivariate/var.md": 5, "docs/en/multivariate/var.md#section_1": 1, "docs/en/multivariate/var.md#section_3": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/multivariate/var.md#section_12": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_4": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_3": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_1": 2, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_3": 1}, "inside": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_1": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "circle": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_1": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 3, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/ma.md#section_1": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 5, "docs/en/time-domain/arma.md#section_1": 2, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/time-domain/ar.md": 3, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/ar.md#section_1": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_6": 2}, "1t": {"docs/en/multivariate/var.md": 5, "docs/en/multivariate/var.md#section_2": 5}, "2t": {"docs/en/multivariate/var.md": 5, "docs/en/multivariate/var.md#section_2": 5}, "pmatrix": {"docs/en/multivariate/var.md": 12, "docs/en/multivariate/var.md#section_2": 10, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ar.md": 6, "docs/en/time-domain/ar.md#section_4": 6, "docs/en/exponential-smoothing/ets.md": 8, "docs/en/exponential-smoothing/ets.md#section_3": 8, "docs/en/state-space/kalman-filter.md": 10, "docs/en/state-space/kalman-filter.md#section_4": 10, "docs/en/foundations/autocorrelation.md": 6, "docs/en/foundations/autocorrelation.md#section_3": 6}, "end": {"docs/en/multivariate/var.md": 6, "docs/en/multivariate/var.md#section_2": 5, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_3": 1, "docs/en/time-domain/ar.md": 3, "docs/en/time-domain/ar.md#section_4": 3, "docs/en/exponential-smoothing/ets.md": 4, "docs/en/exponential-smoothing/ets.md#section_3": 4, "docs/en/state-space/kalman-filter.md": 5, "docs/en/state-space/kalman-filter.md#section_4": 5, "docs/en/decomposition/classical.md": 7, "docs/en/decomposition/classical.md#section_4": 1, "docs/en/decomposition/classical.md#section_5": 1, "docs/en/decomposition/classical.md#section_6": 2, "docs/en/decomposition/classical.md#section_12": 3, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_2": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_1": 1, "docs/en/change-detection/change-point.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_3": 3}, "21": {"docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_2": 3, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1}, "22": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_2": 2, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1}, "written": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_2": 1, "docs/en/multivariate/var.md#section_3": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_4": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_5": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_1": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_2": 1}, "capture": {"docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_2": 1, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_7": 2, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_0": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "spillovers": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_2": 1}, "dimension": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_3": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1}, "xi": {"docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_3": 3}, "modulus": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_3": 1, "docs/en/multivariate/var.md#section_15": 1}, "moving": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_4": 1, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_1": 1, "docs/en/time-domain/arima.md#section_5": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_3": 2, "docs/en/decomposition/classical.md": 10, "docs/en/decomposition/classical.md#section_0": 2, "docs/en/decomposition/classical.md#section_2": 2, "docs/en/decomposition/classical.md#section_4": 2, "docs/en/decomposition/classical.md#section_5": 1, "docs/en/decomposition/classical.md#section_12": 3, "docs/en/decomposition/stl.md": 4, "docs/en/decomposition/stl.md#section_5": 1, "docs/en/decomposition/stl.md#section_13": 3}, "mu": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_4": 1, "docs/en/time-domain/ma.md": 10, "docs/en/time-domain/ma.md#section_1": 2, "docs/en/time-domain/ma.md#section_2": 2, "docs/en/time-domain/ma.md#section_4": 3, "docs/en/time-domain/ma.md#section_13": 3, "docs/en/time-domain/arma.md": 5, "docs/en/time-domain/arma.md#section_1": 1, "docs/en/time-domain/arma.md#section_2": 2, "docs/en/time-domain/arma.md#section_4": 2, "docs/en/time-domain/ar.md": 3, "docs/en/time-domain/ar.md#section_1": 1, "docs/en/time-domain/ar.md#section_2": 1, "docs/en/time-domain/ar.md#section_5": 1, "docs/en/state-space/kalman-filter.md": 8, "docs/en/state-space/kalman-filter.md#section_3": 5, "docs/en/state-space/kalman-filter.md#section_13": 3, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_1": 3, "docs/en/foundations/stationarity.md": 3, "docs/en/foundations/stationarity.md#section_1": 1, "docs/en/foundations/stationarity.md#section_2": 2}, "impulse": {"docs/en/multivariate/var.md": 6, "docs/en/multivariate/var.md#section_4": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/multivariate/var.md#section_7": 1, "docs/en/multivariate/var.md#section_13": 1, "docs/en/multivariate/var.md#section_15": 2}, "response": {"docs/en/multivariate/var.md": 5, "docs/en/multivariate/var.md#section_4": 2, "docs/en/multivariate/var.md#section_13": 2, "docs/en/multivariate/var.md#section_15": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_6": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_4": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "jk": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_4": 1, "docs/en/multivariate/var.md#section_5": 1}, "shock": {"docs/en/multivariate/var.md": 5, "docs/en/multivariate/var.md#section_4": 1, "docs/en/multivariate/var.md#section_13": 1, "docs/en/multivariate/var.md#section_15": 3, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1}, "forecast": {"docs/en/multivariate/var.md": 8, "docs/en/multivariate/var.md#section_5": 2, "docs/en/multivariate/var.md#section_6": 1, "docs/en/multivariate/var.md#section_14": 5, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 5, "docs/en/time-domain/ar.md#section_13": 5, "docs/en/time-domain/arima.md": 12, "docs/en/time-domain/arima.md#section_7": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/arima.md#section_16": 5, "docs/en/time-domain/arima.md#section_17": 5, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/sarima.md": 5, "docs/en/time-domain/sarima.md#section_14": 4, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_5": 1, "docs/en/exponential-smoothing/ets.md": 7, "docs/en/exponential-smoothing/ets.md#section_5": 1, "docs/en/exponential-smoothing/ets.md#section_6": 1, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/ets.md#section_13": 3, "docs/en/exponential-smoothing/holt.md": 22, "docs/en/exponential-smoothing/holt.md#section_1": 1, "docs/en/exponential-smoothing/holt.md#section_2": 1, "docs/en/exponential-smoothing/holt.md#section_4": 1, "docs/en/exponential-smoothing/holt.md#section_6": 2, "docs/en/exponential-smoothing/holt.md#section_11": 7, "docs/en/exponential-smoothing/holt.md#section_12": 2, "docs/en/exponential-smoothing/holt.md#section_13": 1, "docs/en/exponential-smoothing/holt.md#section_14": 7, "docs/en/exponential-smoothing/holt-winters.md": 9, "docs/en/exponential-smoothing/holt-winters.md#section_1": 2, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md#section_12": 6, "docs/en/exponential-smoothing/ses.md": 17, "docs/en/exponential-smoothing/ses.md#section_3": 1, "docs/en/exponential-smoothing/ses.md#section_4": 1, "docs/en/exponential-smoothing/ses.md#section_5": 1, "docs/en/exponential-smoothing/ses.md#section_6": 2, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/exponential-smoothing/ses.md#section_12": 5, "docs/en/exponential-smoothing/ses.md#section_13": 6, "docs/en/forecasting/prediction-intervals.md": 22, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/forecasting/prediction-intervals.md#section_1": 3, "docs/en/forecasting/prediction-intervals.md#section_2": 3, "docs/en/forecasting/prediction-intervals.md#section_5": 1, "docs/en/forecasting/prediction-intervals.md#section_6": 2, "docs/en/forecasting/prediction-intervals.md#section_11": 3, "docs/en/forecasting/prediction-intervals.md#section_13": 9, "docs/en/forecasting/multi-step.md": 11, "docs/en/forecasting/multi-step.md#section_1": 2, "docs/en/forecasting/multi-step.md#section_2": 1, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/forecasting/multi-step.md#section_8": 1, "docs/en/forecasting/multi-step.md#section_12": 1, "docs/en/forecasting/multi-step.md#section_13": 1, "docs/en/forecasting/multi-step.md#section_17": 4, "docs/en/interview/interview-questions.md": 11, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/interview/interview-questions.md#section_10": 2, "docs/en/interview/interview-questions.md#section_15": 4, "docs/en/interview/interview-questions.md#section_16": 4, "docs/en/model-selection/cross-validation.md": 13, "docs/en/model-selection/cross-validation.md#section_2": 1, "docs/en/model-selection/cross-validation.md#section_3": 2, "docs/en/model-selection/cross-validation.md#section_5": 4, "docs/en/model-selection/cross-validation.md#section_7": 2, "docs/en/model-selection/cross-validation.md#section_9": 3, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_13": 3, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 8, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/practical/practical-modeling.md#section_1": 1, "docs/en/practical/practical-modeling.md#section_3": 1, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/practical/practical-modeling.md#section_10": 1, "docs/en/practical/practical-modeling.md#section_12": 1, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_6": 2, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_13": 3}, "contribution": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_5": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_5": 1}, "horizon": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_5": 1, "docs/en/time-domain/arima.md": 4, "docs/en/time-domain/arima.md#section_7": 1, "docs/en/time-domain/arima.md#section_17": 3, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md": 4, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/forecasting/prediction-intervals.md#section_6": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/forecasting/multi-step.md": 16, "docs/en/forecasting/multi-step.md#section_0": 1, "docs/en/forecasting/multi-step.md#section_1": 2, "docs/en/forecasting/multi-step.md#section_6": 2, "docs/en/forecasting/multi-step.md#section_7": 1, "docs/en/forecasting/multi-step.md#section_10": 3, "docs/en/forecasting/multi-step.md#section_12": 1, "docs/en/forecasting/multi-step.md#section_14": 1, "docs/en/forecasting/multi-step.md#section_17": 5, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 20, "docs/en/model-selection/cross-validation.md#section_2": 1, "docs/en/model-selection/cross-validation.md#section_5": 7, "docs/en/model-selection/cross-validation.md#section_6": 2, "docs/en/model-selection/cross-validation.md#section_9": 4, "docs/en/model-selection/cross-validation.md#section_11": 6, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_13": 2, "docs/en/practical/practical-modeling.md": 4, "docs/en/practical/practical-modeling.md#section_2": 1, "docs/en/practical/practical-modeling.md#section_6": 2, "docs/en/practical/practical-modeling.md#section_13": 1}, "minimizing": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_5": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_2": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "estimated": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ma.md#section_11": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_11": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/ar.md#section_12": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_15": 1, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_10": 1, "docs/en/exponential-smoothing/holt.md#section_12": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_10": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_5": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_1": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_2": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_1": 1}, "separately": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "efficient": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_6": 1, "docs/en/multivariate/var.md#section_7": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/forecasting/multi-step.md": 4, "docs/en/forecasting/multi-step.md#section_4": 1, "docs/en/forecasting/multi-step.md#section_5": 1, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "regressors": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_6": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_4": 1}, "residuals": {"docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_6": 1, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_6": 2, "docs/en/time-domain/ar.md": 4, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/ar.md#section_14": 3, "docs/en/time-domain/arima.md": 3, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/time-domain/identification.md": 6, "docs/en/time-domain/identification.md#section_3": 1, "docs/en/time-domain/identification.md#section_5": 2, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/time-domain/sarima.md#section_13": 1, "docs/en/exponential-smoothing/ets.md": 4, "docs/en/exponential-smoothing/ets.md#section_13": 4, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_6": 1, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md": 20, "docs/en/model-selection/residual-diagnostics.md#section_0": 2, "docs/en/model-selection/residual-diagnostics.md#section_1": 2, "docs/en/model-selection/residual-diagnostics.md#section_4": 1, "docs/en/model-selection/residual-diagnostics.md#section_5": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 5, "docs/en/model-selection/residual-diagnostics.md#section_7": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 7, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 10, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_3": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 4, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_14": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/stl.md": 4, "docs/en/decomposition/stl.md#section_4": 1, "docs/en/decomposition/stl.md#section_13": 3, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_10": 1}, "lb": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_14": 3}, "normality": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/forecasting/prediction-intervals.md": 4, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/forecasting/prediction-intervals.md#section_4": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 15, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_3": 2, "docs/en/model-selection/residual-diagnostics.md#section_6": 2, "docs/en/model-selection/residual-diagnostics.md#section_7": 2, "docs/en/model-selection/residual-diagnostics.md#section_14": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 6, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "heteroskedasticity": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_6": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_4": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "responses": {"docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_6": 1, "docs/en/multivariate/var.md#section_7": 1, "docs/en/multivariate/var.md#section_15": 1}, "overfitting": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_7": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/information-criteria.md#section_13": 2, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_7": 1}, "easy": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_7": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "requires": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_7": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 3, "docs/en/time-domain/ma.md#section_0": 2, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 3, "docs/en/time-domain/arma.md#section_14": 3, "docs/en/time-domain/ar.md": 3, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_1": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_3": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_13": 2}, "differences": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_7": 2, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/arima.md": 3, "docs/en/time-domain/arima.md#section_0": 1, "docs/en/time-domain/arima.md#section_1": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_7": 1}, "vecm": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_7": 2}, "structural": {"docs/en/multivariate/var.md": 4, "docs/en/multivariate/var.md#section_7": 2, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_4": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_11": 1, "docs/en/interview/interview-questions.md#section_14": 2, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_0": 1, "docs/en/state-space/kalman-filter.md#section_14": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/foundations/stationarity.md": 4, "docs/en/foundations/stationarity.md#section_7": 2, "docs/en/foundations/stationarity.md#section_13": 2}, "reduced": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_7": 1}, "shows": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_7": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_5": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_0": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_16": 3, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "correlations": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_7": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_10": 1}, "svar": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_7": 1}, "claims": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_7": 1}, "cointegrated": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_7": 1}, "irfs": {"docs/en/multivariate/var.md": 5, "docs/en/multivariate/var.md#section_7": 1, "docs/en/multivariate/var.md#section_13": 1, "docs/en/multivariate/var.md#section_15": 3}, "depend": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_7": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_3": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1}, "ordering": {"docs/en/multivariate/var.md": 9, "docs/en/multivariate/var.md#section_7": 1, "docs/en/multivariate/var.md#section_15": 8}, "cholesky": {"docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_7": 1, "docs/en/multivariate/var.md#section_15": 2}, "assumptions": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_7": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_0": 1}, "forgetting": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_7": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_13": 2}, "diagonal": {"docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_7": 1, "docs/en/multivariate/var.md#section_15": 2, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1}, "shocks": {"docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_7": 1, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_4": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 2}, "correlated": {"docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_7": 1, "docs/en/multivariate/var.md#section_15": 2, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "equations": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_7": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_3": 1, "docs/en/time-domain/ar.md#section_4": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_6": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_0": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_0": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_3": 2}, "array": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_9": 2, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_11": 2, "docs/en/forecasting/multi-step.md": 3, "docs/en/forecasting/multi-step.md#section_10": 2, "docs/en/forecasting/multi-step.md#section_14": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_5": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_12": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_12": 2}, "maxlags": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_11": 1}, "ncoefficient": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_12": 1}, "coefs": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_12": 1}, "ntrue": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_12": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_11": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_15": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_14": 1}, "irf": {"docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_13": 3}, "10": {"docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_13": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/multivariate/var.md#section_16": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_10": 1, "docs/en/time-domain/arma.md": 3, "docs/en/time-domain/arma.md#section_14": 3, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_10": 1, "docs/en/time-domain/ar.md#section_11": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_16": 2, "docs/en/time-domain/identification.md": 4, "docs/en/time-domain/identification.md#section_13": 2, "docs/en/time-domain/identification.md#section_14": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_11": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_10": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 3, "docs/en/exponential-smoothing/holt.md#section_9": 1, "docs/en/exponential-smoothing/holt.md#section_11": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_9": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_12": 1, "docs/en/forecasting/prediction-intervals.md": 4, "docs/en/forecasting/prediction-intervals.md#section_11": 1, "docs/en/forecasting/prediction-intervals.md#section_12": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_12": 1, "docs/en/model-selection/cross-validation.md": 3, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/model-selection/information-criteria.md": 4, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/information-criteria.md#section_13": 3, "docs/en/model-selection/residual-diagnostics.md": 7, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 4, "docs/en/practical/practical-modeling.md": 3, "docs/en/practical/practical-modeling.md#section_6": 2, "docs/en/practical/practical-modeling.md#section_9": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_10": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_12": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_8": 1, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_11": 2, "docs/en/change-detection/change-point.md#section_12": 1}, "nirf": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_13": 1}, "3f": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_13": 1, "docs/en/time-domain/ma.md": 4, "docs/en/time-domain/ma.md#section_11": 2, "docs/en/time-domain/ma.md#section_12": 2, "docs/en/time-domain/arma.md": 3, "docs/en/time-domain/arma.md#section_11": 2, "docs/en/time-domain/arma.md#section_12": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_12": 2, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_15": 2, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_14": 1, "docs/en/exponential-smoothing/holt.md": 3, "docs/en/exponential-smoothing/holt.md#section_10": 2, "docs/en/exponential-smoothing/holt.md#section_12": 1, "docs/en/exponential-smoothing/holt-winters.md": 3, "docs/en/exponential-smoothing/holt-winters.md#section_11": 3, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_10": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_10": 2, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_15": 2, "docs/en/state-space/kalman-filter.md": 3, "docs/en/state-space/kalman-filter.md#section_11": 1, "docs/en/state-space/kalman-filter.md#section_12": 2, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_10": 2}, "steps": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_14": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_13": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_16": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_14": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_11": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_1": 1, "docs/en/forecasting/multi-step.md#section_3": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_5": 1}, "n5": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_14": 1}, "advantage": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1}, "fitting": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_14": 1}, "univariate": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1}, "captures": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/decomposition/stl.md#section_13": 1}, "dynamics": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/model-selection/cross-validation.md": 3, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_13": 2, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_13": 2, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "interactions": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_15": 2}, "affects": {"docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_15": 3, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "future": {"docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_15": 3, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_6": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/forecasting/prediction-intervals.md": 5, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/forecasting/prediction-intervals.md#section_1": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_0": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_8": 2, "docs/en/model-selection/cross-validation.md": 8, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/cross-validation.md#section_1": 1, "docs/en/model-selection/cross-validation.md#section_4": 2, "docs/en/model-selection/cross-validation.md#section_11": 4, "docs/en/practical/practical-modeling.md": 4, "docs/en/practical/practical-modeling.md#section_13": 4, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_0": 1, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "errors": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/arma.md": 3, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_7": 2, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_2": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 7, "docs/en/exponential-smoothing/ets.md#section_5": 2, "docs/en/exponential-smoothing/ets.md#section_6": 1, "docs/en/exponential-smoothing/ets.md#section_13": 4, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_5": 1, "docs/en/forecasting/multi-step.md": 6, "docs/en/forecasting/multi-step.md#section_0": 1, "docs/en/forecasting/multi-step.md#section_5": 1, "docs/en/forecasting/multi-step.md#section_17": 4, "docs/en/interview/interview-questions.md": 4, "docs/en/interview/interview-questions.md#section_9": 2, "docs/en/interview/interview-questions.md#section_10": 2, "docs/en/model-selection/cross-validation.md": 6, "docs/en/model-selection/cross-validation.md#section_5": 4, "docs/en/model-selection/cross-validation.md#section_10": 2, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_1": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 13, "docs/en/practical/practical-modeling.md#section_8": 3, "docs/en/practical/practical-modeling.md#section_9": 1, "docs/en/practical/practical-modeling.md#section_10": 2, "docs/en/practical/practical-modeling.md#section_11": 1, "docs/en/practical/practical-modeling.md#section_12": 3, "docs/en/practical/practical-modeling.md#section_13": 3, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "accounts": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_3": 1}, "wide": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "inflation": {"docs/en/multivariate/var.md": 7, "docs/en/multivariate/var.md#section_15": 7, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1}, "affecting": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1}, "demand": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "supply": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1}, "hitting": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1}, "arimas": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1}, "unrelated": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1}, "wasteful": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1}, "sparse": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1}, "important": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "pp": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_0": 1}, "lower": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/arma.md": 4, "docs/en/time-domain/arma.md#section_14": 4, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_13": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_0": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_11": 2, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/anomaly-detection/anomaly-detection.md": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_5": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_13": 2}, "triangular": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_14": 2}, "implies": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "affected": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "contemporaneously": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1}, "later": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "earlier": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1}, "ones": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_12": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_10": 1}, "orderings": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1}, "vs": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_2": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_0": 2, "docs/en/interview/interview-questions.md#section_6": 1, "docs/en/model-selection/information-criteria.md": 6, "docs/en/model-selection/information-criteria.md#section_0": 1, "docs/en/model-selection/information-criteria.md#section_7": 2, "docs/en/model-selection/information-criteria.md#section_13": 3, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_6": 2, "docs/en/practical/practical-modeling.md": 3, "docs/en/practical/practical-modeling.md#section_13": 3, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/change-detection/change-point.md": 4, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_14": 2, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_10": 2, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_1": 1}, "immediately": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_15": 2}, "second": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_0": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/foundations/stationarity.md": 4, "docs/en/foundations/stationarity.md#section_1": 1, "docs/en/foundations/stationarity.md#section_6": 1, "docs/en/foundations/stationarity.md#section_13": 2}, "solutions": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_3": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "theory": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_12": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1}, "justify": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "explicit": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_3": 1}, "reporting": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "stating": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1}, "justifying": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1}, "driven": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "arbitrary": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1}, "choice": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/information-criteria.md#section_13": 2}, "less": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/forecasting/multi-step.md": 5, "docs/en/forecasting/multi-step.md#section_4": 1, "docs/en/forecasting/multi-step.md#section_5": 1, "docs/en/forecasting/multi-step.md#section_17": 3, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_5": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_5": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "iterating": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1}, "backward": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md#section_14": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "converges": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/forecasting/prediction-intervals.md": 3, "docs/en/forecasting/prediction-intervals.md#section_13": 3}, "iff": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1}, "lambda": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_11": 1, "docs/en/exponential-smoothing/ets.md#section_12": 1}, "pm": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 3, "docs/en/time-domain/ma.md#section_2": 1, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_4": 1, "docs/en/forecasting/prediction-intervals.md": 4, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/forecasting/prediction-intervals.md#section_4": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 5, "docs/en/foundations/autocorrelation.md#section_8": 1, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_16": 3}, "sqrt": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 4, "docs/en/deep-learning/deep-learning-ts.md#section_4": 1, "docs/en/deep-learning/deep-learning-ts.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md#section_16": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_5": 2, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_4": 1, "docs/en/forecasting/prediction-intervals.md": 6, "docs/en/forecasting/prediction-intervals.md#section_3": 1, "docs/en/forecasting/prediction-intervals.md#section_12": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 3, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_2": 1, "docs/en/model-selection/cross-validation.md#section_3": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_15": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_2": 1, "docs/en/practical/practical-modeling.md#section_10": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_3": 1, "docs/en/features/feature-engineering.md#section_14": 1, "docs/en/state-space/kalman-filter.md": 5, "docs/en/state-space/kalman-filter.md#section_11": 1, "docs/en/state-space/kalman-filter.md#section_12": 1, "docs/en/state-space/kalman-filter.md#section_13": 3, "docs/en/foundations/autocorrelation.md": 5, "docs/en/foundations/autocorrelation.md#section_0": 1, "docs/en/foundations/autocorrelation.md#section_8": 1, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_16": 2}, "ad": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 8, "docs/en/exponential-smoothing/ets.md#section_0": 1, "docs/en/exponential-smoothing/ets.md#section_1": 2, "docs/en/exponential-smoothing/ets.md#section_11": 2, "docs/en/exponential-smoothing/ets.md#section_13": 3}, "bc": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1}, "checking": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2}, "elements": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1}, "off": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 3, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/ma.md#section_10": 1, "docs/en/time-domain/arma.md": 6, "docs/en/time-domain/arma.md#section_0": 1, "docs/en/time-domain/arma.md#section_6": 3, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_10": 1, "docs/en/time-domain/ar.md": 3, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/ar.md#section_10": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 14, "docs/en/time-domain/identification.md#section_0": 3, "docs/en/time-domain/identification.md#section_1": 6, "docs/en/time-domain/identification.md#section_6": 3, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_5": 2, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 4, "docs/en/interview/interview-questions.md#section_2": 3, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 13, "docs/en/foundations/autocorrelation.md#section_5": 1, "docs/en/foundations/autocorrelation.md#section_9": 6, "docs/en/foundations/autocorrelation.md#section_10": 3, "docs/en/foundations/autocorrelation.md#section_16": 2}, "terms": {"docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_15": 3, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/ma.md#section_4": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_4": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 10, "docs/en/time-domain/sarima.md#section_0": 1, "docs/en/time-domain/sarima.md#section_6": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_15": 7, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_3": 2, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/features/feature-engineering.md": 7, "docs/en/features/feature-engineering.md#section_0": 1, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_16": 5, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "make": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_1": 1}, "unstable": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_7": 2, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "breakdown": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1}, "covariance": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_15": 2, "docs/en/state-space/kalman-filter.md": 3, "docs/en/state-space/kalman-filter.md#section_1": 1, "docs/en/state-space/kalman-filter.md#section_2": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_0": 1, "docs/en/foundations/stationarity.md#section_1": 1}, "symmetric": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_3": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "constants": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1}, "36": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_2": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1}, "45": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_14": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "implications": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1}, "grow": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 3, "docs/en/exponential-smoothing/holt-winters.md#section_2": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "limited": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "severe": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1}, "bvar": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1}, "large": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_5": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 5, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/time-domain/identification.md#section_5": 1, "docs/en/time-domain/identification.md#section_15": 3, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_5": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/interview/interview-questions.md": 4, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/interview/interview-questions.md#section_6": 1, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/interview/interview-questions.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 4, "docs/en/model-selection/information-criteria.md#section_7": 2, "docs/en/model-selection/information-criteria.md#section_13": 2, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/state-space/kalman-filter.md": 4, "docs/en/state-space/kalman-filter.md#section_13": 4, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_3": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/stl.md": 4, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/decomposition/stl.md#section_13": 3, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_9": 1}, "small": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 5, "docs/en/time-domain/identification.md#section_2": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/identification.md#section_15": 3, "docs/en/deep-learning/deep-learning-ts.md": 6, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/deep-learning/deep-learning-ts.md#section_2": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2, "docs/en/forecasting/prediction-intervals.md": 4, "docs/en/forecasting/prediction-intervals.md#section_5": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/interview/interview-questions.md#section_6": 1, "docs/en/interview/interview-questions.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 9, "docs/en/model-selection/information-criteria.md#section_0": 1, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/information-criteria.md#section_13": 5, "docs/en/model-selection/information-criteria.md#section_14": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/state-space/kalman-filter.md": 5, "docs/en/state-space/kalman-filter.md#section_13": 5, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_7": 2, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_10": 1}, "thumb": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "20": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_14": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/exponential-smoothing/holt.md": 5, "docs/en/exponential-smoothing/holt.md#section_11": 2, "docs/en/exponential-smoothing/holt.md#section_12": 2, "docs/en/exponential-smoothing/holt.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 3, "docs/en/forecasting/prediction-intervals.md#section_11": 2, "docs/en/forecasting/prediction-intervals.md#section_12": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_5": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_6": 2, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_12": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 4, "docs/en/foundations/autocorrelation.md#section_13": 2, "docs/en/foundations/autocorrelation.md#section_16": 2}, "parameter": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 7, "docs/en/time-domain/arma.md#section_5": 2, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_12": 1, "docs/en/time-domain/arma.md#section_14": 3, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/exponential-smoothing/ets.md": 4, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/exponential-smoothing/ets.md#section_13": 3, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_14": 2, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_6": 1, "docs/en/exponential-smoothing/ses.md": 7, "docs/en/exponential-smoothing/ses.md#section_0": 1, "docs/en/exponential-smoothing/ses.md#section_1": 1, "docs/en/exponential-smoothing/ses.md#section_5": 1, "docs/en/exponential-smoothing/ses.md#section_6": 1, "docs/en/exponential-smoothing/ses.md#section_13": 3, "docs/en/forecasting/prediction-intervals.md": 5, "docs/en/forecasting/prediction-intervals.md#section_5": 2, "docs/en/forecasting/prediction-intervals.md#section_7": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_13": 2, "docs/en/state-space/kalman-filter.md": 3, "docs/en/state-space/kalman-filter.md#section_13": 3, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "quarterly": {"docs/en/multivariate/var.md": 3, "docs/en/multivariate/var.md#section_15": 3, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_1": 1, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "macro": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_15": 2}, "options": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "increase": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_17": 3, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "try": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 7, "docs/en/time-domain/identification.md#section_4": 1, "docs/en/time-domain/identification.md#section_6": 3, "docs/en/time-domain/identification.md#section_14": 1, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_11": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_9": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/foundations/stationarity.md": 3, "docs/en/foundations/stationarity.md#section_6": 2, "docs/en/foundations/stationarity.md#section_13": 1}, "add": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_10": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 6, "docs/en/exponential-smoothing/ets.md#section_11": 6, "docs/en/exponential-smoothing/holt-winters.md": 4, "docs/en/exponential-smoothing/holt-winters.md#section_10": 2, "docs/en/exponential-smoothing/holt-winters.md#section_11": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 4, "docs/en/model-selection/residual-diagnostics.md#section_6": 3, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 4, "docs/en/features/feature-engineering.md#section_16": 4, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_9": 1}, "seasonal": {"docs/en/multivariate/var.md": 4, "docs/en/multivariate/var.md#section_15": 4, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/arima.md": 8, "docs/en/time-domain/arima.md#section_8": 2, "docs/en/time-domain/arima.md#section_17": 6, "docs/en/time-domain/identification.md": 4, "docs/en/time-domain/identification.md#section_7": 2, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/time-domain/sarima.md": 57, "docs/en/time-domain/sarima.md#section_0": 6, "docs/en/time-domain/sarima.md#section_1": 8, "docs/en/time-domain/sarima.md#section_2": 1, "docs/en/time-domain/sarima.md#section_3": 1, "docs/en/time-domain/sarima.md#section_4": 3, "docs/en/time-domain/sarima.md#section_5": 6, "docs/en/time-domain/sarima.md#section_7": 5, "docs/en/time-domain/sarima.md#section_8": 5, "docs/en/time-domain/sarima.md#section_9": 1, "docs/en/time-domain/sarima.md#section_11": 3, "docs/en/time-domain/sarima.md#section_13": 2, "docs/en/time-domain/sarima.md#section_15": 14, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/exponential-smoothing/ets.md": 18, "docs/en/exponential-smoothing/ets.md#section_0": 2, "docs/en/exponential-smoothing/ets.md#section_1": 2, "docs/en/exponential-smoothing/ets.md#section_2": 1, "docs/en/exponential-smoothing/ets.md#section_3": 1, "docs/en/exponential-smoothing/ets.md#section_4": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/ets.md#section_10": 2, "docs/en/exponential-smoothing/ets.md#section_11": 4, "docs/en/exponential-smoothing/ets.md#section_13": 3, "docs/en/exponential-smoothing/ets.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 47, "docs/en/exponential-smoothing/holt-winters.md#section_0": 2, "docs/en/exponential-smoothing/holt-winters.md#section_1": 3, "docs/en/exponential-smoothing/holt-winters.md#section_2": 5, "docs/en/exponential-smoothing/holt-winters.md#section_3": 2, "docs/en/exponential-smoothing/holt-winters.md#section_5": 3, "docs/en/exponential-smoothing/holt-winters.md#section_6": 3, "docs/en/exponential-smoothing/holt-winters.md#section_7": 6, "docs/en/exponential-smoothing/holt-winters.md#section_9": 3, "docs/en/exponential-smoothing/holt-winters.md#section_10": 1, "docs/en/exponential-smoothing/holt-winters.md#section_11": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 18, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_7": 2, "docs/en/interview/interview-questions.md": 19, "docs/en/interview/interview-questions.md#section_3": 7, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/interview/interview-questions.md#section_16": 10, "docs/en/model-selection/cross-validation.md": 3, "docs/en/model-selection/cross-validation.md#section_11": 3, "docs/en/model-selection/residual-diagnostics.md": 4, "docs/en/model-selection/residual-diagnostics.md#section_6": 3, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 6, "docs/en/features/feature-engineering.md#section_1": 1, "docs/en/features/feature-engineering.md#section_2": 3, "docs/en/features/feature-engineering.md#section_10": 2, "docs/en/anomaly-detection/anomaly-detection.md": 4, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 3, "docs/en/decomposition/classical.md": 33, "docs/en/decomposition/classical.md#section_0": 3, "docs/en/decomposition/classical.md#section_1": 4, "docs/en/decomposition/classical.md#section_3": 1, "docs/en/decomposition/classical.md#section_5": 4, "docs/en/decomposition/classical.md#section_6": 3, "docs/en/decomposition/classical.md#section_7": 1, "docs/en/decomposition/classical.md#section_8": 1, "docs/en/decomposition/classical.md#section_9": 2, "docs/en/decomposition/classical.md#section_12": 14, "docs/en/decomposition/stl.md": 41, "docs/en/decomposition/stl.md#section_0": 4, "docs/en/decomposition/stl.md#section_1": 5, "docs/en/decomposition/stl.md#section_3": 1, "docs/en/decomposition/stl.md#section_5": 6, "docs/en/decomposition/stl.md#section_6": 5, "docs/en/decomposition/stl.md#section_7": 1, "docs/en/decomposition/stl.md#section_8": 2, "docs/en/decomposition/stl.md#section_10": 3, "docs/en/decomposition/stl.md#section_13": 12, "docs/en/decomposition/stl.md#section_14": 2, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_10": 3, "docs/en/foundations/stationarity.md": 5, "docs/en/foundations/stationarity.md#section_6": 2, "docs/en/foundations/stationarity.md#section_7": 3}, "dummies": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_15": 2, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/features/feature-engineering.md": 7, "docs/en/features/feature-engineering.md#section_16": 7}, "include": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_15": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/model-selection/cross-validation.md": 3, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "indicators": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_0": 1}, "exogenous": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_15": 2}, "seasonally": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "varx": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1}, "diagnostic": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_8": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_13": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_8": 1}, "three": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_0": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_0": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_0": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_10": 1}, "seasonals": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1}, "verify": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ar.md": 3, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "pass": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_13": 2, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_4": 1, "docs/en/decomposition/stl.md": 6, "docs/en/decomposition/stl.md#section_3": 1, "docs/en/decomposition/stl.md#section_5": 1, "docs/en/decomposition/stl.md#section_13": 4}, "consideration": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1}, "prefer": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2, "docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/forecasting/multi-step.md": 3, "docs/en/forecasting/multi-step.md#section_17": 3, "docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_6": 2, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "explicitly": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "chapters": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_16": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_18": 2, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_16": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_17": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "sims": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_16": 1}, "1980": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_16": 1}, "macroeconomics": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_16": 1}, "reality": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_16": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "48": {"docs/en/multivariate/var.md": 2, "docs/en/multivariate/var.md#section_16": 2, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_8": 1}, "watson": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_16": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1}, "2001": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_16": 1}, "perspectives": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_16": 1}, "15": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_16": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_18": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md": 5, "docs/en/model-selection/residual-diagnostics.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 3, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 4, "docs/en/foundations/autocorrelation.md#section_16": 4}, "101": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_16": 1}, "115": {"docs/en/multivariate/var.md": 1, "docs/en/multivariate/var.md#section_16": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "express": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_0": 1}, "linear": {"docs/en/time-domain/ma.md": 4, "docs/en/time-domain/ma.md#section_0": 2, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/arima.md": 3, "docs/en/time-domain/arima.md#section_3": 1, "docs/en/time-domain/arima.md#section_7": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_6": 1, "docs/en/deep-learning/deep-learning-ts.md#section_13": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_1": 1, "docs/en/exponential-smoothing/holt.md": 14, "docs/en/exponential-smoothing/holt.md#section_0": 3, "docs/en/exponential-smoothing/holt.md#section_1": 1, "docs/en/exponential-smoothing/holt.md#section_2": 1, "docs/en/exponential-smoothing/holt.md#section_7": 4, "docs/en/exponential-smoothing/holt.md#section_10": 1, "docs/en/exponential-smoothing/holt.md#section_13": 1, "docs/en/exponential-smoothing/holt.md#section_14": 2, "docs/en/forecasting/multi-step.md": 3, "docs/en/forecasting/multi-step.md#section_17": 3, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/interview/interview-questions.md#section_11": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 3, "docs/en/state-space/kalman-filter.md#section_0": 1, "docs/en/state-space/kalman-filter.md#section_4": 1, "docs/en/state-space/kalman-filter.md#section_14": 1, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/change-detection/change-point.md#section_4": 1, "docs/en/change-detection/change-point.md#section_15": 1, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_1": 2, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_6": 1}, "combination": {"docs/en/time-domain/ma.md": 3, "docs/en/time-domain/ma.md#section_0": 2, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_0": 1}, "cuts": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_6": 2, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/identification.md": 7, "docs/en/time-domain/identification.md#section_0": 2, "docs/en/time-domain/identification.md#section_1": 2, "docs/en/time-domain/identification.md#section_6": 2, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_5": 2, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_2": 2, "docs/en/foundations/autocorrelation.md": 8, "docs/en/foundations/autocorrelation.md#section_5": 1, "docs/en/foundations/autocorrelation.md#section_9": 4, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_16": 2}, "after": {"docs/en/time-domain/ma.md": 5, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ma.md#section_10": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_2": 1, "docs/en/time-domain/ar.md": 5, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/ar.md#section_10": 1, "docs/en/time-domain/ar.md#section_14": 3, "docs/en/time-domain/arima.md": 3, "docs/en/time-domain/arima.md#section_0": 1, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_1": 2, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/exponential-smoothing/holt-winters.md": 4, "docs/en/exponential-smoothing/holt-winters.md#section_3": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 3, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_2": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_4": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_2": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 11, "docs/en/foundations/autocorrelation.md#section_0": 1, "docs/en/foundations/autocorrelation.md#section_1": 1, "docs/en/foundations/autocorrelation.md#section_5": 1, "docs/en/foundations/autocorrelation.md#section_9": 2, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_15": 1, "docs/en/foundations/autocorrelation.md#section_16": 3, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_4": 1}, "pacf": {"docs/en/time-domain/ma.md": 5, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/ma.md#section_5": 2, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/arma.md": 9, "docs/en/time-domain/arma.md#section_0": 1, "docs/en/time-domain/arma.md#section_6": 2, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_8": 1, "docs/en/time-domain/arma.md#section_10": 3, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 12, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/ar.md#section_2": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/ar.md#section_8": 1, "docs/en/time-domain/ar.md#section_10": 3, "docs/en/time-domain/ar.md#section_14": 5, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/identification.md": 16, "docs/en/time-domain/identification.md#section_0": 2, "docs/en/time-domain/identification.md#section_1": 2, "docs/en/time-domain/identification.md#section_6": 3, "docs/en/time-domain/identification.md#section_7": 2, "docs/en/time-domain/identification.md#section_8": 1, "docs/en/time-domain/identification.md#section_13": 4, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/time-domain/sarima.md": 6, "docs/en/time-domain/sarima.md#section_0": 1, "docs/en/time-domain/sarima.md#section_5": 3, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/interview/interview-questions.md": 6, "docs/en/interview/interview-questions.md#section_0": 1, "docs/en/interview/interview-questions.md#section_2": 3, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_6": 2, "docs/en/foundations/autocorrelation.md": 33, "docs/en/foundations/autocorrelation.md#section_0": 4, "docs/en/foundations/autocorrelation.md#section_1": 1, "docs/en/foundations/autocorrelation.md#section_3": 2, "docs/en/foundations/autocorrelation.md#section_7": 1, "docs/en/foundations/autocorrelation.md#section_9": 6, "docs/en/foundations/autocorrelation.md#section_10": 2, "docs/en/foundations/autocorrelation.md#section_13": 2, "docs/en/foundations/autocorrelation.md#section_14": 4, "docs/en/foundations/autocorrelation.md#section_16": 10}, "decays": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/ma.md#section_5": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_2": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/foundations/autocorrelation.md": 4, "docs/en/foundations/autocorrelation.md#section_4": 1, "docs/en/foundations/autocorrelation.md#section_9": 2, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_6": 1}, "exponentially": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/ma.md#section_5": 1, "docs/en/time-domain/ar.md": 3, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/ar.md#section_5": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_5": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_0": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_4": 1}, "optimization": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_14": 2, "docs/en/exponential-smoothing/ses.md": 3, "docs/en/exponential-smoothing/ses.md#section_5": 1, "docs/en/exponential-smoothing/ses.md#section_10": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "mle": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/arma.md": 4, "docs/en/time-domain/arma.md#section_0": 1, "docs/en/time-domain/arma.md#section_6": 3, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_6": 1}, "invertibility": {"docs/en/time-domain/ma.md": 15, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/ma.md#section_1": 1, "docs/en/time-domain/ma.md#section_4": 2, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ma.md#section_13": 9, "docs/en/time-domain/arma.md": 6, "docs/en/time-domain/arma.md#section_0": 1, "docs/en/time-domain/arma.md#section_1": 1, "docs/en/time-domain/arma.md#section_2": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_3": 1}, "outside": {"docs/en/time-domain/ma.md": 4, "docs/en/time-domain/ma.md#section_0": 1, "docs/en/time-domain/ma.md#section_1": 1, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/arma.md": 5, "docs/en/time-domain/arma.md#section_1": 2, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/time-domain/ar.md": 3, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/ar.md#section_1": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_6": 2, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_9": 1}, "wn": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_1": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_1": 1}, "operator": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_1": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_1": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_1": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_1": 1, "docs/en/time-domain/arima.md#section_6": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_1": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_4": 1}, "characteristic": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_1": 1, "docs/en/time-domain/ar.md": 4, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/ar.md#section_1": 1, "docs/en/time-domain/ar.md#section_3": 1, "docs/en/time-domain/ar.md#section_14": 1}, "polynomial": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_1": 1, "docs/en/time-domain/arma.md": 4, "docs/en/time-domain/arma.md#section_1": 2, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/ar.md#section_1": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/decomposition/stl.md": 4, "docs/en/decomposition/stl.md#section_1": 2, "docs/en/decomposition/stl.md#section_2": 1, "docs/en/decomposition/stl.md#section_13": 1}, "lie": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_1": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_1": 1}, "geq": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_2": 2, "docs/en/time-domain/arma.md": 5, "docs/en/time-domain/arma.md#section_2": 1, "docs/en/time-domain/arma.md#section_14": 4, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_7": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_3": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_4": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_2": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_1": 1, "docs/en/foundations/autocorrelation.md": 4, "docs/en/foundations/autocorrelation.md#section_2": 1, "docs/en/foundations/autocorrelation.md#section_16": 3, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_13": 2}, "rho": {"docs/en/time-domain/ma.md": 17, "docs/en/time-domain/ma.md#section_2": 3, "docs/en/time-domain/ma.md#section_3": 1, "docs/en/time-domain/ma.md#section_12": 2, "docs/en/time-domain/ma.md#section_13": 11, "docs/en/time-domain/arma.md": 10, "docs/en/time-domain/arma.md#section_2": 4, "docs/en/time-domain/arma.md#section_14": 6, "docs/en/time-domain/ar.md": 7, "docs/en/time-domain/ar.md#section_2": 1, "docs/en/time-domain/ar.md#section_3": 5, "docs/en/time-domain/ar.md#section_4": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 19, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/time-domain/identification.md#section_5": 1, "docs/en/time-domain/identification.md#section_15": 17, "docs/en/time-domain/sarima.md": 4, "docs/en/time-domain/sarima.md#section_15": 4, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_2": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/foundations/autocorrelation.md": 32, "docs/en/foundations/autocorrelation.md#section_1": 2, "docs/en/foundations/autocorrelation.md#section_2": 6, "docs/en/foundations/autocorrelation.md#section_3": 9, "docs/en/foundations/autocorrelation.md#section_4": 1, "docs/en/foundations/autocorrelation.md#section_5": 2, "docs/en/foundations/autocorrelation.md#section_6": 1, "docs/en/foundations/autocorrelation.md#section_8": 3, "docs/en/foundations/autocorrelation.md#section_16": 8, "docs/en/foundations/stationarity.md": 8, "docs/en/foundations/stationarity.md#section_3": 5, "docs/en/foundations/stationarity.md#section_13": 3}, "quad": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_2": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_1": 1, "docs/en/state-space/kalman-filter.md": 3, "docs/en/state-space/kalman-filter.md#section_1": 2, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_5": 1}, "note": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_2": 1, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_2": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "general": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_3": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_3": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_4": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_6": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_2": 1, "docs/en/exponential-smoothing/ets.md#section_6": 1}, "cases": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_3": 2, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_0": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_2": 2, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_1": 2}, "leq": {"docs/en/time-domain/ma.md": 4, "docs/en/time-domain/ma.md#section_3": 2, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_2": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_2": 1, "docs/en/foundations/stationarity.md#section_3": 1}, "invertible": {"docs/en/time-domain/ma.md": 10, "docs/en/time-domain/ma.md#section_4": 1, "docs/en/time-domain/ma.md#section_7": 3, "docs/en/time-domain/ma.md#section_13": 6, "docs/en/time-domain/arma.md": 6, "docs/en/time-domain/arma.md#section_4": 3, "docs/en/time-domain/arma.md#section_14": 3, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_6": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_13": 2}, "allows": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_4": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_1": 1}, "expressing": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_4": 1}, "observables": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_4": 1, "docs/en/time-domain/ma.md#section_13": 1}, "required": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_4": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_13": 1}, "proper": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_4": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/exponential-smoothing/ets.md": 4, "docs/en/exponential-smoothing/ets.md#section_0": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "hh": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_5": 2, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_2": 1, "docs/en/foundations/autocorrelation.md": 4, "docs/en/foundations/autocorrelation.md#section_1": 1, "docs/en/foundations/autocorrelation.md#section_7": 1, "docs/en/foundations/autocorrelation.md#section_16": 2}, "innovation": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/state-space/kalman-filter.md": 7, "docs/en/state-space/kalman-filter.md#section_2": 1, "docs/en/state-space/kalman-filter.md#section_5": 1, "docs/en/state-space/kalman-filter.md#section_13": 5}, "recursive": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/forecasting/multi-step.md": 38, "docs/en/forecasting/multi-step.md#section_0": 3, "docs/en/forecasting/multi-step.md#section_1": 2, "docs/en/forecasting/multi-step.md#section_2": 1, "docs/en/forecasting/multi-step.md#section_4": 1, "docs/en/forecasting/multi-step.md#section_5": 3, "docs/en/forecasting/multi-step.md#section_6": 2, "docs/en/forecasting/multi-step.md#section_9": 3, "docs/en/forecasting/multi-step.md#section_13": 1, "docs/en/forecasting/multi-step.md#section_16": 1, "docs/en/forecasting/multi-step.md#section_17": 21, "docs/en/interview/interview-questions.md": 4, "docs/en/interview/interview-questions.md#section_9": 3, "docs/en/interview/interview-questions.md#section_11": 1}, "autocovariances": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1}, "conditional": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_6": 2, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "squares": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_1": 1}, "css": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1}, "minimize": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_6": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_5": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_7": 1}, "fast": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1}, "biased": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_4": 1}, "exact": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_6": 2, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_4": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/change-detection/change-point.md": 6, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/change-detection/change-point.md#section_4": 1, "docs/en/change-detection/change-point.md#section_14": 4, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "likelihood": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_6": 2, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_2": 1, "docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_0": 1, "docs/en/exponential-smoothing/ets.md#section_5": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/model-selection/information-criteria.md": 7, "docs/en/model-selection/information-criteria.md#section_1": 1, "docs/en/model-selection/information-criteria.md#section_3": 1, "docs/en/model-selection/information-criteria.md#section_7": 2, "docs/en/model-selection/information-criteria.md#section_13": 3, "docs/en/state-space/kalman-filter.md": 7, "docs/en/state-space/kalman-filter.md#section_5": 2, "docs/en/state-space/kalman-filter.md#section_13": 5}, "initial": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/arma.md": 3, "docs/en/time-domain/arma.md#section_3": 1, "docs/en/time-domain/arma.md#section_6": 2, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/exponential-smoothing/holt.md": 5, "docs/en/exponential-smoothing/holt.md#section_1": 2, "docs/en/exponential-smoothing/holt.md#section_14": 3, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_1": 1, "docs/en/exponential-smoothing/ses.md#section_10": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1}, "conditions": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/ar.md": 5, "docs/en/time-domain/ar.md#section_3": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_13": 2}, "direct": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/ar.md": 3, "docs/en/time-domain/ar.md#section_14": 3, "docs/en/forecasting/multi-step.md": 44, "docs/en/forecasting/multi-step.md#section_0": 3, "docs/en/forecasting/multi-step.md#section_1": 2, "docs/en/forecasting/multi-step.md#section_3": 1, "docs/en/forecasting/multi-step.md#section_4": 1, "docs/en/forecasting/multi-step.md#section_5": 2, "docs/en/forecasting/multi-step.md#section_6": 2, "docs/en/forecasting/multi-step.md#section_9": 6, "docs/en/forecasting/multi-step.md#section_10": 1, "docs/en/forecasting/multi-step.md#section_14": 1, "docs/en/forecasting/multi-step.md#section_16": 1, "docs/en/forecasting/multi-step.md#section_17": 22, "docs/en/forecasting/multi-step.md#section_18": 2, "docs/en/interview/interview-questions.md": 4, "docs/en/interview/interview-questions.md#section_2": 1, "docs/en/interview/interview-questions.md#section_9": 3, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_16": 2}, "asymptotically": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "challenges": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_0": 1}, "unlike": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_0": 1}, "optima": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_7": 1}, "good": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "starting": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "constraints": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "enforced": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "examine": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_8": 2, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "cutoff": {"docs/en/time-domain/ma.md": 3, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/ma.md#section_7": 2, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/foundations/autocorrelation.md": 6, "docs/en/foundations/autocorrelation.md#section_0": 2, "docs/en/foundations/autocorrelation.md#section_9": 2, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "candidate": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_6": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/arima.md": 3, "docs/en/time-domain/arima.md#section_8": 2, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_14": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/information-criteria.md#section_10": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_9": 1}, "enforce": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_7": 1}, "get": {"docs/en/time-domain/ma.md": 3, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_13": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_14": 2, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_11": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1, "docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_4": 1, "docs/en/decomposition/stl.md#section_13": 2, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_13": 2}, "difficulty": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_7": 1}, "harder": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_5": 1}, "poor": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "lead": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "convergence": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_3": 1, "docs/en/decomposition/stl.md#section_13": 1}, "innovations": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_7": 2, "docs/en/state-space/kalman-filter.md": 5, "docs/en/state-space/kalman-filter.md#section_5": 1, "docs/en/state-space/kalman-filter.md#section_13": 4}, "differencing": {"docs/en/time-domain/ma.md": 3, "docs/en/time-domain/ma.md#section_7": 3, "docs/en/time-domain/arima.md": 18, "docs/en/time-domain/arima.md#section_0": 2, "docs/en/time-domain/arima.md#section_1": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_9": 4, "docs/en/time-domain/arima.md#section_17": 10, "docs/en/time-domain/identification.md": 5, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/time-domain/identification.md#section_4": 1, "docs/en/time-domain/identification.md#section_6": 2, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 12, "docs/en/time-domain/sarima.md#section_0": 1, "docs/en/time-domain/sarima.md#section_1": 2, "docs/en/time-domain/sarima.md#section_7": 2, "docs/en/time-domain/sarima.md#section_8": 4, "docs/en/time-domain/sarima.md#section_15": 3, "docs/en/interview/interview-questions.md": 4, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/interview/interview-questions.md#section_3": 1, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/model-selection/residual-diagnostics.md": 9, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_5": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 5, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 13, "docs/en/foundations/stationarity.md#section_0": 1, "docs/en/foundations/stationarity.md#section_1": 1, "docs/en/foundations/stationarity.md#section_4": 1, "docs/en/foundations/stationarity.md#section_6": 2, "docs/en/foundations/stationarity.md#section_7": 4, "docs/en/foundations/stationarity.md#section_13": 4}, "negative": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/arima.md": 3, "docs/en/time-domain/arima.md#section_9": 2, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 4, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/ets.md#section_13": 3, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_3": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_10": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "spike": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 3, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/model-selection/residual-diagnostics.md": 4, "docs/en/model-selection/residual-diagnostics.md#section_6": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/anomaly-detection/anomaly-detection.md": 4, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_10": 3, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_9": 1}, "often": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 4, "docs/en/deep-learning/deep-learning-ts.md#section_8": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_8": 2, "docs/en/exponential-smoothing/holt.md": 6, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/exponential-smoothing/holt.md#section_14": 5, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 5, "docs/en/forecasting/multi-step.md#section_0": 1, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/forecasting/multi-step.md#section_17": 3, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 5, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 4, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_0": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "indicates": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_3": 2, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_5": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "misinterpreting": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "abrupt": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_10": 1}, "drop": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/features/feature-engineering.md": 4, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/features/feature-engineering.md#section_12": 2, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_10": 1}, "either": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_7": 1}, "flip": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ma.md#section_13": 1}, "reconsider": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "root": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/arma.md": 4, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_14": 3, "docs/en/time-domain/ar.md": 3, "docs/en/time-domain/ar.md#section_14": 3, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_4": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_0": 1, "docs/en/time-domain/identification.md#section_4": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/interview/interview-questions.md": 8, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/interview/interview-questions.md#section_3": 1, "docs/en/interview/interview-questions.md#section_16": 6, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_3": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_16": 2, "docs/en/foundations/stationarity.md": 16, "docs/en/foundations/stationarity.md#section_1": 1, "docs/en/foundations/stationarity.md#section_4": 3, "docs/en/foundations/stationarity.md#section_5": 4, "docs/en/foundations/stationarity.md#section_6": 1, "docs/en/foundations/stationarity.md#section_7": 2, "docs/en/foundations/stationarity.md#section_13": 4, "docs/en/foundations/stationarity.md#section_14": 1}, "boundary": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "breaks": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/model-selection/cross-validation.md": 3, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_7": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "down": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_7": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_15": 1}, "300": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_9": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_9": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_11": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_12": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_9": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_12": 1}, "theta1": {"docs/en/time-domain/ma.md": 8, "docs/en/time-domain/ma.md#section_9": 2, "docs/en/time-domain/ma.md#section_11": 3, "docs/en/time-domain/ma.md#section_12": 3}, "theta2": {"docs/en/time-domain/ma.md": 8, "docs/en/time-domain/ma.md#section_9": 2, "docs/en/time-domain/ma.md#section_11": 3, "docs/en/time-domain/ma.md#section_12": 3}, "eps": {"docs/en/time-domain/ma.md": 4, "docs/en/time-domain/ma.md#section_9": 4, "docs/en/time-domain/arma.md": 5, "docs/en/time-domain/arma.md#section_9": 5, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_9": 2, "docs/en/time-domain/arima.md": 5, "docs/en/time-domain/arima.md#section_11": 1, "docs/en/time-domain/arima.md#section_12": 4, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_9": 1, "docs/en/time-domain/identification.md#section_10": 2, "docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_9": 3, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_10": 3}, "cut": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_10": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_10": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1}, "nlags": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_10": 1, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_10": 2, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_10": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_13": 2, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_14": 1}, "round": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_10": 1, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_10": 2, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_10": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_13": 2, "docs/en/forecasting/multi-step.md": 4, "docs/en/forecasting/multi-step.md#section_16": 4, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_9": 1}, "maparams": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_11": 2, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_11": 1, "docs/en/time-domain/arma.md#section_12": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_15": 1}, "theoretical": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_12": 2, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_12": 1, "docs/en/forecasting/multi-step.md": 3, "docs/en/forecasting/multi-step.md#section_5": 1, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_10": 2, "docs/en/foundations/autocorrelation.md#section_17": 1}, "comparison": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_12": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_14": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_11": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/forecasting/multi-step.md": 4, "docs/en/forecasting/multi-step.md#section_5": 1, "docs/en/forecasting/multi-step.md#section_16": 1, "docs/en/forecasting/multi-step.md#section_18": 2, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_5": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "gamma0": {"docs/en/time-domain/ma.md": 3, "docs/en/time-domain/ma.md#section_12": 3}, "rho1": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_12": 2}, "rho2": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_12": 2}, "regardless": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_1": 1, "docs/en/decomposition/classical.md#section_12": 2}, "satisfied": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1}, "because": {"docs/en/time-domain/ma.md": 3, "docs/en/time-domain/ma.md#section_13": 3, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_5": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_17": 3, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/foundations/autocorrelation.md": 5, "docs/en/foundations/autocorrelation.md#section_16": 5, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "combinations": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/exponential-smoothing/ets.md": 7, "docs/en/exponential-smoothing/ets.md#section_7": 3, "docs/en/exponential-smoothing/ets.md#section_13": 4}, "preserve": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1}, "concept": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/practical/practical-modeling.md": 5, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/practical/practical-modeling.md#section_1": 1, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/practical/practical-modeling.md#section_14": 1}, "care": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_13": 2}, "unobservable": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1}, "convergent": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1}, "ensures": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_12": 2, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "enables": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_6": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1}, "computing": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_6": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "updates": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_2": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_3": 1}, "detail": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1}, "expansion": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_13": 2}, "diverges": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_13": 2}, "produce": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_9": 1}, "identical": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_3": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_1": 1}, "acfs": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_13": 2}, "forecasts": {"docs/en/time-domain/ma.md": 3, "docs/en/time-domain/ma.md#section_13": 3, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_7": 2, "docs/en/time-domain/arima.md": 7, "docs/en/time-domain/arima.md#section_7": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_9": 2, "docs/en/time-domain/arima.md#section_17": 3, "docs/en/exponential-smoothing/ets.md": 10, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/exponential-smoothing/ets.md#section_8": 2, "docs/en/exponential-smoothing/ets.md#section_13": 7, "docs/en/exponential-smoothing/holt.md": 7, "docs/en/exponential-smoothing/holt.md#section_0": 1, "docs/en/exponential-smoothing/holt.md#section_4": 1, "docs/en/exponential-smoothing/holt.md#section_6": 2, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/exponential-smoothing/holt.md#section_14": 2, "docs/en/exponential-smoothing/holt-winters.md": 3, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/exponential-smoothing/ses.md": 12, "docs/en/exponential-smoothing/ses.md#section_0": 2, "docs/en/exponential-smoothing/ses.md#section_6": 3, "docs/en/exponential-smoothing/ses.md#section_7": 2, "docs/en/exponential-smoothing/ses.md#section_13": 5, "docs/en/forecasting/prediction-intervals.md": 6, "docs/en/forecasting/prediction-intervals.md#section_6": 2, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_11": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/forecasting/multi-step.md": 7, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/forecasting/multi-step.md#section_8": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/forecasting/multi-step.md#section_16": 1, "docs/en/forecasting/multi-step.md#section_17": 3, "docs/en/interview/interview-questions.md": 4, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/interview/interview-questions.md#section_15": 3, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_5": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 5, "docs/en/model-selection/residual-diagnostics.md#section_17": 5, "docs/en/practical/practical-modeling.md": 13, "docs/en/practical/practical-modeling.md#section_1": 1, "docs/en/practical/practical-modeling.md#section_6": 4, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/practical/practical-modeling.md#section_9": 2, "docs/en/practical/practical-modeling.md#section_10": 3, "docs/en/practical/practical-modeling.md#section_11": 2, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "enforcing": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1}, "performs": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1}, "poorly": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1}, "satisfies": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_3": 1, "docs/en/time-domain/arma.md#section_14": 1}, "taking": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1}, "derivative": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1}, "setting": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "finds": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "equal": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "therefore": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "equality": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1}, "unlikely": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1}, "mixed": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_5": 1}, "since": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_1": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_5": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_13": 2}, "formula": {"docs/en/time-domain/ma.md": 2, "docs/en/time-domain/ma.md#section_13": 2, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_0": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_13": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_8": 1}, "convention": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1}, "comes": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1}, "term": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_4": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_13": 2, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_6": 2, "docs/en/model-selection/residual-diagnostics.md#section_11": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_13": 2}, "region": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1}, "833": {"docs/en/time-domain/ma.md": 4, "docs/en/time-domain/ma.md#section_13": 4}, "maybe": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "better": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/model-selection/cross-validation.md": 5, "docs/en/model-selection/cross-validation.md#section_3": 1, "docs/en/model-selection/cross-validation.md#section_11": 4, "docs/en/model-selection/information-criteria.md": 5, "docs/en/model-selection/information-criteria.md#section_0": 1, "docs/en/model-selection/information-criteria.md#section_6": 2, "docs/en/model-selection/information-criteria.md#section_13": 2, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_13": 2, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_14": 3}, "44": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1}, "694": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1}, "492": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1}, "plan": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1}, "enforces": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1}, "manually": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1}, "cdot": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/deep-learning/deep-learning-ts.md": 10, "docs/en/deep-learning/deep-learning-ts.md#section_1": 4, "docs/en/deep-learning/deep-learning-ts.md#section_3": 2, "docs/en/deep-learning/deep-learning-ts.md#section_5": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 3, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 6, "docs/en/exponential-smoothing/holt-winters.md#section_13": 6, "docs/en/exponential-smoothing/ses.md": 3, "docs/en/exponential-smoothing/ses.md#section_2": 1, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_4": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_13": 2}, "warnings": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "box": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_14": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_15": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/ar.md#section_15": 1, "docs/en/time-domain/arima.md": 3, "docs/en/time-domain/arima.md#section_8": 2, "docs/en/time-domain/arima.md#section_18": 1, "docs/en/time-domain/identification.md": 8, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/time-domain/identification.md#section_5": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_15": 3, "docs/en/time-domain/identification.md#section_16": 2, "docs/en/time-domain/sarima.md": 4, "docs/en/time-domain/sarima.md#section_4": 1, "docs/en/time-domain/sarima.md#section_13": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/interview/interview-questions.md#section_17": 1, "docs/en/model-selection/residual-diagnostics.md": 9, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_2": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 3, "docs/en/model-selection/residual-diagnostics.md#section_18": 2, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_17": 1}, "jenkins": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_14": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_15": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_18": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_16": 1, "docs/en/time-domain/sarima.md": 3, "docs/en/time-domain/sarima.md#section_4": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/interview/interview-questions.md#section_17": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_17": 1}, "reinsel": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_14": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_15": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_18": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_16": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_17": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_17": 1}, "ljung": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_14": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_15": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/ar.md#section_15": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_18": 1, "docs/en/time-domain/identification.md": 6, "docs/en/time-domain/identification.md#section_5": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/time-domain/identification.md#section_16": 2, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/interview/interview-questions.md#section_17": 1, "docs/en/model-selection/residual-diagnostics.md": 9, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_2": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 3, "docs/en/model-selection/residual-diagnostics.md#section_18": 2, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_17": 1}, "2015": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_14": 1, "docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_15": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_15": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_18": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_16": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_17": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_17": 1}, "wiley": {"docs/en/time-domain/ma.md": 1, "docs/en/time-domain/ma.md#section_14": 1, "docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_15": 2, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_15": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_18": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_16": 2, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_17": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_18": 2, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_17": 1}, "combines": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_0": 1}, "tail": {"docs/en/time-domain/arma.md": 4, "docs/en/time-domain/arma.md#section_0": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_10": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_0": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_2": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1}, "part": {"docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_0": 2, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1}, "parsimonious": {"docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_0": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_6": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_0": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_2": 1}, "present": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_0": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "assuming": {"docs/en/time-domain/arma.md": 3, "docs/en/time-domain/arma.md#section_2": 1, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "simplicity": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_2": 1}, "solving": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_2": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "differs": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_2": 1}, "recursion": {"docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_3": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_2": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "come": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_4": 1}, "psi": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_4": 1}, "redundancy": {"docs/en/time-domain/arma.md": 7, "docs/en/time-domain/arma.md#section_5": 2, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_12": 1, "docs/en/time-domain/arma.md#section_14": 3, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1}, "critical": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_5": 1, "docs/en/model-selection/residual-diagnostics.md": 4, "docs/en/model-selection/residual-diagnostics.md#section_2": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_4": 1}, "polynomials": {"docs/en/time-domain/arma.md": 3, "docs/en/time-domain/arma.md#section_5": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_1": 1, "docs/en/time-domain/sarima.md#section_15": 1}, "share": {"docs/en/time-domain/arma.md": 3, "docs/en/time-domain/arma.md#section_5": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/interview/interview-questions.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_5": 1}, "factors": {"docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_5": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_12": 3}, "5l": {"docs/en/time-domain/arma.md": 5, "docs/en/time-domain/arma.md#section_5": 2, "docs/en/time-domain/arma.md#section_14": 3}, "simplifies": {"docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_5": 1, "docs/en/time-domain/arma.md#section_6": 1}, "called": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_5": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1}, "likely": {"docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_6": 2, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_17": 3, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_16": 3}, "eacf": {"docs/en/time-domain/arma.md": 3, "docs/en/time-domain/arma.md#section_6": 3, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_3": 2}, "iteratively": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_3": 1}, "removing": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_2": 1, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_0": 1, "docs/en/foundations/autocorrelation.md#section_1": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "resulting": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "table": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_3": 1}, "indicating": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "maximize": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "properly": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_6": 1}, "default": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_6": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_13": 1}, "parameterization": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_7": 1}, "parsimony": {"docs/en/time-domain/arma.md": 3, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_7": 1}, "modes": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_7": 1}, "cancellation": {"docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_7": 1}, "close": {"docs/en/time-domain/arma.md": 3, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 3, "docs/en/exponential-smoothing/ses.md#section_6": 1, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_13": 2, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "canceling": {"docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/time-domain/arma.md#section_14": 1}, "inflated": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_7": 1}, "confusion": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_7": 1}, "overall": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_12": 2}, "arparams": {"docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_11": 1, "docs/en/time-domain/arma.md#section_12": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_15": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_12": 1}, "nphi": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_12": 1}, "abs": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_12": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_7": 2, "docs/en/practical/practical-modeling.md": 4, "docs/en/practical/practical-modeling.md#section_8": 2, "docs/en/practical/practical-modeling.md#section_12": 2, "docs/en/anomaly-detection/anomaly-detection.md": 6, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_11": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_12": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_14": 1}, "naic": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_13": 1}, "autoregressive": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_1": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "momentum": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "dissipate": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1}, "while": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_5": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_13": 3, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "fewer": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_4": 1}, "requiring": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1}, "well": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/multi-step.md": 4, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_12": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "approximated": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1}, "infinite": {"docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/time-domain/ar.md": 3, "docs/en/time-domain/ar.md#section_5": 3, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "achieving": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1}, "parsimoniously": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1}, "overfits": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1}, "simple": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_5": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/deep-learning/deep-learning-ts.md": 5, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 2, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md#section_13": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_1": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 3, "docs/en/exponential-smoothing/ses.md#section_0": 1, "docs/en/exponential-smoothing/ses.md#section_1": 1, "docs/en/interview/interview-questions.md": 4, "docs/en/interview/interview-questions.md#section_13": 1, "docs/en/interview/interview-questions.md#section_16": 3, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_3": 1, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 5, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_11": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_14": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_0": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "frequently": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_0": 1}, "sufficient": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/foundations/stationarity.md": 4, "docs/en/foundations/stationarity.md#section_0": 1, "docs/en/foundations/stationarity.md#section_7": 1, "docs/en/foundations/stationarity.md#section_13": 2}, "problematic": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_6": 1}, "occurs": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "causing": {"docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "cancel": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "sides": {"docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "problems": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_14": 1}, "becomes": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "nearly": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "singular": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1}, "hessian": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1}, "explode": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1}, "misleading": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "complexity": {"docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_2": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_0": 1, "docs/en/model-selection/information-criteria.md#section_13": 2, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_4": 1}, "suggest": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "suffices": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "leading": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_13": 2}, "multiply": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "take": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_6": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_6": 2, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "expectations": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "uncorrelated": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "itself": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "simply": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1}, "component": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_1": 1, "docs/en/decomposition/classical.md": 4, "docs/en/decomposition/classical.md#section_5": 1, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/classical.md#section_12": 2, "docs/en/decomposition/stl.md": 5, "docs/en/decomposition/stl.md#section_1": 2, "docs/en/decomposition/stl.md#section_10": 1, "docs/en/decomposition/stl.md#section_13": 2, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "modifies": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1}, "subsequent": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_4": 1}, "follow": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_0": 1}, "rightarrow": {"docs/en/time-domain/arma.md": 2, "docs/en/time-domain/arma.md#section_14": 2, "docs/en/time-domain/ar.md": 3, "docs/en/time-domain/ar.md#section_3": 1, "docs/en/time-domain/ar.md#section_14": 2}, "combined": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_4": 1, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_15": 2}, "improper": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1}, "several": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_14": 2}, "520": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1}, "525": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1}, "515": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1}, "514": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1}, "despite": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "having": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1}, "reasoning": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1}, "negligible": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1}, "principle": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1}, "simpler": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_7": 2, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/information-criteria.md": 5, "docs/en/model-selection/information-criteria.md#section_13": 5, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2}, "preferred": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_2": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_0": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "performance": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/model-selection/cross-validation.md": 3, "docs/en/model-selection/cross-validation.md#section_1": 1, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "stable": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_7": 2, "docs/en/exponential-smoothing/holt.md": 5, "docs/en/exponential-smoothing/holt.md#section_14": 5, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "interpretable": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "additional": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_5": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_13": 2, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_16": 2}, "decision": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_2": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_2": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "penalizes": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_2": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "accuracy": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/model-selection/cross-validation.md": 3, "docs/en/model-selection/cross-validation.md#section_3": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "candidates": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_0": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/model-selection/information-criteria.md": 4, "docs/en/model-selection/information-criteria.md#section_3": 1, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/information-criteria.md#section_10": 2, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_6": 3}, "blindly": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1}, "choosing": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/ses.md": 3, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "lowest": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_7": 1}, "meaningful": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_16": 2}, "holdout": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_14": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/practical/practical-modeling.md": 5, "docs/en/practical/practical-modeling.md#section_6": 3, "docs/en/practical/practical-modeling.md#section_7": 2, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "tsay": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_15": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1}, "2010": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_15": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1}, "financial": {"docs/en/time-domain/arma.md": 1, "docs/en/time-domain/arma.md#section_15": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_16": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1}, "plus": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "equivalently": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_2": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "sinusoidally": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_0": 1}, "yule": {"docs/en/time-domain/ar.md": 4, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/ar.md#section_3": 1, "docs/en/time-domain/ar.md#section_4": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_3": 2}, "walker": {"docs/en/time-domain/ar.md": 4, "docs/en/time-domain/ar.md#section_0": 1, "docs/en/time-domain/ar.md#section_3": 1, "docs/en/time-domain/ar.md#section_4": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_3": 2}, "hold": {"docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_3": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "monotonic": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_3": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_9": 1}, "damped": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_3": 1, "docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_1": 3, "docs/en/exponential-smoothing/holt.md": 14, "docs/en/exponential-smoothing/holt.md#section_1": 1, "docs/en/exponential-smoothing/holt.md#section_4": 1, "docs/en/exponential-smoothing/holt.md#section_6": 1, "docs/en/exponential-smoothing/holt.md#section_7": 3, "docs/en/exponential-smoothing/holt.md#section_12": 2, "docs/en/exponential-smoothing/holt.md#section_13": 1, "docs/en/exponential-smoothing/holt.md#section_14": 5, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_5": 2}, "vdots": {"docs/en/time-domain/ar.md": 4, "docs/en/time-domain/ar.md#section_4": 4, "docs/en/foundations/autocorrelation.md": 4, "docs/en/foundations/autocorrelation.md#section_3": 4}, "ddots": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_4": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_3": 1}, "autocorrelations": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_4": 1, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_2": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 2}, "memory": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_5": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1}, "decaying": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_5": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_0": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "weights": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_5": 1, "docs/en/deep-learning/deep-learning-ts.md": 4, "docs/en/deep-learning/deep-learning-ts.md#section_4": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 3, "docs/en/exponential-smoothing/ses.md": 5, "docs/en/exponential-smoothing/ses.md#section_0": 1, "docs/en/exponential-smoothing/ses.md#section_2": 2, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_6": 2, "docs/en/decomposition/stl.md": 9, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_2": 1, "docs/en/decomposition/stl.md#section_3": 1, "docs/en/decomposition/stl.md#section_4": 2, "docs/en/decomposition/stl.md#section_5": 1, "docs/en/decomposition/stl.md#section_13": 3}, "moments": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/foundations/stationarity.md": 3, "docs/en/foundations/stationarity.md#section_13": 3}, "replace": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1}, "solve": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1}, "yields": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1}, "inefficient": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "ordinary": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1}, "regress": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "loses": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_4": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "distributional": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "assumption": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_0": 1}, "gaussian": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_2": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_4": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_4": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_0": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "numerical": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_5": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1}, "spikes": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 2, "docs/en/foundations/autocorrelation.md": 6, "docs/en/foundations/autocorrelation.md#section_9": 1, "docs/en/foundations/autocorrelation.md#section_10": 2, "docs/en/foundations/autocorrelation.md#section_16": 3}, "criterion": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_6": 1, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_2": 2, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 5, "docs/en/deep-learning/deep-learning-ts.md#section_7": 2, "docs/en/deep-learning/deep-learning-ts.md#section_13": 1, "docs/en/deep-learning/deep-learning-ts.md#section_14": 1, "docs/en/deep-learning/deep-learning-ts.md#section_15": 1, "docs/en/model-selection/information-criteria.md": 4, "docs/en/model-selection/information-criteria.md#section_1": 2, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "satisfy": {"docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "leads": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "explosive": {"docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1}, "favor": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "larger": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_0": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "predicting": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "neglecting": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "invalid": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "hac": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1}, "alternating": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1}, "signs": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1}, "reversion": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_7": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "autoreg": {"docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_8": 1, "docs/en/time-domain/ar.md#section_12": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_13": 2, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_14": 2}, "phi1": {"docs/en/time-domain/ar.md": 5, "docs/en/time-domain/ar.md#section_9": 2, "docs/en/time-domain/ar.md#section_12": 3, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_9": 1, "docs/en/time-domain/identification.md#section_10": 1}, "phi2": {"docs/en/time-domain/ar.md": 5, "docs/en/time-domain/ar.md#section_9": 2, "docs/en/time-domain/ar.md#section_12": 3, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_9": 1, "docs/en/time-domain/identification.md#section_10": 1}, "sel": {"docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_11": 2}, "ic": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "selected": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_6": 1}, "params": {"docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_12": 2, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_11": 2, "docs/en/exponential-smoothing/holt.md": 3, "docs/en/exponential-smoothing/holt.md#section_10": 2, "docs/en/exponential-smoothing/holt.md#section_12": 1, "docs/en/exponential-smoothing/holt-winters.md": 3, "docs/en/exponential-smoothing/holt-winters.md#section_11": 3, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_10": 2, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "intuitively": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1}, "keeping": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1}, "bounded": {"docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/forecasting/prediction-intervals.md": 3, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "walk": {"docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/arima.md": 5, "docs/en/time-domain/arima.md#section_2": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/arima.md#section_17": 3, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/ses.md": 3, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/forecasting/prediction-intervals.md": 5, "docs/en/forecasting/prediction-intervals.md#section_3": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 4, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_1": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 5, "docs/en/foundations/stationarity.md#section_4": 1, "docs/en/foundations/stationarity.md#section_9": 1, "docs/en/foundations/stationarity.md#section_11": 1, "docs/en/foundations/stationarity.md#section_12": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "persist": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "forever": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "explodes": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "99": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/forecasting/prediction-intervals.md": 4, "docs/en/forecasting/prediction-intervals.md#section_13": 4}, "enough": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "technically": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1}, "behave": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1}, "walks": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1}, "predictions": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_5": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/forecasting/multi-step.md": 3, "docs/en/forecasting/multi-step.md#section_1": 1, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "degrade": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1}, "quickly": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "gradually": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "controlling": {"docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_14": 2}, "intermediate": {"docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_2": 1, "docs/en/foundations/autocorrelation.md": 4, "docs/en/foundations/autocorrelation.md#section_0": 1, "docs/en/foundations/autocorrelation.md#section_1": 1, "docs/en/foundations/autocorrelation.md#section_16": 2}, "definition": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1}, "dependence": {"docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_1": 1}, "includes": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/features/feature-engineering.md": 4, "docs/en/features/feature-engineering.md#section_16": 4, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "indirect": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_2": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "through": {"docs/en/time-domain/ar.md": 4, "docs/en/time-domain/ar.md#section_14": 4, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_2": 1, "docs/en/forecasting/multi-step.md#section_4": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_2": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_16": 3, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_0": 1}, "correlates": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1}, "chain": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1}, "perfectly": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1}, "ll": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1}, "nonzero": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "confidence": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/foundations/autocorrelation.md": 5, "docs/en/foundations/autocorrelation.md#section_8": 1, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_16": 3}, "bands": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/foundations/autocorrelation.md": 7, "docs/en/foundations/autocorrelation.md#section_8": 1, "docs/en/foundations/autocorrelation.md#section_9": 1, "docs/en/foundations/autocorrelation.md#section_10": 2, "docs/en/foundations/autocorrelation.md#section_16": 3}, "judge": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1}, "making": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_6": 1}, "volatile": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/exponential-smoothing/holt.md": 3, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/exponential-smoothing/holt.md#section_14": 2, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "plane": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1}, "vertices": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1}, "define": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_6": 1}, "discriminant": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1}, "passes": {"docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_14": 2}, "connects": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1}, "simultaneously": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_1": 1}, "two": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/exponential-smoothing/holt.md": 3, "docs/en/exponential-smoothing/holt.md#section_0": 2, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "fail": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 4, "docs/en/time-domain/identification.md#section_4": 4, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_6": 1}, "third": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1}, "25": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/features/feature-engineering.md": 5, "docs/en/features/feature-engineering.md#section_5": 2, "docs/en/features/feature-engineering.md#section_11": 2, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_12": 1}, "misspecification": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 6, "docs/en/forecasting/multi-step.md#section_5": 2, "docs/en/forecasting/multi-step.md#section_17": 4, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_7": 1}, "instead": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_8": 2, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "break": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_4": 1, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_14": 3, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_1": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_7": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "outliers": {"docs/en/time-domain/ar.md": 2, "docs/en/time-domain/ar.md#section_14": 2, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 6, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_0": 1, "docs/en/decomposition/stl.md": 13, "docs/en/decomposition/stl.md#section_0": 1, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_4": 1, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/decomposition/stl.md#section_8": 1, "docs/en/decomposition/stl.md#section_9": 1, "docs/en/decomposition/stl.md#section_11": 1, "docs/en/decomposition/stl.md#section_13": 6, "docs/en/change-detection/change-point.md": 4, "docs/en/change-detection/change-point.md#section_14": 4}, "fully": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "addressed": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1}, "plot": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_8": 2, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_6": 2, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/holt-winters.md": 3, "docs/en/exponential-smoothing/holt-winters.md#section_2": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md": 9, "docs/en/model-selection/residual-diagnostics.md#section_0": 2, "docs/en/model-selection/residual-diagnostics.md#section_6": 6, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_12": 2, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_9": 1, "docs/en/foundations/autocorrelation.md#section_13": 1, "docs/en/foundations/stationarity.md": 4, "docs/en/foundations/stationarity.md#section_6": 2, "docs/en/foundations/stationarity.md#section_13": 2}, "shifts": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 3, "docs/en/exponential-smoothing/ses.md#section_13": 3, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_14": 2, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_1": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "original": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "hasn": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "captured": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/decomposition/stl.md#section_13": 1}, "fix": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/deep-learning/deep-learning-ts.md": 7, "docs/en/deep-learning/deep-learning-ts.md#section_17": 7, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 2}, "sometimes": {"docs/en/time-domain/ar.md": 1, "docs/en/time-domain/ar.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1}, "extends": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_0": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_0": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_0": 1}, "stands": {"docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_0": 1, "docs/en/time-domain/arima.md#section_17": 1}, "meaning": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_0": 1}, "needs": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_0": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_0": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_4": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "become": {"docs/en/time-domain/arima.md": 3, "docs/en/time-domain/arima.md#section_0": 1, "docs/en/time-domain/arima.md#section_1": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1}, "differenced": {"docs/en/time-domain/arima.md": 11, "docs/en/time-domain/arima.md#section_0": 1, "docs/en/time-domain/arima.md#section_4": 1, "docs/en/time-domain/arima.md#section_8": 2, "docs/en/time-domain/arima.md#section_9": 2, "docs/en/time-domain/arima.md#section_12": 1, "docs/en/time-domain/arima.md#section_14": 1, "docs/en/time-domain/arima.md#section_17": 3, "docs/en/time-domain/identification.md": 4, "docs/en/time-domain/identification.md#section_10": 1, "docs/en/time-domain/identification.md#section_12": 2, "docs/en/time-domain/identification.md#section_13": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_12": 1}, "commonly": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_0": 1}, "th": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_1": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "degree": {"docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_1": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_1": 1}, "nabla": {"docs/en/time-domain/arima.md": 6, "docs/en/time-domain/arima.md#section_1": 2, "docs/en/time-domain/arima.md#section_2": 1, "docs/en/time-domain/arima.md#section_4": 2, "docs/en/time-domain/arima.md#section_5": 1}, "drift": {"docs/en/time-domain/arima.md": 6, "docs/en/time-domain/arima.md#section_3": 2, "docs/en/time-domain/arima.md#section_7": 1, "docs/en/time-domain/arima.md#section_9": 2, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/exponential-smoothing/holt-winters.md": 3, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/practical/practical-modeling.md": 6, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/practical/practical-modeling.md#section_1": 1, "docs/en/practical/practical-modeling.md#section_13": 3, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_13": 2, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "ct": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_3": 1}, "expanding": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_4": 1, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_4": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_2": 1, "docs/en/model-selection/cross-validation.md": 6, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/cross-validation.md#section_1": 1, "docs/en/model-selection/cross-validation.md#section_11": 4, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_1": 1}, "ima": {"docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_5": 1, "docs/en/time-domain/arima.md#section_17": 1}, "weighted": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_5": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_0": 1, "docs/en/exponential-smoothing/ses.md#section_1": 1, "docs/en/decomposition/stl.md": 4, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_2": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/decomposition/stl.md#section_14": 1}, "ewma": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_5": 1}, "forms": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_5": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_1": 1}, "basis": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_5": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1}, "notation": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_6": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_1": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "contributes": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_6": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "property": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_7": 1}, "revert": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_7": 1}, "growth": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_7": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_5": 1, "docs/en/change-detection/change-point.md#section_13": 1}, "widen": {"docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_7": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "accumulated": {"docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_7": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_0": 1}, "uncertainty": {"docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_7": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_5": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 9, "docs/en/forecasting/prediction-intervals.md#section_0": 2, "docs/en/forecasting/prediction-intervals.md#section_5": 4, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/forecasting/multi-step.md": 3, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/practical/practical-modeling.md": 4, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/state-space/kalman-filter.md": 7, "docs/en/state-space/kalman-filter.md#section_2": 1, "docs/en/state-space/kalman-filter.md#section_13": 6, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "methodology": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "adf": {"docs/en/time-domain/arima.md": 3, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_14": 2, "docs/en/time-domain/identification.md": 5, "docs/en/time-domain/identification.md#section_0": 1, "docs/en/time-domain/identification.md#section_4": 2, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_11": 1, "docs/en/interview/interview-questions.md": 5, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/interview/interview-questions.md#section_16": 4, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 15, "docs/en/foundations/stationarity.md#section_0": 1, "docs/en/foundations/stationarity.md#section_5": 2, "docs/en/foundations/stationarity.md#section_6": 1, "docs/en/foundations/stationarity.md#section_7": 3, "docs/en/foundations/stationarity.md#section_11": 3, "docs/en/foundations/stationarity.md#section_12": 1, "docs/en/foundations/stationarity.md#section_13": 4}, "kpss": {"docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/identification.md": 9, "docs/en/time-domain/identification.md#section_0": 1, "docs/en/time-domain/identification.md#section_4": 2, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_8": 1, "docs/en/time-domain/identification.md#section_11": 2, "docs/en/time-domain/identification.md#section_12": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 10, "docs/en/foundations/stationarity.md#section_0": 1, "docs/en/foundations/stationarity.md#section_5": 2, "docs/en/foundations/stationarity.md#section_6": 1, "docs/en/foundations/stationarity.md#section_7": 1, "docs/en/foundations/stationarity.md#section_8": 1, "docs/en/foundations/stationarity.md#section_13": 4}, "until": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_3": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_3": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_9": 1}, "orders": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_0": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_0": 1}, "determining": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_1": 1}, "symptom": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_6": 1}, "wanders": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1}, "persists": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1}, "already": {"docs/en/time-domain/arima.md": 3, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "fluctuates": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_2": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "around": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_8": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_2": 1, "docs/en/change-detection/change-point.md#section_14": 2}, "introduces": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_4": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "misspecifying": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1}, "long": {"docs/en/time-domain/arima.md": 4, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/arima.md#section_17": 3, "docs/en/deep-learning/deep-learning-ts.md": 11, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md#section_2": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 5, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1, "docs/en/exponential-smoothing/holt.md": 6, "docs/en/exponential-smoothing/holt.md#section_1": 1, "docs/en/exponential-smoothing/holt.md#section_6": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/exponential-smoothing/holt.md#section_14": 3, "docs/en/forecasting/prediction-intervals.md": 3, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/forecasting/multi-step.md": 3, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/interview/interview-questions.md#section_13": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_1": 1}, "rarely": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "transformation": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "types": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_1": 1}, "deterministic": {"docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_13": 2}, "regression": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_11": 1, "docs/en/time-domain/identification.md#section_12": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_6": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/information-criteria.md#section_14": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/decomposition/stl.md": 5, "docs/en/decomposition/stl.md#section_0": 1, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_14": 3, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_3": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_1": 1}, "stochastic": {"docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_13": 2}, "counts": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_5": 1}, "negatives": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1}, "constrained": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_9": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1}, "adfuller": {"docs/en/time-domain/arima.md": 3, "docs/en/time-domain/arima.md#section_10": 1, "docs/en/time-domain/arima.md#section_14": 2, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_8": 1, "docs/en/time-domain/identification.md#section_11": 1, "docs/en/time-domain/identification.md#section_12": 1, "docs/en/foundations/stationarity.md": 4, "docs/en/foundations/stationarity.md#section_8": 1, "docs/en/foundations/stationarity.md#section_11": 2, "docs/en/foundations/stationarity.md#section_12": 1}, "dx": {"docs/en/time-domain/arima.md": 5, "docs/en/time-domain/arima.md#section_12": 4, "docs/en/time-domain/arima.md#section_13": 1, "docs/en/time-domain/identification.md": 5, "docs/en/time-domain/identification.md#section_10": 5}, "integrate": {"docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_13": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_10": 1}, "cumsum": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_13": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_10": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_11": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_8": 1, "docs/en/practical/practical-modeling.md": 3, "docs/en/practical/practical-modeling.md#section_8": 3, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_9": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_9": 1}, "diff": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_14": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_12": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_7": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_12": 1}, "n10": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_16": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_12": 1}, "2f": {"docs/en/time-domain/arima.md": 3, "docs/en/time-domain/arima.md#section_16": 3, "docs/en/exponential-smoothing/holt.md": 3, "docs/en/exponential-smoothing/holt.md#section_11": 2, "docs/en/exponential-smoothing/holt.md#section_12": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_10": 1, "docs/en/forecasting/prediction-intervals.md": 5, "docs/en/forecasting/prediction-intervals.md#section_11": 3, "docs/en/forecasting/prediction-intervals.md#section_12": 2, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_14": 1, "docs/en/practical/practical-modeling.md": 5, "docs/en/practical/practical-modeling.md#section_10": 5, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_14": 1, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_11": 2, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_10": 1}, "95": {"docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_16": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_8": 2, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/forecasting/prediction-intervals.md": 12, "docs/en/forecasting/prediction-intervals.md#section_4": 1, "docs/en/forecasting/prediction-intervals.md#section_6": 1, "docs/en/forecasting/prediction-intervals.md#section_11": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 8, "docs/en/practical/practical-modeling.md": 5, "docs/en/practical/practical-modeling.md#section_5": 2, "docs/en/practical/practical-modeling.md#section_11": 1, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_8": 1, "docs/en/foundations/autocorrelation.md#section_16": 2}, "ci": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_16": 1}, "iloc": {"docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_16": 2, "docs/en/exponential-smoothing/holt.md": 3, "docs/en/exponential-smoothing/holt.md#section_11": 2, "docs/en/exponential-smoothing/holt.md#section_12": 1, "docs/en/forecasting/prediction-intervals.md": 5, "docs/en/forecasting/prediction-intervals.md#section_11": 3, "docs/en/forecasting/prediction-intervals.md#section_12": 2, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_12": 2}, "stand": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1}, "denoted": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1}, "exactly": {"docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_15": 3, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "inverse": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1}, "continuous": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "analogy": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "discrete": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_16": 2}, "summed": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1}, "trends": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/exponential-smoothing/holt.md": 5, "docs/en/exponential-smoothing/holt.md#section_0": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt.md#section_15": 2, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_2": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_6": 1}, "tell": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1}, "looks": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "corrected": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_2": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_6": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_1": 1}, "excessive": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1}, "alternation": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1}, "adds": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_0": 1, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_5": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/information-criteria.md": 4, "docs/en/model-selection/information-criteria.md#section_13": 4, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "behaves": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1}, "everyone": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1}, "especially": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "alpha": {"docs/en/time-domain/arima.md": 7, "docs/en/time-domain/arima.md#section_17": 7, "docs/en/exponential-smoothing/ets.md": 7, "docs/en/exponential-smoothing/ets.md#section_3": 2, "docs/en/exponential-smoothing/ets.md#section_4": 1, "docs/en/exponential-smoothing/ets.md#section_13": 4, "docs/en/exponential-smoothing/holt.md": 7, "docs/en/exponential-smoothing/holt.md#section_1": 3, "docs/en/exponential-smoothing/holt.md#section_3": 1, "docs/en/exponential-smoothing/holt.md#section_5": 2, "docs/en/exponential-smoothing/holt.md#section_10": 1, "docs/en/exponential-smoothing/holt-winters.md": 4, "docs/en/exponential-smoothing/holt-winters.md#section_1": 4, "docs/en/exponential-smoothing/ses.md": 58, "docs/en/exponential-smoothing/ses.md#section_0": 3, "docs/en/exponential-smoothing/ses.md#section_1": 8, "docs/en/exponential-smoothing/ses.md#section_2": 14, "docs/en/exponential-smoothing/ses.md#section_3": 2, "docs/en/exponential-smoothing/ses.md#section_4": 3, "docs/en/exponential-smoothing/ses.md#section_5": 1, "docs/en/exponential-smoothing/ses.md#section_6": 2, "docs/en/exponential-smoothing/ses.md#section_10": 1, "docs/en/exponential-smoothing/ses.md#section_11": 5, "docs/en/exponential-smoothing/ses.md#section_13": 19, "docs/en/forecasting/prediction-intervals.md": 5, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/forecasting/prediction-intervals.md#section_1": 3, "docs/en/forecasting/prediction-intervals.md#section_11": 1, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_16": 3, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_3": 2, "docs/en/state-space/kalman-filter.md": 3, "docs/en/state-space/kalman-filter.md#section_12": 1, "docs/en/state-space/kalman-filter.md#section_13": 2, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_5": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "recursively": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_0": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_3": 1}, "update": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_3": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_11": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 5, "docs/en/state-space/kalman-filter.md#section_0": 1, "docs/en/state-space/kalman-filter.md#section_2": 1, "docs/en/state-space/kalman-filter.md#section_5": 1, "docs/en/state-space/kalman-filter.md#section_7": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_5": 1}, "rearranging": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1}, "correspond": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1}, "unknown": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "effect": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_2": 2, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "grows": {"docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_5": 1, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 5, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/forecasting/prediction-intervals.md#section_3": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_1": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_5": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_13": 2}, "permanent": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1}, "bound": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/forecasting/prediction-intervals.md": 5, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 4, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "96": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_1": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 3, "docs/en/exponential-smoothing/holt-winters.md#section_9": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/forecasting/prediction-intervals.md": 4, "docs/en/forecasting/prediction-intervals.md#section_4": 1, "docs/en/forecasting/prediction-intervals.md#section_12": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_15": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_11": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 4, "docs/en/foundations/autocorrelation.md#section_8": 1, "docs/en/foundations/autocorrelation.md#section_9": 1, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "interval": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 4, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/ets.md#section_13": 3, "docs/en/exponential-smoothing/holt-winters.md": 3, "docs/en/exponential-smoothing/holt-winters.md#section_13": 3, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_4": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 6, "docs/en/forecasting/prediction-intervals.md#section_1": 1, "docs/en/forecasting/prediction-intervals.md#section_6": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 3, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_5": 1, "docs/en/practical/practical-modeling.md#section_12": 1}, "width": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 8, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/forecasting/prediction-intervals.md#section_3": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_12": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 3, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "becoming": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1}, "arbitrarily": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1}, "narrow": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 3, "docs/en/practical/practical-modeling.md#section_13": 3}, "provide": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_9": 1}, "tight": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_9": 1}, "judgment": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "scenarios": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1}, "matter": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "planning": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_15": 1}, "showing": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1}, "clear": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "upward": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1}, "13": {"docs/en/time-domain/arima.md": 2, "docs/en/time-domain/arima.md#section_17": 2, "docs/en/time-domain/sarima.md": 9, "docs/en/time-domain/sarima.md#section_4": 3, "docs/en/time-domain/sarima.md#section_6": 1, "docs/en/time-domain/sarima.md#section_15": 5, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_14": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "appropriate": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_9": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "handles": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/sarima.md": 7, "docs/en/time-domain/sarima.md#section_4": 2, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_15": 4, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_0": 1, "docs/en/decomposition/stl.md#section_13": 1}, "interaction": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/sarima.md": 5, "docs/en/time-domain/sarima.md#section_5": 1, "docs/en/time-domain/sarima.md#section_6": 1, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/time-domain/sarima.md#section_15": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_13": 1}, "airline": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/sarima.md": 7, "docs/en/time-domain/sarima.md#section_0": 1, "docs/en/time-domain/sarima.md#section_4": 2, "docs/en/time-domain/sarima.md#section_15": 4}, "next": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1}, "remaining": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1}, "degrading": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_17": 1}, "control": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_18": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_14": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_17": 1}, "17": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_18": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "hyndman": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_18": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_16": 1, "docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_16": 2, "docs/en/exponential-smoothing/ets.md": 4, "docs/en/exponential-smoothing/ets.md#section_14": 4, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_14": 2, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_5": 1, "docs/en/forecasting/multi-step.md#section_18": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_17": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_13": 2, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_14": 2}, "athanasopoulos": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_18": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_16": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_14": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_17": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_13": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1}, "2021": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_18": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_16": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_14": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_17": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_13": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1}, "principles": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_18": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_16": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_14": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_17": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_13": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1}, "practice": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_18": 1, "docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/identification.md#section_16": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/exponential-smoothing/ses.md#section_14": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/interview/interview-questions.md#section_17": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_13": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_1": 1}, "otexts": {"docs/en/time-domain/arima.md": 1, "docs/en/time-domain/arima.md#section_18": 1, "docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_16": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_14": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_17": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_13": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1}, "determines": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_0": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_10": 1}, "cox": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_1": 1}, "tails": {"docs/en/time-domain/identification.md": 5, "docs/en/time-domain/identification.md#section_1": 4, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 4, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 3, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 4, "docs/en/foundations/autocorrelation.md#section_9": 2, "docs/en/foundations/autocorrelation.md#section_10": 2}, "akaike": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_2": 1, "docs/en/model-selection/information-criteria.md": 4, "docs/en/model-selection/information-criteria.md#section_1": 1, "docs/en/model-selection/information-criteria.md#section_2": 1, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/information-criteria.md#section_14": 1}, "ln": {"docs/en/time-domain/identification.md": 7, "docs/en/time-domain/identification.md#section_2": 4, "docs/en/time-domain/identification.md#section_15": 3, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_6": 1, "docs/en/model-selection/information-criteria.md": 27, "docs/en/model-selection/information-criteria.md#section_1": 3, "docs/en/model-selection/information-criteria.md#section_2": 4, "docs/en/model-selection/information-criteria.md#section_3": 3, "docs/en/model-selection/information-criteria.md#section_4": 3, "docs/en/model-selection/information-criteria.md#section_13": 14}, "2k": {"docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_2": 2, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_6": 1, "docs/en/model-selection/information-criteria.md": 18, "docs/en/model-selection/information-criteria.md#section_0": 1, "docs/en/model-selection/information-criteria.md#section_1": 2, "docs/en/model-selection/information-criteria.md#section_2": 1, "docs/en/model-selection/information-criteria.md#section_4": 1, "docs/en/model-selection/information-criteria.md#section_5": 4, "docs/en/model-selection/information-criteria.md#section_13": 9, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "bayesian": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_2": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_1": 1, "docs/en/change-detection/change-point.md": 7, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/change-detection/change-point.md#section_5": 1, "docs/en/change-detection/change-point.md#section_13": 2, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "schwarz": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_2": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_1": 1, "docs/en/model-selection/information-criteria.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_2": 1}, "heavily": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_2": 1}, "aicc": {"docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_2": 2, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_6": 2, "docs/en/model-selection/information-criteria.md": 17, "docs/en/model-selection/information-criteria.md#section_0": 1, "docs/en/model-selection/information-criteria.md#section_1": 2, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/information-criteria.md#section_13": 12}, "removes": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_3": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_0": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_3": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_4": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "rows": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_3": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "columns": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_3": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_15": 1}, "insignificant": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_3": 1}, "top": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_3": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_15": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1}, "corner": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_3": 1}, "triangle": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_3": 1}, "strategy": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_4": 1, "docs/en/forecasting/multi-step.md": 10, "docs/en/forecasting/multi-step.md#section_2": 1, "docs/en/forecasting/multi-step.md#section_3": 1, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/forecasting/multi-step.md#section_13": 1, "docs/en/forecasting/multi-step.md#section_14": 1, "docs/en/forecasting/multi-step.md#section_15": 1, "docs/en/forecasting/multi-step.md#section_17": 3, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_0": 1}, "conclusion": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_4": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "05": {"docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_4": 1, "docs/en/time-domain/identification.md#section_6": 2, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_10": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md": 6, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 3, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_8": 1}, "inconclusive": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_4": 1}, "null": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_5": 1, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_1": 2, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_8": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "complete": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_3": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "preliminary": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_6": 1}, "obvious": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_6": 1}, "changes": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_7": 2, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 3, "docs/en/exponential-smoothing/ses.md#section_6": 1, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/practical/practical-modeling.md": 3, "docs/en/practical/practical-modeling.md#section_1": 1, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_13": 2, "docs/en/change-detection/change-point.md": 12, "docs/en/change-detection/change-point.md#section_1": 2, "docs/en/change-detection/change-point.md#section_3": 1, "docs/en/change-detection/change-point.md#section_7": 5, "docs/en/change-detection/change-point.md#section_14": 4}, "final": {"docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/state-space/kalman-filter.md": 3, "docs/en/state-space/kalman-filter.md#section_11": 3}, "among": {"docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_3": 1}, "auto": {"docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_6": 2, "docs/en/time-domain/identification.md#section_7": 1}, "tools": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_6": 1}, "package": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_6": 1}, "pmdarima": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_6": 1}, "stepwise": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_6": 1}, "search": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_6": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_3": 1}, "mechanical": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1}, "clean": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1}, "noisy": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "interpretations": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1}, "relying": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "automated": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1}, "features": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/forecasting/multi-step.md": 6, "docs/en/forecasting/multi-step.md#section_9": 3, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/interview/interview-questions.md": 6, "docs/en/interview/interview-questions.md#section_3": 1, "docs/en/interview/interview-questions.md#section_8": 4, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/practical/practical-modeling.md": 7, "docs/en/practical/practical-modeling.md#section_1": 1, "docs/en/practical/practical-modeling.md#section_6": 3, "docs/en/practical/practical-modeling.md#section_13": 3, "docs/en/features/feature-engineering.md": 73, "docs/en/features/feature-engineering.md#section_0": 3, "docs/en/features/feature-engineering.md#section_1": 4, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_5": 27, "docs/en/features/feature-engineering.md#section_8": 4, "docs/en/features/feature-engineering.md#section_11": 17, "docs/en/features/feature-engineering.md#section_12": 2, "docs/en/features/feature-engineering.md#section_15": 1, "docs/en/features/feature-engineering.md#section_16": 14, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "autocorrelated": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_17": 1}, "unreliable": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "conservative": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md": 3, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "pandas": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_8": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_9": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_11": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_9": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "pd": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_8": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_9": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_11": 2, "docs/en/features/feature-engineering.md": 4, "docs/en/features/feature-engineering.md#section_9": 1, "docs/en/features/feature-engineering.md#section_10": 2, "docs/en/features/feature-engineering.md#section_15": 1}, "stats": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_8": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 4, "docs/en/model-selection/residual-diagnostics.md#section_8": 3, "docs/en/model-selection/residual-diagnostics.md#section_14": 1, "docs/en/anomaly-detection/anomaly-detection.md": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_8": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_11": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_14": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_13": 1}, "exercise": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_9": 1}, "nadf": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_12": 1}, "nstep": {"docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_13": 1, "docs/en/time-domain/identification.md#section_14": 1}, "name": {"docs/en/time-domain/identification.md": 3, "docs/en/time-domain/identification.md#section_14": 3, "docs/en/exponential-smoothing/ets.md": 5, "docs/en/exponential-smoothing/ets.md#section_11": 5, "docs/en/model-selection/cross-validation.md": 5, "docs/en/model-selection/cross-validation.md#section_9": 3, "docs/en/model-selection/cross-validation.md#section_10": 2, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_10": 2}, "items": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_14": 1, "docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_11": 2, "docs/en/exponential-smoothing/ets.md#section_12": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_10": 1}, "resid": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_14": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 8, "docs/en/model-selection/residual-diagnostics.md#section_11": 2, "docs/en/model-selection/residual-diagnostics.md#section_12": 1, "docs/en/model-selection/residual-diagnostics.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md#section_14": 1, "docs/en/model-selection/residual-diagnostics.md#section_15": 2, "docs/en/model-selection/residual-diagnostics.md#section_16": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_14": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_10": 2, "docs/en/decomposition/stl.md": 4, "docs/en/decomposition/stl.md#section_10": 1, "docs/en/decomposition/stl.md#section_11": 2, "docs/en/decomposition/stl.md#section_13": 1}, "except": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_14": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_11": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_9": 1}, "failed": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_14": 1}, "involves": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_0": 1}, "though": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "dependencies": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_13": 1}, "versus": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1}, "lightly": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1}, "penalty": {"docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/model-selection/information-criteria.md": 10, "docs/en/model-selection/information-criteria.md#section_5": 4, "docs/en/model-selection/information-criteria.md#section_13": 6, "docs/en/change-detection/change-point.md": 9, "docs/en/change-detection/change-point.md#section_4": 1, "docs/en/change-detection/change-point.md#section_6": 2, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_14": 5}, "preference": {"docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_15": 2}, "situations": {"docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_15": 2}, "sizes": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "optimizes": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_6": 1}, "beats": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "guides": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1}, "arbiters": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1}, "approximate": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_5": 1, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_0": 1, "docs/en/foundations/autocorrelation.md#section_8": 1}, "bartlett": {"docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_8": 1, "docs/en/foundations/autocorrelation.md#section_17": 1}, "approximation": {"docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "approximately": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_2": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_1": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "holds": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "expressions": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1}, "contribute": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "precise": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1}, "pierce": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1}, "modified": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/anomaly-detection/anomaly-detection.md": 6, "docs/en/anomaly-detection/anomaly-detection.md#section_2": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 5, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "improves": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_12": 1}, "undersized": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1}, "choices": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "2s": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 3, "docs/en/time-domain/sarima.md#section_5": 2, "docs/en/time-domain/sarima.md#section_7": 1}, "slowly": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "drops": {"docs/en/time-domain/identification.md": 2, "docs/en/time-domain/identification.md#section_15": 2, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_1": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "favors": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/model-selection/information-criteria.md": 4, "docs/en/model-selection/information-criteria.md#section_0": 2, "docs/en/model-selection/information-criteria.md#section_13": 2}, "inadequacy": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_6": 1}, "forming": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1}, "simplify": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1}, "isn": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "checks": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1}, "accepting": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_13": 3, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1}, "early": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_2": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "1978": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_16": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1}, "measure": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1}, "lack": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1}, "biometrika": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_16": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "65": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1}, "297": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_16": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1}, "303": {"docs/en/time-domain/identification.md": 1, "docs/en/time-domain/identification.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1}, "benchmark": {"docs/en/time-domain/sarima.md": 3, "docs/en/time-domain/sarima.md#section_0": 1, "docs/en/time-domain/sarima.md#section_15": 2, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_11": 2}, "regular": {"docs/en/time-domain/sarima.md": 3, "docs/en/time-domain/sarima.md#section_0": 1, "docs/en/time-domain/sarima.md#section_7": 2, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "ps": {"docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_1": 1, "docs/en/time-domain/sarima.md#section_5": 1}, "qs": {"docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_1": 1, "docs/en/time-domain/sarima.md#section_5": 1}, "elsewhere": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_3": 1}, "classic": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_4": 1}, "passenger": {"docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_4": 1, "docs/en/time-domain/sarima.md#section_15": 1}, "smooths": {"docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_4": 2, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_4": 1}, "3s": {"docs/en/time-domain/sarima.md": 3, "docs/en/time-domain/sarima.md#section_5": 2, "docs/en/time-domain/sarima.md#section_7": 1}, "multiplicative": {"docs/en/time-domain/sarima.md": 7, "docs/en/time-domain/sarima.md#section_6": 2, "docs/en/time-domain/sarima.md#section_8": 2, "docs/en/time-domain/sarima.md#section_15": 3, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_2": 1, "docs/en/exponential-smoothing/ets.md": 25, "docs/en/exponential-smoothing/ets.md#section_1": 6, "docs/en/exponential-smoothing/ets.md#section_4": 1, "docs/en/exponential-smoothing/ets.md#section_5": 1, "docs/en/exponential-smoothing/ets.md#section_7": 2, "docs/en/exponential-smoothing/ets.md#section_8": 4, "docs/en/exponential-smoothing/ets.md#section_10": 1, "docs/en/exponential-smoothing/ets.md#section_13": 10, "docs/en/exponential-smoothing/holt-winters.md": 20, "docs/en/exponential-smoothing/holt-winters.md#section_0": 2, "docs/en/exponential-smoothing/holt-winters.md#section_1": 1, "docs/en/exponential-smoothing/holt-winters.md#section_2": 4, "docs/en/exponential-smoothing/holt-winters.md#section_3": 1, "docs/en/exponential-smoothing/holt-winters.md#section_6": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md#section_11": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 9, "docs/en/decomposition/classical.md": 22, "docs/en/decomposition/classical.md#section_0": 1, "docs/en/decomposition/classical.md#section_1": 2, "docs/en/decomposition/classical.md#section_3": 1, "docs/en/decomposition/classical.md#section_5": 4, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/classical.md#section_8": 1, "docs/en/decomposition/classical.md#section_9": 2, "docs/en/decomposition/classical.md#section_10": 1, "docs/en/decomposition/classical.md#section_11": 1, "docs/en/decomposition/classical.md#section_12": 8, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_6": 2}, "sar": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_6": 1}, "weekly": {"docs/en/time-domain/sarima.md": 7, "docs/en/time-domain/sarima.md#section_7": 2, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_15": 4, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_16": 3, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_16": 2}, "52": {"docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_7": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "double": {"docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_15": 1}, "seasonalities": {"docs/en/time-domain/sarima.md": 3, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_15": 2, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1}, "alternative": {"docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_8": 2, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_1": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "365": {"docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_8": 2, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/model-selection/cross-validation.md": 5, "docs/en/model-selection/cross-validation.md#section_11": 5, "docs/en/features/feature-engineering.md": 9, "docs/en/features/feature-engineering.md#section_5": 2, "docs/en/features/feature-engineering.md#section_10": 2, "docs/en/features/feature-engineering.md#section_11": 2, "docs/en/features/feature-engineering.md#section_16": 3, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_6": 1}, "difficult": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_8": 1}, "exist": {"docs/en/time-domain/sarima.md": 3, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_15": 2, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "integer": {"docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_8": 2, "docs/en/exponential-smoothing/holt-winters.md": 3, "docs/en/exponential-smoothing/holt-winters.md#section_7": 3, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_6": 2, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_0": 1, "docs/en/decomposition/stl.md#section_1": 1}, "days": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/cross-validation.md": 4, "docs/en/model-selection/cross-validation.md#section_11": 4, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_1": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_6": 2, "docs/en/decomposition/classical.md#section_12": 1}, "year": {"docs/en/time-domain/sarima.md": 4, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/time-domain/sarima.md#section_15": 3, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_6": 1}, "directly": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_8": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_3": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "trigonometric": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_8": 1}, "statespace": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_9": 1}, "sarimax": {"docs/en/time-domain/sarima.md": 4, "docs/en/time-domain/sarima.md#section_9": 2, "docs/en/time-domain/sarima.md#section_12": 1, "docs/en/time-domain/sarima.md#section_15": 1}, "disp": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_12": 1}, "tables": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_12": 1}, "nljung": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_13": 1}, "n12": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_12": 1}, "month": {"docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_14": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_12": 1, "docs/en/features/feature-engineering.md": 5, "docs/en/features/feature-engineering.md#section_1": 1, "docs/en/features/feature-engineering.md#section_5": 2, "docs/en/features/feature-engineering.md#section_11": 2, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_12": 2}, "affect": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "refers": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1}, "multiplying": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1}, "wouldn": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1}, "additive": {"docs/en/time-domain/sarima.md": 3, "docs/en/time-domain/sarima.md#section_15": 3, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_2": 1, "docs/en/exponential-smoothing/ets.md": 15, "docs/en/exponential-smoothing/ets.md#section_0": 1, "docs/en/exponential-smoothing/ets.md#section_1": 6, "docs/en/exponential-smoothing/ets.md#section_3": 2, "docs/en/exponential-smoothing/ets.md#section_5": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/ets.md#section_13": 4, "docs/en/exponential-smoothing/holt-winters.md": 21, "docs/en/exponential-smoothing/holt-winters.md#section_0": 2, "docs/en/exponential-smoothing/holt-winters.md#section_1": 1, "docs/en/exponential-smoothing/holt-winters.md#section_2": 3, "docs/en/exponential-smoothing/holt-winters.md#section_3": 1, "docs/en/exponential-smoothing/holt-winters.md#section_4": 1, "docs/en/exponential-smoothing/holt-winters.md#section_6": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md#section_10": 1, "docs/en/exponential-smoothing/holt-winters.md#section_11": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 9, "docs/en/decomposition/classical.md": 21, "docs/en/decomposition/classical.md#section_0": 1, "docs/en/decomposition/classical.md#section_1": 2, "docs/en/decomposition/classical.md#section_3": 1, "docs/en/decomposition/classical.md#section_5": 4, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/classical.md#section_10": 3, "docs/en/decomposition/classical.md#section_12": 9, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_6": 1}, "implication": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1}, "separation": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1}, "blends": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1}, "confuse": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_15": 1}, "originally": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1}, "yet": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "fits": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_5": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "overlap": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1}, "constraint": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_7": 1}, "observed": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1}, "write": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "yesterday": {"docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_15": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "last": {"docs/en/time-domain/sarima.md": 3, "docs/en/time-domain/sarima.md#section_15": 3, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_6": 1, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_9": 2, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_5": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_3": 1}, "day": {"docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_15": 2, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_1": 1, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/anomaly-detection/anomaly-detection.md": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 3}, "says": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1}, "repeat": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_9": 1, "docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_3": 1, "docs/en/decomposition/stl.md#section_13": 2}, "continuing": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1}, "simplistic": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1}, "benefits": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1}, "electricity": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1}, "24h": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1}, "168h": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1}, "alternatives": {"docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_15": 2, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "external": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "tbats": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1}, "prophet": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "neural": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1}, "approaches": {"docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_15": 2, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_3": 1, "docs/en/interview/interview-questions.md#section_15": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "week": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_1": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "exog": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1}, "168": {"docs/en/time-domain/sarima.md": 2, "docs/en/time-domain/sarima.md#section_15": 2}, "hierarchical": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "de": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1}, "livera": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1}, "snyder": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_14": 3, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_14": 1}, "2011": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1}, "jasa": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "106": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1}, "496": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1}, "1513": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1}, "1527": {"docs/en/time-domain/sarima.md": 1, "docs/en/time-domain/sarima.md#section_16": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1}, "transformer": {"docs/en/deep-learning/deep-learning-ts.md": 5, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md#section_4": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "addresses": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1}, "vanishing": {"docs/en/deep-learning/deep-learning-ts.md": 5, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/deep-learning/deep-learning-ts.md#section_2": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 3}, "gradients": {"docs/en/deep-learning/deep-learning-ts.md": 7, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/deep-learning/deep-learning-ts.md#section_2": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 5}, "gates": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1}, "dilated": {"docs/en/deep-learning/deep-learning-ts.md": 6, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md#section_3": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 3}, "convolutions": {"docs/en/deep-learning/deep-learning-ts.md": 4, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md#section_3": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "attention": {"docs/en/deep-learning/deep-learning-ts.md": 11, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md#section_4": 4, "docs/en/deep-learning/deep-learning-ts.md#section_17": 4, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_13": 1}, "mechanisms": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_13": 1}, "dl": {"docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 2, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_13": 2}, "shines": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1}, "underperform": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_0": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_5": 1}, "recurrent": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1}, "network": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_1": 2}, "tanh": {"docs/en/deep-learning/deep-learning-ts.md": 4, "docs/en/deep-learning/deep-learning-ts.md#section_1": 3, "docs/en/deep-learning/deep-learning-ts.md#section_2": 1}, "forget": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "gate": {"docs/en/deep-learning/deep-learning-ts.md": 4, "docs/en/deep-learning/deep-learning-ts.md#section_1": 3, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "input": {"docs/en/deep-learning/deep-learning-ts.md": 5, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md#section_4": 1, "docs/en/deep-learning/deep-learning-ts.md#section_5": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_6": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_6": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_0": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_5": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_5": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_5": 1}, "cell": {"docs/en/deep-learning/deep-learning-ts.md": 5, "docs/en/deep-learning/deep-learning-ts.md#section_1": 2, "docs/en/deep-learning/deep-learning-ts.md#section_2": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2}, "tilde": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_1": 2, "docs/en/anomaly-detection/anomaly-detection.md": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 3}, "output": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_6": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_6": 1, "docs/en/forecasting/multi-step.md": 4, "docs/en/forecasting/multi-step.md#section_0": 1, "docs/en/forecasting/multi-step.md#section_7": 1, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_5": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_5": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_5": 1}, "convolutional": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1}, "connections": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1}, "sequence": {"docs/en/deep-learning/deep-learning-ts.md": 6, "docs/en/deep-learning/deep-learning-ts.md#section_1": 1, "docs/en/deep-learning/deep-learning-ts.md#section_4": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 3, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1}, "gradient": {"docs/en/deep-learning/deep-learning-ts.md": 9, "docs/en/deep-learning/deep-learning-ts.md#section_2": 3, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 5, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/forecasting/multi-step.md#section_18": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "loss": {"docs/en/deep-learning/deep-learning-ts.md": 9, "docs/en/deep-learning/deep-learning-ts.md#section_2": 1, "docs/en/deep-learning/deep-learning-ts.md#section_5": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 2, "docs/en/deep-learning/deep-learning-ts.md#section_14": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 3, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "respect": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_2": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_0": 1}, "activation": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_2": 1}, "typically": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_2": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_3": 1}, "product": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_2": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_15": 1}, "numbers": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_2": 1}, "vanishes": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_2": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "preserving": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_2": 1}, "sequences": {"docs/en/deep-learning/deep-learning-ts.md": 4, "docs/en/deep-learning/deep-learning-ts.md#section_2": 1, "docs/en/deep-learning/deep-learning-ts.md#section_11": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1}, "dilation": {"docs/en/deep-learning/deep-learning-ts.md": 6, "docs/en/deep-learning/deep-learning-ts.md#section_3": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 4}, "receptive": {"docs/en/deep-learning/deep-learning-ts.md": 6, "docs/en/deep-learning/deep-learning-ts.md#section_3": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 5}, "field": {"docs/en/deep-learning/deep-learning-ts.md": 6, "docs/en/deep-learning/deep-learning-ts.md#section_3": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 5}, "layers": {"docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_3": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2}, "softmax": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_4": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "qk": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_4": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "derived": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_4": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "timesteps": {"docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_4": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2}, "position": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_4": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_5": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "encoding": {"docs/en/deep-learning/deep-learning-ts.md": 5, "docs/en/deep-learning/deep-learning-ts.md#section_4": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 4}, "added": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_4": 1}, "maintain": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_4": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_5": 1}, "training": {"docs/en/deep-learning/deep-learning-ts.md": 9, "docs/en/deep-learning/deep-learning-ts.md#section_5": 2, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 5, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 8, "docs/en/interview/interview-questions.md#section_8": 4, "docs/en/interview/interview-questions.md#section_16": 4, "docs/en/model-selection/cross-validation.md": 15, "docs/en/model-selection/cross-validation.md#section_1": 4, "docs/en/model-selection/cross-validation.md#section_4": 2, "docs/en/model-selection/cross-validation.md#section_5": 1, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 7, "docs/en/practical/practical-modeling.md": 5, "docs/en/practical/practical-modeling.md#section_6": 2, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/features/feature-engineering.md": 9, "docs/en/features/feature-engineering.md#section_0": 1, "docs/en/features/feature-engineering.md#section_4": 1, "docs/en/features/feature-engineering.md#section_8": 2, "docs/en/features/feature-engineering.md#section_16": 5, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 1}, "teacher": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_5": 1}, "forcing": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_5": 1}, "during": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_5": 1, "docs/en/model-selection/cross-validation.md": 3, "docs/en/model-selection/cross-validation.md#section_1": 1, "docs/en/model-selection/cross-validation.md#section_4": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "actual": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_5": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 5, "docs/en/forecasting/prediction-intervals.md#section_1": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 2, "docs/en/forecasting/prediction-intervals.md#section_12": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_16": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_7": 2}, "optimize": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_5": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_6": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_6": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "horizons": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_5": 1, "docs/en/exponential-smoothing/holt.md": 4, "docs/en/exponential-smoothing/holt.md#section_1": 1, "docs/en/exponential-smoothing/holt.md#section_6": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_0": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/multi-step.md": 12, "docs/en/forecasting/multi-step.md#section_1": 1, "docs/en/forecasting/multi-step.md#section_3": 1, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/forecasting/multi-step.md#section_9": 2, "docs/en/forecasting/multi-step.md#section_17": 6, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "torch": {"docs/en/deep-learning/deep-learning-ts.md": 11, "docs/en/deep-learning/deep-learning-ts.md#section_6": 2, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md#section_9": 3, "docs/en/deep-learning/deep-learning-ts.md#section_11": 2, "docs/en/deep-learning/deep-learning-ts.md#section_13": 1, "docs/en/deep-learning/deep-learning-ts.md#section_15": 2}, "nn": {"docs/en/deep-learning/deep-learning-ts.md": 13, "docs/en/deep-learning/deep-learning-ts.md#section_6": 5, "docs/en/deep-learning/deep-learning-ts.md#section_7": 2, "docs/en/deep-learning/deep-learning-ts.md#section_9": 2, "docs/en/deep-learning/deep-learning-ts.md#section_13": 4}, "lstmforecaster": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_6": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1}, "module": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_6": 1, "docs/en/deep-learning/deep-learning-ts.md#section_13": 1}, "def": {"docs/en/deep-learning/deep-learning-ts.md": 5, "docs/en/deep-learning/deep-learning-ts.md#section_6": 2, "docs/en/deep-learning/deep-learning-ts.md#section_11": 1, "docs/en/deep-learning/deep-learning-ts.md#section_13": 2, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_10": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_5": 1, "docs/en/model-selection/cross-validation.md#section_7": 1, "docs/en/practical/practical-modeling.md": 3, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/practical/practical-modeling.md#section_8": 2, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/features/feature-engineering.md#section_11": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_6": 2, "docs/en/change-detection/change-point.md#section_13": 1}, "super": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_6": 1, "docs/en/deep-learning/deep-learning-ts.md#section_13": 1}, "fc": {"docs/en/deep-learning/deep-learning-ts.md": 4, "docs/en/deep-learning/deep-learning-ts.md#section_6": 2, "docs/en/deep-learning/deep-learning-ts.md#section_13": 2}, "forward": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_6": 1, "docs/en/deep-learning/deep-learning-ts.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_1": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_1": 1, "docs/en/state-space/kalman-filter.md": 3, "docs/en/state-space/kalman-filter.md#section_0": 1, "docs/en/state-space/kalman-filter.md#section_13": 2}, "batch": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_6": 1}, "return": {"docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_6": 1, "docs/en/deep-learning/deep-learning-ts.md#section_11": 1, "docs/en/deep-learning/deep-learning-ts.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_6": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_6": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_10": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_5": 1, "docs/en/model-selection/cross-validation.md#section_7": 1, "docs/en/practical/practical-modeling.md": 3, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/practical/practical-modeling.md#section_8": 2, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/features/feature-engineering.md#section_11": 1, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_5": 1, "docs/en/state-space/kalman-filter.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 3, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_5": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_5": 1, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_6": 2, "docs/en/change-detection/change-point.md#section_13": 1}, "loop": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/decomposition/stl.md": 4, "docs/en/decomposition/stl.md#section_3": 4}, "64": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "optimizer": {"docs/en/deep-learning/deep-learning-ts.md": 6, "docs/en/deep-learning/deep-learning-ts.md#section_7": 3, "docs/en/deep-learning/deep-learning-ts.md#section_13": 1, "docs/en/deep-learning/deep-learning-ts.md#section_14": 2}, "optim": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md#section_13": 1}, "adam": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md#section_13": 1}, "lr": {"docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md#section_13": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "001": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1}, "mseloss": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md#section_13": 1}, "epoch": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md#section_14": 1}, "100": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_10": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_9": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_9": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/model-selection/cross-validation.md": 4, "docs/en/model-selection/cross-validation.md#section_3": 1, "docs/en/model-selection/cross-validation.md#section_9": 1, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_5": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/practical/practical-modeling.md": 5, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/practical/practical-modeling.md#section_8": 1, "docs/en/practical/practical-modeling.md#section_9": 1, "docs/en/practical/practical-modeling.md#section_11": 1, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_8": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_10": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_8": 1, "docs/en/change-detection/change-point.md": 10, "docs/en/change-detection/change-point.md#section_10": 3, "docs/en/change-detection/change-point.md#section_11": 1, "docs/en/change-detection/change-point.md#section_14": 5, "docs/en/change-detection/change-point.md#section_15": 1, "docs/en/foundations/autocorrelation.md": 4, "docs/en/foundations/autocorrelation.md#section_16": 4}, "dataloader": {"docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md#section_9": 1, "docs/en/deep-learning/deep-learning-ts.md#section_14": 1}, "scenario": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "recommendation": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1}, "1000": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_5": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_7": 1}, "medium": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_5": 1}, "related": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_5": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_3": 1}, "transfer": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1}, "latency": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1}, "parallelizable": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_7": 1}, "little": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "thousands": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_13": 1}, "wins": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_13": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "complicated": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1}, "architecture": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_9": 1}, "baselines": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_0": 1}, "naive": {"docs/en/deep-learning/deep-learning-ts.md": 5, "docs/en/deep-learning/deep-learning-ts.md#section_8": 2, "docs/en/deep-learning/deep-learning-ts.md#section_16": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/holt.md": 3, "docs/en/exponential-smoothing/holt.md#section_14": 3, "docs/en/interview/interview-questions.md": 6, "docs/en/interview/interview-questions.md#section_10": 3, "docs/en/interview/interview-questions.md#section_16": 3, "docs/en/model-selection/cross-validation.md": 5, "docs/en/model-selection/cross-validation.md#section_3": 1, "docs/en/model-selection/cross-validation.md#section_11": 4, "docs/en/practical/practical-modeling.md": 3, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/practical/practical-modeling.md#section_7": 2}, "before": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_16": 2}, "success": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "lookback": {"docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_8": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "aware": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "origin": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 11, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/cross-validation.md#section_1": 1, "docs/en/model-selection/cross-validation.md#section_2": 2, "docs/en/model-selection/cross-validation.md#section_5": 2, "docs/en/model-selection/cross-validation.md#section_6": 3, "docs/en/model-selection/cross-validation.md#section_9": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_2": 1}, "split": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md#section_12": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_12": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_8": 2, "docs/en/practical/practical-modeling.md": 5, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/practical/practical-modeling.md#section_13": 3, "docs/en/features/feature-engineering.md": 5, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/features/feature-engineering.md#section_6": 1, "docs/en/features/feature-engineering.md#section_7": 1, "docs/en/features/feature-engineering.md#section_12": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "clipping": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "scheduling": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1}, "careful": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_9": 1}, "initialization": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_8": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 3, "docs/en/exponential-smoothing/holt-winters.md#section_6": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 3, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/state-space/kalman-filter.md": 3, "docs/en/state-space/kalman-filter.md#section_6": 2, "docs/en/state-space/kalman-filter.md#section_13": 1}, "utils": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_9": 1}, "tensordataset": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_9": 1, "docs/en/deep-learning/deep-learning-ts.md#section_14": 1}, "2000": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_10": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "len": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_11": 1, "docs/en/deep-learning/deep-learning-ts.md#section_12": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_10": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_5": 1, "docs/en/model-selection/cross-validation.md#section_9": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_15": 2, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_7": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_7": 1, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_6": 2, "docs/en/change-detection/change-point.md#section_13": 1}, "append": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_11": 2, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_10": 2, "docs/en/model-selection/cross-validation.md": 3, "docs/en/model-selection/cross-validation.md#section_5": 1, "docs/en/model-selection/cross-validation.md#section_9": 2, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_10": 1, "docs/en/change-detection/change-point.md": 4, "docs/en/change-detection/change-point.md#section_6": 4}, "floattensor": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_11": 2}, "unsqueeze": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_11": 2}, "train": {"docs/en/deep-learning/deep-learning-ts.md": 4, "docs/en/deep-learning/deep-learning-ts.md#section_12": 1, "docs/en/deep-learning/deep-learning-ts.md#section_14": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2, "docs/en/forecasting/multi-step.md": 10, "docs/en/forecasting/multi-step.md#section_1": 1, "docs/en/forecasting/multi-step.md#section_2": 1, "docs/en/forecasting/multi-step.md#section_3": 1, "docs/en/forecasting/multi-step.md#section_12": 1, "docs/en/forecasting/multi-step.md#section_13": 1, "docs/en/forecasting/multi-step.md#section_14": 2, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/model-selection/cross-validation.md": 20, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/cross-validation.md#section_1": 3, "docs/en/model-selection/cross-validation.md#section_5": 5, "docs/en/model-selection/cross-validation.md#section_7": 2, "docs/en/model-selection/cross-validation.md#section_9": 3, "docs/en/model-selection/cross-validation.md#section_11": 6, "docs/en/practical/practical-modeling.md": 12, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/practical/practical-modeling.md#section_6": 4, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/practical/practical-modeling.md#section_13": 6, "docs/en/features/feature-engineering.md": 12, "docs/en/features/feature-engineering.md#section_4": 7, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/features/feature-engineering.md#section_12": 4}, "simplelstm": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_13": 2}, "32": {"docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_13": 2, "docs/en/deep-learning/deep-learning-ts.md#section_14": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1}, "shuffle": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_14": 1}, "evaluate": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_15": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_1": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_14": 1}, "eval": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_15": 1}, "rmse": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_15": 1, "docs/en/deep-learning/deep-learning-ts.md#section_16": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/model-selection/cross-validation.md": 4, "docs/en/model-selection/cross-validation.md#section_2": 1, "docs/en/model-selection/cross-validation.md#section_3": 2, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/practical/practical-modeling.md": 3, "docs/en/practical/practical-modeling.md#section_2": 1, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/practical/practical-modeling.md#section_10": 1, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_14": 3}, "address": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_12": 2}, "vanilla": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "multiplied": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "timestep": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2}, "vanish": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2}, "preventing": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "updated": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "additively": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "multiplicatively": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "controls": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_0": 1}, "keep": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "flows": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "unchanged": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "hundreds": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "acts": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "highway": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "bypassing": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "completely": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "solves": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "structures": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "parallelization": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "sequential": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/change-detection/change-point.md#section_1": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "efficiently": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1}, "easily": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "speed": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_7": 1}, "length": {"docs/en/deep-learning/deep-learning-ts.md": 4, "docs/en/deep-learning/deep-learning-ts.md#section_17": 4, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/anomaly-detection/anomaly-detection.md": 6, "docs/en/anomaly-detection/anomaly-detection.md#section_4": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 3, "docs/en/change-detection/change-point.md": 9, "docs/en/change-detection/change-point.md#section_5": 1, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_13": 3, "docs/en/change-detection/change-point.md#section_14": 4}, "truly": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "processing": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "online": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_11": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/change-detection/change-point.md": 15, "docs/en/change-detection/change-point.md#section_0": 2, "docs/en/change-detection/change-point.md#section_1": 1, "docs/en/change-detection/change-point.md#section_5": 1, "docs/en/change-detection/change-point.md#section_6": 1, "docs/en/change-detection/change-point.md#section_7": 2, "docs/en/change-detection/change-point.md#section_13": 2, "docs/en/change-detection/change-point.md#section_14": 5, "docs/en/change-detection/change-point.md#section_15": 1}, "streaming": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "naturally": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "lengths": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "tracking": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_11": 1, "docs/en/practical/practical-modeling.md": 6, "docs/en/practical/practical-modeling.md#section_4": 1, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/practical/practical-modeling.md#section_10": 1, "docs/en/practical/practical-modeling.md#section_12": 1, "docs/en/practical/practical-modeling.md#section_13": 2}, "smaller": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_10": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_11": 1}, "research": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1}, "finding": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_14": 2}, "matches": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "benchmarks": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "defaulting": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "comparable": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "convolution": {"docs/en/deep-learning/deep-learning-ts.md": 4, "docs/en/deep-learning/deep-learning-ts.md#section_17": 4}, "increasing": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "gaps": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "inputs": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_1": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_0": 1}, "layer": {"docs/en/deep-learning/deep-learning-ts.md": 4, "docs/en/deep-learning/deep-learning-ts.md#section_17": 4}, "rf": {"docs/en/deep-learning/deep-learning-ts.md": 5, "docs/en/deep-learning/deep-learning-ts.md#section_17": 5, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_5": 1}, "255": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "511": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "actually": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "positional": {"docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_17": 3}, "permutation": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "invariant": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "treats": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2}, "permute": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "pairwise": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "similarities": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "embeddings": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "pe": {"docs/en/deep-learning/deep-learning-ts.md": 3, "docs/en/deep-learning/deep-learning-ts.md#section_17": 3}, "2i": {"docs/en/deep-learning/deep-learning-ts.md": 4, "docs/en/deep-learning/deep-learning-ts.md#section_17": 4}, "10000": {"docs/en/deep-learning/deep-learning-ts.md": 2, "docs/en/deep-learning/deep-learning-ts.md#section_17": 2}, "bag": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "vectors": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "losing": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "entirely": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "energy": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "oscillate": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1}, "defaults": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "scheduler": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "occur": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_1": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "norms": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "scaled": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_3": 1, "docs/en/model-selection/cross-validation.md#section_7": 1, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_4": 3, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_2": 1}, "saturation": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "standardize": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_0": 1}, "targets": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_9": 1}, "epochs": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "learned": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "longer": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "curve": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "mse": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "dominated": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "diagnosis": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "decreasing": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1}, "magnitudes": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "visualize": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1}, "actuals": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/practical/practical-modeling.md": 8, "docs/en/practical/practical-modeling.md#section_9": 3, "docs/en/practical/practical-modeling.md#section_10": 3, "docs/en/practical/practical-modeling.md#section_11": 2}, "failure": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_17": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1}, "hochreiter": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1}, "schmidhuber": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1}, "1997": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1}, "computation": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "1735": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1}, "1780": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1}, "bai": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1}, "kolter": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1}, "koltun": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1}, "2018": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_17": 2}, "empirical": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1}, "generic": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1}, "networks": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1}, "arxiv": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "1803": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1}, "01271": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1}, "vaswani": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1}, "et": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1}, "al": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1}, "neurips": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1}, "lim": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1}, "zohren": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1}, "survey": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "philosophical": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1}, "transactions": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1}, "379": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1}, "2194": {"docs/en/deep-learning/deep-learning-ts.md": 1, "docs/en/deep-learning/deep-learning-ts.md#section_18": 1}, "unifying": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_0": 1}, "named": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_0": 1}, "30": {"docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_0": 1, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/forecasting/multi-step.md": 5, "docs/en/forecasting/multi-step.md#section_17": 4, "docs/en/forecasting/multi-step.md#section_18": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1}, "variants": {"docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_0": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_5": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "taxonomy": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_1": 1}, "none": {"docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_1": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_7": 2}, "transitions": {"docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_3": 1, "docs/en/exponential-smoothing/ets.md#section_4": 1}, "exp": {"docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_5": 2, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_6": 2}, "ahead": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_5": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_6": 1, "docs/en/forecasting/prediction-intervals.md": 4, "docs/en/forecasting/prediction-intervals.md#section_13": 4, "docs/en/forecasting/multi-step.md": 4, "docs/en/forecasting/multi-step.md#section_1": 1, "docs/en/forecasting/multi-step.md#section_3": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/forecasting/multi-step.md#section_18": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_1": 1}, "formulation": {"docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_6": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1}, "analytical": {"docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_6": 2}, "simulation": {"docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_6": 2, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_1": 1}, "paths": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "percentiles": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_6": 1, "docs/en/forecasting/prediction-intervals.md": 4, "docs/en/forecasting/prediction-intervals.md#section_6": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 2}, "valid": {"docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_7": 2}, "certain": {"docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1}, "implementations": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_6": 1}, "restrict": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_7": 1}, "admissible": {"docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_7": 1, "docs/en/exponential-smoothing/ets.md#section_13": 2}, "broader": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1}, "proportional": {"docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_2": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "averaging": {"docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_8": 2, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "impractical": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1}, "coverage": {"docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_8": 2, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 6, "docs/en/forecasting/prediction-intervals.md#section_1": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 3, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/practical/practical-modeling.md": 13, "docs/en/practical/practical-modeling.md#section_5": 3, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/practical/practical-modeling.md#section_11": 4, "docs/en/practical/practical-modeling.md#section_12": 2, "docs/en/practical/practical-modeling.md#section_13": 3}, "nominal": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_8": 1, "docs/en/forecasting/prediction-intervals.md": 4, "docs/en/forecasting/prediction-intervals.md#section_1": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 1}, "holtwinters": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_9": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_8": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_8": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_8": 1}, "exponentialsmoothing": {"docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_9": 1, "docs/en/exponential-smoothing/ets.md#section_11": 1, "docs/en/exponential-smoothing/holt-winters.md": 4, "docs/en/exponential-smoothing/holt-winters.md#section_8": 1, "docs/en/exponential-smoothing/holt-winters.md#section_10": 1, "docs/en/exponential-smoothing/holt-winters.md#section_11": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "120": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_10": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_8": 1}, "various": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_11": 1}, "mul": {"docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_11": 2, "docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_11": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "inf": {"docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_11": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_6": 1}, "sorted": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_11": 1}, "best": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_12": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_0": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 3, "docs/en/forecasting/prediction-intervals.md#section_1": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_12": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_1": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "min": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_12": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_12": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_11": 2, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_0": 1, "docs/en/features/feature-engineering.md#section_4": 1, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_10": 2, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_6": 2}, "nbest": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_12": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_12": 1}, "traditional": {"docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_13": 3}, "foundation": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1}, "comparing": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_7": 1}, "systematically": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "hoc": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1}, "formulas": {"docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_13": 3, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "unified": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_11": 1}, "automatic": {"docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/ets.md#section_14": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "algorithmically": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1}, "gave": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1}, "lacked": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1}, "principled": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1}, "scales": {"docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_0": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_1": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "absolute": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 4, "docs/en/model-selection/cross-validation.md#section_3": 3, "docs/en/model-selection/cross-validation.md#section_7": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "percentage": {"docs/en/exponential-smoothing/ets.md": 3, "docs/en/exponential-smoothing/ets.md#section_13": 3, "docs/en/exponential-smoothing/holt-winters.md": 3, "docs/en/exponential-smoothing/holt-winters.md#section_13": 3, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_3": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "business": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "contexts": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "fitted": {"docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_6": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_2": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "natural": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_11": 1}, "underestimates": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "transition": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_1": 1}, "inadmissible": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1}, "identifiability": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1}, "specifically": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "bounds": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1}, "let": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_7": 1}, "admissibility": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1}, "much": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_5": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "standardized": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_1": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "justifies": {"docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_13": 2, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_1": 1}, "reasonable": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_7": 1}, "reasonableness": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1}, "damping": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "98": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "boundaries": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "risk": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 3, "docs/en/forecasting/prediction-intervals.md#section_13": 3, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "koehler": {"docs/en/exponential-smoothing/ets.md": 2, "docs/en/exponential-smoothing/ets.md#section_14": 2, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_14": 1}, "grose": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1}, "2002": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1}, "ijf": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1, "docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_14": 2}, "18": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1}, "439": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1}, "454": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1}, "ord": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_14": 1}, "2008": {"docs/en/exponential-smoothing/ets.md": 1, "docs/en/exponential-smoothing/ets.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_14": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "trajectory": {"docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_0": 1, "docs/en/exponential-smoothing/holt.md#section_2": 1}, "persistent": {"docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_0": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "variant": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_1": 1}, "dampens": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_1": 1}, "intercept": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_2": 1}, "slope": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_2": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_1": 1}, "connection": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_3": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_4": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_3": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_11": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "asymptote": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_4": 1}, "dies": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_4": 1}, "2h": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_5": 1, "docs/en/forecasting/prediction-intervals.md": 3, "docs/en/forecasting/prediction-intervals.md#section_3": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 2}, "initialize": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_6": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_6": 1, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_5": 1, "docs/en/state-space/kalman-filter.md#section_7": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_3": 1, "docs/en/decomposition/stl.md#section_5": 1}, "flatten": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_6": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1}, "historical": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_6": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/practical/practical-modeling.md": 3, "docs/en/practical/practical-modeling.md#section_1": 1, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/state-space/kalman-filter.md": 3, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/state-space/kalman-filter.md#section_13": 2, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "reversals": {"docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_6": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1}, "generally": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_6": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "safer": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_6": 1}, "production": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_6": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/practical/practical-modeling.md": 15, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/practical/practical-modeling.md#section_1": 1, "docs/en/practical/practical-modeling.md#section_6": 2, "docs/en/practical/practical-modeling.md#section_9": 1, "docs/en/practical/practical-modeling.md#section_13": 10, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "extrapolating": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1}, "far": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1}, "continue": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_3": 1}, "indefinitely": {"docs/en/exponential-smoothing/holt.md": 3, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/exponential-smoothing/holt.md#section_14": 2}, "frequent": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_13": 2}, "makes": {"docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_7": 2, "docs/en/exponential-smoothing/holt-winters.md": 3, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_13": 2, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "sticky": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1}, "recognize": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1}, "extrapolation": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_5": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "transforms": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_0": 1}, "outperforms": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_7": 1}, "optimized": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_10": 1, "docs/en/exponential-smoothing/ses.md": 3, "docs/en/exponential-smoothing/ses.md#section_10": 1, "docs/en/exponential-smoothing/ses.md#section_11": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_6": 1}, "19": {"docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_11": 1, "docs/en/exponential-smoothing/holt.md#section_12": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1}, "ndamped": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_12": 1}, "responsive": {"docs/en/exponential-smoothing/holt.md": 3, "docs/en/exponential-smoothing/holt.md#section_14": 3, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1}, "separating": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "couldn": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "behaviors": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "aspects": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "optimizing": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "independently": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "realistic": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "quantities": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_2": 1}, "populations": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "linearly": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_3": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "toward": {"docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_14": 2, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_13": 2}, "safety": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "prevents": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "extreme": {"docs/en/exponential-smoothing/holt.md": 2, "docs/en/exponential-smoothing/holt.md#section_14": 2, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 3, "docs/en/forecasting/prediction-intervals.md#section_13": 3, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "competitions": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "hedges": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "continues": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_5": 1}, "stops": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "closer": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "limit": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "asymptotes": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "compound": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1}, "recent": {"docs/en/exponential-smoothing/holt.md": 4, "docs/en/exponential-smoothing/holt.md#section_14": 4, "docs/en/exponential-smoothing/ses.md": 4, "docs/en/exponential-smoothing/ses.md#section_6": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_13": 2}, "never": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/model-selection/cross-validation.md": 3, "docs/en/model-selection/cross-validation.md#section_1": 1, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_0": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "fixed": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 4, "docs/en/model-selection/cross-validation.md#section_1": 1, "docs/en/model-selection/cross-validation.md#section_6": 2, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_13": 2, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "pushes": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "quality": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_13": 2}, "track": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "closely": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "adapts": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_3": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1}, "keeps": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1}, "throughout": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_14": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_6": 1}, "1957": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1}, "averages": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_0": 2, "docs/en/decomposition/classical.md#section_4": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_13": 2, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_1": 2}, "onr": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1}, "memorandum": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1}, "gardner": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_14": 1}, "mckenzie": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1}, "1985": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_14": 1}, "management": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1}, "science": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1, "docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1}, "31": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1}, "1237": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1}, "1246": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1}, "makridakis": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_13": 1}, "hibon": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1}, "m3": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1}, "competition": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_18": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1}, "16": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1}, "451": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1}, "476": {"docs/en/exponential-smoothing/holt.md": 1, "docs/en/exponential-smoothing/holt.md#section_15": 1}, "version": {"docs/en/exponential-smoothing/holt-winters.md": 3, "docs/en/exponential-smoothing/holt-winters.md#section_0": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "ell": {"docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_0": 2}, "hb": {"docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_0": 2}, "variation": {"docs/en/exponential-smoothing/holt-winters.md": 6, "docs/en/exponential-smoothing/holt-winters.md#section_0": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 5, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_1": 2, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_6": 1}, "lfloor": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_1": 1}, "rfloor": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_1": 1}, "amount": {"docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_2": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "swings": {"docs/en/exponential-smoothing/holt-winters.md": 5, "docs/en/exponential-smoothing/holt-winters.md#section_2": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 3, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_12": 2}, "ratio": {"docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_2": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_13": 2, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "std": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_2": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_10": 2, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_11": 1, "docs/en/practical/practical-modeling.md#section_12": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/features/feature-engineering.md#section_11": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_11": 1, "docs/en/anomaly-detection/anomaly-detection.md": 10, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_12": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 5, "docs/en/decomposition/classical.md": 4, "docs/en/decomposition/classical.md#section_10": 1, "docs/en/decomposition/classical.md#section_11": 1, "docs/en/decomposition/classical.md#section_12": 2, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_10": 2}, "indices": {"docs/en/exponential-smoothing/holt-winters.md": 12, "docs/en/exponential-smoothing/holt-winters.md#section_3": 2, "docs/en/exponential-smoothing/holt-winters.md#section_6": 2, "docs/en/exponential-smoothing/holt-winters.md#section_7": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 6, "docs/en/decomposition/classical.md": 7, "docs/en/decomposition/classical.md#section_1": 1, "docs/en/decomposition/classical.md#section_5": 2, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/classical.md#section_9": 1, "docs/en/decomposition/classical.md#section_12": 2}, "normalization": {"docs/en/exponential-smoothing/holt-winters.md": 6, "docs/en/exponential-smoothing/holt-winters.md#section_3": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 3, "docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_12": 3}, "applied": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_3": 1}, "equivalence": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_4": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1}, "combine": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_5": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_15": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_6": 1}, "flattens": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_5": 1}, "2nd": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_6": 1}, "1st": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_6": 1}, "normalize": {"docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_6": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/decomposition/classical.md": 4, "docs/en/decomposition/classical.md#section_3": 2, "docs/en/decomposition/classical.md#section_5": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_13": 1}, "sse": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_6": 1, "docs/en/exponential-smoothing/ses.md": 4, "docs/en/exponential-smoothing/ses.md#section_5": 1, "docs/en/exponential-smoothing/ses.md#section_11": 3}, "mae": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_6": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 8, "docs/en/model-selection/cross-validation.md#section_3": 3, "docs/en/model-selection/cross-validation.md#section_7": 2, "docs/en/model-selection/cross-validation.md#section_11": 3, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_10": 1}, "typical": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_6": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_6": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "ranges": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_6": 1}, "degrades": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1}, "significantly": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_5": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "insufficient": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1}, "full": {"docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/model-selection/cross-validation.md": 5, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 4, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_15": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/features/feature-engineering.md": 4, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/features/feature-engineering.md#section_16": 3, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_4": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_13": 2, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "reliable": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_7": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "years": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_9": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_8": 1, "docs/en/decomposition/classical.md#section_12": 1}, "hw": {"docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_11": 2}, "nmultiplicative": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_11": 1}, "decide": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "roughly": {"docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "decreases": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "proportionally": {"docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2}, "visual": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_6": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "inspection": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_6": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "subperiods": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "normalized": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_1": 1}, "systematic": {"docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 6, "docs/en/practical/practical-modeling.md#section_3": 2, "docs/en/practical/practical-modeling.md#section_13": 4, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_12": 2}, "shift": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_8": 2, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/features/feature-engineering.md": 16, "docs/en/features/feature-engineering.md#section_5": 5, "docs/en/features/feature-engineering.md#section_8": 3, "docs/en/features/feature-engineering.md#section_11": 3, "docs/en/features/feature-engineering.md#section_16": 5, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/change-detection/change-point.md": 5, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/change-detection/change-point.md#section_1": 1, "docs/en/change-detection/change-point.md#section_2": 1, "docs/en/change-detection/change-point.md#section_14": 2}, "implementing": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "develop": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "assume": {"docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_7": 1}, "simplification": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "2m": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_2": 1, "docs/en/decomposition/classical.md#section_5": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_3": 1, "docs/en/decomposition/stl.md#section_5": 1}, "case": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 5, "docs/en/forecasting/prediction-intervals.md#section_13": 5, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_14": 2}, "chase": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "simplified": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_3": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_13": 2}, "accurate": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "widening": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_7": 2}, "narrowing": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "retail": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "december": {"docs/en/exponential-smoothing/holt-winters.md": 3, "docs/en/exponential-smoothing/holt-winters.md#section_13": 3, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "grown": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "100k": {"docs/en/exponential-smoothing/holt-winters.md": 2, "docs/en/exponential-smoothing/holt-winters.md#section_13": 2}, "200k": {"docs/en/exponential-smoothing/holt-winters.md": 3, "docs/en/exponential-smoothing/holt-winters.md#section_13": 3}, "doubled": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "grew": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "400k": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "deviation": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_3": 1, "docs/en/anomaly-detection/anomaly-detection.md": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 3, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "150k": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "match": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_1": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "verification": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "percentages": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "growing": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "looking": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "deviations": {"docs/en/exponential-smoothing/holt-winters.md": 3, "docs/en/exponential-smoothing/holt-winters.md#section_13": 3}, "question": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1}, "scale": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_12": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 3, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "1960": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_14": 1}, "324": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1}, "342": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1}, "chatfield": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1}, "yar": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1}, "1988": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1}, "statistician": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1}, "129": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1}, "140": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1}, "2006": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1}, "art": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1, "docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_14": 1}, "ii": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1}, "637": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1}, "666": {"docs/en/exponential-smoothing/holt-winters.md": 1, "docs/en/exponential-smoothing/holt-winters.md#section_14": 1}, "responsiveness": {"docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_0": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1}, "closed": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_5": 1}, "heavy": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_6": 1, "docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 4, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 3, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "light": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_6": 1}, "trending": {"docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1}, "bar": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/features/feature-engineering.md": 4, "docs/en/features/feature-engineering.md#section_3": 2, "docs/en/features/feature-engineering.md#section_16": 2, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_2": 1, "docs/en/decomposition/classical.md": 7, "docs/en/decomposition/classical.md#section_3": 6, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_3": 1, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_1": 3}, "weight": {"docs/en/exponential-smoothing/ses.md": 5, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/exponential-smoothing/ses.md#section_13": 4, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_2": 1, "docs/en/decomposition/stl.md": 4, "docs/en/decomposition/stl.md#section_13": 4}, "practitioners": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1}, "expect": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_15": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "simpleexpsmoothing": {"docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_8": 1, "docs/en/exponential-smoothing/ses.md#section_10": 1}, "alphas": {"docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_11": 2}, "fittedvalues": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_11": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1}, "modeled": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "reason": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1}, "distant": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "adapt": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/classical.md#section_12": 1}, "potential": {"docs/en/exponential-smoothing/ses.md": 2, "docs/en/exponential-smoothing/ses.md#section_13": 2, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_7": 1}, "balances": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "considerations": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_11": 2}, "found": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_3": 1}, "smoother": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 8, "docs/en/state-space/kalman-filter.md#section_5": 1, "docs/en/state-space/kalman-filter.md#section_6": 2, "docs/en/state-space/kalman-filter.md#section_13": 5, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_2": 1}, "behind": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "geometric": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1}, "goes": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1}, "almost": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "volatility": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "pulling": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1}, "inflate": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "chasing": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "confirming": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1}, "investigate": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_6": 1}, "28": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_14": 1, "docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_4": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_11": 1}, "brown": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_14": 1}, "1959": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_14": 1}, "inventory": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_14": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_15": 1}, "mcgraw": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_14": 1}, "hill": {"docs/en/exponential-smoothing/ses.md": 1, "docs/en/exponential-smoothing/ses.md#section_14": 1}, "quantify": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_5": 1}, "giving": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_4": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "fall": {"docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "bootstrap": {"docs/en/forecasting/prediction-intervals.md": 8, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/forecasting/prediction-intervals.md#section_6": 4, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "parametric": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_0": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "containing": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_1": 1}, "probability": {"docs/en/forecasting/prediction-intervals.md": 3, "docs/en/forecasting/prediction-intervals.md#section_1": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_3": 1, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/change-detection/change-point.md": 7, "docs/en/change-detection/change-point.md#section_5": 3, "docs/en/change-detection/change-point.md#section_13": 4}, "proportion": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_1": 1}, "falling": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_1": 1}, "specific": {"docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_3": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_13": 2, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "80": {"docs/en/forecasting/prediction-intervals.md": 3, "docs/en/forecasting/prediction-intervals.md#section_4": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 2, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/practical/practical-modeling.md": 3, "docs/en/practical/practical-modeling.md#section_6": 2, "docs/en/practical/practical-modeling.md#section_12": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_9": 1, "docs/en/decomposition/stl.md#section_11": 2, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_14": 2}, "accounting": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_5": 1}, "intrinsic": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_5": 1}, "calculate": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_6": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_10": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "refit": {"docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_6": 2, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "replacement": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_6": 1}, "97": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_6": 1}, "pis": {"docs/en/forecasting/prediction-intervals.md": 7, "docs/en/forecasting/prediction-intervals.md#section_7": 3, "docs/en/forecasting/prediction-intervals.md#section_13": 4}, "undercoverage": {"docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1}, "misspecified": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 1}, "underestimated": {"docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1}, "overcoverage": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 1}, "skewed": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 1}, "tailed": {"docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "uncertain": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "wider": {"docs/en/forecasting/prediction-intervals.md": 4, "docs/en/forecasting/prediction-intervals.md#section_7": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 3, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "accumulate": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "know": {"docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 2}, "exceptions": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1}, "reverting": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1}, "unconditional": {"docs/en/forecasting/prediction-intervals.md": 4, "docs/en/forecasting/prediction-intervals.md#section_13": 4}, "85": {"docs/en/forecasting/prediction-intervals.md": 2, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "supposed": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1}, "tested": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "ignored": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "remedies": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1}, "heavier": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1}, "distributions": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/practical/practical-modeling.md": 3, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "trusting": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1}, "stakeholders": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "want": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_4": 1}, "worst": {"docs/en/forecasting/prediction-intervals.md": 4, "docs/en/forecasting/prediction-intervals.md#section_13": 4}, "translate": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1}, "upper": {"docs/en/forecasting/prediction-intervals.md": 4, "docs/en/forecasting/prediction-intervals.md#section_13": 4, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_11": 2, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_13": 2}, "exceeds": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_12": 1}, "quantiles": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1}, "exceeding": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1}, "message": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1}, "statements": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1}, "acceptable": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1}, "exceeded": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_13": 1}, "calculating": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1}, "statistics": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1, "docs/en/features/feature-engineering.md": 8, "docs/en/features/feature-engineering.md#section_0": 1, "docs/en/features/feature-engineering.md#section_1": 1, "docs/en/features/feature-engineering.md#section_3": 1, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/features/feature-engineering.md#section_16": 4, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_13": 2, "docs/en/anomaly-detection/anomaly-detection.md": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_10": 1, "docs/en/decomposition/stl.md#section_14": 1}, "121": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1}, "135": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1}, "thombs": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1}, "schucany": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1}, "1990": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_14": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1}, "410": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1}, "486": {"docs/en/forecasting/prediction-intervals.md": 1, "docs/en/forecasting/prediction-intervals.md#section_14": 1}, "iterate": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_0": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "mimo": {"docs/en/forecasting/multi-step.md": 9, "docs/en/forecasting/multi-step.md#section_0": 1, "docs/en/forecasting/multi-step.md#section_1": 1, "docs/en/forecasting/multi-step.md#section_6": 2, "docs/en/forecasting/multi-step.md#section_10": 1, "docs/en/forecasting/multi-step.md#section_15": 1, "docs/en/forecasting/multi-step.md#section_16": 1, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_9": 2}, "accumulates": {"docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_0": 1, "docs/en/forecasting/multi-step.md#section_2": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "accumulation": {"docs/en/forecasting/multi-step.md": 3, "docs/en/forecasting/multi-step.md#section_0": 1, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_9": 1}, "ml": {"docs/en/forecasting/multi-step.md": 4, "docs/en/forecasting/multi-step.md#section_0": 1, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 4, "docs/en/interview/interview-questions.md#section_0": 1, "docs/en/interview/interview-questions.md#section_5": 2, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_0": 1, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1}, "given": {"docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_1": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1}, "iterated": {"docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_1": 1, "docs/en/forecasting/multi-step.md#section_18": 1}, "repeatedly": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_1": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_1": 1}, "feeding": {"docs/en/forecasting/multi-step.md": 3, "docs/en/forecasting/multi-step.md#section_1": 1, "docs/en/forecasting/multi-step.md#section_17": 2}, "outputs": {"docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_1": 1, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_9": 1}, "dirrec": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_1": 1}, "hybrid": {"docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_1": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "underlying": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_2": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_1": 1}, "dgp": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_2": 1}, "iterations": {"docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_2": 1, "docs/en/forecasting/multi-step.md#section_4": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_4": 1}, "propagation": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_3": 1}, "trained": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_3": 1}, "target": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_3": 1, "docs/en/practical/practical-modeling.md": 4, "docs/en/practical/practical-modeling.md#section_1": 1, "docs/en/practical/practical-modeling.md#section_13": 3, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_13": 1}, "violate": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_3": 1}, "rec": {"docs/en/forecasting/multi-step.md": 4, "docs/en/forecasting/multi-step.md#section_4": 1, "docs/en/forecasting/multi-step.md#section_17": 3}, "compounds": {"docs/en/forecasting/multi-step.md": 5, "docs/en/forecasting/multi-step.md#section_4": 1, "docs/en/forecasting/multi-step.md#section_5": 1, "docs/en/forecasting/multi-step.md#section_17": 3, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "dir": {"docs/en/forecasting/multi-step.md": 3, "docs/en/forecasting/multi-step.md#section_4": 1, "docs/en/forecasting/multi-step.md#section_17": 2}, "compounding": {"docs/en/forecasting/multi-step.md": 3, "docs/en/forecasting/multi-step.md#section_4": 1, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_9": 1}, "ben": {"docs/en/forecasting/multi-step.md": 3, "docs/en/forecasting/multi-step.md#section_5": 1, "docs/en/forecasting/multi-step.md#section_18": 2}, "taieb": {"docs/en/forecasting/multi-step.md": 3, "docs/en/forecasting/multi-step.md#section_5": 1, "docs/en/forecasting/multi-step.md#section_18": 2}, "correct": {"docs/en/forecasting/multi-step.md": 4, "docs/en/forecasting/multi-step.md#section_5": 1, "docs/en/forecasting/multi-step.md#section_17": 3, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_16": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_16": 3, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_6": 1}, "msfe": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_5": 1}, "outperform": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_5": 1}, "guidelines": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_6": 1}, "optimality": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_6": 1}, "quantification": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "elif": {"docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_6": 2, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 2}, "nonparametric": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_6": 1}, "implementation": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_6": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "once": {"docs/en/forecasting/multi-step.md": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_16": 2}, "multioutputregressor": {"docs/en/forecasting/multi-step.md": 4, "docs/en/forecasting/multi-step.md#section_7": 1, "docs/en/forecasting/multi-step.md#section_10": 1, "docs/en/forecasting/multi-step.md#section_15": 1, "docs/en/forecasting/multi-step.md#section_17": 1}, "tree": {"docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/anomaly-detection/anomaly-detection.md": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_4": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2}, "extrapolate": {"docs/en/forecasting/multi-step.md": 3, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/forecasting/multi-step.md#section_17": 2}, "exploding": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_9": 1}, "trust": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/state-space/kalman-filter.md": 3, "docs/en/state-space/kalman-filter.md#section_13": 3}, "inconsistency": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "computational": {"docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/interview/interview-questions.md#section_13": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "cost": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1, "docs/en/change-detection/change-point.md": 9, "docs/en/change-detection/change-point.md#section_3": 1, "docs/en/change-detection/change-point.md#section_6": 3, "docs/en/change-detection/change-point.md#section_14": 4, "docs/en/change-detection/change-point.md#section_15": 1}, "expensive": {"docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "suboptimal": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_9": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_4": 1}, "sklearn": {"docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_10": 2, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_8": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_9": 2, "docs/en/features/feature-engineering.md#section_16": 1}, "ridge": {"docs/en/forecasting/multi-step.md": 3, "docs/en/forecasting/multi-step.md#section_10": 1, "docs/en/forecasting/multi-step.md#section_14": 1, "docs/en/forecasting/multi-step.md#section_15": 1}, "multioutput": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_10": 1}, "dataset": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_10": 1, "docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_13": 2}, "400": {"docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_12": 2}, "reshape": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_14": 1}, "predicted": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1}, "theoretically": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "mainly": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "trees": {"docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_17": 2, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_4": 1}, "nns": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1}, "mode": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_15": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_12": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/decomposition/stl.md#section_13": 1}, "anchors": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1}, "efficiency": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "boosting": {"docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/forecasting/multi-step.md#section_18": 1}, "expectation": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1}, "correctly": {"docs/en/forecasting/multi-step.md": 2, "docs/en/forecasting/multi-step.md#section_17": 2}, "place": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1}, "rao": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1}, "effectively": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1}, "xgboost": {"docs/en/forecasting/multi-step.md": 5, "docs/en/forecasting/multi-step.md#section_17": 5, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "back": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_9": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "erratic": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1}, "computationally": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "xgbregressor": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1}, "custom": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "lightgbm": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_17": 1}, "2014": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1}, "kaggle": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1}, "load": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1}, "382": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1}, "394": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1}, "chevillon": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1}, "2007": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "surveys": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "746": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1}, "785": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1}, "marcellino": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1}, "multistep": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1}, "macroeconomic": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1}, "499": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1}, "526": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1}, "bontempi": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1}, "atiya": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1}, "sorjamaa": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1}, "2012": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_14": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "review": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1, "docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_0": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "nn5": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1}, "expert": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1}, "39": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1}, "7067": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1}, "7083": {"docs/en/forecasting/multi-step.md": 1, "docs/en/forecasting/multi-step.md#section_18": 1}, "compiles": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_0": 1}, "asked": {"docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_0": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "concise": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_0": 1}, "topics": {"docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_0": 2}, "span": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_0": 1}, "fundamentals": {"docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_0": 1}, "advanced": {"docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_0": 1}, "interviews": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_0": 1}, "unpredictable": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_1": 1}, "conclusions": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_5": 1}, "detrending": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_6": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "changing": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_1": 1, "docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_3": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/foundations/stationarity.md": 3, "docs/en/foundations/stationarity.md#section_6": 2, "docs/en/foundations/stationarity.md#section_7": 1}, "separates": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_3": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_0": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_0": 1}, "remainder": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_3": 1, "docs/en/decomposition/stl.md": 15, "docs/en/decomposition/stl.md#section_0": 2, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_5": 1, "docs/en/decomposition/stl.md#section_6": 2, "docs/en/decomposition/stl.md#section_10": 1, "docs/en/decomposition/stl.md#section_11": 1, "docs/en/decomposition/stl.md#section_13": 7}, "dummy": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_3": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_2": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "arimax": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_4": 1}, "interpretability": {"docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/interview/interview-questions.md#section_13": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_16": 2}, "primary": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_4": 1}, "goal": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_0": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "whichever": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_4": 1}, "implement": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_4": 1}, "context": {"docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_4": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 8, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 5, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "machine": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_5": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1}, "calibrated": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_5": 1}, "lots": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_5": 1}, "q6": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_6": 1}, "2ln": {"docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_6": 2}, "overfit": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_6": 1}, "40": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_6": 1, "docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_9": 1}, "guideline": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_6": 1}, "q7": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_7": 1}, "interpolation": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_0": 1}, "sporadic": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_7": 1}, "fill": {"docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_7": 2, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "imputation": {"docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_7": 2, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_16": 2}, "previous": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_7": 1}, "em": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_7": 1}, "indicator": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "binary": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/change-detection/change-point.md": 6, "docs/en/change-detection/change-point.md#section_3": 1, "docs/en/change-detection/change-point.md#section_12": 1, "docs/en/change-detection/change-point.md#section_14": 4}, "delete": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_7": 1}, "continuity": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_7": 1}, "globally": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_7": 1}, "q8": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_8": 1}, "prevent": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "sources": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "prevention": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_8": 1}, "temporally": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_8": 1}, "scalers": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_8": 1}, "availability": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_8": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "q9": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_9": 1}, "feed": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_9": 1}, "offs": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_9": 1}, "q10": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_10": 1}, "mape": {"docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 9, "docs/en/model-selection/cross-validation.md#section_3": 3, "docs/en/model-selection/cross-validation.md#section_11": 6, "docs/en/practical/practical-modeling.md": 4, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/practical/practical-modeling.md#section_8": 1, "docs/en/practical/practical-modeling.md#section_10": 2}, "undefined": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "mase": {"docs/en/interview/interview-questions.md": 3, "docs/en/interview/interview-questions.md#section_10": 2, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 11, "docs/en/model-selection/cross-validation.md#section_3": 3, "docs/en/model-selection/cross-validation.md#section_7": 1, "docs/en/model-selection/cross-validation.md#section_9": 1, "docs/en/model-selection/cross-validation.md#section_10": 1, "docs/en/model-selection/cross-validation.md#section_11": 5}, "free": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_10": 1}, "compares": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_10": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_5": 1}, "q11": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_11": 1}, "filtering": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_11": 1, "docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/state-space/kalman-filter.md#section_14": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "arrives": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_11": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_7": 1}, "q12": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_12": 1}, "limitations": {"docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_12": 2}, "missed": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_12": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "q13": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_13": 1}, "occam": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_13": 1}, "razor": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_13": 1}, "popular": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_13": 1}, "architectures": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_13": 1}, "cnn": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_13": 1}, "q14": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_14": 1}, "chow": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "cusum": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/change-detection/change-point.md": 11, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/change-detection/change-point.md#section_2": 2, "docs/en/change-detection/change-point.md#section_6": 1, "docs/en/change-detection/change-point.md#section_14": 7, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "cumulative": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/change-detection/change-point.md#section_2": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "pelt": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_14": 1, "docs/en/change-detection/change-point.md": 9, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/change-detection/change-point.md#section_4": 1, "docs/en/change-detection/change-point.md#section_6": 2, "docs/en/change-detection/change-point.md#section_11": 2, "docs/en/change-detection/change-point.md#section_14": 3}, "markov": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_14": 1}, "post": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_14": 1}, "q15": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_15": 1}, "reconciliation": {"docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_15": 2, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_1": 1}, "ensuring": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_15": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_1": 1}, "aggregation": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_15": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_1": 1}, "category": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_15": 1}, "sums": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_15": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "aggregate": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_15": 1}, "distribute": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_15": 1}, "bottom": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_15": 1}, "individuals": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_15": 1}, "optimally": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_15": 1}, "mint": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_15": 1}, "budgets": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_15": 1}, "plots": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "skipping": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "underfitting": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "manifestation": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "great": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "regularization": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "penalize": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "relate": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "deeply": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "connected": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "equivalents": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "presence": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "hegy": {"docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "canova": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "hansen": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "ocsb": {"docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_16": 2}, "retailer": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "understand": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "granularity": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "holidays": {"docs/en/interview/interview-questions.md": 2, "docs/en/interview/interview-questions.md#section_16": 2, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "promotions": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "cv": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/model-selection/cross-validation.md": 23, "docs/en/model-selection/cross-validation.md#section_0": 5, "docs/en/model-selection/cross-validation.md#section_4": 2, "docs/en/model-selection/cross-validation.md#section_5": 2, "docs/en/model-selection/cross-validation.md#section_6": 4, "docs/en/model-selection/cross-validation.md#section_9": 1, "docs/en/model-selection/cross-validation.md#section_11": 9, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_7": 1}, "weeks": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "communication": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "monitoring": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/practical/practical-modeling.md": 9, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/practical/practical-modeling.md#section_4": 1, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/practical/practical-modeling.md#section_9": 2, "docs/en/practical/practical-modeling.md#section_10": 2, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/change-detection/change-point.md#section_14": 2}, "retraining": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/practical/practical-modeling.md": 6, "docs/en/practical/practical-modeling.md#section_1": 1, "docs/en/practical/practical-modeling.md#section_4": 1, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/practical/practical-modeling.md#section_12": 1, "docs/en/practical/practical-modeling.md#section_13": 2}, "schedule": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "jumping": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "hard": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "beat": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1}, "justified": {"docs/en/interview/interview-questions.md": 1, "docs/en/interview/interview-questions.md#section_16": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "fold": {"docs/en/model-selection/cross-validation.md": 10, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/cross-validation.md#section_4": 3, "docs/en/model-selection/cross-validation.md#section_5": 2, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 3}, "windows": {"docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_1": 1, "docs/en/features/feature-engineering.md#section_8": 2, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1}, "blocked": {"docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/model-selection/cross-validation.md#section_5": 1}, "hyperparameter": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_0": 1}, "tuning": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_0": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "seen": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_1": 1}, "sliding": {"docs/en/model-selection/cross-validation.md": 8, "docs/en/model-selection/cross-validation.md#section_1": 1, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 6}, "oldest": {"docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_1": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "origins": {"docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_2": 1, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_2": 1}, "smape": {"docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_3": 2}, "fails": {"docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_4": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "randomly": {"docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_4": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "splits": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_4": 1, "docs/en/practical/practical-modeling.md": 4, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/anomaly-detection/anomaly-detection.md": 4, "docs/en/anomaly-detection/anomaly-detection.md#section_4": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2}, "sees": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_4": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "optimistic": {"docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_4": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "iteration": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_5": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_4": 1}, "store": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_5": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_5": 1}, "blocks": {"docs/en/model-selection/cross-validation.md": 3, "docs/en/model-selection/cross-validation.md#section_5": 3}, "contiguous": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_5": 1}, "ideal": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_5": 1}, "accidentally": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_6": 1}, "overlapping": {"docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "guarantee": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "refitting": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_6": 1}, "representative": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_6": 1}, "old": {"docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "mislead": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_6": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_7": 1}, "nan": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_9": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_8": 1}, "isnan": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_10": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_9": 2}, "assigns": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "folds": {"docs/en/model-selection/cross-validation.md": 4, "docs/en/model-selection/cross-validation.md#section_11": 4}, "breaking": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "unrealistic": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "implicitly": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "learns": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "inflating": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "apparent": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "effective": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "independence": {"docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_11": 2, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "contains": {"docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_11": 2}, "60": {"docs/en/model-selection/cross-validation.md": 3, "docs/en/model-selection/cross-validation.md#section_11": 3}, "55": {"docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_11": 2}, "timeseriessplit": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "evolving": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_0": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_13": 2}, "refits": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "misleads": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "advantages": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_3": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "division": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "asymmetrically": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "defined": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "intermittent": {"docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_11": 2}, "asymmetric": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_7": 1}, "150": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_10": 1}, "treated": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "differently": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "dependent": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "requirements": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "guidance": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "5k": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "leaves": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "1095": {"docs/en/model-selection/cross-validation.md": 4, "docs/en/model-selection/cross-validation.md#section_11": 4}, "design": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "scheme": {"docs/en/model-selection/cross-validation.md": 2, "docs/en/model-selection/cross-validation.md#section_11": 2}, "103": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "sets": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "gap": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "skip": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1}, "simulate": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_9": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_12": 1}, "delay": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_7": 2, "docs/en/change-detection/change-point.md#section_14": 1}, "723": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_11": 1, "docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1}, "bergmeir": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1}, "predictor": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "sciences": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1}, "191": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1}, "192": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1}, "213": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1}, "tashman": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1}, "437": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1}, "450": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1}, "cerqueira": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1}, "torgo": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1}, "2020": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "evaluating": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1}, "109": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1}, "2028": {"docs/en/model-selection/cross-validation.md": 1, "docs/en/model-selection/cross-validation.md#section_12": 1}, "2log": {"docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_0": 2}, "recovery": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_0": 1}, "tends": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_0": 1}, "disagree": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_0": 1}, "minimizes": {"docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_2": 2, "docs/en/model-selection/information-criteria.md#section_13": 1}, "kullback": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_2": 1}, "leibler": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_2": 1}, "divergence": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_2": 1}, "kl": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_2": 1}, "showed": {"docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_2": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "approximates": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_3": 1}, "marginal": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_3": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "08k": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_5": 1}, "00k": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_5": 1}, "61k": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_5": 1}, "91k": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_5": 1}, "rank": {"docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_6": 1, "docs/en/model-selection/information-criteria.md#section_12": 1}, "support": {"docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_6": 2, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_13": 2}, "delta": {"docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_6": 2, "docs/en/foundations/stationarity.md": 5, "docs/en/foundations/stationarity.md#section_1": 1, "docs/en/foundations/stationarity.md#section_4": 1, "docs/en/foundations/stationarity.md#section_5": 2, "docs/en/foundations/stationarity.md#section_13": 1}, "1002": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_7": 1}, "transformations": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_7": 1, "docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_6": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "adjusting": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_7": 1}, "increasingly": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_7": 1}, "ties": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_7": 1}, "display": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_11": 1}, "dataframe": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_11": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_10": 1}, "stronger": {"docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_13": 2}, "6k": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "improvement": {"docs/en/model-selection/information-criteria.md": 3, "docs/en/model-selection/information-criteria.md#section_13": 3, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "underfit": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "capturing": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "underpenalizes": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "overly": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "hurvich": {"docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/information-criteria.md#section_14": 1}, "tsai": {"docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/information-criteria.md#section_14": 1}, "1989": {"docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/model-selection/information-criteria.md#section_14": 1}, "2kn": {"docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_13": 2}, "corrections": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "nested": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_7": 1}, "offset": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_14": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_12": 2}, "identifies": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_16": 2}, "goals": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "lean": {"docs/en/model-selection/information-criteria.md": 2, "docs/en/model-selection/information-criteria.md#section_13": 2}, "ensemble": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_9": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_1": 1}, "picking": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "dogmatically": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "alongside": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_13": 1}, "1974": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1}, "ieee": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1}, "716": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1}, "estimating": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_1": 1}, "annals": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1}, "461": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1}, "464": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1}, "burnham": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1}, "anderson": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1}, "multimodel": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1}, "76": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1}, "307": {"docs/en/model-selection/information-criteria.md": 1, "docs/en/model-selection/information-criteria.md#section_14": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1}, "homoskedasticity": {"docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_4": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "garch": {"docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_0": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "adjusted": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_2": 1}, "jarque": {"docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_3": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_14": 1}, "bera": {"docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_3": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_14": 1}, "jb": {"docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_3": 2}, "skewness": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_3": 1}, "kurtosis": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_3": 1}, "arch": {"docs/en/model-selection/residual-diagnostics.md": 6, "docs/en/model-selection/residual-diagnostics.md#section_4": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 4}, "lm": {"docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_4": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 2}, "nr": {"docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_4": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "runs": {"docs/en/model-selection/residual-diagnostics.md": 3, "docs/en/model-selection/residual-diagnostics.md#section_5": 3}, "randomness": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_5": 1}, "consecutive": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_5": 1}, "checklist": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1}, "fanning": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1}, "clustering": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1}, "histogram": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1}, "bell": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1}, "shaped": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1}, "mostly": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1}, "violations": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1}, "violation": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1}, "outlier": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_5": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_6": 2, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_11": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_14": 3}, "treatment": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_6": 1}, "misses": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "obsession": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1}, "14": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_7": 1, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/features/feature-engineering.md#section_11": 1}, "graphics": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_8": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_11": 1}, "tsaplots": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_8": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_11": 1}, "intentionally": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_9": 1}, "n2": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_13": 1}, "n3": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_14": 1}, "stat": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_14": 1}, "correlate": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_15": 1}, "n4": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_15": 1}, "remain": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "unexploited": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "fixable": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "clt": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "ignorable": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "spending": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "effort": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "reflexively": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "subtract": {"docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_5": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_13": 2}, "account": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "computed": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "constrain": {"docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2}, "oversized": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "subtracted": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "otherwise": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_8": 1}, "clusters": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "ok": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "stabilizing": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "adjustment": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_13": 2}, "calm": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "02": {"docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2}, "slight": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_9": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "conclude": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "borderline": {"docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "fine": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "minor": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "invalidate": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "actions": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "visually": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "isolated": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2, "docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_16": 2}, "ultimate": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "accept": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "unless": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_17": 1}, "perfect": {"docs/en/model-selection/residual-diagnostics.md": 2, "docs/en/model-selection/residual-diagnostics.md#section_17": 2}, "engle": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1}, "1982": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1}, "heteroscedasticity": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1}, "united": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1}, "kingdom": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1}, "987": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1}, "1007": {"docs/en/model-selection/residual-diagnostics.md": 1, "docs/en/model-selection/residual-diagnostics.md#section_18": 1}, "concerns": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_0": 1}, "tips": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_0": 1}, "document": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_0": 1}, "monitor": {"docs/en/practical/practical-modeling.md": 3, "docs/en/practical/practical-modeling.md#section_0": 1, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "fallback": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_0": 1}, "performed": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_1": 1}, "mimicking": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_1": 1}, "updating": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_1": 1}, "periodically": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_1": 1}, "mad": {"docs/en/practical/practical-modeling.md": 6, "docs/en/practical/practical-modeling.md#section_4": 1, "docs/en/practical/practical-modeling.md#section_8": 3, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/anomaly-detection/anomaly-detection.md": 16, "docs/en/anomaly-detection/anomaly-detection.md#section_2": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 14}, "miscalibration": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_5": 1}, "pipeline": {"docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_6": 2, "docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "config": {"docs/en/practical/practical-modeling.md": 6, "docs/en/practical/practical-modeling.md#section_6": 6}, "float": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_6": 1}, "score": {"docs/en/practical/practical-modeling.md": 3, "docs/en/practical/practical-modeling.md#section_6": 3, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_4": 1, "docs/en/anomaly-detection/anomaly-detection.md": 23, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_2": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_4": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_11": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_12": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 14}, "retrain": {"docs/en/practical/practical-modeling.md": 5, "docs/en/practical/practical-modeling.md#section_6": 3, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "dashboard": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_6": 1}, "metric": {"docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/practical/practical-modeling.md#section_7": 1}, "warning": {"docs/en/practical/practical-modeling.md": 4, "docs/en/practical/practical-modeling.md#section_6": 1, "docs/en/practical/practical-modeling.md#section_12": 3}, "recalibrate": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_6": 1}, "claim": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_7": 1}, "static": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_7": 1}, "regularly": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_7": 1}, "makers": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_7": 1}, "tune": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_7": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1}, "else": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_8": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_7": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_4": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_6": 1}, "simulated": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_11": 1}, "alert": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_12": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "alerts": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_12": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "consequences": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "overoptimistic": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "incorrectly": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "kfold": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "manual": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_13": 1}, "sudden": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_7": 1}, "covid": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "impact": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "customer": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "evolution": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "recurring": {"docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_13": 2}, "oscillates": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "states": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 5, "docs/en/state-space/kalman-filter.md#section_0": 2, "docs/en/state-space/kalman-filter.md#section_5": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/state-space/kalman-filter.md#section_9": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "charts": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "flag": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_3": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "adaptive": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "rsfe": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "normalizes": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "comparability": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "distinguishes": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "reacting": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "fluctuation": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "wait": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "sustained": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_14": 2}, "75": {"docs/en/practical/practical-modeling.md": 2, "docs/en/practical/practical-modeling.md#section_13": 2, "docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_12": 1}, "unmodeled": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "quantile": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "975": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "875": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "seem": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "truthful": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "decisions": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "worse": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "strictly": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_1": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "historically": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "delayed": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "unusual": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/anomaly-detection/anomaly-detection.md": 8, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2}, "lucky": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "faces": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "delays": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "evaluated": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "convenient": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "generalizes": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1, "docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "spanning": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_13": 1}, "spiliotis": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1}, "assimakopoulos": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1}, "m4": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1}, "000": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1}, "61": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1}, "54": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "74": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "gama": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1}, "bifet": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1}, "pechenizkiy": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1}, "bouchachia": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1}, "adaptation": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1}, "acm": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "46": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1}, "kolassa": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1}, "count": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1}, "788": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1}, "803": {"docs/en/practical/practical-modeling.md": 1, "docs/en/practical/practical-modeling.md#section_14": 1}, "ready": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_0": 1}, "date": {"docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_0": 1, "docs/en/features/feature-engineering.md#section_1": 1, "docs/en/features/feature-engineering.md#section_5": 1}, "predictors": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_1": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_1": 1}, "roll": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_1": 1}, "extracted": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_1": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_0": 1}, "timestamp": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_1": 1}, "quarter": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_1": 1}, "event": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_1": 1}, "sine": {"docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_1": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "cosine": {"docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_1": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "ema": {"docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_3": 2}, "standardization": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_4": 1}, "median": {"docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_4": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/anomaly-detection/anomaly-detection.md": 10, "docs/en/anomaly-detection/anomaly-detection.md#section_2": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 6, "docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_4": 1, "docs/en/decomposition/stl.md#section_13": 2, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_14": 2}, "iqr": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_4": 1, "docs/en/anomaly-detection/anomaly-detection.md": 9, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_2": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_13": 5}, "scaler": {"docs/en/features/feature-engineering.md": 8, "docs/en/features/feature-engineering.md#section_4": 1, "docs/en/features/feature-engineering.md#section_8": 1, "docs/en/features/feature-engineering.md#section_16": 6}, "copy": {"docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/features/feature-engineering.md#section_11": 1}, "datetime": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_5": 1}, "dayofweek": {"docs/en/features/feature-engineering.md": 5, "docs/en/features/feature-engineering.md#section_5": 3, "docs/en/features/feature-engineering.md#section_11": 2}, "dayofyear": {"docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/features/feature-engineering.md#section_11": 1}, "dropna": {"docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_5": 1, "docs/en/features/feature-engineering.md#section_11": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "leak": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_8": 1}, "impute": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_8": 1}, "dimensionality": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_8": 1}, "inherit": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_8": 1}, "randomforestregressor": {"docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_9": 1, "docs/en/features/feature-engineering.md#section_13": 1}, "dates": {"docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_10": 2}, "2023": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_10": 1}, "freq": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_10": 1}, "calendar": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_11": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/classical.md#section_12": 1}, "doy": {"docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_11": 3}, "axis": {"docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_12": 2}, "importance": {"docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_15": 3}, "ntop": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_15": 1}, "ascending": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_15": 1}, "head": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_15": 1}, "influence": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_13": 3}, "influenced": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "knows": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "world": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "manageable": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "monday": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "excludes": {"docs/en/features/feature-engineering.md": 2, "docs/en/features/feature-engineering.md#section_16": 2}, "moves": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "represented": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "alias": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "182": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "overkill": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "simplest": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1, "docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_1": 1}, "plenty": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "fillna": {"docs/en/features/feature-engineering.md": 3, "docs/en/features/feature-engineering.md#section_16": 3}, "bfill": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "earliest": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "preserves": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "neutral": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "isna": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "astype": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "flexible": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "shorter": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "warmup": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "maximizes": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "dropping": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_16": 1}, "christ": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1}, "braun": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1}, "neuffer": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1}, "kempa": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1}, "liehr": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1}, "extraction": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1, "docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_2": 1, "docs/en/decomposition/classical.md#section_5": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_3": 1}, "scalable": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1}, "hypothesis": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_8": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "neurocomputing": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1}, "72": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1}, "77": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1}, "fulcher": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1}, "jones": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1}, "hctsa": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1}, "phenotyping": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1}, "brownlee": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1}, "mastery": {"docs/en/features/feature-engineering.md": 1, "docs/en/features/feature-engineering.md#section_17": 1}, "propagate": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_0": 1}, "incorporate": {"docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_0": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "unifies": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_0": 1}, "eta": {"docs/en/state-space/kalman-filter.md": 6, "docs/en/state-space/kalman-filter.md#section_1": 1, "docs/en/state-space/kalman-filter.md#section_3": 2, "docs/en/state-space/kalman-filter.md#section_13": 3}, "recursions": {"docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_2": 1, "docs/en/state-space/kalman-filter.md#section_3": 1}, "filtered": {"docs/en/state-space/kalman-filter.md": 4, "docs/en/state-space/kalman-filter.md#section_2": 1, "docs/en/state-space/kalman-filter.md#section_5": 1, "docs/en/state-space/kalman-filter.md#section_13": 2, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_5": 1}, "gain": {"docs/en/state-space/kalman-filter.md": 10, "docs/en/state-space/kalman-filter.md#section_2": 1, "docs/en/state-space/kalman-filter.md#section_3": 1, "docs/en/state-space/kalman-filter.md#section_12": 2, "docs/en/state-space/kalman-filter.md#section_13": 6}, "scalar": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_3": 1}, "steady": {"docs/en/state-space/kalman-filter.md": 8, "docs/en/state-space/kalman-filter.md#section_3": 1, "docs/en/state-space/kalman-filter.md#section_12": 1, "docs/en/state-space/kalman-filter.md#section_13": 6}, "x0": {"docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_5": 2}, "p0": {"docs/en/state-space/kalman-filter.md": 4, "docs/en/state-space/kalman-filter.md#section_5": 2, "docs/en/state-space/kalman-filter.md#section_7": 2}, "inv": {"docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_5": 2}, "det": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_5": 1}, "definite": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_2": 1}, "square": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1}, "ud": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1}, "factorization": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1}, "diffuse": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1}, "extensions": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1}, "ekf": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1}, "ukf": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1}, "particle": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_6": 1}, "mu0": {"docs/en/state-space/kalman-filter.md": 3, "docs/en/state-space/kalman-filter.md#section_7": 3, "docs/en/change-detection/change-point.md": 6, "docs/en/change-detection/change-point.md#section_6": 3, "docs/en/change-detection/change-point.md#section_13": 3}, "nsteady": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_12": 1}, "trusted": {"docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_13": 2}, "evolves": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "prior": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "stabilizes": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "recognizing": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "4qr": {"docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_13": 2}, "2q": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "reach": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "smoothed": {"docs/en/state-space/kalman-filter.md": 2, "docs/en/state-space/kalman-filter.md#section_13": 2, "docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_5": 1, "docs/en/decomposition/stl.md#section_13": 1}, "retrospective": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "refine": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "gps": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "measurements": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "vehicle": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "treat": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "minus": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "grid": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "overconfident": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_13": 1}, "harvey": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_14": 1}, "durbin": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_14": 1}, "koopman": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_14": 1}, "oxford": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_14": 1}, "basic": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_14": 1}, "82": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_14": 1}, "35": {"docs/en/state-space/kalman-filter.md": 1, "docs/en/state-space/kalman-filter.md#section_14": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_9": 1}, "anomalies": {"docs/en/anomaly-detection/anomaly-detection.md": 22, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_3": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_4": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_9": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_10": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_14": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 13}, "collective": {"docs/en/anomaly-detection/anomaly-detection.md": 4, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "contextual": {"docs/en/anomaly-detection/anomaly-detection.md": 7, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_3": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 4}, "distance": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1, "docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_2": 1, "docs/en/decomposition/stl.md#section_13": 1}, "lof": {"docs/en/anomaly-detection/anomaly-detection.md": 5, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_5": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "isolation": {"docs/en/anomaly-detection/anomaly-detection.md": 7, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_4": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 4, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "forest": {"docs/en/anomaly-detection/anomaly-detection.md": 6, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_4": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "autoencoders": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1}, "challenge": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1}, "defining": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_0": 1}, "ac": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 1}, "usage": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 1}, "winter": {"docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "settings": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_1": 1}, "supervised": {"docs/en/anomaly-detection/anomaly-detection.md": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "labeled": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 1}, "semi": {"docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_2": 1}, "unsupervised": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 1}, "labels": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_1": 1}, "chosen": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_2": 1}, "6745": {"docs/en/anomaly-detection/anomaly-detection.md": 5, "docs/en/anomaly-detection/anomaly-detection.md#section_2": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 4}, "isolate": {"docs/en/anomaly-detection/anomaly-detection.md": 4, "docs/en/anomaly-detection/anomaly-detection.md#section_4": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2}, "build": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_4": 1}, "neighbors": {"docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_5": 2}, "lrd": {"docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_5": 2}, "zscore": {"docs/en/anomaly-detection/anomaly-detection.md": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_11": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_14": 1}, "events": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_6": 1}, "masking": {"docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "hiding": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1}, "swamping": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1}, "flagged": {"docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "cleaning": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1}, "global": {"docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_14": 2}, "saturday": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1}, "weekday": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_7": 1}, "insert": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_10": 1}, "convolve": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_12": 1}, "thresholds": {"docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2, "docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "anomalous": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "distinction": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_10": 1}, "compared": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "scores": {"docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2}, "masks": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "robustness": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1, "docs/en/decomposition/stl.md": 6, "docs/en/decomposition/stl.md#section_3": 2, "docs/en/decomposition/stl.md#section_4": 1, "docs/en/decomposition/stl.md#section_5": 1, "docs/en/decomposition/stl.md#section_13": 2}, "4826": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "hides": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "distances": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "rest": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "partition": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "surrounded": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "clustered": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "deploy": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "triggers": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "investigation": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "precision": {"docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2}, "raise": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "detectors": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "majority": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "agree": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "label": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "stage": {"docs/en/anomaly-detection/anomaly-detection.md": 3, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 3}, "catch": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "rules": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "logic": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "filters": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "recall": {"docs/en/anomaly-detection/anomaly-detection.md": 2, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 2}, "f1": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "catching": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "fatigue": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_15": 1}, "chandola": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "banerjee": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "kumar": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "2009": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "41": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_17": 1}, "58": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "liu": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "ting": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "zhou": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "icdm": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "413": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "422": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "breunig": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "kriegel": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "ng": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "sander": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "identifying": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "sigmod": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "93": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "104": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "hochenbaum": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "vallis": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "kejariwal": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "cloud": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "1704": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "07706": {"docs/en/anomaly-detection/anomaly-detection.md": 1, "docs/en/anomaly-detection/anomaly-detection.md#section_16": 1}, "irregular": {"docs/en/decomposition/classical.md": 11, "docs/en/decomposition/classical.md#section_0": 1, "docs/en/decomposition/classical.md#section_1": 1, "docs/en/decomposition/classical.md#section_5": 2, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/classical.md#section_12": 6}, "centered": {"docs/en/decomposition/classical.md": 4, "docs/en/decomposition/classical.md#section_0": 1, "docs/en/decomposition/classical.md#section_2": 1, "docs/en/decomposition/classical.md#section_5": 1, "docs/en/decomposition/classical.md#section_12": 1}, "repeating": {"docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_1": 1}, "cma": {"docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_2": 1, "docs/en/decomposition/classical.md#section_5": 1}, "odd": {"docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_2": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_0": 1, "docs/en/decomposition/stl.md#section_1": 1}, "center": {"docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_2": 1}, "calculation": {"docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_3": 1}, "detrended": {"docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_3": 1}, "season": {"docs/en/decomposition/classical.md": 3, "docs/en/decomposition/classical.md#section_3": 2, "docs/en/decomposition/classical.md#section_5": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_3": 1, "docs/en/decomposition/stl.md#section_13": 1}, "jm": {"docs/en/decomposition/classical.md": 4, "docs/en/decomposition/classical.md#section_3": 4}, "ratios": {"docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_3": 1}, "attenuates": {"docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_4": 1}, "leave": {"docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_5": 1}, "mod": {"docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_5": 1}, "replicate": {"docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_5": 1}, "trading": {"docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_6": 1, "docs/en/decomposition/classical.md#section_12": 1}, "easter": {"docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_6": 1}, "ntrend": {"docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_9": 1}, "ncompare": {"docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_10": 1}, "nanstd": {"docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_10": 2}, "temp": {"docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "locally": {"docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_14": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "cancels": {"docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "removed": {"docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "lost": {"docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "positions": {"docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_3": 1, "docs/en/decomposition/stl.md#section_11": 1}, "half": {"docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "seasons": {"docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_12": 2}, "deflate": {"docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1}, "decompose": {"docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_12": 1, "docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_6": 1}, "wheelwright": {"docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_13": 1}, "1998": {"docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_13": 1}, "census": {"docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_13": 2}, "bureau": {"docs/en/decomposition/classical.md": 2, "docs/en/decomposition/classical.md#section_13": 2}, "13arima": {"docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_13": 1}, "seats": {"docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_13": 1}, "reference": {"docs/en/decomposition/classical.md": 1, "docs/en/decomposition/classical.md#section_13": 1}, "loess": {"docs/en/decomposition/stl.md": 11, "docs/en/decomposition/stl.md#section_0": 1, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/decomposition/stl.md#section_2": 1, "docs/en/decomposition/stl.md#section_3": 3, "docs/en/decomposition/stl.md#section_5": 2, "docs/en/decomposition/stl.md#section_13": 2, "docs/en/decomposition/stl.md#section_14": 1}, "robustly": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_0": 1}, "movement": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_1": 1}, "everything": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_1": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "scatterplot": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_1": 1}, "downweight": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_1": 1}, "tricube": {"docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_2": 1, "docs/en/decomposition/stl.md#section_13": 2}, "bandwidth": {"docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_2": 1, "docs/en/decomposition/stl.md#section_13": 2}, "outer": {"docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_3": 2, "docs/en/decomposition/stl.md#section_4": 1}, "inner": {"docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_3": 2}, "subseries": {"docs/en/decomposition/stl.md": 6, "docs/en/decomposition/stl.md#section_3": 1, "docs/en/decomposition/stl.md#section_5": 2, "docs/en/decomposition/stl.md#section_13": 3}, "deseasonalize": {"docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_3": 1, "docs/en/decomposition/stl.md#section_5": 1}, "deseasonalized": {"docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_3": 1, "docs/en/decomposition/stl.md#section_5": 1}, "assign": {"docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_4": 1, "docs/en/decomposition/stl.md#section_13": 1}, "bisquare": {"docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_4": 1, "docs/en/decomposition/stl.md#section_13": 1}, "downweighted": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_4": 1}, "extract": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_5": 1}, "absorbs": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_6": 1}, "distort": {"docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_6": 1, "docs/en/decomposition/stl.md#section_13": 1}, "exponentiate": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_6": 1}, "nremainder": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_11": 1}, "fig": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_12": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_13": 1}, "flexibility": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "adjustable": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "smoothness": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "ends": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "work": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "iterative": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "reweighting": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "normally": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "wondering": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "distorts": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "differentiable": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "curves": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "downweighting": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "smoothly": {"docs/en/decomposition/stl.md": 2, "docs/en/decomposition/stl.md#section_13": 2}, "nice": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "jagged": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "leaked": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "januaries": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "drifts": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1, "docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "leaving": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "remains": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "biases": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_13": 1}, "cleveland": {"docs/en/decomposition/stl.md": 3, "docs/en/decomposition/stl.md#section_14": 3}, "mcrae": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1}, "terpenning": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1}, "official": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1}, "73": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1}, "devlin": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1}, "83": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1}, "403": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1}, "596": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1}, "610": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1}, "dokumentov": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1}, "str": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1}, "monash": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1}, "working": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1}, "papers": {"docs/en/decomposition/stl.md": 1, "docs/en/decomposition/stl.md#section_14": 1}, "penalized": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_0": 1}, "offline": {"docs/en/change-detection/change-point.md": 8, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/change-detection/change-point.md#section_1": 1, "docs/en/change-detection/change-point.md#section_6": 1, "docs/en/change-detection/change-point.md#section_14": 4, "docs/en/change-detection/change-point.md#section_15": 1}, "programming": {"docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_0": 1, "docs/en/change-detection/change-point.md#section_4": 1}, "tau": {"docs/en/change-detection/change-point.md": 7, "docs/en/change-detection/change-point.md#section_1": 3, "docs/en/change-detection/change-point.md#section_14": 4}, "away": {"docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_2": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "allowance": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_2": 1}, "segmentation": {"docs/en/change-detection/change-point.md": 9, "docs/en/change-detection/change-point.md#section_3": 1, "docs/en/change-detection/change-point.md#section_6": 1, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_12": 1, "docs/en/change-detection/change-point.md#section_14": 5}, "greedy": {"docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_3": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "rss": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_3": 1}, "pruned": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_4": 1}, "partitioning": {"docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_4": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "pruning": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_4": 1}, "eliminate": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_4": 1}, "segmentations": {"docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_4": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "achieve": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_4": 1}, "cp": {"docs/en/change-detection/change-point.md": 4, "docs/en/change-detection/change-point.md#section_6": 4}, "alarms": {"docs/en/change-detection/change-point.md": 4, "docs/en/change-detection/change-point.md#section_6": 3, "docs/en/change-detection/change-point.md#section_14": 1}, "reset": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_6": 1}, "penalties": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_7": 1}, "segment": {"docs/en/change-detection/change-point.md": 10, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_14": 9}, "segments": {"docs/en/change-detection/change-point.md": 4, "docs/en/change-detection/change-point.md#section_7": 1, "docs/en/change-detection/change-point.md#section_10": 1, "docs/en/change-detection/change-point.md#section_14": 2}, "ruptures": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_8": 1}, "rpt": {"docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_8": 1, "docs/en/change-detection/change-point.md#section_11": 1, "docs/en/change-detection/change-point.md#section_12": 1}, "concatenate": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_10": 1}, "algo": {"docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_11": 2}, "l2": {"docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_11": 1, "docs/en/change-detection/change-point.md#section_12": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "pen": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_11": 1}, "binseg": {"docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_12": 2}, "norm": {"docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_13": 2}, "hazard": {"docs/en/change-detection/change-point.md": 4, "docs/en/change-detection/change-point.md#section_13": 4}, "cpd": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_13": 1}, "probabilities": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_13": 1}, "predprob": {"docs/en/change-detection/change-point.md": 3, "docs/en/change-detection/change-point.md#section_13": 3}, "pdf": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_13": 1}, "argmax": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_13": 1}, "revise": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "fraud": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "locations": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "building": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "providing": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "capability": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "style": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "intensive": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "elbow": {"docs/en/change-detection/change-point.md": 2, "docs/en/change-detection/change-point.md#section_14": 2}, "diminishing": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "detections": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "sic": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "mbic": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "datasets": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "considering": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "searches": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "signals": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "server": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "turns": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "functions": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_10": 1}, "l1": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "huber": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "removal": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "resolutions": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "confirmation": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "alarm": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "dominate": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "trigger": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_14": 1}, "truong": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "oudre": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "vayatis": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "selective": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "167": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "107299": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "killick": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "fearnhead": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "eckley": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "changepoints": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "107": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "1590": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "1598": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "adams": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "mackay": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "0710": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "3742": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "1954": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "schemes": {"docs/en/change-detection/change-point.md": 1, "docs/en/change-detection/change-point.md#section_15": 1}, "acvf": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_1": 1}, "corr": {"docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_1": 2}, "symmetry": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_2": 1, "docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_2": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "kk": {"docs/en/foundations/autocorrelation.md": 4, "docs/en/foundations/autocorrelation.md#section_3": 3, "docs/en/foundations/autocorrelation.md#section_16": 1}, "k1": {"docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_3": 2}, "k2": {"docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_3": 2}, "geometrically": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_4": 1}, "exceed": {"docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "applying": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_10": 1, "docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "meaningless": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_10": 1}, "validating": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_10": 1}, "armaprocess": {"docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_11": 1, "docs/en/foundations/autocorrelation.md#section_12": 1}, "75l": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_12": 1}, "25l": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_12": 1}, "nsample": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_12": 1}, "axes": {"docs/en/foundations/autocorrelation.md": 3, "docs/en/foundations/autocorrelation.md#section_13": 3}, "subplots": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_13": 1}, "figsize": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_13": 1}, "ax": {"docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_13": 2}, "title": {"docs/en/foundations/autocorrelation.md": 2, "docs/en/foundations/autocorrelation.md#section_13": 2}, "staying": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "strongly": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "relatively": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "formally": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "successive": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "highly": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "conditioning": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "thus": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "33": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "induction": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "variability": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "individual": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "se": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "band": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "196": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_16": 1}, "1946": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_17": 1}, "jrss": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_17": 1}, "27": {"docs/en/foundations/autocorrelation.md": 1, "docs/en/foundations/autocorrelation.md#section_17": 1}, "made": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_0": 1}, "strict": {"docs/en/foundations/stationarity.md": 5, "docs/en/foundations/stationarity.md#section_1": 1, "docs/en/foundations/stationarity.md#section_7": 2, "docs/en/foundations/stationarity.md#section_13": 2}, "weakly": {"docs/en/foundations/stationarity.md": 4, "docs/en/foundations/stationarity.md#section_1": 1, "docs/en/foundations/stationarity.md#section_2": 1, "docs/en/foundations/stationarity.md#section_13": 2}, "ergodicity": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_1": 1}, "ergodic": {"docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_1": 2}, "population": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_1": 1}, "realization": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_1": 1}, "encountered": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_1": 1}, "cauchy": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_2": 1}, "dickey": {"docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_5": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "fuller": {"docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_5": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "exists": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_5": 1}, "complementary": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_5": 1}, "quadratic": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_6": 1}, "unnecessary": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_7": 1}, "ar1": {"docs/en/foundations/stationarity.md": 4, "docs/en/foundations/stationarity.md#section_10": 3, "docs/en/foundations/stationarity.md#section_11": 1}, "rw": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_12": 1}, "covariances": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "gaussianity": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "constrains": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "violating": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "summation": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "differentiation": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "recurrence": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "08": {"docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_13": 2}, "03": {"docs/en/foundations/stationarity.md": 2, "docs/en/foundations/stationarity.md#section_13": 2}, "contradictory": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "inspect": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "fractionally": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "supports": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "serial": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_13": 1}, "1979": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "estimators": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "366": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "427": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "431": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "kwiatkowski": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "phillips": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "schmidt": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "shin": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "1992": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "159": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}, "178": {"docs/en/foundations/stationarity.md": 1, "docs/en/foundations/stationarity.md#section_14": 1}}}}